<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Category: study - 제목</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="제목"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="제목"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta description="설명"><meta property="og:type" content="blog"><meta property="og:title" content="제목"><meta property="og:url" content="https://github.com/idanjfhgkwl/idanjfhgkwl.github.io"><meta property="og:site_name" content="제목"><meta property="og:description" content="설명"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://github.com/img/og_image.png"><meta property="article:author" content="JustY"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://github.com/idanjfhgkwl/idanjfhgkwl.github.io"},"headline":"제목","image":["https://github.com/img/og_image.png"],"author":{"@type":"Person","name":"JustY"},"description":"설명"}</script><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.2.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="제목" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/categories">Categories</a></li><li class="is-active"><a href="#" aria-current="page">study</a></li></ul></nav></div></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-12-01T02:54:47.536Z" title="2020-12-01T02:54:47.536Z">2020-12-01</time></span><span class="level-item">Updated&nbsp;<time dateTime="2020-12-01T02:54:47.827Z" title="2020-12-01T02:54:47.827Z">2020-12-01</time></span><span class="level-item"><a class="link-muted" href="/categories/study/">study</a></span><span class="level-item">19 minutes read (About 2812 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/12/01/study/kaggle_how_to_choose_right_metric_for_evaluating_ml_model_vipulgandhi/">ML 모델 을 평가하기 위한 올바른 측정 항목을 선택하는 방법</a></h1><div class="content"><h1 id="ML-모델-을-평가하기-위한-올바른-측정-항목을-선택하는-방법"><a href="#ML-모델-을-평가하기-위한-올바른-측정-항목을-선택하는-방법" class="headerlink" title="ML 모델 을 평가하기 위한 올바른 측정 항목을 선택하는 방법"></a>ML 모델 을 평가하기 위한 올바른 측정 항목을 선택하는 방법</h1><p>(How to Choose Right Metric for Evaluating ML Model)</p>
<h2 id="도입부"><a href="#도입부" class="headerlink" title="도입부"></a>도입부</h2><p><a target="_blank" rel="noopener" href="https://www.kaggle.com/vipulgandhi/how-to-choose-right-metric-for-evaluating-ml-model">https://www.kaggle.com/vipulgandhi/how-to-choose-right-metric-for-evaluating-ml-model</a></p>
<p>이 <a target="_blank" rel="noopener" href="https://scikit-learn.org/stable/modules/model_evaluation.html">Scikit-learn 페이지</a>는 훌륭한 참조를 제공합니다.</p>
<p>일반적인 기능 엔지니어링, 선택, 모델 구현을 수행하고 확률 또는 클래스 형태로 출력을 얻은 후 다음 단계는 테스트 데이터 세트를 사용하여 일부 메트릭을 기반으로 모델이 얼마나 효과적인지 확인하는 것입니다. 메트릭은 모델의 성능을 설명합니다.<br><br><br>모델은 정확도 _ 점수라는 메트릭을 사용하여 평가할 때 만족스러운 결과를 제공 할 수 있지만 logarithmic_loss와 같은 다른 메트릭 또는 다른 이러한 메트릭에 대해 평가할 때 좋지 않은 결과를 제공 할 수 있습니다. 따라서 기계 학습 모델을 평가하기 위해 올바른 메트릭을 선택하는 것이 매우 중요합니다.<br><br><br>측정 항목 선택은 기계 학습 알고리즘의 성능을 측정하고 비교하는 방법에 영향을줍니다. 결과에서 다른 특성의 중요성에 가중치를 부여하는 방법에 영향을줍니다.<br><br><br><strong>분류 메트릭</strong></p>
<ul>
<li>정확성.</li>
<li>대수 손실.</li>
<li>ROC, AUC.</li>
<li>혼란 매트릭스.</li>
<li>분류 보고서.</li>
</ul>
<p><strong>회귀 지표</strong></p>
<ul>
<li>평균 절대 오차.</li>
<li>평균 제곱 오차.</li>
<li>평균 제곱근 오차.</li>
<li>루트 평균 제곱 로그 오류.</li>
<li>R 광장.</li>
<li>R 제곱을 조정했습니다.</li>
</ul>
<p><strong>분류 문제</strong>에서는 , 우리는 (자신이 생성 출력의 종류에 따라) 알고리즘의 두 가지 유형을 사용</p>
<ul>
<li><strong>클래스 출력</strong> : SVM 및 KNN과 같은 알고리즘은 클래스 출력을 생성합니다. 예를 들어, 이진 분류 문제에서 출력은 0 또는 1입니다. SKLearn의 / 기타 알고리즘은 이러한 클래스 출력을 확률로 변환 할 수 있습니다.</li>
<li><strong>확률 출력</strong> : 로지스틱 회귀, 랜덤 포레스트, 그라디언트 부스팅, Adaboost 등과 같은 알고리즘은 확률 출력을 제공합니다. 확률 출력은 임계 확률을 생성하여 클래스 출력으로 변환 할 수 있습니다.</li>
</ul>
<p>회귀 문제에서 출력은 본질적으로 항상 연속적이며 추가 처리가 필요하지 않습니다.</p>
<h2 id="분류-메트릭"><a href="#분류-메트릭" class="headerlink" title="분류 메트릭"></a>분류 메트릭</h2><p>(Classification Metrices)</p>
<ul>
<li>데이터 세트 : 피마 인디언 당뇨병 예측.  </li>
<li>평가 알고리즘 : 로지스틱 회귀, SGDClassifier, RandomForestClassifier.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> google.colab <span class="keyword">import</span> drive <span class="comment"># 패키지 불러오기 </span></span><br><span class="line"><span class="keyword">from</span> os.path <span class="keyword">import</span> join  </span><br><span class="line"></span><br><span class="line"><span class="comment"># 구글 드라이브 마운트</span></span><br><span class="line">ROOT = <span class="string">&quot;/content/drive&quot;</span>     <span class="comment"># 드라이브 기본 경로</span></span><br><span class="line">print(ROOT)                 <span class="comment"># print content of ROOT (Optional)</span></span><br><span class="line">drive.mount(ROOT)           <span class="comment"># 드라이브 기본 경로 </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 프로젝트 파일 생성 및 다운받을 경로 이동</span></span><br><span class="line">MY_GOOGLE_DRIVE_PATH = <span class="string">&#x27;My Drive/Colab Notebooks/python_basic/kaggle_how-to-choose-right-metric-for-evaluating-ml-model_vipulgandhi/data&#x27;</span></span><br><span class="line">PROJECT_PATH = join(ROOT, MY_GOOGLE_DRIVE_PATH)</span><br><span class="line">print(PROJECT_PATH)</span><br></pre></td></tr></table></figure>

<pre><code>/content/drive
Mounted at /content/drive
/content/drive/My Drive/Colab Notebooks/python_basic/kaggle_how-to-choose-right-metric-for-evaluating-ml-model_vipulgandhi/data</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">%cd <span class="string">&quot;&#123;PROJECT_PATH&#125;&quot;</span></span><br></pre></td></tr></table></figure>

<pre><code>/content/drive/My Drive/Colab Notebooks/python_basic/kaggle_how-to-choose-right-metric-for-evaluating-ml-model_vipulgandhi/data</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!ls</span><br></pre></td></tr></table></figure>

<pre><code>diabetes.csv</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression, SGDClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split, cross_val_score, cross_val_predict, StratifiedKFold</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">&#x27;ignore&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">diabetes_data = pd.read_csv(<span class="string">&#x27;diabetes.csv&#x27;</span>)</span><br><span class="line"></span><br><span class="line">X =  diabetes_data.drop([<span class="string">&quot;Outcome&quot;</span>],axis = <span class="number">1</span>)</span><br><span class="line">y = diabetes_data[<span class="string">&quot;Outcome&quot;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 훈련 세트를 사용하여 다양한 하이퍼 파라미터로 여러 모델을 훈련하고 검증 세트에서 가장 잘 수행되는 모델과 하이퍼 파라미터를 선택합니다.</span></span><br><span class="line"><span class="comment"># 모델 유형과 하이퍼 파라미터가 선택되면 전체 훈련 세트에서 이러한 하이퍼 파라미터를 사용하여 최종 모델을 훈련시키고 일반화 된 오류는 테스트 세트에서 최종적으로 측정됩니다.</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, random_state = <span class="number">56</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># StratifiedKFold 클래스는 계층화 된 샘플링을 수행하여 각 클래스의 대표 비율을 포함하는 폴드를 생성합니다.</span></span><br><span class="line">cv = StratifiedKFold(n_splits=<span class="number">10</span>, shuffle = <span class="literal">False</span>, random_state = <span class="number">76</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 로지스틱 회귀</span></span><br><span class="line">clf_logreg = LogisticRegression()</span><br><span class="line"><span class="comment"># 적합 모델</span></span><br><span class="line">clf_logreg.fit(X_train, y_train)</span><br><span class="line"><span class="comment"># 검증 세트에 대한 클래스 예측을합니다.</span></span><br><span class="line">y_pred_class_logreg = cross_val_predict(clf_logreg, X_train, y_train, cv = cv)</span><br><span class="line"><span class="comment"># 클래스 1에 대한 예측 확률, 양성 클래스의 확률</span></span><br><span class="line">y_pred_prob_logreg = cross_val_predict(clf_logreg, X_train, y_train, cv = cv, method=<span class="string">&quot;predict_proba&quot;</span>)</span><br><span class="line">y_pred_prob_logreg_class1 = y_pred_prob_logreg[:, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># SGD 분류기</span></span><br><span class="line">clf_SGD = SGDClassifier()</span><br><span class="line"><span class="comment"># 적합 모델</span></span><br><span class="line">clf_SGD.fit(X_train, y_train)</span><br><span class="line"><span class="comment"># 검증 세트에 대한 클래스 예측을합니다.</span></span><br><span class="line">y_pred_class_SGD = cross_val_predict(clf_SGD, X_train, y_train, cv = cv)</span><br><span class="line"><span class="comment"># 클래스 1에 대한 예측 확률</span></span><br><span class="line">y_pred_prob_SGD = cross_val_predict(clf_SGD, X_train, y_train, cv = cv, method=<span class="string">&quot;decision_function&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 랜덤 포레스트 분류기</span></span><br><span class="line">clf_rfc = RandomForestClassifier()</span><br><span class="line"><span class="comment"># 적합 모델</span></span><br><span class="line">clf_rfc.fit(X_train, y_train)</span><br><span class="line"><span class="comment"># 검증 세트에 대한 클래스 예측을합니다.</span></span><br><span class="line">y_pred_class_rfc = cross_val_predict(clf_rfc, X_train, y_train, cv = cv)</span><br><span class="line"><span class="comment"># 클래스 1에 대한 예측 확률</span></span><br><span class="line">y_pred_prob_rfc = cross_val_predict(clf_rfc, X_train, y_train, cv = cv, method=<span class="string">&quot;predict_proba&quot;</span>)</span><br><span class="line">y_pred_prob_rfc_class1 = y_pred_prob_rfc[:, <span class="number">1</span>]</span><br></pre></td></tr></table></figure>

<p><strong>빠른 참고</strong> : SkLearn의 “predict_log_proba”는 확률의 로그를 제공합니다. 확률이 매우 작아 질 수 있으므로 종종 더 편리합니다.</p>
<h3 id="Null-정확도"><a href="#Null-정확도" class="headerlink" title="Null 정확도"></a>Null 정확도</h3><p>(Null accuracy)</p>
<ul>
<li>항상 가장 빈번한 클래스를 예측하여 얻을 수있는 정확도.</li>
<li>이것은 항상 0/1을 예측하는 멍청한 모델이 “null_accuracy”%의 시간에 맞을 것이라는 것을 의미합니다.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.base <span class="keyword">import</span> BaseEstimator</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BaseClassifier</span>(<span class="params">BaseEstimator</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span>(<span class="params">self, X, y=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span>(<span class="params">self, X</span>):</span></span><br><span class="line">        <span class="keyword">return</span> np.zeros((<span class="built_in">len</span>(X), <span class="number">1</span>), dtype=<span class="built_in">bool</span>)</span><br><span class="line">    </span><br><span class="line">base_clf = BaseClassifier()</span><br><span class="line">cross_val_score(base_clf, X_train, y_train, cv=<span class="number">10</span>, scoring=<span class="string">&quot;accuracy&quot;</span>).mean()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Method 2</span></span><br><span class="line"><span class="comment"># calculate null accuracy (for binary / multi-class classification problems)</span></span><br><span class="line"><span class="comment"># null_accuracy = y_train.value_counts().head(1) / len(y_train)</span></span><br></pre></td></tr></table></figure>




<pre><code>0.6509981851179674</code></pre>
<h3 id="분류-정확도"><a href="#분류-정확도" class="headerlink" title="분류 정확도"></a>분류 정확도</h3><p>(Classification Accuracy)</p>
<p>분류 정확도 또는 정확도는 총 입력 샘플 수에 대한 올바른 예측 수의 비율입니다.  </p>
<p>$$Accuracy = \frac{Number\ of\ correct\ predictions}{Total\ number\ of\ predictions\ made} = \frac{TP + TN}{TP + TN + FP + FN}$$<br><img src="https://user-images.githubusercontent.com/72365720/100689145-3b388380-33c7-11eb-9f61-aec4666ba452.png" alt="다운로드">  </p>
<p>정확도 측정 항목을 사용하는 경우: 각 클래스에 속하는 샘플 수가 거의 동일한 경우<br>정확도 측정 항목을 사용하지 않는 경우: 하나의 클래스 만 대부분의 샘플을 보유 할 때.<br><br><br><strong>예</strong>:<br>훈련 세트에 클래스 A의 샘플이 98 %이고 클래스 B의 샘플이 2 %라고 가정합니다. 그러면 우리 모델은 클래스 A에 속하는 모든 훈련 샘플을 간단히 예측하여 98 %의 훈련 정확도를 쉽게 얻을 수 있습니다.<br>동일한 모델이 클래스 A의 60 % 샘플과 클래스 B의 40 % 샘플이있는 테스트 세트에서 테스트되면 테스트 정확도가 60 %로 떨어집니다. 분류 정확도는 우리에게 높은 정확도를 달성한다는 잘못된 감각을 줄 수 있습니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 정확도 계산</span></span><br><span class="line"></span><br><span class="line">acc_logreg = cross_val_score(clf_logreg, X_train, y_train, cv = cv, scoring = <span class="string">&#x27;accuracy&#x27;</span>).mean()</span><br><span class="line">acc_SGD = cross_val_score(clf_SGD, X_train, y_train, cv = cv, scoring = <span class="string">&#x27;accuracy&#x27;</span>).mean()</span><br><span class="line">acc_rfc = cross_val_score(clf_rfc, X_train, y_train, cv = cv, scoring = <span class="string">&#x27;accuracy&#x27;</span>).mean()</span><br><span class="line"></span><br><span class="line">acc_logreg, acc_SGD, acc_rfc</span><br></pre></td></tr></table></figure>




<pre><code>(0.7797035692679977, 0.611222020568663, 0.7606473079249849)</code></pre>
<h3 id="로그-손실-로그-손실-로지스틱-손실-교차-엔트로피-손실"><a href="#로그-손실-로그-손실-로지스틱-손실-교차-엔트로피-손실" class="headerlink" title="로그 손실 / 로그 손실 / 로지스틱 손실 / 교차 엔트로피 손실"></a>로그 손실 / 로그 손실 / 로지스틱 손실 / 교차 엔트로피 손실</h3><ul>
<li>로그 손실로 작업 할 때 분류기는 모든 샘플에 대해 각 클래스에 확률을 할당해야합니다.</li>
<li>로그 손실은 실제 레이블과 비교하고 잘못된 분류에 페널티를 적용하여 모델 확률의 불확실성을 측정합니다.</li>
<li>로그 손실은 둘 이상의 레이블에 대해서만 정의됩니다.</li>
<li>로그 손실은 예측 확률이 향상됨에 따라 점차 감소하므로 로그 손실이 0에 가까울수록 정확도가 높아지고 로그 손실이 0에서 멀어지면 정확도가 낮아집니다.</li>
<li>로그 손실은 (0, ∞] 범위에 있습니다.</li>
</ul>
<p>M 클래스에 속하는 N 개의 샘플이 있다고 가정하면 로그 손실은 다음과 같이 계산됩니다.<br>$$ Log\ Loss = \frac{-1}{N} \sum_{i=1}^{N} \sum_{i=1}^{M}  y_{ij} * \log(\hat{y_{ij}})$$   </p>
<ul>
<li>$y_{ij}$ ,샘플 i가 클래스 j에 속하는지 여부를 나타냅니다.</li>
<li>$p_{ij}$ ,샘플 i가 클래스 j에 속하는 확률을 나타냅니다.</li>
</ul>
<p>음수 부호 부정  $\log(\hat{y_{ij}})$  항상 음수 인 출력.  $\hat{y_{ij}}$  확률 (0-1)을 출력하고,  $\log(x)$  0 &lt;x &lt;1 인 경우 음수입니다.  </p>
<p><strong>예</strong>:<br>학습 레이블은 0과 1이지만 학습 예측은 0.4, 0.6, 0.89 등입니다. 모델의 오류 측정 값을 계산하기 위해 0.5보다 큰 값을 갖는 모든 관측 값을 1로 분류 할 수 있습니다. 우리는 오 분류를 증가시킬 위험이 높습니다. 확률이 0.4, 0.45, 0.49 인 많은 값이 1의 참값을 가질 수 있기 때문입니다.<br>이것이 logLoss가 등장하는 곳입니다.<br>이제 LogLoss의 공식을 자세히 살펴 보겠습니다. 값에 대한 4 가지 주요 사례가있을 수 있습니다. $y_{ij}$  과  $p_{ij}$ </p>
<ul>
<li>사례 1 :  $y_{ij}$j =1 ,  $p_{ij}$  = 높음</li>
<li>사례 2 :  $y_{ij}$ =1 ,  $p_{ij}$  = 낮음</li>
<li>사례 3 :  $y_{ij}$ =0 ,  $p_{ij}$  = 낮음</li>
<li>사례 4 :  $y_{ij}$ =0 ,  $p_{ij}$  = 높음</li>
</ul>
<p>LogLoss는 불확실성을 어떻게 측정합니까?<br>케이스 1과 케이스 3이 더 많이있는 경우 로그 로스 공식 내부의 합계 (및 평균)는 케이스 2와 케이스 4가 추가 된 경우에 비해 훨씬 더 커질 것입니다. 이제이 값은 좋은 예측을 나타내는 Case 1 및 Case 3만큼 큽니다. (-1)을 곱하면 값을 가능한 한 작게 만듭니다. 이것은 이제 직관적으로 의미합니다.-값이 작을수록 모델이 더 좋습니다. 즉, 로그 손실이 더 작고, 모델이 더 좋습니다. 즉, 불확실성이 더 작고, 모델이 더 좋습니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># logloss 계산</span></span><br><span class="line"></span><br><span class="line">logloss_logreg = cross_val_score(clf_logreg, X_train, y_train, cv = cv, scoring = <span class="string">&#x27;neg_log_loss&#x27;</span>).mean()</span><br><span class="line">logloss_rfc = cross_val_score(clf_rfc, X_train, y_train, cv = cv, scoring = <span class="string">&#x27;neg_log_loss&#x27;</span>).mean()</span><br><span class="line"></span><br><span class="line"><span class="comment"># SGDClassifier의 힌지 손실은 확률 추정을 지원하지 않습니다.</span></span><br><span class="line"><span class="comment"># Scikit-learn의 CalibratedClassifierCV에서 SGDClassifier를 기본 추정기로 설정하여 확률 추정치를 생성 할 수 있습니다.</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.calibration <span class="keyword">import</span> CalibratedClassifierCV</span><br><span class="line"></span><br><span class="line">new_clf_SGD = CalibratedClassifierCV(clf_SGD)</span><br><span class="line">new_clf_SGD.fit(X_train, y_train)</span><br><span class="line">logloss_SGD = cross_val_score(new_clf_SGD, X_train, y_train, cv = cv, scoring = <span class="string">&#x27;neg_log_loss&#x27;</span>).mean()</span><br><span class="line"></span><br><span class="line">logloss_logreg, logloss_SGD, logloss_rfc</span><br></pre></td></tr></table></figure>




<pre><code>(-0.48368646454082465, -0.6383384003043665, -0.4664817973667718)</code></pre>
<h3 id="ROC-곡선"><a href="#ROC-곡선" class="headerlink" title="ROC 곡선"></a>ROC 곡선</h3><h3 id="AUC"><a href="#AUC" class="headerlink" title="AUC"></a>AUC</h3><h3 id="혼동-매트릭스"><a href="#혼동-매트릭스" class="headerlink" title="혼동 매트릭스"></a>혼동 매트릭스</h3><h3 id="분류-보고서"><a href="#분류-보고서" class="headerlink" title="분류 보고서"></a>분류 보고서</h3><h3 id="정밀도-재현율-트레이드-오프"><a href="#정밀도-재현율-트레이드-오프" class="headerlink" title="정밀도-재현율 트레이드 오프"></a>정밀도-재현율 트레이드 오프</h3><h3 id="결론"><a href="#결론" class="headerlink" title="결론"></a>결론</h3><h2 id="회귀-지표"><a href="#회귀-지표" class="headerlink" title="회귀 지표"></a>회귀 지표</h2><h3 id="평균-절대-오차"><a href="#평균-절대-오차" class="headerlink" title="평균 절대 오차"></a>평균 절대 오차</h3><h3 id="평균-제곱-오차"><a href="#평균-제곱-오차" class="headerlink" title="평균 제곱 오차"></a>평균 제곱 오차</h3><h3 id="RMSE"><a href="#RMSE" class="headerlink" title="RMSE"></a>RMSE</h3><h3 id="평균-제곱근-로그-오차"><a href="#평균-제곱근-로그-오차" class="headerlink" title="평균 제곱근 로그 오차"></a>평균 제곱근 로그 오차</h3><h3 id="R-제곱"><a href="#R-제곱" class="headerlink" title="R_ 제곱"></a>R_ 제곱</h3><h3 id="조정-된-R-제곱"><a href="#조정-된-R-제곱" class="headerlink" title="조정 된 R- 제곱"></a>조정 된 R- 제곱</h3><h2 id="NLP-메트릭"><a href="#NLP-메트릭" class="headerlink" title="NLP 메트릭"></a>NLP 메트릭</h2><h2 id="보너스"><a href="#보너스" class="headerlink" title="보너스"></a>보너스</h2><h3 id="다중-클래스-분류"><a href="#다중-클래스-분류" class="headerlink" title="다중 클래스 분류"></a>다중 클래스 분류</h3><h3 id="다중-라벨-분류"><a href="#다중-라벨-분류" class="headerlink" title="다중 라벨 분류"></a>다중 라벨 분류</h3><h3 id="다중-출력-분류"><a href="#다중-출력-분류" class="headerlink" title="다중 출력 분류"></a>다중 출력 분류</h3></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-12-01T02:54:41.038Z" title="2020-12-01T02:54:41.038Z">2020-12-01</time></span><span class="level-item">Updated&nbsp;<time dateTime="2020-12-01T02:55:08.810Z" title="2020-12-01T02:55:08.810Z">2020-12-01</time></span><span class="level-item"><a class="link-muted" href="/categories/study/">study</a></span><span class="level-item">14 minutes read (About 2044 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/12/01/study/Tour_of_Evaluation_Metrics_for_Imbalanced_Classification/">불균형 분류에 대한 평가 지표 둘러보기 (번역)</a></h1><div class="content"><h1 id="불균형-분류에-대한-평가-지표-둘러보기"><a href="#불균형-분류에-대한-평가-지표-둘러보기" class="headerlink" title="불균형 분류에 대한 평가 지표 둘러보기"></a>불균형 분류에 대한 평가 지표 둘러보기</h1><p>(Tour of Evaluation Metrics for Imbalanced Classification)</p>
<p>분류기는 평가에 사용되는 측정 항목만큼만 우수합니다.<br><br><br>모델을 평가하기 위해 잘못된 메트릭을 선택하면 불량 모델을 선택하거나 최악의 경우 모델의 예상 성능에 대해 오해 할 가능성이 있습니다.<br><br><br>적절한 측정 항목을 선택하는 것은 일반적으로 응용 기계 학습에서 어렵지만 불균형 분류 문제의 경우 특히 어렵습니다. 첫째, 널리 사용되는 대부분의 표준 메트릭은 균형 잡힌 클래스 분포를 가정하고 일반적으로 모든 클래스가 아니므로 모든 예측 오류가 아닌 불균형 분류에 대해 동일하기 때문입니다.<br><br><br>이 자습서에서는 불균형 분류에 사용할 수있는 메트릭을 발견합니다.<br>이 자습서를 완료하면 다음을 알게됩니다.  </p>
<ul>
<li>분류를 위한 메트릭 선택의 문제 및 편향된 클래스 분포가있을 때 특히 어떻게 어려운지에 대해 설명합니다.</li>
<li>등급, 임계 값 및 확률이라고하는 분류기 모델을 평가하기위한 세 가지 주요 메트릭 유형이있는 방법</li>
<li>어디서부터 시작해야할지 모르는 경우 불균형 분류에 대한 메트릭을 선택하는 방법.  </li>
</ul>
<p>모든 예제에 대한 단계별 자습서 및 Python 소스 코드 파일을 포함하여 저의 새로운 저서 <a target="_blank" rel="noopener" href="https://machinelearningmastery.com/imbalanced-classification-with-python/">Imbalanced Classification with Python</a>으로 프로젝트 를 시작하십시오 .</p>
<h2 id="평가-지표의-과제"><a href="#평가-지표의-과제" class="headerlink" title="평가 지표의 과제"></a>평가 지표의 과제</h2><p>(Challenge of Evaluation Metrics)</p>
<p>평가 메트릭은 예측 모델의 성능을 정량화합니다.<br><br><br>여기에는 일반적으로 데이터 세트에서 모델을 학습시키고, 모델을 사용하여 학습 중에 사용되지 않은 홀드 아웃 데이터 세트에 대한 예측을 수행 한 다음 예측을 홀드 아웃 데이터 세트의 예상 값과 비교합니다.<br><br><br>분류 문제의 경우 메트릭에는 예상 클래스 레이블을 예측 된 클래스 레이블과 비교하거나 문제에 대한 클래스 레이블의 예측 확률을 해석하는 작업이 포함됩니다.<br><br><br>모델을 선택하고 데이터 준비 방법을 함께 사용하는 것은 평가 메트릭에 의해 안내되는 검색 문제입니다. 실험은 다른 모델로 수행되며 각 실험의 결과는 측정 항목으로 정량화됩니다.</p>
<blockquote>
<p>평가 측정은 분류 성능을 평가하고 분류 자 ​​모델링을 안내하는 데 중요한 역할을합니다. — <a target="_blank" rel="noopener" href="https://www.worldscientific.com/doi/abs/10.1142/S0218001409007326">불균형 데이터 분류 : 검토</a> , 2009.  </p>
</blockquote>
<p>분류 정확도 또는 분류 오류와 같은 분류 예측 모델을 평가하는 데 널리 사용되는 표준 메트릭이 있습니다.<br><br><br>표준 메트릭은 대부분의 문제에서 잘 작동하므로 널리 채택됩니다. 그러나 모든 메트릭은 문제 또는 문제에서 중요한 것에 대해 가정합니다. 따라서 사용자 또는 프로젝트 이해 관계자가 모델 또는 예측에 대해 중요하다고 생각하는 것을 가장 잘 포착하는 평가 지표를 선택해야하므로 모델 평가 지표를 선택하기가 어렵습니다.<br><br><br>이 문제는 클래스 분포에 왜곡이있을 때 더욱 어려워집니다. 그 이유는 소수 클래스와 다수 클래스 간의 비율이 1 : 100 또는 1 : 1000과 같이 클래스가 불균형하거나 심각하게 불균형 할 때 많은 표준 메트릭이 신뢰할 수 없거나 오해의 소지가 있기 때문입니다.</p>
<blockquote>
<p>클래스 불균형의 경우, 데이터가 왜곡 될 때 왜곡되지 않은 데이터에 사용되는 상대적으로 강력한 기본 절차가 비참하게 무너질 수 있기 때문에 문제는 훨씬 더 심각합니다. — 페이지 187, <a target="_blank" rel="noopener" href="https://amzn.to/32K9K6d">불균형 학습 : 기초, 알고리즘 및 응용 프로그램</a> , 2013.  </p>
</blockquote>
<p>예를 들어, 심각하게 불균형 한 분류 문제에 대한 분류 정확도를보고하는 것은 위험 할 정도로 오해의 소지가 있습니다. 프로젝트 이해 관계자가 결과를 사용하여 결론을 도출하거나 새로운 프로젝트를 계획하는 경우입니다.</p>
<blockquote>
<p>실제로 불균형 도메인에서 공통 메트릭을 사용하면 최적이 아닌 분류 모델로 이어질 수 있으며 이러한 측정은 왜곡 된 도메인에 민감하지 않기 때문에 잘못된 결론을 내릴 수 있습니다. — <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1505.01658">불균형 분포 하에서의 예측 모델링 조사</a> , 2015.</p>
</blockquote>
<p>중요한 것은 불균형 분류로 작업 할 때 종종 다른 평가 지표가 필요하다는 것입니다.<br><br><br>모든 클래스를 동등하게 중요하게 취급하는 표준 평가 메트릭과 달리 불균형 분류 문제는 일반적으로 소수 클래스의 분류 오류를 다수 클래스의 분류 오류보다 더 중요하게 평가합니다. 소수 클래스에 초점을 맞춘 성능 메트릭이 필요할 수 있으며, 이는 효과적인 모델을 훈련하는 데 필요한 관찰이 부족한 소수 클래스이기 때문에 어렵습니다.</p>
<blockquote>
<p>불균형 데이터 세트의 주된 문제는 사용 가능한 데이터 샘플에서 제대로 표현되지 않은 케이스의 성능에 대한 사용자 선호도 편향과 종종 관련된다는 사실에 있습니다. — <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1505.01658">불균형 분포 하에서의 예측 모델링 조사</a> , 2015.</p>
</blockquote>
<p>이제 모델 평가 메트릭을 선택하는 문제에 익숙해 졌으므로 선택할 수있는 여러 메트릭의 몇 가지 예를 살펴 보겠습니다.</p>
<h2 id="분류-자-평가-지표의-분류"><a href="#분류-자-평가-지표의-분류" class="headerlink" title="분류 자 평가 지표의 분류"></a>분류 자 평가 지표의 분류</h2><p>(Taxonomy of Classifier Evaluation Metrics)</p>
<p>분류기 모델을 평가할 때 선택할 수있는 메트릭은 수십 가지가 있으며 학계에서 제안한 메트릭의 모든 애완 동물 버전을 고려할 경우 수백 가지가 있습니다.<br><br><br>선택할 수있는 메트릭을 처리하기 위해 <a target="_blank" rel="noopener" href="http://personales.upv.es/ceferra/">Cesar Ferri</a> 등이 제안한 분류법을 사용합니다 . “ <a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/abs/pii/S0167865508002687">분류를위한 성능 측정의 실험적 비교</a> “라는 제목의 2008 년 논문에서 . 2013 년 책“ <a target="_blank" rel="noopener" href="https://amzn.to/32K9K6d">불균형 학습</a> ” 에서도 채택되었으며 유용하다고 생각합니다.<br><br><br>평가 지표를 세 가지 유용한 그룹으로 나눌 수 있습니다. 그들은:</p>
<ul>
<li>임계 값 메트릭</li>
<li>순위 지표</li>
<li>확률 메트릭.</li>
</ul>
<p>이 구분은 일반적으로 분류 자에 대해 실무자가 사용하는 상위 메트릭, 특히 불균형 분류가 분류 체계에 깔끔하게 적합하기 때문에 유용합니다.</p>
<blockquote>
<p>여러 기계 학습 연구자들이 분류 맥락에서 사용되는 세 가지 평가 지표 제품군을 식별했습니다. 이들은 임계 메트릭 (예 : 정확도 및 F- 측정), 순위 지정 방법 및 메트릭 (예 : 수신기 작동 특성 (ROC) 분석 및 AUC), 확률 적 메트릭 (예 : 평균 제곱근 오차)입니다. — 페이지 189, <a target="_blank" rel="noopener" href="https://amzn.to/32K9K6d">불균형 학습 : 기초, 알고리즘 및 응용 프로그램</a> , 2013.</p>
</blockquote>
<p>차례로 각 그룹을 자세히 살펴 보겠습니다.</p>
<h3 id="불균형-분류에-대한-임계-값-메트릭"><a href="#불균형-분류에-대한-임계-값-메트릭" class="headerlink" title="불균형 분류에 대한 임계 값 메트릭"></a>불균형 분류에 대한 임계 값 메트릭</h3><p>(Threshold Metrics for Imbalanced Classification)</p>
<h2 id="How-to-Choose-an-Evaluation-Metric"><a href="#How-to-Choose-an-Evaluation-Metric" class="headerlink" title="How to Choose an Evaluation Metric"></a>How to Choose an Evaluation Metric</h2></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-12-01T01:06:39.359Z" title="2020-12-01T01:06:39.359Z">2020-12-01</time></span><span class="level-item">Updated&nbsp;<time dateTime="2020-12-01T01:10:53.653Z" title="2020-12-01T01:10:53.653Z">2020-12-01</time></span><span class="level-item"><a class="link-muted" href="/categories/study/">study</a></span><span class="level-item">6 hours read (About 57733 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/12/01/study/%EC%82%AC%EC%9D%B4%ED%82%B7%EB%9F%B0%EC%9C%BC%EB%A1%9C_%EC%8B%9C%EC%9E%91%ED%95%98%EB%8A%94_%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D/">파이썬 머신러닝 완벽가이드 2장</a></h1><div class="content"><h1 id="사이킷런으로-시작하는-머신러닝"><a href="#사이킷런으로-시작하는-머신러닝" class="headerlink" title="사이킷런으로 시작하는 머신러닝"></a>사이킷런으로 시작하는 머신러닝</h1><h2 id="정리"><a href="#정리" class="headerlink" title="정리"></a>정리</h2><p><strong>사이킷런</strong>  </p>
<ul>
<li>많은 머신러닝 알고리즘을 제공</li>
<li>쉽고 직관적인 API 프레임워크</li>
<li>편리하고 다양한 모듈 지원</li>
</ul>
<p><strong>머신러닝 애플리케이션</strong>  </p>
<ul>
<li><strong>전처리 작업</strong>: 데이터의 가공 및 변환 과정</li>
<li><strong>데이터 시트 분리 작업</strong>: 데이터를 학습 데이터와 테스트 데이터로 분리</li>
<li><strong>모델 학습</strong>: 학습 데이터를 기반으로 머신러닝 알고리즘을 적용</li>
<li><strong>예측</strong>: 학습된 모델을 기반으로 테스트 데이터에 대한 예측을 수행</li>
<li><strong>평가</strong>: 예측된 결과값을 실제 결과값과 비교해 머신러닝 모델에 대한 평가를 수행</li>
</ul>
<p><strong>데이터 전처리 작업</strong>  </p>
<ul>
<li>오류 데이터의 보정이나 결손값(Null) 처리 등의 다양한 데이터 클렌징 작업</li>
<li>레이블 인코딩이나 원-핫 인코딩 같은 인코딩 작업</li>
<li>데이터의 스케일링/정규화 작업 등으로 머신러닝 알고리즘이 최적으로 수행될 수 있데 데이터를 사전 처리하는 것</li>
</ul>
<p><strong>머신러닝 모델</strong></p>
<ul>
<li>학습 데이터 세트로 학습한 뒤 반드시 별도의 테스트 데이터 세트로 평가되어야 한다.  </li>
<li>테스트 데이터의 건수 부족이나 고정된 테스트 데이터 세트를 이용한 반복적인 모델의 학습과 평가는 해당 테스트 데이터 세트에만 치우치는 빈약한 머신러닝 모델을 만들 가능성이 높다.  </li>
<li>이를 해결하기 위해 학습 데이터 세트를 학습 데이터와 검증 데이터로 구성된 여러 개의 폴드 세트로 분리해 교차검증을 수행한다. 사이킷런은 교차 검증을 지원하기 위해 KFord, StratifiedKFold, cross_val_score 등의 다양한 클래스와 함수를 제공한다. 또한 머신러닝 모델의 최적의 하이퍼 파라미터를 교차 검증을 통해 추출하기 위해 GridSearchCV를 제공한다.</li>
</ul>
<h2 id="사이킷런-소개와-특징"><a href="#사이킷런-소개와-특징" class="headerlink" title="사이킷런 소개와 특징"></a>사이킷런 소개와 특징</h2><p>사이킷런(scikit-learn)은 파이썬 머신러닝 라이브러리 중 가장 많이 사용되는 라이브러리이다. 사이킷런은 파이썬 기반의 머신러닝을 위한 가장 쉽고 효율적인 개발 라이브러리를 제공한다.</p>
<p><strong>사이킷런의 특징</strong></p>
<ul>
<li>파이썬 기반의 다른 머신러닝 패키지도 사이킷런 스타일의 API를 지향할 정도로 쉽고 가장 파이썬스러운 API를 제공한다. <code># API: 운영체제가 제공하는 함수의 집합체</code></li>
<li>머신러닝을 위한 매우 다양한 알고리즘과 개발을 위한 편리한 프레임워크와 API를 제공한다.</li>
</ul>
<h2 id="colab에서-anaconda-install-설치-방법"><a href="#colab에서-anaconda-install-설치-방법" class="headerlink" title="colab에서 anaconda install 설치 방법"></a>colab에서 anaconda install 설치 방법</h2><p>Conda + Google Colab<br>A guide to installing Conda when using Google Colab<br><a target="_blank" rel="noopener" href="https://towardsdatascience.com/conda-google-colab-75f7c867a522">https://towardsdatascience.com/conda-google-colab-75f7c867a522</a></p>
<p>먼저 Google Colab에서 기본적으로 사용되는 Python을 확인해야합니다. 다음 명령을 실행하면 기본 Python 실행 파일의 절대 경로가 반환됩니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!which python <span class="comment"># should return /usr/local/bin/python</span></span><br></pre></td></tr></table></figure>

<pre><code>/usr/local/bin/python</code></pre>
<p>이제 기본 Python의 버전 번호를 확인하십시오.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!python --version</span><br></pre></td></tr></table></figure>

<pre><code>Python 3.6.9</code></pre>
<p>마지막으로 PYTHONPATH변수가 설정 되었는지 확인합니다 .<br>이 명령을 작성하는 시점에는이 명령 만 반환됩니다 /env/python(이상하게도 이것은 Google Colab 파일 시스템 내에 존재하지 않는 것으로 보이는 디렉토리입니다).</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!echo $PYTHONPATH</span><br></pre></td></tr></table></figure>

<pre><code>/env/python</code></pre>
<p>Miniconda 설치<br>Google Colab 셀에서 실행되는 경우 아래 코드는 해당 Miniconda 버전에 대한 설치 프로그램 스크립트를 다운로드하여 /usr/local. /usr/local기본 위치가 아닌에 직접 설치 ~/miniconda3하면 Conda 및 모든 필수 종속성을 Google Colab 내에서 자동으로 사용할 수 있습니다.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">%%bash</span><br><span class="line">MINICONDA_INSTALLER_SCRIPT=Miniconda3-4.5.4-Linux-x86_64.sh</span><br><span class="line">MINICONDA_PREFIX=/usr/<span class="built_in">local</span></span><br><span class="line">wget https://repo.continuum.io/miniconda/<span class="variable">$MINICONDA_INSTALLER_SCRIPT</span></span><br><span class="line">chmod +x <span class="variable">$MINICONDA_INSTALLER_SCRIPT</span></span><br><span class="line">./<span class="variable">$MINICONDA_INSTALLER_SCRIPT</span> -b -f -p <span class="variable">$MINICONDA_PREFIX</span></span><br></pre></td></tr></table></figure>

<pre><code>PREFIX=/usr/local
installing: python-3.6.5-hc3d631a_2 ...
installing: ca-certificates-2018.03.07-0 ...
installing: conda-env-2.6.0-h36134e3_1 ...
installing: libgcc-ng-7.2.0-hdf63c60_3 ...
installing: libstdcxx-ng-7.2.0-hdf63c60_3 ...
installing: libffi-3.2.1-hd88cf55_4 ...
installing: ncurses-6.1-hf484d3e_0 ...
installing: openssl-1.0.2o-h20670df_0 ...
installing: tk-8.6.7-hc745277_3 ...
installing: xz-5.2.4-h14c3975_4 ...
installing: yaml-0.1.7-had09818_2 ...
installing: zlib-1.2.11-ha838bed_2 ...
installing: libedit-3.1.20170329-h6b74fdf_2 ...
installing: readline-7.0-ha6073c6_4 ...
installing: sqlite-3.23.1-he433501_0 ...
installing: asn1crypto-0.24.0-py36_0 ...
installing: certifi-2018.4.16-py36_0 ...
installing: chardet-3.0.4-py36h0f667ec_1 ...
installing: idna-2.6-py36h82fb2a8_1 ...
installing: pycosat-0.6.3-py36h0a5515d_0 ...
installing: pycparser-2.18-py36hf9f622e_1 ...
installing: pysocks-1.6.8-py36_0 ...
installing: ruamel_yaml-0.15.37-py36h14c3975_2 ...
installing: six-1.11.0-py36h372c433_1 ...
installing: cffi-1.11.5-py36h9745a5d_0 ...
installing: setuptools-39.2.0-py36_0 ...
installing: cryptography-2.2.2-py36h14c3975_0 ...
installing: wheel-0.31.1-py36_0 ...
installing: pip-10.0.1-py36_0 ...
installing: pyopenssl-18.0.0-py36_0 ...
installing: urllib3-1.22-py36hbe7ace6_0 ...
installing: requests-2.18.4-py36he2e5f8d_1 ...
installing: conda-4.5.4-py36_0 ...
installation finished.
WARNING:
    You currently have a PYTHONPATH environment variable set. This may cause
    unexpected behavior when running the Python interpreter in Miniconda3.
    For best results, please verify that your PYTHONPATH only points to
    directories of packages that are compatible with the Python interpreter
    in Miniconda3: /usr/local


--2020-11-30 00:23:29--  https://repo.continuum.io/miniconda/Miniconda3-4.5.4-Linux-x86_64.sh
Resolving repo.continuum.io (repo.continuum.io)... 104.18.201.79, 104.18.200.79, 2606:4700::6812:c94f, ...
Connecting to repo.continuum.io (repo.continuum.io)|104.18.201.79|:443... connected.
HTTP request sent, awaiting response... 301 Moved Permanently
Location: https://repo.anaconda.com/miniconda/Miniconda3-4.5.4-Linux-x86_64.sh [following]
--2020-11-30 00:23:29--  https://repo.anaconda.com/miniconda/Miniconda3-4.5.4-Linux-x86_64.sh
Resolving repo.anaconda.com (repo.anaconda.com)... 104.16.131.3, 104.16.130.3, 2606:4700::6810:8303, ...
Connecting to repo.anaconda.com (repo.anaconda.com)|104.16.131.3|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 58468498 (56M) [application/x-sh]
Saving to: ‘Miniconda3-4.5.4-Linux-x86_64.sh’

     0K .......... .......... .......... .......... ..........  0% 62.9M 1s
    50K .......... .......... .......... .......... ..........  0% 3.74M 8s
   100K .......... .......... .......... .......... ..........  0% 3.90M 10s
   150K .......... .......... .......... .......... ..........  0% 3.98M 11s
   200K .......... .......... .......... .......... ..........  0% 82.5M 9s
   250K .......... .......... .......... .......... ..........  0%  138M 7s
   300K .......... .......... .......... .......... ..........  0% 4.21M 8s
   350K .......... .......... .......... .......... ..........  0% 35.7M 7s
   400K .......... .......... .......... .......... ..........  0%  137M 7s
   450K .......... .......... .......... .......... ..........  0%  187M 6s
   500K .......... .......... .......... .......... ..........  0% 31.4M 6s
   550K .......... .......... .......... .......... ..........  1%  159M 5s
   600K .......... .......... .......... .......... ..........  1%  182M 5s
   650K .......... .......... .......... .......... ..........  1%  175M 4s
   700K .......... .......... .......... .......... ..........  1%  167M 4s
   750K .......... .......... .......... .......... ..........  1% 6.16M 4s
   800K .......... .......... .......... .......... ..........  1%  154M 4s
   850K .......... .......... .......... .......... ..........  1%  149M 4s
   900K .......... .......... .......... .......... ..........  1%  110M 4s
   950K .......... .......... .......... .......... ..........  1% 47.8M 4s
  1000K .......... .......... .......... .......... ..........  1%  121M 4s
  1050K .......... .......... .......... .......... ..........  1%  187M 3s
  1100K .......... .......... .......... .......... ..........  2% 53.5M 3s
  1150K .......... .......... .......... .......... ..........  2% 78.1M 3s
  1200K .......... .......... .......... .......... ..........  2% 74.5M 3s
  1250K .......... .......... .......... .......... ..........  2% 66.5M 3s
  1300K .......... .......... .......... .......... ..........  2%  136M 3s
  1350K .......... .......... .......... .......... ..........  2% 68.3M 3s
  1400K .......... .......... .......... .......... ..........  2% 86.2M 3s
  1450K .......... .......... .......... .......... ..........  2% 62.2M 3s
  1500K .......... .......... .......... .......... ..........  2% 10.7M 3s
  1550K .......... .......... .......... .......... ..........  2%  134M 3s
  1600K .......... .......... .......... .......... ..........  2%  163M 3s
  1650K .......... .......... .......... .......... ..........  2% 82.2M 3s
  1700K .......... .......... .......... .......... ..........  3%  123M 2s
  1750K .......... .......... .......... .......... ..........  3% 94.5M 2s
  1800K .......... .......... .......... .......... ..........  3%  199M 2s
  1850K .......... .......... .......... .......... ..........  3% 88.1M 2s
  1900K .......... .......... .......... .......... ..........  3%  151M 2s
  1950K .......... .......... .......... .......... ..........  3% 84.9M 2s
  2000K .......... .......... .......... .......... ..........  3% 84.4M 2s
  2050K .......... .......... .......... .......... ..........  3%  143M 2s
  2100K .......... .......... .......... .......... ..........  3%  135M 2s
  2150K .......... .......... .......... .......... ..........  3%  157M 2s
  2200K .......... .......... .......... .......... ..........  3%  154M 2s
  2250K .......... .......... .......... .......... ..........  4%  149M 2s
  2300K .......... .......... .......... .......... ..........  4%  186M 2s
  2350K .......... .......... .......... .......... ..........  4% 38.1M 2s
  2400K .......... .......... .......... .......... ..........  4%  159M 2s
  2450K .......... .......... .......... .......... ..........  4%  143M 2s
  2500K .......... .......... .......... .......... ..........  4%  145M 2s
  2550K .......... .......... .......... .......... ..........  4% 30.8M 2s
  2600K .......... .......... .......... .......... ..........  4%  151M 2s
  2650K .......... .......... .......... .......... ..........  4%  166M 2s
  2700K .......... .......... .......... .......... ..........  4%  148M 2s
  2750K .......... .......... .......... .......... ..........  4%  142M 2s
  2800K .......... .......... .......... .......... ..........  4%  192M 2s
  2850K .......... .......... .......... .......... ..........  5%  187M 2s
  2900K .......... .......... .......... .......... ..........  5%  159M 2s
  2950K .......... .......... .......... .......... ..........  5%  160M 2s
  3000K .......... .......... .......... .......... ..........  5%  178M 2s
  3050K .......... .......... .......... .......... ..........  5% 30.6M 2s
  3100K .......... .......... .......... .......... ..........  5%  124M 2s
  3150K .......... .......... .......... .......... ..........  5%  122M 2s
  3200K .......... .......... .......... .......... ..........  5%  185M 2s
  3250K .......... .......... .......... .......... ..........  5%  156M 2s
  3300K .......... .......... .......... .......... ..........  5%  136M 1s
  3350K .......... .......... .......... .......... ..........  5%  122M 1s
  3400K .......... .......... .......... .......... ..........  6%  141M 1s
  3450K .......... .......... .......... .......... ..........  6%  139M 1s
  3500K .......... .......... .......... .......... ..........  6%  144M 1s
  3550K .......... .......... .......... .......... ..........  6%  109M 1s
  3600K .......... .......... .......... .......... ..........  6%  134M 1s
  3650K .......... .......... .......... .......... ..........  6%  143M 1s
  3700K .......... .......... .......... .......... ..........  6%  123M 1s
  3750K .......... .......... .......... .......... ..........  6%  122M 1s
  3800K .......... .......... .......... .......... ..........  6%  143M 1s
  3850K .......... .......... .......... .......... ..........  6%  133M 1s
  3900K .......... .......... .......... .......... ..........  6%  158M 1s
  3950K .......... .......... .......... .......... ..........  7%  121M 1s
  4000K .......... .......... .......... .......... ..........  7%  131M 1s
  4050K .......... .......... .......... .......... ..........  7%  128M 1s
  4100K .......... .......... .......... .......... ..........  7%  128M 1s
  4150K .......... .......... .......... .......... ..........  7%  130M 1s
  4200K .......... .......... .......... .......... ..........  7%  131M 1s
  4250K .......... .......... .......... .......... ..........  7%  126M 1s
  4300K .......... .......... .......... .......... ..........  7%  154M 1s
  4350K .......... .......... .......... .......... ..........  7%  118M 1s
  4400K .......... .......... .......... .......... ..........  7% 86.9M 1s
  4450K .......... .......... .......... .......... ..........  7%  151M 1s
  4500K .......... .......... .......... .......... ..........  7%  137M 1s
  4550K .......... .......... .......... .......... ..........  8%  166M 1s
  4600K .......... .......... .......... .......... ..........  8% 33.4M 1s
  4650K .......... .......... .......... .......... ..........  8%  185M 1s
  4700K .......... .......... .......... .......... ..........  8%  192M 1s
  4750K .......... .......... .......... .......... ..........  8%  126M 1s
  4800K .......... .......... .......... .......... ..........  8%  121M 1s
  4850K .......... .......... .......... .......... ..........  8%  145M 1s
  4900K .......... .......... .......... .......... ..........  8%  186M 1s
  4950K .......... .......... .......... .......... ..........  8%  165M 1s
  5000K .......... .......... .......... .......... ..........  8%  182M 1s
  5050K .......... .......... .......... .......... ..........  8%  190M 1s
  5100K .......... .......... .......... .......... ..........  9%  186M 1s
  5150K .......... .......... .......... .......... ..........  9%  102M 1s
  5200K .......... .......... .......... .......... ..........  9%  142M 1s
  5250K .......... .......... .......... .......... ..........  9%  120M 1s
  5300K .......... .......... .......... .......... ..........  9%  134M 1s
  5350K .......... .......... .......... .......... ..........  9%  115M 1s
  5400K .......... .......... .......... .......... ..........  9%  123M 1s
  5450K .......... .......... .......... .......... ..........  9%  118M 1s
  5500K .......... .......... .......... .......... ..........  9%  130M 1s
  5550K .......... .......... .......... .......... ..........  9%  108M 1s
  5600K .......... .......... .......... .......... ..........  9%  133M 1s
  5650K .......... .......... .......... .......... ..........  9%  126M 1s
  5700K .......... .......... .......... .......... .......... 10%  202M 1s
  5750K .......... .......... .......... .......... .......... 10%  109M 1s
  5800K .......... .......... .......... .......... .......... 10%  163M 1s
  5850K .......... .......... .......... .......... .......... 10%  183M 1s
  5900K .......... .......... .......... .......... .......... 10%  164M 1s
  5950K .......... .......... .......... .......... .......... 10%  101M 1s
  6000K .......... .......... .......... .......... .......... 10%  154M 1s
  6050K .......... .......... .......... .......... .......... 10%  176M 1s
  6100K .......... .......... .......... .......... .......... 10%  156M 1s
  6150K .......... .......... .......... .......... .......... 10%  166M 1s
  6200K .......... .......... .......... .......... .......... 10%  150M 1s
  6250K .......... .......... .......... .......... .......... 11%  177M 1s
  6300K .......... .......... .......... .......... .......... 11%  192M 1s
  6350K .......... .......... .......... .......... .......... 11%  152M 1s
  6400K .......... .......... .......... .......... .......... 11%  176M 1s
  6450K .......... .......... .......... .......... .......... 11%  184M 1s
  6500K .......... .......... .......... .......... .......... 11%  188M 1s
  6550K .......... .......... .......... .......... .......... 11%  163M 1s
  6600K .......... .......... .......... .......... .......... 11%  178M 1s
  6650K .......... .......... .......... .......... .......... 11% 31.7M 1s
  6700K .......... .......... .......... .......... .......... 11%  194M 1s
  6750K .......... .......... .......... .......... .......... 11%  147M 1s
  6800K .......... .......... .......... .......... .......... 11%  193M 1s
  6850K .......... .......... .......... .......... .......... 12%  146M 1s
  6900K .......... .......... .......... .......... .......... 12%  177M 1s
  6950K .......... .......... .......... .......... .......... 12%  172M 1s
  7000K .......... .......... .......... .......... .......... 12%  188M 1s
  7050K .......... .......... .......... .......... .......... 12%  190M 1s
  7100K .......... .......... .......... .......... .......... 12%  153M 1s
  7150K .......... .......... .......... .......... .......... 12%  156M 1s
  7200K .......... .......... .......... .......... .......... 12%  161M 1s
  7250K .......... .......... .......... .......... .......... 12%  118M 1s
  7300K .......... .......... .......... .......... .......... 12%  188M 1s
  7350K .......... .......... .......... .......... .......... 12%  167M 1s
  7400K .......... .......... .......... .......... .......... 13%  171M 1s
  7450K .......... .......... .......... .......... .......... 13%  189M 1s
  7500K .......... .......... .......... .......... .......... 13%  184M 1s
  7550K .......... .......... .......... .......... .......... 13%  103M 1s
  7600K .......... .......... .......... .......... .......... 13%  105M 1s
  7650K .......... .......... .......... .......... .......... 13%  103M 1s
  7700K .......... .......... .......... .......... .......... 13%  127M 1s
  7750K .......... .......... .......... .......... .......... 13%  148M 1s
  7800K .......... .......... .......... .......... .......... 13%  106M 1s
  7850K .......... .......... .......... .......... .......... 13%  119M 1s
  7900K .......... .......... .......... .......... .......... 13%  134M 1s
  7950K .......... .......... .......... .......... .......... 14%  158M 1s
  8000K .......... .......... .......... .......... .......... 14%  197M 1s
  8050K .......... .......... .......... .......... .......... 14%  189M 1s
  8100K .......... .......... .......... .......... .......... 14%  180M 1s
  8150K .......... .......... .......... .......... .......... 14%  170M 1s
  8200K .......... .......... .......... .......... .......... 14%  192M 1s
  8250K .......... .......... .......... .......... .......... 14%  194M 1s
  8300K .......... .......... .......... .......... .......... 14%  183M 1s
  8350K .......... .......... .......... .......... .......... 14%  160M 1s
  8400K .......... .......... .......... .......... .......... 14%  191M 1s
  8450K .......... .......... .......... .......... .......... 14%  194M 1s
  8500K .......... .......... .......... .......... .......... 14%  184M 1s
  8550K .......... .......... .......... .......... .......... 15%  174M 1s
  8600K .......... .......... .......... .......... .......... 15%  192M 1s
  8650K .......... .......... .......... .......... .......... 15%  158M 1s
  8700K .......... .......... .......... .......... .......... 15% 23.0M 1s
  8750K .......... .......... .......... .......... .......... 15%  164M 1s
  8800K .......... .......... .......... .......... .......... 15%  193M 1s
  8850K .......... .......... .......... .......... .......... 15%  189M 1s
  8900K .......... .......... .......... .......... .......... 15%  197M 1s
  8950K .......... .......... .......... .......... .......... 15%  122M 1s
  9000K .......... .......... .......... .......... .......... 15%  171M 1s
  9050K .......... .......... .......... .......... .......... 15%  173M 1s
  9100K .......... .......... .......... .......... .......... 16%  199M 1s
  9150K .......... .......... .......... .......... .......... 16%  159M 1s
  9200K .......... .......... .......... .......... .......... 16%  183M 1s
  9250K .......... .......... .......... .......... .......... 16%  161M 1s
  9300K .......... .......... .......... .......... .......... 16%  117M 1s
  9350K .......... .......... .......... .......... .......... 16%  132M 1s
  9400K .......... .......... .......... .......... .......... 16%  166M 1s
  9450K .......... .......... .......... .......... .......... 16%  128M 1s
  9500K .......... .......... .......... .......... .......... 16%  171M 1s
  9550K .......... .......... .......... .......... .......... 16%  147M 1s
  9600K .......... .......... .......... .......... .......... 16%  124M 1s
  9650K .......... .......... .......... .......... .......... 16%  171M 1s
  9700K .......... .......... .......... .......... .......... 17%  197M 1s
  9750K .......... .......... .......... .......... .......... 17%  168M 1s
  9800K .......... .......... .......... .......... .......... 17%  127M 1s
  9850K .......... .......... .......... .......... .......... 17%  167M 1s
  9900K .......... .......... .......... .......... .......... 17%  181M 1s
  9950K .......... .......... .......... .......... .......... 17%  153M 1s
 10000K .......... .......... .......... .......... .......... 17%  174M 1s
 10050K .......... .......... .......... .......... .......... 17%  179M 1s
 10100K .......... .......... .......... .......... .......... 17%  183M 1s
 10150K .......... .......... .......... .......... .......... 17%  154M 1s
 10200K .......... .......... .......... .......... .......... 17%  185M 1s
 10250K .......... .......... .......... .......... .......... 18%  180M 1s
 10300K .......... .......... .......... .......... .......... 18%  182M 1s
 10350K .......... .......... .......... .......... .......... 18%  147M 1s
 10400K .......... .......... .......... .......... .......... 18%  187M 1s
 10450K .......... .......... .......... .......... .......... 18%  140M 1s
 10500K .......... .......... .......... .......... .......... 18%  149M 1s
 10550K .......... .......... .......... .......... .......... 18%  168M 1s
 10600K .......... .......... .......... .......... .......... 18%  161M 1s
 10650K .......... .......... .......... .......... .......... 18%  142M 1s
 10700K .......... .......... .......... .......... .......... 18%  165M 1s
 10750K .......... .......... .......... .......... .......... 18% 26.5M 1s
 10800K .......... .......... .......... .......... .......... 19%  134M 1s
 10850K .......... .......... .......... .......... .......... 19%  181M 1s
 10900K .......... .......... .......... .......... .......... 19%  183M 1s
 10950K .......... .......... .......... .......... .......... 19%  134M 1s
 11000K .......... .......... .......... .......... .......... 19%  186M 1s
 11050K .......... .......... .......... .......... .......... 19%  133M 1s
 11100K .......... .......... .......... .......... .......... 19%  143M 1s
 11150K .......... .......... .......... .......... .......... 19%  153M 1s
 11200K .......... .......... .......... .......... .......... 19%  149M 1s
 11250K .......... .......... .......... .......... .......... 19%  167M 1s
 11300K .......... .......... .......... .......... .......... 19%  183M 1s
 11350K .......... .......... .......... .......... .......... 19%  134M 1s
 11400K .......... .......... .......... .......... .......... 20%  105M 1s
 11450K .......... .......... .......... .......... .......... 20%  170M 1s
 11500K .......... .......... .......... .......... .......... 20%  177M 1s
 11550K .......... .......... .......... .......... .......... 20%  124M 1s
 11600K .......... .......... .......... .......... .......... 20%  127M 1s
 11650K .......... .......... .......... .......... .......... 20%  170M 1s
 11700K .......... .......... .......... .......... .......... 20%  192M 1s
 11750K .......... .......... .......... .......... .......... 20%  169M 1s
 11800K .......... .......... .......... .......... .......... 20%  189M 1s
 11850K .......... .......... .......... .......... .......... 20%  177M 1s
 11900K .......... .......... .......... .......... .......... 20%  191M 1s
 11950K .......... .......... .......... .......... .......... 21%  150M 1s
 12000K .......... .......... .......... .......... .......... 21%  180M 1s
 12050K .......... .......... .......... .......... .......... 21%  187M 1s
 12100K .......... .......... .......... .......... .......... 21%  191M 1s
 12150K .......... .......... .......... .......... .......... 21%  170M 1s
 12200K .......... .......... .......... .......... .......... 21%  180M 1s
 12250K .......... .......... .......... .......... .......... 21%  109M 1s
 12300K .......... .......... .......... .......... .......... 21%  181M 1s
 12350K .......... .......... .......... .......... .......... 21% 91.5M 1s
 12400K .......... .......... .......... .......... .......... 21%  144M 1s
 12450K .......... .......... .......... .......... .......... 21%  129M 1s
 12500K .......... .......... .......... .......... .......... 21%  197M 1s
 12550K .......... .......... .......... .......... .......... 22%  154M 1s
 12600K .......... .......... .......... .......... .......... 22%  184M 1s
 12650K .......... .......... .......... .......... .......... 22%  183M 1s
 12700K .......... .......... .......... .......... .......... 22%  146M 1s
 12750K .......... .......... .......... .......... .......... 22%  138M 1s
 12800K .......... .......... .......... .......... .......... 22% 32.5M 1s
 12850K .......... .......... .......... .......... .......... 22%  128M 1s
 12900K .......... .......... .......... .......... .......... 22%  148M 1s
 12950K .......... .......... .......... .......... .......... 22%  171M 1s
 13000K .......... .......... .......... .......... .......... 22%  193M 1s
 13050K .......... .......... .......... .......... .......... 22%  147M 1s
 13100K .......... .......... .......... .......... .......... 23%  167M 1s
 13150K .......... .......... .......... .......... .......... 23%  139M 1s
 13200K .......... .......... .......... .......... .......... 23%  149M 1s
 13250K .......... .......... .......... .......... .......... 23%  154M 1s
 13300K .......... .......... .......... .......... .......... 23%  190M 1s
 13350K .......... .......... .......... .......... .......... 23%  148M 1s
 13400K .......... .......... .......... .......... .......... 23%  141M 1s
 13450K .......... .......... .......... .......... .......... 23%  190M 1s
 13500K .......... .......... .......... .......... .......... 23%  186M 1s
 13550K .......... .......... .......... .......... .......... 23%  119M 1s
 13600K .......... .......... .......... .......... .......... 23%  185M 1s
 13650K .......... .......... .......... .......... .......... 23%  189M 1s
 13700K .......... .......... .......... .......... .......... 24%  182M 1s
 13750K .......... .......... .......... .......... .......... 24%  160M 1s
 13800K .......... .......... .......... .......... .......... 24%  190M 1s
 13850K .......... .......... .......... .......... .......... 24%  185M 1s
 13900K .......... .......... .......... .......... .......... 24%  186M 1s
 13950K .......... .......... .......... .......... .......... 24%  150M 1s
 14000K .......... .......... .......... .......... .......... 24%  190M 1s
 14050K .......... .......... .......... .......... .......... 24%  186M 1s
 14100K .......... .......... .......... .......... .......... 24%  130M 1s
 14150K .......... .......... .......... .......... .......... 24%  113M 1s
 14200K .......... .......... .......... .......... .......... 24%  117M 1s
 14250K .......... .......... .......... .......... .......... 25%  180M 1s
 14300K .......... .......... .......... .......... .......... 25%  189M 1s
 14350K .......... .......... .......... .......... .......... 25%  123M 1s
 14400K .......... .......... .......... .......... .......... 25%  171M 1s
 14450K .......... .......... .......... .......... .......... 25%  188M 1s
 14500K .......... .......... .......... .......... .......... 25%  187M 1s
 14550K .......... .......... .......... .......... .......... 25% 31.8M 1s
 14600K .......... .......... .......... .......... .......... 25%  187M 1s
 14650K .......... .......... .......... .......... .......... 25%  164M 1s
 14700K .......... .......... .......... .......... .......... 25%  137M 0s
 14750K .......... .......... .......... .......... .......... 25%  134M 0s
 14800K .......... .......... .......... .......... .......... 26%  191M 0s
 14850K .......... .......... .......... .......... .......... 26%  184M 0s
 14900K .......... .......... .......... .......... .......... 26%  194M 0s
 14950K .......... .......... .......... .......... .......... 26%  128M 0s
 15000K .......... .......... .......... .......... .......... 26%  151M 0s
 15050K .......... .......... .......... .......... .......... 26%  173M 0s
 15100K .......... .......... .......... .......... .......... 26%  160M 0s
 15150K .......... .......... .......... .......... .......... 26%  160M 0s
 15200K .......... .......... .......... .......... .......... 26%  167M 0s
 15250K .......... .......... .......... .......... .......... 26%  158M 0s
 15300K .......... .......... .......... .......... .......... 26%  188M 0s
 15350K .......... .......... .......... .......... .......... 26%  166M 0s
 15400K .......... .......... .......... .......... .......... 27%  129M 0s
 15450K .......... .......... .......... .......... .......... 27%  179M 0s
 15500K .......... .......... .......... .......... .......... 27%  181M 0s
 15550K .......... .......... .......... .......... .......... 27%  159M 0s
 15600K .......... .......... .......... .......... .......... 27%  188M 0s
 15650K .......... .......... .......... .......... .......... 27%  186M 0s
 15700K .......... .......... .......... .......... .......... 27%  178M 0s
 15750K .......... .......... .......... .......... .......... 27%  170M 0s
 15800K .......... .......... .......... .......... .......... 27%  182M 0s
 15850K .......... .......... .......... .......... .......... 27%  186M 0s
 15900K .......... .......... .......... .......... .......... 27%  176M 0s
 15950K .......... .......... .......... .......... .......... 28%  126M 0s
 16000K .......... .......... .......... .......... .......... 28%  107M 0s
 16050K .......... .......... .......... .......... .......... 28%  176M 0s
 16100K .......... .......... .......... .......... .......... 28%  188M 0s
 16150K .......... .......... .......... .......... .......... 28%  164M 0s
 16200K .......... .......... .......... .......... .......... 28%  178M 0s
 16250K .......... .......... .......... .......... .......... 28%  188M 0s
 16300K .......... .......... .......... .......... .......... 28%  188M 0s
 16350K .......... .......... .......... .......... .......... 28%  144M 0s
 16400K .......... .......... .......... .......... .......... 28%  186M 0s
 16450K .......... .......... .......... .......... .......... 28%  191M 0s
 16500K .......... .......... .......... .......... .......... 28%  183M 0s
 16550K .......... .......... .......... .......... .......... 29%  161M 0s
 16600K .......... .......... .......... .......... .......... 29% 28.0M 0s
 16650K .......... .......... .......... .......... .......... 29%  180M 0s
 16700K .......... .......... .......... .......... .......... 29%  108M 0s
 16750K .......... .......... .......... .......... .......... 29% 95.9M 0s
 16800K .......... .......... .......... .......... .......... 29%  139M 0s
 16850K .......... .......... .......... .......... .......... 29%  180M 0s
 16900K .......... .......... .......... .......... .......... 29%  155M 0s
 16950K .......... .......... .......... .......... .......... 29%  170M 0s
 17000K .......... .......... .......... .......... .......... 29%  195M 0s
 17050K .......... .......... .......... .......... .......... 29%  144M 0s
 17100K .......... .......... .......... .......... .......... 30%  158M 0s
 17150K .......... .......... .......... .......... .......... 30%  158M 0s
 17200K .......... .......... .......... .......... .......... 30%  156M 0s
 17250K .......... .......... .......... .......... .......... 30%  143M 0s
 17300K .......... .......... .......... .......... .......... 30%  192M 0s
 17350K .......... .......... .......... .......... .......... 30%  165M 0s
 17400K .......... .......... .......... .......... .......... 30%  152M 0s
 17450K .......... .......... .......... .......... .......... 30%  196M 0s
 17500K .......... .......... .......... .......... .......... 30%  161M 0s
 17550K .......... .......... .......... .......... .......... 30%  153M 0s
 17600K .......... .......... .......... .......... .......... 30%  171M 0s
 17650K .......... .......... .......... .......... .......... 30%  192M 0s
 17700K .......... .......... .......... .......... .......... 31%  188M 0s
 17750K .......... .......... .......... .......... .......... 31%  159M 0s
 17800K .......... .......... .......... .......... .......... 31%  116M 0s
 17850K .......... .......... .......... .......... .......... 31%  135M 0s
 17900K .......... .......... .......... .......... .......... 31%  174M 0s
 17950K .......... .......... .......... .......... .......... 31%  160M 0s
 18000K .......... .......... .......... .......... .......... 31%  187M 0s
 18050K .......... .......... .......... .......... .......... 31%  189M 0s
 18100K .......... .......... .......... .......... .......... 31%  172M 0s
 18150K .......... .......... .......... .......... .......... 31%  166M 0s
 18200K .......... .......... .......... .......... .......... 31%  143M 0s
 18250K .......... .......... .......... .......... .......... 32%  177M 0s
 18300K .......... .......... .......... .......... .......... 32%  189M 0s
 18350K .......... .......... .......... .......... .......... 32%  152M 0s
 18400K .......... .......... .......... .......... .......... 32%  181M 0s
 18450K .......... .......... .......... .......... .......... 32%  190M 0s
 18500K .......... .......... .......... .......... .......... 32%  176M 0s
 18550K .......... .......... .......... .......... .......... 32%  166M 0s
 18600K .......... .......... .......... .......... .......... 32%  179M 0s
 18650K .......... .......... .......... .......... .......... 32% 25.8M 0s
 18700K .......... .......... .......... .......... .......... 32%  166M 0s
 18750K .......... .......... .......... .......... .......... 32% 68.1M 0s
 18800K .......... .......... .......... .......... .......... 33%  184M 0s
 18850K .......... .......... .......... .......... .......... 33%  175M 0s
 18900K .......... .......... .......... .......... .......... 33%  185M 0s
 18950K .......... .......... .......... .......... .......... 33%  108M 0s
 19000K .......... .......... .......... .......... .......... 33%  101M 0s
 19050K .......... .......... .......... .......... .......... 33%  178M 0s
 19100K .......... .......... .......... .......... .......... 33%  157M 0s
 19150K .......... .......... .......... .......... .......... 33%  153M 0s
 19200K .......... .......... .......... .......... .......... 33%  190M 0s
 19250K .......... .......... .......... .......... .......... 33%  145M 0s
 19300K .......... .......... .......... .......... .......... 33%  190M 0s
 19350K .......... .......... .......... .......... .......... 33%  165M 0s
 19400K .......... .......... .......... .......... .......... 34%  183M 0s
 19450K .......... .......... .......... .......... .......... 34%  188M 0s
 19500K .......... .......... .......... .......... .......... 34%  190M 0s
 19550K .......... .......... .......... .......... .......... 34% 96.0M 0s
 19600K .......... .......... .......... .......... .......... 34%  142M 0s
 19650K .......... .......... .......... .......... .......... 34%  184M 0s
 19700K .......... .......... .......... .......... .......... 34%  187M 0s
 19750K .......... .......... .......... .......... .......... 34%  158M 0s
 19800K .......... .......... .......... .......... .......... 34%  197M 0s
 19850K .......... .......... .......... .......... .......... 34%  193M 0s
 19900K .......... .......... .......... .......... .......... 34%  176M 0s
 19950K .......... .......... .......... .......... .......... 35%  107M 0s
 20000K .......... .......... .......... .......... .......... 35%  144M 0s
 20050K .......... .......... .......... .......... .......... 35%  173M 0s
 20100K .......... .......... .......... .......... .......... 35%  193M 0s
 20150K .......... .......... .......... .......... .......... 35%  169M 0s
 20200K .......... .......... .......... .......... .......... 35%  192M 0s
 20250K .......... .......... .......... .......... .......... 35%  179M 0s
 20300K .......... .......... .......... .......... .......... 35%  189M 0s
 20350K .......... .......... .......... .......... .......... 35%  157M 0s
 20400K .......... .......... .......... .......... .......... 35%  183M 0s
 20450K .......... .......... .......... .......... .......... 35%  186M 0s
 20500K .......... .......... .......... .......... .......... 35%  188M 0s
 20550K .......... .......... .......... .......... .......... 36%  163M 0s
 20600K .......... .......... .......... .......... .......... 36%  178M 0s
 20650K .......... .......... .......... .......... .......... 36%  188M 0s
 20700K .......... .......... .......... .......... .......... 36% 32.0M 0s
 20750K .......... .......... .......... .......... .......... 36%  139M 0s
 20800K .......... .......... .......... .......... .......... 36%  196M 0s
 20850K .......... .......... .......... .......... .......... 36%  185M 0s
 20900K .......... .......... .......... .......... .......... 36%  183M 0s
 20950K .......... .......... .......... .......... .......... 36%  174M 0s
 21000K .......... .......... .......... .......... .......... 36%  196M 0s
 21050K .......... .......... .......... .......... .......... 36%  158M 0s
 21100K .......... .......... .......... .......... .......... 37%  179M 0s
 21150K .......... .......... .......... .......... .......... 37% 90.5M 0s
 21200K .......... .......... .......... .......... .......... 37%  101M 0s
 21250K .......... .......... .......... .......... .......... 37%  119M 0s
 21300K .......... .......... .......... .......... .......... 37%  175M 0s
 21350K .......... .......... .......... .......... .......... 37%  157M 0s
 21400K .......... .......... .......... .......... .......... 37%  137M 0s
 21450K .......... .......... .......... .......... .......... 37%  124M 0s
 21500K .......... .......... .......... .......... .......... 37%  168M 0s
 21550K .......... .......... .......... .......... .......... 37%  156M 0s
 21600K .......... .......... .......... .......... .......... 37%  184M 0s
 21650K .......... .......... .......... .......... .......... 38%  179M 0s
 21700K .......... .......... .......... .......... .......... 38%  184M 0s
 21750K .......... .......... .......... .......... .......... 38%  168M 0s
 21800K .......... .......... .......... .......... .......... 38%  121M 0s
 21850K .......... .......... .......... .......... .......... 38%  186M 0s
 21900K .......... .......... .......... .......... .......... 38%  187M 0s
 21950K .......... .......... .......... .......... .......... 38%  153M 0s
 22000K .......... .......... .......... .......... .......... 38%  179M 0s
 22050K .......... .......... .......... .......... .......... 38%  188M 0s
 22100K .......... .......... .......... .......... .......... 38%  184M 0s
 22150K .......... .......... .......... .......... .......... 38%  168M 0s
 22200K .......... .......... .......... .......... .......... 38%  180M 0s
 22250K .......... .......... .......... .......... .......... 39%  189M 0s
 22300K .......... .......... .......... .......... .......... 39%  189M 0s
 22350K .......... .......... .......... .......... .......... 39%  150M 0s
 22400K .......... .......... .......... .......... .......... 39%  185M 0s
 22450K .......... .......... .......... .......... .......... 39%  193M 0s
 22500K .......... .......... .......... .......... .......... 39%  115M 0s
 22550K .......... .......... .......... .......... .......... 39%  155M 0s
 22600K .......... .......... .......... .......... .......... 39%  188M 0s
 22650K .......... .......... .......... .......... .......... 39%  179M 0s
 22700K .......... .......... .......... .......... .......... 39%  180M 0s
 22750K .......... .......... .......... .......... .......... 39% 32.5M 0s
 22800K .......... .......... .......... .......... .......... 40%  193M 0s
 22850K .......... .......... .......... .......... .......... 40%  183M 0s
 22900K .......... .......... .......... .......... .......... 40%  190M 0s
 22950K .......... .......... .......... .......... .......... 40%  172M 0s
 23000K .......... .......... .......... .......... .......... 40%  194M 0s
 23050K .......... .......... .......... .......... .......... 40%  141M 0s
 23100K .......... .......... .......... .......... .......... 40%  136M 0s
 23150K .......... .......... .......... .......... .......... 40%  159M 0s
 23200K .......... .......... .......... .......... .......... 40%  178M 0s
 23250K .......... .......... .......... .......... .......... 40%  195M 0s
 23300K .......... .......... .......... .......... .......... 40%  191M 0s
 23350K .......... .......... .......... .......... .......... 40%  163M 0s
 23400K .......... .......... .......... .......... .......... 41%  184M 0s
 23450K .......... .......... .......... .......... .......... 41%  192M 0s
 23500K .......... .......... .......... .......... .......... 41%  140M 0s
 23550K .......... .......... .......... .......... .......... 41% 93.9M 0s
 23600K .......... .......... .......... .......... .......... 41%  184M 0s
 23650K .......... .......... .......... .......... .......... 41% 97.9M 0s
 23700K .......... .......... .......... .......... .......... 41%  118M 0s
 23750K .......... .......... .......... .......... .......... 41% 99.4M 0s
 23800K .......... .......... .......... .......... .......... 41%  116M 0s
 23850K .......... .......... .......... .......... .......... 41%  135M 0s
 23900K .......... .......... .......... .......... .......... 41%  171M 0s
 23950K .......... .......... .......... .......... .......... 42%  154M 0s
 24000K .......... .......... .......... .......... .......... 42%  186M 0s
 24050K .......... .......... .......... .......... .......... 42%  183M 0s
 24100K .......... .......... .......... .......... .......... 42%  191M 0s
 24150K .......... .......... .......... .......... .......... 42%  164M 0s
 24200K .......... .......... .......... .......... .......... 42%  181M 0s
 24250K .......... .......... .......... .......... .......... 42%  180M 0s
 24300K .......... .......... .......... .......... .......... 42%  142M 0s
 24350K .......... .......... .......... .......... .......... 42%  134M 0s
 24400K .......... .......... .......... .......... .......... 42%  177M 0s
 24450K .......... .......... .......... .......... .......... 42%  189M 0s
 24500K .......... .......... .......... .......... .......... 42%  189M 0s
 24550K .......... .......... .......... .......... .......... 43%  118M 0s
 24600K .......... .......... .......... .......... .......... 43%  120M 0s
 24650K .......... .......... .......... .......... .......... 43%  173M 0s
 24700K .......... .......... .......... .......... .......... 43%  191M 0s
 24750K .......... .......... .......... .......... .......... 43%  111M 0s
 24800K .......... .......... .......... .......... .......... 43% 34.5M 0s
 24850K .......... .......... .......... .......... .......... 43%  176M 0s
 24900K .......... .......... .......... .......... .......... 43%  190M 0s
 24950K .......... .......... .......... .......... .......... 43%  177M 0s
 25000K .......... .......... .......... .......... .......... 43%  189M 0s
 25050K .......... .......... .......... .......... .......... 43%  109M 0s
 25100K .......... .......... .......... .......... .......... 44%  169M 0s
 25150K .......... .......... .......... .......... .......... 44%  164M 0s
 25200K .......... .......... .......... .......... .......... 44%  178M 0s
 25250K .......... .......... .......... .......... .......... 44%  191M 0s
 25300K .......... .......... .......... .......... .......... 44%  123M 0s
 25350K .......... .......... .......... .......... .......... 44%  141M 0s
 25400K .......... .......... .......... .......... .......... 44%  137M 0s
 25450K .......... .......... .......... .......... .......... 44%  164M 0s
 25500K .......... .......... .......... .......... .......... 44%  153M 0s
 25550K .......... .......... .......... .......... .......... 44% 80.0M 0s
 25600K .......... .......... .......... .......... .......... 44%  150M 0s
 25650K .......... .......... .......... .......... .......... 45%  172M 0s
 25700K .......... .......... .......... .......... .......... 45%  186M 0s
 25750K .......... .......... .......... .......... .......... 45%  170M 0s
 25800K .......... .......... .......... .......... .......... 45%  182M 0s
 25850K .......... .......... .......... .......... .......... 45%  185M 0s
 25900K .......... .......... .......... .......... .......... 45%  189M 0s
 25950K .......... .......... .......... .......... .......... 45%  149M 0s
 26000K .......... .......... .......... .......... .......... 45%  189M 0s
 26050K .......... .......... .......... .......... .......... 45%  192M 0s
 26100K .......... .......... .......... .......... .......... 45%  186M 0s
 26150K .......... .......... .......... .......... .......... 45%  113M 0s
 26200K .......... .......... .......... .......... .......... 45%  123M 0s
 26250K .......... .......... .......... .......... .......... 46%  186M 0s
 26300K .......... .......... .......... .......... .......... 46%  180M 0s
 26350K .......... .......... .......... .......... .......... 46%  117M 0s
 26400K .......... .......... .......... .......... .......... 46%  140M 0s
 26450K .......... .......... .......... .......... .......... 46%  165M 0s
 26500K .......... .......... .......... .......... .......... 46%  161M 0s
 26550K .......... .......... .......... .......... .......... 46%  145M 0s
 26600K .......... .......... .......... .......... .......... 46%  177M 0s
 26650K .......... .......... .......... .......... .......... 46%  185M 0s
 26700K .......... .......... .......... .......... .......... 46%  183M 0s
 26750K .......... .......... .......... .......... .......... 46%  152M 0s
 26800K .......... .......... .......... .......... .......... 47%  183M 0s
 26850K .......... .......... .......... .......... .......... 47% 33.0M 0s
 26900K .......... .......... .......... .......... .......... 47%  161M 0s
 26950K .......... .......... .......... .......... .......... 47%  173M 0s
 27000K .......... .......... .......... .......... .......... 47%  198M 0s
 27050K .......... .......... .......... .......... .......... 47%  190M 0s
 27100K .......... .......... .......... .......... .......... 47%  186M 0s
 27150K .......... .......... .......... .......... .......... 47%  155M 0s
 27200K .......... .......... .......... .......... .......... 47%  183M 0s
 27250K .......... .......... .......... .......... .......... 47%  190M 0s
 27300K .......... .......... .......... .......... .......... 47%  137M 0s
 27350K .......... .......... .......... .......... .......... 47%  103M 0s
 27400K .......... .......... .......... .......... .......... 48%  134M 0s
 27450K .......... .......... .......... .......... .......... 48%  132M 0s
 27500K .......... .......... .......... .......... .......... 48%  186M 0s
 27550K .......... .......... .......... .......... .......... 48%  105M 0s
 27600K .......... .......... .......... .......... .......... 48%  121M 0s
 27650K .......... .......... .......... .......... .......... 48%  185M 0s
 27700K .......... .......... .......... .......... .......... 48%  176M 0s
 27750K .......... .......... .......... .......... .......... 48%  167M 0s
 27800K .......... .......... .......... .......... .......... 48%  186M 0s
 27850K .......... .......... .......... .......... .......... 48%  179M 0s
 27900K .......... .......... .......... .......... .......... 48%  183M 0s
 27950K .......... .......... .......... .......... .......... 49%  131M 0s
 28000K .......... .......... .......... .......... .......... 49%  110M 0s
 28050K .......... .......... .......... .......... .......... 49%  184M 0s
 28100K .......... .......... .......... .......... .......... 49%  191M 0s
 28150K .......... .......... .......... .......... .......... 49%  162M 0s
 28200K .......... .......... .......... .......... .......... 49%  178M 0s
 28250K .......... .......... .......... .......... .......... 49%  192M 0s
 28300K .......... .......... .......... .......... .......... 49%  126M 0s
 28350K .......... .......... .......... .......... .......... 49% 87.5M 0s
 28400K .......... .......... .......... .......... .......... 49%  168M 0s
 28450K .......... .......... .......... .......... .......... 49%  187M 0s
 28500K .......... .......... .......... .......... .......... 50%  177M 0s
 28550K .......... .......... .......... .......... .......... 50%  172M 0s
 28600K .......... .......... .......... .......... .......... 50%  189M 0s
 28650K .......... .......... .......... .......... .......... 50%  180M 0s
 28700K .......... .......... .......... .......... .......... 50%  185M 0s
 28750K .......... .......... .......... .......... .......... 50%  159M 0s
 28800K .......... .......... .......... .......... .......... 50%  183M 0s
 28850K .......... .......... .......... .......... .......... 50%  180M 0s
 28900K .......... .......... .......... .......... .......... 50% 31.9M 0s
 28950K .......... .......... .......... .......... .......... 50%  160M 0s
 29000K .......... .......... .......... .......... .......... 50%  190M 0s
 29050K .......... .......... .......... .......... .......... 50%  193M 0s
 29100K .......... .......... .......... .......... .......... 51%  145M 0s
 29150K .......... .......... .......... .......... .......... 51% 93.2M 0s
 29200K .......... .......... .......... .......... .......... 51%  147M 0s
 29250K .......... .......... .......... .......... .......... 51%  158M 0s
 29300K .......... .......... .......... .......... .......... 51%  187M 0s
 29350K .......... .......... .......... .......... .......... 51%  170M 0s
 29400K .......... .......... .......... .......... .......... 51%  134M 0s
 29450K .......... .......... .......... .......... .......... 51%  176M 0s
 29500K .......... .......... .......... .......... .......... 51%  192M 0s
 29550K .......... .......... .......... .......... .......... 51%  157M 0s
 29600K .......... .......... .......... .......... .......... 51%  180M 0s
 29650K .......... .......... .......... .......... .......... 52%  189M 0s
 29700K .......... .......... .......... .......... .......... 52%  190M 0s
 29750K .......... .......... .......... .......... .......... 52%  167M 0s
 29800K .......... .......... .......... .......... .......... 52%  149M 0s
 29850K .......... .......... .......... .......... .......... 52%  133M 0s
 29900K .......... .......... .......... .......... .......... 52%  141M 0s
 29950K .......... .......... .......... .......... .......... 52%  151M 0s
 30000K .......... .......... .......... .......... .......... 52%  192M 0s
 30050K .......... .......... .......... .......... .......... 52%  185M 0s
 30100K .......... .......... .......... .......... .......... 52%  183M 0s
 30150K .......... .......... .......... .......... .......... 52%  181M 0s
 30200K .......... .......... .......... .......... .......... 52%  109M 0s
 30250K .......... .......... .......... .......... .......... 53%  137M 0s
 30300K .......... .......... .......... .......... .......... 53%  188M 0s
 30350K .......... .......... .......... .......... .......... 53%  152M 0s
 30400K .......... .......... .......... .......... .......... 53%  192M 0s
 30450K .......... .......... .......... .......... .......... 53%  176M 0s
 30500K .......... .......... .......... .......... .......... 53%  194M 0s
 30550K .......... .......... .......... .......... .......... 53%  134M 0s
 30600K .......... .......... .......... .......... .......... 53%  176M 0s
 30650K .......... .......... .......... .......... .......... 53%  189M 0s
 30700K .......... .......... .......... .......... .......... 53%  185M 0s
 30750K .......... .......... .......... .......... .......... 53%  156M 0s
 30800K .......... .......... .......... .......... .......... 54%  128M 0s
 30850K .......... .......... .......... .......... .......... 54%  184M 0s
 30900K .......... .......... .......... .......... .......... 54%  183M 0s
 30950K .......... .......... .......... .......... .......... 54% 31.4M 0s
 31000K .......... .......... .......... .......... .......... 54%  112M 0s
 31050K .......... .......... .......... .......... .......... 54%  104M 0s
 31100K .......... .......... .......... .......... .......... 54%  158M 0s
 31150K .......... .......... .......... .......... .......... 54%  155M 0s
 31200K .......... .......... .......... .......... .......... 54%  181M 0s
 31250K .......... .......... .......... .......... .......... 54%  137M 0s
 31300K .......... .......... .......... .......... .......... 54%  181M 0s
 31350K .......... .......... .......... .......... .......... 54%  164M 0s
 31400K .......... .......... .......... .......... .......... 55%  195M 0s
 31450K .......... .......... .......... .......... .......... 55%  192M 0s
 31500K .......... .......... .......... .......... .......... 55%  188M 0s
 31550K .......... .......... .......... .......... .......... 55%  152M 0s
 31600K .......... .......... .......... .......... .......... 55%  193M 0s
 31650K .......... .......... .......... .......... .......... 55%  188M 0s
 31700K .......... .......... .......... .......... .......... 55%  112M 0s
 31750K .......... .......... .......... .......... .......... 55%  124M 0s
 31800K .......... .......... .......... .......... .......... 55%  178M 0s
 31850K .......... .......... .......... .......... .......... 55%  184M 0s
 31900K .......... .......... .......... .......... .......... 55%  197M 0s
 31950K .......... .......... .......... .......... .......... 56%  164M 0s
 32000K .......... .......... .......... .......... .......... 56%  188M 0s
 32050K .......... .......... .......... .......... .......... 56%  135M 0s
 32100K .......... .......... .......... .......... .......... 56%  126M 0s
 32150K .......... .......... .......... .......... .......... 56%  168M 0s
 32200K .......... .......... .......... .......... .......... 56%  180M 0s
 32250K .......... .......... .......... .......... .......... 56%  197M 0s
 32300K .......... .......... .......... .......... .......... 56%  198M 0s
 32350K .......... .......... .......... .......... .......... 56%  154M 0s
 32400K .......... .......... .......... .......... .......... 56%  189M 0s
 32450K .......... .......... .......... .......... .......... 56%  193M 0s
 32500K .......... .......... .......... .......... .......... 57%  189M 0s
 32550K .......... .......... .......... .......... .......... 57%  163M 0s
 32600K .......... .......... .......... .......... .......... 57%  189M 0s
 32650K .......... .......... .......... .......... .......... 57%  172M 0s
 32700K .......... .......... .......... .......... .......... 57%  139M 0s
 32750K .......... .......... .......... .......... .......... 57%  110M 0s
 32800K .......... .......... .......... .......... .......... 57%  188M 0s
 32850K .......... .......... .......... .......... .......... 57%  188M 0s
 32900K .......... .......... .......... .......... .......... 57%  177M 0s
 32950K .......... .......... .......... .......... .......... 57%  166M 0s
 33000K .......... .......... .......... .......... .......... 57% 33.4M 0s
 33050K .......... .......... .......... .......... .......... 57%  193M 0s
 33100K .......... .......... .......... .......... .......... 58%  186M 0s
 33150K .......... .......... .......... .......... .......... 58%  161M 0s
 33200K .......... .......... .......... .......... .......... 58%  185M 0s
 33250K .......... .......... .......... .......... .......... 58%  191M 0s
 33300K .......... .......... .......... .......... .......... 58%  196M 0s
 33350K .......... .......... .......... .......... .......... 58%  168M 0s
 33400K .......... .......... .......... .......... .......... 58% 94.2M 0s
 33450K .......... .......... .......... .......... .......... 58%  182M 0s
 33500K .......... .......... .......... .......... .......... 58%  179M 0s
 33550K .......... .......... .......... .......... .......... 58%  154M 0s
 33600K .......... .......... .......... .......... .......... 58%  186M 0s
 33650K .......... .......... .......... .......... .......... 59%  187M 0s
 33700K .......... .......... .......... .......... .......... 59%  112M 0s
 33750K .......... .......... .......... .......... .......... 59% 94.1M 0s
 33800K .......... .......... .......... .......... .......... 59%  105M 0s
 33850K .......... .......... .......... .......... .......... 59%  180M 0s
 33900K .......... .......... .......... .......... .......... 59%  130M 0s
 33950K .......... .......... .......... .......... .......... 59%  122M 0s
 34000K .......... .......... .......... .......... .......... 59%  190M 0s
 34050K .......... .......... .......... .......... .......... 59%  186M 0s
 34100K .......... .......... .......... .......... .......... 59%  180M 0s
 34150K .......... .......... .......... .......... .......... 59%  169M 0s
 34200K .......... .......... .......... .......... .......... 59%  193M 0s
 34250K .......... .......... .......... .......... .......... 60%  180M 0s
 34300K .......... .......... .......... .......... .......... 60%  181M 0s
 34350K .......... .......... .......... .......... .......... 60%  156M 0s
 34400K .......... .......... .......... .......... .......... 60%  186M 0s
 34450K .......... .......... .......... .......... .......... 60%  187M 0s
 34500K .......... .......... .......... .......... .......... 60%  180M 0s
 34550K .......... .......... .......... .......... .......... 60%  100M 0s
 34600K .......... .......... .......... .......... .......... 60%  114M 0s
 34650K .......... .......... .......... .......... .......... 60%  181M 0s
 34700K .......... .......... .......... .......... .......... 60%  191M 0s
 34750K .......... .......... .......... .......... .......... 60%  150M 0s
 34800K .......... .......... .......... .......... .......... 61%  179M 0s
 34850K .......... .......... .......... .......... .......... 61%  187M 0s
 34900K .......... .......... .......... .......... .......... 61%  181M 0s
 34950K .......... .......... .......... .......... .......... 61%  163M 0s
 35000K .......... .......... .......... .......... .......... 61%  184M 0s
 35050K .......... .......... .......... .......... .......... 61% 31.9M 0s
 35100K .......... .......... .......... .......... .......... 61%  177M 0s
 35150K .......... .......... .......... .......... .......... 61%  164M 0s
 35200K .......... .......... .......... .......... .......... 61%  129M 0s
 35250K .......... .......... .......... .......... .......... 61%  129M 0s
 35300K .......... .......... .......... .......... .......... 61%  188M 0s
 35350K .......... .......... .......... .......... .......... 61%  168M 0s
 35400K .......... .......... .......... .......... .......... 62%  185M 0s
 35450K .......... .......... .......... .......... .......... 62%  192M 0s
 35500K .......... .......... .......... .......... .......... 62%  142M 0s
 35550K .......... .......... .......... .......... .......... 62%  111M 0s
 35600K .......... .......... .......... .......... .......... 62%  187M 0s
 35650K .......... .......... .......... .......... .......... 62%  138M 0s
 35700K .......... .......... .......... .......... .......... 62%  181M 0s
 35750K .......... .......... .......... .......... .......... 62%  114M 0s
 35800K .......... .......... .......... .......... .......... 62%  159M 0s
 35850K .......... .......... .......... .......... .......... 62%  186M 0s
 35900K .......... .......... .......... .......... .......... 62%  178M 0s
 35950K .......... .......... .......... .......... .......... 63%  161M 0s
 36000K .......... .......... .......... .......... .......... 63%  178M 0s
 36050K .......... .......... .......... .......... .......... 63%  152M 0s
 36100K .......... .......... .......... .......... .......... 63%  190M 0s
 36150K .......... .......... .......... .......... .......... 63%  148M 0s
 36200K .......... .......... .......... .......... .......... 63%  191M 0s
 36250K .......... .......... .......... .......... .......... 63%  178M 0s
 36300K .......... .......... .......... .......... .......... 63%  190M 0s
 36350K .......... .......... .......... .......... .......... 63%  157M 0s
 36400K .......... .......... .......... .......... .......... 63%  104M 0s
 36450K .......... .......... .......... .......... .......... 63%  114M 0s
 36500K .......... .......... .......... .......... .......... 64%  176M 0s
 36550K .......... .......... .......... .......... .......... 64%  158M 0s
 36600K .......... .......... .......... .......... .......... 64%  190M 0s
 36650K .......... .......... .......... .......... .......... 64%  188M 0s
 36700K .......... .......... .......... .......... .......... 64%  184M 0s
 36750K .......... .......... .......... .......... .......... 64%  155M 0s
 36800K .......... .......... .......... .......... .......... 64%  186M 0s
 36850K .......... .......... .......... .......... .......... 64%  187M 0s
 36900K .......... .......... .......... .......... .......... 64%  113M 0s
 36950K .......... .......... .......... .......... .......... 64%  169M 0s
 37000K .......... .......... .......... .......... .......... 64%  184M 0s
 37050K .......... .......... .......... .......... .......... 64%  181M 0s
 37100K .......... .......... .......... .......... .......... 65% 33.2M 0s
 37150K .......... .......... .......... .......... .......... 65%  147M 0s
 37200K .......... .......... .......... .......... .......... 65%  183M 0s
 37250K .......... .......... .......... .......... .......... 65%  184M 0s
 37300K .......... .......... .......... .......... .......... 65%  187M 0s
 37350K .......... .......... .......... .......... .......... 65%  130M 0s
 37400K .......... .......... .......... .......... .......... 65%  131M 0s
 37450K .......... .......... .......... .......... .......... 65%  182M 0s
 37500K .......... .......... .......... .......... .......... 65%  141M 0s
 37550K .......... .......... .......... .......... .......... 65%  158M 0s
 37600K .......... .......... .......... .......... .......... 65%  158M 0s
 37650K .......... .......... .......... .......... .......... 66%  191M 0s
 37700K .......... .......... .......... .......... .......... 66%  167M 0s
 37750K .......... .......... .......... .......... .......... 66%  170M 0s
 37800K .......... .......... .......... .......... .......... 66%  183M 0s
 37850K .......... .......... .......... .......... .......... 66%  176M 0s
 37900K .......... .......... .......... .......... .......... 66%  189M 0s
 37950K .......... .......... .......... .......... .......... 66%  121M 0s
 38000K .......... .......... .......... .......... .......... 66%  129M 0s
 38050K .......... .......... .......... .......... .......... 66%  192M 0s
 38100K .......... .......... .......... .......... .......... 66%  188M 0s
 38150K .......... .......... .......... .......... .......... 66%  166M 0s
 38200K .......... .......... .......... .......... .......... 66%  180M 0s
 38250K .......... .......... .......... .......... .......... 67%  185M 0s
 38300K .......... .......... .......... .......... .......... 67%  128M 0s
 38350K .......... .......... .......... .......... .......... 67% 96.4M 0s
 38400K .......... .......... .......... .......... .......... 67%  138M 0s
 38450K .......... .......... .......... .......... .......... 67%  177M 0s
 38500K .......... .......... .......... .......... .......... 67%  185M 0s
 38550K .......... .......... .......... .......... .......... 67%  170M 0s
 38600K .......... .......... .......... .......... .......... 67%  188M 0s
 38650K .......... .......... .......... .......... .......... 67%  177M 0s
 38700K .......... .......... .......... .......... .......... 67%  188M 0s
 38750K .......... .......... .......... .......... .......... 67%  108M 0s
 38800K .......... .......... .......... .......... .......... 68%  139M 0s
 38850K .......... .......... .......... .......... .......... 68%  178M 0s
 38900K .......... .......... .......... .......... .......... 68%  187M 0s
 38950K .......... .......... .......... .......... .......... 68%  169M 0s
 39000K .......... .......... .......... .......... .......... 68%  180M 0s
 39050K .......... .......... .......... .......... .......... 68%  182M 0s
 39100K .......... .......... .......... .......... .......... 68%  187M 0s
 39150K .......... .......... .......... .......... .......... 68% 31.7M 0s
 39200K .......... .......... .......... .......... .......... 68%  170M 0s
 39250K .......... .......... .......... .......... .......... 68%  137M 0s
 39300K .......... .......... .......... .......... .......... 68%  149M 0s
 39350K .......... .......... .......... .......... .......... 69%  149M 0s
 39400K .......... .......... .......... .......... .......... 69%  159M 0s
 39450K .......... .......... .......... .......... .......... 69%  179M 0s
 39500K .......... .......... .......... .......... .......... 69%  185M 0s
 39550K .......... .......... .......... .......... .......... 69%  152M 0s
 39600K .......... .......... .......... .......... .......... 69%  179M 0s
 39650K .......... .......... .......... .......... .......... 69%  181M 0s
 39700K .......... .......... .......... .......... .......... 69%  193M 0s
 39750K .......... .......... .......... .......... .......... 69%  165M 0s
 39800K .......... .......... .......... .......... .......... 69%  126M 0s
 39850K .......... .......... .......... .......... .......... 69%  105M 0s
 39900K .......... .......... .......... .......... .......... 69%  156M 0s
 39950K .......... .......... .......... .......... .......... 70%  158M 0s
 40000K .......... .......... .......... .......... .......... 70%  204M 0s
 40050K .......... .......... .......... .......... .......... 70%  152M 0s
 40100K .......... .......... .......... .......... .......... 70%  173M 0s
 40150K .......... .......... .......... .......... .......... 70%  116M 0s
 40200K .......... .......... .......... .......... .......... 70%  172M 0s
 40250K .......... .......... .......... .......... .......... 70%  154M 0s
 40300K .......... .......... .......... .......... .......... 70% 89.7M 0s
 40350K .......... .......... .......... .......... .......... 70%  116M 0s
 40400K .......... .......... .......... .......... .......... 70%  187M 0s
 40450K .......... .......... .......... .......... .......... 70%  187M 0s
 40500K .......... .......... .......... .......... .......... 71%  190M 0s
 40550K .......... .......... .......... .......... .......... 71%  107M 0s
 40600K .......... .......... .......... .......... .......... 71%  165M 0s
 40650K .......... .......... .......... .......... .......... 71%  189M 0s
 40700K .......... .......... .......... .......... .......... 71%  189M 0s
 40750K .......... .......... .......... .......... .......... 71%  155M 0s
 40800K .......... .......... .......... .......... .......... 71%  184M 0s
 40850K .......... .......... .......... .......... .......... 71%  175M 0s
 40900K .......... .......... .......... .......... .......... 71%  187M 0s
 40950K .......... .......... .......... .......... .......... 71%  167M 0s
 41000K .......... .......... .......... .......... .......... 71%  189M 0s
 41050K .......... .......... .......... .......... .......... 71%  147M 0s
 41100K .......... .......... .......... .......... .......... 72%  183M 0s
 41150K .......... .......... .......... .......... .......... 72%  106M 0s
 41200K .......... .......... .......... .......... .......... 72% 32.3M 0s
 41250K .......... .......... .......... .......... .......... 72%  189M 0s
 41300K .......... .......... .......... .......... .......... 72%  190M 0s
 41350K .......... .......... .......... .......... .......... 72%  165M 0s
 41400K .......... .......... .......... .......... .......... 72%  119M 0s
 41450K .......... .......... .......... .......... .......... 72% 32.9M 0s
 41500K .......... .......... .......... .......... .......... 72%  195M 0s
 41550K .......... .......... .......... .......... .......... 72%  155M 0s
 41600K .......... .......... .......... .......... .......... 72%  179M 0s
 41650K .......... .......... .......... .......... .......... 73%  186M 0s
 41700K .......... .......... .......... .......... .......... 73%  193M 0s
 41750K .......... .......... .......... .......... .......... 73%  105M 0s
 41800K .......... .......... .......... .......... .......... 73%  106M 0s
 41850K .......... .......... .......... .......... .......... 73%  112M 0s
 41900K .......... .......... .......... .......... .......... 73%  171M 0s
 41950K .......... .......... .......... .......... .......... 73%  145M 0s
 42000K .......... .......... .......... .......... .......... 73%  167M 0s
 42050K .......... .......... .......... .......... .......... 73%  176M 0s
 42100K .......... .......... .......... .......... .......... 73%  189M 0s
 42150K .......... .......... .......... .......... .......... 73%  116M 0s
 42200K .......... .......... .......... .......... .......... 73%  114M 0s
 42250K .......... .......... .......... .......... .......... 74%  189M 0s
 42300K .......... .......... .......... .......... .......... 74%  184M 0s
 42350K .......... .......... .......... .......... .......... 74%  150M 0s
 42400K .......... .......... .......... .......... .......... 74%  193M 0s
 42450K .......... .......... .......... .......... .......... 74%  209M 0s
 42500K .......... .......... .......... .......... .......... 74%  185M 0s
 42550K .......... .......... .......... .......... .......... 74%  159M 0s
 42600K .......... .......... .......... .......... .......... 74%  192M 0s
 42650K .......... .......... .......... .......... .......... 74%  143M 0s
 42700K .......... .......... .......... .......... .......... 74%  137M 0s
 42750K .......... .......... .......... .......... .......... 74%  123M 0s
 42800K .......... .......... .......... .......... .......... 75%  157M 0s
 42850K .......... .......... .......... .......... .......... 75%  180M 0s
 42900K .......... .......... .......... .......... .......... 75%  189M 0s
 42950K .......... .......... .......... .......... .......... 75%  176M 0s
 43000K .......... .......... .......... .......... .......... 75%  191M 0s
 43050K .......... .......... .......... .......... .......... 75%  124M 0s
 43100K .......... .......... .......... .......... .......... 75%  183M 0s
 43150K .......... .......... .......... .......... .......... 75%  140M 0s
 43200K .......... .......... .......... .......... .......... 75%  141M 0s
 43250K .......... .......... .......... .......... .......... 75% 32.0M 0s
 43300K .......... .......... .......... .......... .......... 75%  196M 0s
 43350K .......... .......... .......... .......... .......... 76%  168M 0s
 43400K .......... .......... .......... .......... .......... 76%  194M 0s
 43450K .......... .......... .......... .......... .......... 76%  190M 0s
 43500K .......... .......... .......... .......... .......... 76%  188M 0s
 43550K .......... .......... .......... .......... .......... 76%  131M 0s
 43600K .......... .......... .......... .......... .......... 76%  181M 0s
 43650K .......... .......... .......... .......... .......... 76%  183M 0s
 43700K .......... .......... .......... .......... .......... 76%  178M 0s
 43750K .......... .......... .......... .......... .......... 76%  167M 0s
 43800K .......... .......... .......... .......... .......... 76%  186M 0s
 43850K .......... .......... .......... .......... .......... 76%  183M 0s
 43900K .......... .......... .......... .......... .......... 76%  183M 0s
 43950K .......... .......... .......... .......... .......... 77%  159M 0s
 44000K .......... .......... .......... .......... .......... 77%  185M 0s
 44050K .......... .......... .......... .......... .......... 77%  129M 0s
 44100K .......... .......... .......... .......... .......... 77%  142M 0s
 44150K .......... .......... .......... .......... .......... 77%  165M 0s
 44200K .......... .......... .......... .......... .......... 77%  182M 0s
 44250K .......... .......... .......... .......... .......... 77%  189M 0s
 44300K .......... .......... .......... .......... .......... 77%  181M 0s
 44350K .......... .......... .......... .......... .......... 77%  138M 0s
 44400K .......... .......... .......... .......... .......... 77% 97.1M 0s
 44450K .......... .......... .......... .......... .......... 77%  139M 0s
 44500K .......... .......... .......... .......... .......... 78%  152M 0s
 44550K .......... .......... .......... .......... .......... 78%  116M 0s
 44600K .......... .......... .......... .......... .......... 78%  121M 0s
 44650K .......... .......... .......... .......... .......... 78%  174M 0s
 44700K .......... .......... .......... .......... .......... 78%  193M 0s
 44750K .......... .......... .......... .......... .......... 78%  160M 0s
 44800K .......... .......... .......... .......... .......... 78%  181M 0s
 44850K .......... .......... .......... .......... .......... 78%  188M 0s
 44900K .......... .......... .......... .......... .......... 78%  126M 0s
 44950K .......... .......... .......... .......... .......... 78%  160M 0s
 45000K .......... .......... .......... .......... .......... 78%  153M 0s
 45050K .......... .......... .......... .......... .......... 78%  148M 0s
 45100K .......... .......... .......... .......... .......... 79%  188M 0s
 45150K .......... .......... .......... .......... .......... 79%  149M 0s
 45200K .......... .......... .......... .......... .......... 79%  190M 0s
 45250K .......... .......... .......... .......... .......... 79%  187M 0s
 45300K .......... .......... .......... .......... .......... 79% 33.5M 0s
 45350K .......... .......... .......... .......... .......... 79%  162M 0s
 45400K .......... .......... .......... .......... .......... 79%  145M 0s
 45450K .......... .......... .......... .......... .......... 79%  142M 0s
 45500K .......... .......... .......... .......... .......... 79%  187M 0s
 45550K .......... .......... .......... .......... .......... 79%  157M 0s
 45600K .......... .......... .......... .......... .......... 79%  181M 0s
 45650K .......... .......... .......... .......... .......... 80%  195M 0s
 45700K .......... .......... .......... .......... .......... 80%  196M 0s
 45750K .......... .......... .......... .......... .......... 80%  163M 0s
 45800K .......... .......... .......... .......... .......... 80%  181M 0s
 45850K .......... .......... .......... .......... .......... 80%  189M 0s
 45900K .......... .......... .......... .......... .......... 80%  127M 0s
 45950K .......... .......... .......... .......... .......... 80%  112M 0s
 46000K .......... .......... .......... .......... .......... 80%  187M 0s
 46050K .......... .......... .......... .......... .......... 80%  186M 0s
 46100K .......... .......... .......... .......... .......... 80%  178M 0s
 46150K .......... .......... .......... .......... .......... 80%  167M 0s
 46200K .......... .......... .......... .......... .......... 81%  146M 0s
 46250K .......... .......... .......... .......... .......... 81%  129M 0s
 46300K .......... .......... .......... .......... .......... 81%  167M 0s
 46350K .......... .......... .......... .......... .......... 81%  123M 0s
 46400K .......... .......... .......... .......... .......... 81%  149M 0s
 46450K .......... .......... .......... .......... .......... 81%  157M 0s
 46500K .......... .......... .......... .......... .......... 81%  182M 0s
 46550K .......... .......... .......... .......... .......... 81%  169M 0s
 46600K .......... .......... .......... .......... .......... 81%  180M 0s
 46650K .......... .......... .......... .......... .......... 81%  195M 0s
 46700K .......... .......... .......... .......... .......... 81%  183M 0s
 46750K .......... .......... .......... .......... .......... 81%  110M 0s
 46800K .......... .......... .......... .......... .......... 82%  138M 0s
 46850K .......... .......... .......... .......... .......... 82%  159M 0s
 46900K .......... .......... .......... .......... .......... 82%  153M 0s
 46950K .......... .......... .......... .......... .......... 82%  169M 0s
 47000K .......... .......... .......... .......... .......... 82%  185M 0s
 47050K .......... .......... .......... .......... .......... 82%  188M 0s
 47100K .......... .......... .......... .......... .......... 82%  179M 0s
 47150K .......... .......... .......... .......... .......... 82%  155M 0s
 47200K .......... .......... .......... .......... .......... 82%  185M 0s
 47250K .......... .......... .......... .......... .......... 82%  181M 0s
 47300K .......... .......... .......... .......... .......... 82%  191M 0s
 47350K .......... .......... .......... .......... .......... 83% 31.7M 0s
 47400K .......... .......... .......... .......... .......... 83%  193M 0s
 47450K .......... .......... .......... .......... .......... 83%  193M 0s
 47500K .......... .......... .......... .......... .......... 83%  200M 0s
 47550K .......... .......... .......... .......... .......... 83%  160M 0s
 47600K .......... .......... .......... .......... .......... 83%  184M 0s
 47650K .......... .......... .......... .......... .......... 83%  202M 0s
 47700K .......... .......... .......... .......... .......... 83%  190M 0s
 47750K .......... .......... .......... .......... .......... 83%  178M 0s
 47800K .......... .......... .......... .......... .......... 83%  139M 0s
 47850K .......... .......... .......... .......... .......... 83% 80.2M 0s
 47900K .......... .......... .......... .......... .......... 83%  180M 0s
 47950K .......... .......... .......... .......... .......... 84%  163M 0s
 48000K .......... .......... .......... .......... .......... 84%  191M 0s
 48050K .......... .......... .......... .......... .......... 84%  185M 0s
 48100K .......... .......... .......... .......... .......... 84%  122M 0s
 48150K .......... .......... .......... .......... .......... 84%  142M 0s
 48200K .......... .......... .......... .......... .......... 84%  180M 0s
 48250K .......... .......... .......... .......... .......... 84%  130M 0s
 48300K .......... .......... .......... .......... .......... 84%  161M 0s
 48350K .......... .......... .......... .......... .......... 84%  151M 0s
 48400K .......... .......... .......... .......... .......... 84%  191M 0s
 48450K .......... .......... .......... .......... .......... 84%  192M 0s
 48500K .......... .......... .......... .......... .......... 85%  188M 0s
 48550K .......... .......... .......... .......... .......... 85%  156M 0s
 48600K .......... .......... .......... .......... .......... 85%  131M 0s
 48650K .......... .......... .......... .......... .......... 85%  138M 0s
 48700K .......... .......... .......... .......... .......... 85%  169M 0s
 48750K .......... .......... .......... .......... .......... 85%  110M 0s
 48800K .......... .......... .......... .......... .......... 85%  185M 0s
 48850K .......... .......... .......... .......... .......... 85%  175M 0s
 48900K .......... .......... .......... .......... .......... 85%  188M 0s
 48950K .......... .......... .......... .......... .......... 85%  168M 0s
 49000K .......... .......... .......... .......... .......... 85%  189M 0s
 49050K .......... .......... .......... .......... .......... 85%  180M 0s
 49100K .......... .......... .......... .......... .......... 86%  177M 0s
 49150K .......... .......... .......... .......... .......... 86%  153M 0s
 49200K .......... .......... .......... .......... .......... 86%  181M 0s
 49250K .......... .......... .......... .......... .......... 86%  187M 0s
 49300K .......... .......... .......... .......... .......... 86%  192M 0s
 49350K .......... .......... .......... .......... .......... 86%  165M 0s
 49400K .......... .......... .......... .......... .......... 86% 30.5M 0s
 49450K .......... .......... .......... .......... .......... 86%  192M 0s
 49500K .......... .......... .......... .......... .......... 86%  183M 0s
 49550K .......... .......... .......... .......... .......... 86%  160M 0s
 49600K .......... .......... .......... .......... .......... 86%  199M 0s
 49650K .......... .......... .......... .......... .......... 87%  142M 0s
 49700K .......... .......... .......... .......... .......... 87%  119M 0s
 49750K .......... .......... .......... .......... .......... 87%  142M 0s
 49800K .......... .......... .......... .......... .......... 87%  191M 0s
 49850K .......... .......... .......... .......... .......... 87%  177M 0s
 49900K .......... .......... .......... .......... .......... 87%  193M 0s
 49950K .......... .......... .......... .......... .......... 87%  133M 0s
 50000K .......... .......... .......... .......... .......... 87%  121M 0s
 50050K .......... .......... .......... .......... .......... 87%  191M 0s
 50100K .......... .......... .......... .......... .......... 87%  185M 0s
 50150K .......... .......... .......... .......... .......... 87%  114M 0s
 50200K .......... .......... .......... .......... .......... 88%  157M 0s
 50250K .......... .......... .......... .......... .......... 88%  185M 0s
 50300K .......... .......... .......... .......... .......... 88%  178M 0s
 50350K .......... .......... .......... .......... .......... 88%  155M 0s
 50400K .......... .......... .......... .......... .......... 88%  192M 0s
 50450K .......... .......... .......... .......... .......... 88%  155M 0s
 50500K .......... .......... .......... .......... .......... 88%  128M 0s
 50550K .......... .......... .......... .......... .......... 88%  179M 0s
 50600K .......... .......... .......... .......... .......... 88%  188M 0s
 50650K .......... .......... .......... .......... .......... 88%  170M 0s
 50700K .......... .......... .......... .......... .......... 88% 49.4M 0s
 50750K .......... .......... .......... .......... .......... 88%  160M 0s
 50800K .......... .......... .......... .......... .......... 89%  194M 0s
 50850K .......... .......... .......... .......... .......... 89%  189M 0s
 50900K .......... .......... .......... .......... .......... 89%  176M 0s
 50950K .......... .......... .......... .......... .......... 89%  162M 0s
 51000K .......... .......... .......... .......... .......... 89%  193M 0s
 51050K .......... .......... .......... .......... .......... 89%  186M 0s
 51100K .......... .......... .......... .......... .......... 89%  186M 0s
 51150K .......... .......... .......... .......... .......... 89%  156M 0s
 51200K .......... .......... .......... .......... .......... 89%  193M 0s
 51250K .......... .......... .......... .......... .......... 89%  176M 0s
 51300K .......... .......... .......... .......... .......... 89%  183M 0s
 51350K .......... .......... .......... .......... .......... 90%  177M 0s
 51400K .......... .......... .......... .......... .......... 90%  195M 0s
 51450K .......... .......... .......... .......... .......... 90%  151M 0s
 51500K .......... .......... .......... .......... .......... 90%  197M 0s
 51550K .......... .......... .......... .......... .......... 90%  159M 0s
 51600K .......... .......... .......... .......... .......... 90% 31.8M 0s
 51650K .......... .......... .......... .......... .......... 90%  191M 0s
 51700K .......... .......... .......... .......... .......... 90%  201M 0s
 51750K .......... .......... .......... .......... .......... 90%  138M 0s
 51800K .......... .......... .......... .......... .......... 90%  165M 0s
 51850K .......... .......... .......... .......... .......... 90%  163M 0s
 51900K .......... .......... .......... .......... .......... 90%  158M 0s
 51950K .......... .......... .......... .......... .......... 91%  152M 0s
 52000K .......... .......... .......... .......... .......... 91%  179M 0s
 52050K .......... .......... .......... .......... .......... 91%  169M 0s
 52100K .......... .......... .......... .......... .......... 91%  175M 0s
 52150K .......... .......... .......... .......... .......... 91%  176M 0s
 52200K .......... .......... .......... .......... .......... 91%  195M 0s
 52250K .......... .......... .......... .......... .......... 91%  185M 0s
 52300K .......... .......... .......... .......... .......... 91%  189M 0s
 52350K .......... .......... .......... .......... .......... 91%  134M 0s
 52400K .......... .......... .......... .......... .......... 91%  184M 0s
 52450K .......... .......... .......... .......... .......... 91%  178M 0s
 52500K .......... .......... .......... .......... .......... 92%  172M 0s
 52550K .......... .......... .......... .......... .......... 92%  170M 0s
 52600K .......... .......... .......... .......... .......... 92%  183M 0s
 52650K .......... .......... .......... .......... .......... 92%  189M 0s
 52700K .......... .......... .......... .......... .......... 92%  195M 0s
 52750K .......... .......... .......... .......... .......... 92%  161M 0s
 52800K .......... .......... .......... .......... .......... 92%  179M 0s
 52850K .......... .......... .......... .......... .......... 92%  180M 0s
 52900K .......... .......... .......... .......... .......... 92%  195M 0s
 52950K .......... .......... .......... .......... .......... 92% 96.2M 0s
 53000K .......... .......... .......... .......... .......... 92%  168M 0s
 53050K .......... .......... .......... .......... .......... 92%  186M 0s
 53100K .......... .......... .......... .......... .......... 93%  178M 0s
 53150K .......... .......... .......... .......... .......... 93%  145M 0s
 53200K .......... .......... .......... .......... .......... 93%  180M 0s
 53250K .......... .......... .......... .......... .......... 93%  175M 0s
 53300K .......... .......... .......... .......... .......... 93%  169M 0s
 53350K .......... .......... .......... .......... .......... 93%  161M 0s
 53400K .......... .......... .......... .......... .......... 93%  145M 0s
 53450K .......... .......... .......... .......... .......... 93%  138M 0s
 53500K .......... .......... .......... .......... .......... 93%  179M 0s
 53550K .......... .......... .......... .......... .......... 93%  152M 0s
 53600K .......... .......... .......... .......... .......... 93%  112M 0s
 53650K .......... .......... .......... .......... .......... 94% 29.2M 0s
 53700K .......... .......... .......... .......... .......... 94%  149M 0s
 53750K .......... .......... .......... .......... .......... 94%  149M 0s
 53800K .......... .......... .......... .......... .......... 94%  171M 0s
 53850K .......... .......... .......... .......... .......... 94%  164M 0s
 53900K .......... .......... .......... .......... .......... 94%  164M 0s
 53950K .......... .......... .......... .......... .......... 94%  145M 0s
 54000K .......... .......... .......... .......... .......... 94%  168M 0s
 54050K .......... .......... .......... .......... .......... 94%  174M 0s
 54100K .......... .......... .......... .......... .......... 94%  189M 0s
 54150K .......... .......... .......... .......... .......... 94%  171M 0s
 54200K .......... .......... .......... .......... .......... 95%  171M 0s
 54250K .......... .......... .......... .......... .......... 95%  157M 0s
 54300K .......... .......... .......... .......... .......... 95%  185M 0s
 54350K .......... .......... .......... .......... .......... 95%  135M 0s
 54400K .......... .......... .......... .......... .......... 95%  185M 0s
 54450K .......... .......... .......... .......... .......... 95%  187M 0s
 54500K .......... .......... .......... .......... .......... 95%  178M 0s
 54550K .......... .......... .......... .......... .......... 95%  160M 0s
 54600K .......... .......... .......... .......... .......... 95%  191M 0s
 54650K .......... .......... .......... .......... .......... 95%  186M 0s
 54700K .......... .......... .......... .......... .......... 95%  191M 0s
 54750K .......... .......... .......... .......... .......... 95%  142M 0s
 54800K .......... .......... .......... .......... .......... 96%  186M 0s
 54850K .......... .......... .......... .......... .......... 96%  159M 0s
 54900K .......... .......... .......... .......... .......... 96%  164M 0s
 54950K .......... .......... .......... .......... .......... 96%  165M 0s
 55000K .......... .......... .......... .......... .......... 96%  181M 0s
 55050K .......... .......... .......... .......... .......... 96%  188M 0s
 55100K .......... .......... .......... .......... .......... 96%  167M 0s
 55150K .......... .......... .......... .......... .......... 96%  150M 0s
 55200K .......... .......... .......... .......... .......... 96%  173M 0s
 55250K .......... .......... .......... .......... .......... 96%  178M 0s
 55300K .......... .......... .......... .......... .......... 96%  180M 0s
 55350K .......... .......... .......... .......... .......... 97%  145M 0s
 55400K .......... .......... .......... .......... .......... 97%  159M 0s
 55450K .......... .......... .......... .......... .......... 97%  153M 0s
 55500K .......... .......... .......... .......... .......... 97%  155M 0s
 55550K .......... .......... .......... .......... .......... 97%  138M 0s
 55600K .......... .......... .......... .......... .......... 97%  182M 0s
 55650K .......... .......... .......... .......... .......... 97%  189M 0s
 55700K .......... .......... .......... .......... .......... 97% 32.7M 0s
 55750K .......... .......... .......... .......... .......... 97%  169M 0s
 55800K .......... .......... .......... .......... .......... 97%  193M 0s
 55850K .......... .......... .......... .......... .......... 97%  188M 0s
 55900K .......... .......... .......... .......... .......... 97%  182M 0s
 55950K .......... .......... .......... .......... .......... 98%  160M 0s
 56000K .......... .......... .......... .......... .......... 98%  192M 0s
 56050K .......... .......... .......... .......... .......... 98%  194M 0s
 56100K .......... .......... .......... .......... .......... 98%  183M 0s
 56150K .......... .......... .......... .......... .......... 98%  174M 0s
 56200K .......... .......... .......... .......... .......... 98%  187M 0s
 56250K .......... .......... .......... .......... .......... 98%  185M 0s
 56300K .......... .......... .......... .......... .......... 98%  189M 0s
 56350K .......... .......... .......... .......... .......... 98%  160M 0s
 56400K .......... .......... .......... .......... .......... 98%  189M 0s
 56450K .......... .......... .......... .......... .......... 98%  176M 0s
 56500K .......... .......... .......... .......... .......... 99%  192M 0s
 56550K .......... .......... .......... .......... .......... 99%  174M 0s
 56600K .......... .......... .......... .......... .......... 99%  184M 0s
 56650K .......... .......... .......... .......... .......... 99%  184M 0s
 56700K .......... .......... .......... .......... .......... 99%  187M 0s
 56750K .......... .......... .......... .......... .......... 99%  154M 0s
 56800K .......... .......... .......... .......... .......... 99%  183M 0s
 56850K .......... .......... .......... .......... .......... 99%  185M 0s
 56900K .......... .......... .......... .......... .......... 99%  192M 0s
 56950K .......... .......... .......... .......... .......... 99%  167M 0s
 57000K .......... .......... .......... .......... .......... 99%  179M 0s
 57050K .......... .......... .......... .......... ........  100%  170M=0.5s

2020-11-30 00:23:30 (122 MB/s) - ‘Miniconda3-4.5.4-Linux-x86_64.sh’ saved [58468498/58468498]

Python 3.6.5 :: Anaconda, Inc.</code></pre>
<p>사이킷런 버전은 다음과 같이 확인할 수 있다. version앞뒤의 “__”는 언더스코어가 연이어 두개 있는 것이다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sklearn</span><br><span class="line"></span><br><span class="line">print(sklearn.__version__)</span><br></pre></td></tr></table></figure>

<pre><code>0.22.2.post1</code></pre>
<h2 id="첫-번째-머신러닝-만들어-보기-붓꽃-품종-예측하기"><a href="#첫-번째-머신러닝-만들어-보기-붓꽃-품종-예측하기" class="headerlink" title="첫 번째 머신러닝 만들어 보기 - 붓꽃 품종 예측하기"></a>첫 번째 머신러닝 만들어 보기 - 붓꽃 품종 예측하기</h2><p>첫 번째로 만들어 볼 머신러링 모델은 붓꽃 데이터 세트로 붓꽃의 품종을 분류하는 것이다. 붓꽃 데이터 세트는 꽃잎의 길이와 너비, 꽃받침의 길이와 너비 피처를 기반으로 꽃의 품종을 예측하기 위한 것이다.<br>    # 피쳐(Feature)는 기계 학습과 패턴 인식의 용어이다. 관찰 대상에게서 발견된 개별적이고 측정가능한 경험적(heuristic) 속성을 말한다.<br>분류는 <strong>대표적인 지도학습 방법</strong>의 하나이다. 지도학습은 학습을 위한 다양한 피처와 분류 결정값인 레이블 데이터로 모델을 학습한 뒤, 별도의 테스트 데이터 세트에서 미지의 레이블을 예측하는 것. 즉, <strong>명확한 정답이 주어진 데이터를 먼저 학습한 뒤 미지의 정답을 예측하는 방식</strong>. 이 때, 학습을 위해 주어진 데이터 세트를 학습 데이터 세트, 머신러닝 모델의 예측 성능을 평가하기 위해 별도로 주어진 데이터 세트를 테스트 데이터 세트로 지칭한다.</p>
<p>새로운 주피터 노트북을 생성하고 사이킷런에서 사용할 모듈을 임포트 한다. 사이킷런 패키지 내의 모듈명은 sklearn으로 시작하는 명명규칙이 있다. 항목은 아래와 같다.</p>
<ul>
<li>sklearn.datasets: 자체적으로 제공하는 데이터 세트를 생성하는 모듈의 모임</li>
<li>sklearn.tree: 트리 기반 ML 알고리즘을 구현한 클래스의 모임 * ML:Machine Learning</li>
</ul>
<p>예측 데이터로 데이터를 분리하거나 최적의 하이퍼 파라미터로 평가하기 위한 다양한 모듈의 모임이다. 하이퍼 파라미터를 통해 머신러닝 알고리즘의 성능을 튜닝할 수 있다. 붓꽃 데이터 세트를 생성하는 데는 load_iris()를 이용하며, ML 알고리즘은 의사 결정 트리 알고리즘으로, 이를 구현한 DecisionTreeClassifier를 적용한다.</p>
<p>데이터 세트를 학습 데이터와 테스트 데이터로 분리하는 데는 train_test_split()함수를 사용 한다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris </span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier </span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br></pre></td></tr></table></figure>

<p>load_iris() 함수를 이용해 붓꽃 데이터 세트를 로딩한 후, 피처들과 데이터 값이 어떻게 구성돼 있는지 확인하기 위해 DataFrame으로 변환한다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd </span><br><span class="line"></span><br><span class="line"><span class="comment"># 붓꽃 데이터 세트를 로딩합니다.</span></span><br><span class="line">iris = load_iris() </span><br><span class="line"> </span><br><span class="line"><span class="comment"># iris.data는 Iris 데이터 세트에서 피처(feature)만으로 된 데이터를 numpy로 가지고 있습니다. </span></span><br><span class="line">iris_data = iris.data </span><br><span class="line"></span><br><span class="line"><span class="comment"># iris.target은 붓꽃 데이터 세트에서 레이블(결정 값) 데이터를 numpy로 가지고 있습니다. *numpy: 벡터 및 행렬 연산에 있어서 매우 편리한 기능을 제공</span></span><br><span class="line">iris_label = iris.target </span><br><span class="line">print(<span class="string">&#x27;iris target값:&#x27;</span>, iris_label) </span><br><span class="line">print(<span class="string">&#x27;iris target명:&#x27;</span>, iris.target_names) </span><br><span class="line"></span><br><span class="line"><span class="comment"># 붓꽃 데이터 세트를 자세히 보기 위해 DataFrame으로 변환합니다. </span></span><br><span class="line">iris_df = pd.DataFrame(data=iris_data, columns=iris.feature_names) </span><br><span class="line">iris_df[<span class="string">&#x27;label&#x27;</span>] = iris.target </span><br><span class="line">iris_df.head(<span class="number">3</span>)</span><br></pre></td></tr></table></figure>

<pre><code>iris target값: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2
 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
 2 2]
iris target명: [&#39;setosa&#39; &#39;versicolor&#39; &#39;virginica&#39;]</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sepal length (cm)</th>
      <th>sepal width (cm)</th>
      <th>petal length (cm)</th>
      <th>petal width (cm)</th>
      <th>label</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>5.1</td>
      <td>3.5</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>4.9</td>
      <td>3.0</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4.7</td>
      <td>3.2</td>
      <td>1.3</td>
      <td>0.2</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>



<p>학습용 데이터와 테스트용 데이터는 반드시 <strong>분리</strong>해야 한다. 학습 데이터로 학습된 모델이 얼마나 뛰어난 성능을 가지는지 평가하려면 테스트 데이터 세트가 필요하기 때문이다. 이를 위해서 사이킷런은 train_test_split( ) API를 제공한다. train_test_split( )을 이용하면 학습 데이터와 테스트 데이터를 test_size 파라미터 입력 값의 비율로 쉽게 분할한다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x_train, x_test, y_train, y_test = train_test_split(iris_data, iris_label, </span><br><span class="line">                                                    test_size=<span class="number">0.2</span>, random_state=<span class="number">11</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>train_test_split()의 파라미터<br></p>
<ul>
<li>iris_data는 피처 데이터 세트<br></li>
<li>iris_label은 레이블 데이터 세트<br></li>
<li>test_size=0.2는 전체 데이터 세트 중 테스트 데이터 세트 비율(테스트 20%, 학습 데이터 80%)<br></li>
<li>random_state는 호출할 때마다 같은 학습/테스트 용 데이터 세트를 생성하기 위해 주어지는 난수 발생 값. 지정하지 않으면 수행 할때마다 다른 학습/데이터 용 데이터 생성됨</li>
</ul>
<p>위 예제에서 train_test_split()은 학습용 피처 데이터 세트를 x_train으로, 테스트용 피처 테이터 세트를 x_test로, 학습용 레이블 데이터 세트는 y_train으로, 테스트용 레이블 데이터 세트를 y_test로  반환한다.</p>
<p>학습 데이터를 확보 했으니 이 데이터를 기반으로 머신러닝 분류 알고리즘의 하나인 의사 결정을 트리를 이용해 학습과 예측을 수행해 보자. 먼저 사이킷런의 의사 결정 트리 클래스인 DecisionTreeClassifier를 객체로 생성한다. 생성된 DecisionTreeClassifier 객체의 fit() 메서드에 학습용 피처 데이터 속성과 결정값 데이터 세트를 입력해 호출하면 학습을 수행한다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># DecisionTreeClassifier 객체 생성</span></span><br><span class="line">dt_clf = DecisionTreeClassifier(random_state=<span class="number">11</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 학습 수행</span></span><br><span class="line">dt_clf.fit(x_train, y_train)</span><br></pre></td></tr></table></figure>




<pre><code>DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion=&#39;gini&#39;,
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_impurity_split=None,
                       min_samples_leaf=1, min_samples_split=2,
                       min_weight_fraction_leaf=0.0, presort=&#39;deprecated&#39;,
                       random_state=11, splitter=&#39;best&#39;)</code></pre>
<p>의사 결정 트리 기반의 DecisionTreeClassifier 객체는 학습 데이터를 기반으로 학습 완료한다. 예측은 반드시 학습 데이터가 아닌 다른 데이터를 이용해야 하며, 일반적으로 <strong>테스트 데이터 세트</strong>를 이용한다. DecisionTreeClassifier 객체의 predict() 메서드 테스트용 피처 데이터 세트를 입력해 호출하면 학습된 모델 기반에서 테스트 데이터 세트에 대한 예측값을 반환하게 된다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 학습이 완료된 DecisionTreeClassifier 객체에서 테스트 데이터 세트로 예측 수행.</span></span><br><span class="line">pred = dt_clf.predict(x_test)</span><br></pre></td></tr></table></figure>

<p>일반적으로 머신러닝 모델의 성능 평가 방법은 여러 가지가 있으나, 여기서는 정확도를 측정해 보겠다. 정확도는 예측 결과가 실제 레이블 값과 얼마나 정확하게 맞는지 평가하는 지표이다. 예측한 붓꽃 품종과 실제 테스트 데이터 세트의 붓꽃 품종이 얼마나 일치하는지 확인해 보겠다. 사이킷런은 정확도 측정을 위해 accuracy_score() 함수를 제공한다. accuaracy_score()의 첫 번째 파라미터로 실제 레이블 데이터 세트, 두 번째 파라미터로 예측 레이블 데이터 세트를 입력하면 된다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span>  accuracy_score</span><br><span class="line">print(<span class="string">&#x27;예측 정확도: &#123;0:.4f&#125;&#x27;</span>.<span class="built_in">format</span>(accuracy_score(y_test, pred)))</span><br></pre></td></tr></table></figure>

<pre><code>예측 정확도: 0.9333</code></pre>
<p>학습한 의사 결정 트리의 알고리즘 예측 정확도가 약0.9333(93.33%)으로 측정됐다.</p>
<p>앞의 붓꽃 데이터 세트로 분류를 예측한 프로세스를 정리하면 다음과 같다.</p>
<ul>
<li><strong>데이터 세트 분리</strong>: 데이터를 학습 데이터와 테스트 데이터로 분리한다.</li>
<li><strong>모델 학습</strong>: 학습 데이터를 기반으로 ML 알고리즘을 적용해 모델을 학습시킨다.</li>
<li><strong>예측 수행</strong>: 학습된 ML 모델을 이용해 테스트 데이터의 분류를 예측한다.</li>
<li><strong>평가</strong>: 이렇게 예측된 결과값과 테스트 데이터의 실제 결과값을 비교해 ML 모델 성능을 평가한다.</li>
</ul>
<h2 id="사이킷런의-기반-프레임워크-익히기"><a href="#사이킷런의-기반-프레임워크-익히기" class="headerlink" title="사이킷런의 기반 프레임워크 익히기"></a>사이킷런의 기반 프레임워크 익히기</h2><p><strong>Estimator 이해 및 fit(), predict() 메서드</strong><br><br>사이킷런은 API 일관성과 개발 편의성을 제공하기 위한 노력이 엿보이는 패키지이다. 사이킷런은 ML 모델 학습을 위해서 fit( )을, 학습된 모델의 예측을 위해 predict( ) 메서드를 제공한다. 지도학습의 주요 두 축인 분류(Classification)와 회귀(Regression)의 다양한 알고리즘을 구현한 모든 사이킷런 클래스는 fit( )과 predict( ) 만을 이용해 간단하게 학습과 예측 결과를 반환한다. 사이킷런에서는 분류 알고리즘을 구현한 클래스를 Classifier로, 그리고 회귀 알고리즘을 구현한 클래스를 Regressor로 지칭한다. 사이킷런은 매우 많은 유형의 Classifier와 Regressor 클래스를 제공한다. 이들 Classifier와 Regressor를 합쳐서 Estimator 클래스라고 부른다. 즉, <strong>지도학습의 모든 알고리즘을 구현한 클래스를 통칭해서 Estimator라고 부른다.</strong></p>
<p>cross_val_score( )(간편하게 교차검증 결과 평가 지표(정확도, 정밀도등)를 반환)와 같은 evaluation 함수, GridSearchSV와 같은 하이퍼 파라미터 튜닝을 지원하는 클래스의 경우 이 Estimator를 인자로 받는다. 인자로 받은 Estimator에 대해서 cross_val_score( ), GridSearchCV.fit( ) 함수 내에서 이 Estimator의 fit( )과 predict( )를 호출해서 평가를 하거나 하이퍼 파라미터 튜닝을 수행하는 것이다.</p>
<p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAA4IAAAG+CAYAAADGLrqeAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAAEnQAABJ0Ad5mH3gAAOt8SURBVHhe7N0FvGVV3f/x/+Ojj90tKmKLXdjd2ImJioqFDdiKTYitICoqCNjdQUiDtDRDDDnUdM+w/ue95q5hz7nnztxh7szc+H5e/rzMOTvWXnvfe9bn/Fb8vxJCCCGEEEIIYUoREQwhhBBCCCGEKUZEMIQQQgghhBCmGBHBEEIIIYQQQphiRARDCCGEEEIIYYoREQwhhBBCCCGEKUZEMIQQQgghhBCmGBHBEEIIIYQQQphiRARDCCGEEEIIYYoREQwhhBBCCCGEKUZEMIQQQgghhBCmGBHBEEIIIYQQQphiRARDCCGEEEIIYYoREQwhhBBCCCGEKUZEMIQQQgghhBCmGBHBEEIIIYQQQphiRARDCCGEEEIIYYoREQwhhBBCCCGEKUZEMIQQQgghhBCmGBHBEEIIIYQQQphiRARDCCGEEEIIYYoREQwhhBBCCCGEKUZEMIQQQgghhBCmGBHBEEIIIYQQQphiRARDCCGEEEIIYYoREQwhhBBCCCGEKUZEMIQQQgghhBCmGBHBEEIIIYQQQphiRARDCCGEEEIIYYoREQwhhBBCCCGEKUZEMIQQQgghhBCmGBHBEEIIIYQQQphiRARDCCGEEEIIYYoREQwhhBBCCCGEKUZEMIQQQgghhBCmGBHBEEIIIYQQQphiRARDCCGEEEIIYYoREQwhhBBCCCGEKUZEMIQQQgghhBCmGBHBEMJGY/k1pcxbtKxcOmtxmXb5gnLWjPnl1EvmlVMunldOumhuOfHCRGJyxUm9+G/v+T7j0vnl/CsX1md/0dLl5Zre70JYwTW9yuiPjcWSJUvKGWecUb72ta+Vww47bK3KsnTp0nLwwQeX3XffvZx11ln1WBuK/vobKcYjI5XNaxdeeGH5/Oc/X/7whz+Uq666auidjcexxx5bvvrVr5aTTjqpLFq0aOjVFSjfn//85/LhD3+4fPSjHy3HH398Ofvss8vXv/71+izNmTNnaMsQNh4RwRDCBmdpzwCvnr+knNOTP43jo8+dXQ47e1b591kzy0FnziwHnnF1OfD0q8u/EolJFp7rg3rP9yG95/zwc2bVZ//ki+aWc69YUK6cu6QsWTZ1jXDWrFnlH//4R/nQhz5U3va2t5VtttlmZbz3ve8tu+66a33/oosuKosXLx7aa/0ye/bs8vOf/7w86EEPKh//+MfLvHnzRi1QCxYsKB/72MfK5ptvXvbbb78yc+bMoXfWHwsXLiz//Oc/63lb3b3lLW8pb3/722v47/Y6QTn66KOrsG5MnH/69OnlZz/7WfnEJz5Ry6l87373u8tuu+1WDj/88Hpd6v2EE04oD3vYw8qnP/3pcv755w8dYeOgPF/5ylfKQx/60PLtb3+7zJgxY+idUi6++OLy3e9+tzz1qU8t97rXvcrDH/7w8vvf/7788pe/LI9+9KPLJz/5yXLuuecObR3CxiMiGELYYGg/zV20rGZCTpg+p4rfoMZyIjHVwhchx10wp345Mmfhsvq7MtW49NJLa8P6bne7W7n73e9eHvKQh6wMje1HPOIR5TnPeU55//vfXxvVGyIjRPxkdZ797GfXsjUhGQ0yRLKBz3jGM8of//jHDZIBUl7S+cIXvrDW24Mf/OByn/vcp9zqVrcqt7nNbcoDH/jAlXX6vOc9r/zlL3/ZYFLdj3q8+uqry69+9avyrne9qzz96U8vj3rUo6roKZ+fT3jCE6oQnn766XV7GTjPhy8LNrZIKc/ee+9dn8n9999/5fPodTKufl2PDOb3v//9mhX++9//Xl7wghfUDLMvNELY2EQEQwgbBFnAq+YtLadfOr8cds6sgY3hRGKqhy9HdI++at6Sskzf6SnEJZdcUrN+xEXm6sc//vHK2GOPPWrjf8stt6xyQ3RkkIjP+mTZsmU1u/OnP/2pdhEdrQRi+fLl5bTTTqvdGEmuY61vdD91zt/85je13ojKZz7zmXL/+9+/ZqVkqX70ox/V9wjYeeedt0HK1Y96dO/23HPPKsqPe9zjypve9KaaAVRm5dtrr72qRMlunnjiiXWf//znP1UEt99++3GRUTvnnHOq5F9wwQUrM6vuAfEjszLZMt2Nyy67rH6x4FnypUIIG5uIYAhhvaNBe/ncJbUb6MFnJguYSKwudB09YfrccvmcqSWDTQQJyw9+8IOhV1dAVowPO+igg8oHPvCBmtl6xSteUY455pihLcIgyMlxxx1Xs20vetGLyty5c9dKZtcXsqWyY6Re1u873/lOOfnkk2v5Gu65LJtMmszheBTBQbgGmWBZ7C9/+ctDr4YwPokIhhDWK9ocshsatsb+DWr4JhKJ4XH8BXPKlb3fnanigk0EdQmUuRqJU089tbzjHe+o2+28885l/vz5Q++s4Morr6zyYzyhro+HHnpoHU82qAsk2ZClMVaOmNjeRB6kk0R5XzaPgMqeNYnyU1fP//73v+XAAw+s+/3rX/+qZTM2EDKCZ555Zt1Xmfy74bheM+btb3/7W93/r3/9axXbyy+/vL7fRVbyqKOOql0kjTV0ffYR5Igo9e8D13zEEUfUsWq6JDpnVwRdk3PKbDnHkUceWbs1ylg1SBsZO+SQQ2oZvU/aZLr6pdI16vLoOK5LOL46btfvpwzay172srLpppvW7N9oxk+OJILtXiizum714n64rv77LmPnWXPPW73/+9//XqWMkLHsHtN1mxSmK6vqz/ueEcd1XTJ+xjgaG+g5dXzjHD1P7q16tF93chnndQ9lPttzq/yusVt+/23CGV1knat9OWIfZeiWP4TREBEMIaxX5i9eXk6+aF4ygYnEWoYvTmTRjaudCoxWBKHroyyXrGCTFkIgg6RbnvFZJOMOd7hDzTB+6lOfKqeccspKWbKthjixkr15ylOeUgXD9o95zGPqGC7lIQO6UN7znvessz22/QnVb3/72/KGN7yh3Pe+96376dK67bbbVlmA45v8RDlIRGv4EwIioAskOdtkk03q/ne6053Kk5/85Hoe4tUVAOP+vOf4Jhx58YtfXPe57W1vW572tKeVn/70p+WKK64YJmZrEkHnMv5xhx12qLNf2u4BD3hA+dznPlfftz8xfuc731mzd3e+853LZpttVrbaaqt6D0ym01A306ZNq905Zfnuete7lrvc5S71mN/61reqIDo3cded8uY3v3kdG6jco8G+g0TQ8czO6lgyxerljne8Y7nHPe5RtttuuyrrrS7Jmn/rgqpc6tw1eUa+973vVaF0HtdFYk1c047pul/72tdWybWNUH+ejV/84hdV5NxT3VxvdrObletf//r1p/v7rGc9qz5LJNHkQZ4v4gnyRuKMM3RflVv5iaTu0N0ZSdWVrr7u2Re+8IX6RYj7cu9737uOm+3KZQijISIYQlhvLO99UJ41Y0E59OyMCUwkrksYM3jmZfPr79JkZ21EUCZN1kVjXjZHo1ymRhdD8mFmxte85jV13NmTnvSkOkZOo9o50ISASN7udrer27zuda+rx9QYJxAEyDGNRSRcu+yyS5UdccABB9Suf8aBOYb9ZLiEDBeMASMsJmqR4WkTzchOfvazny33u9/9arlIlf1JpdlJb3rTm9axZbKJLcNjXB+BsA+p0M3zzW9+cx0zeetb37psscUWNSvULwJrEkFCZBIeQZ5f/vKXl/e9731VNp2bPKsbUvL85z+/1udLXvKSWpZnPvOZVZZs12SGjKl/ku56iJP7qa5MtkOGzK5JZsjVr3/962EZ3ZFQ7kEi6Cf5dI3tXmy99dZVRm94wxuWj3zkIzVzZn/bqltSR6zVoW3VjZ/uDVE3JtT+pPilL31pPeYrX/nK+myQWs+A43lePT9tVljZYVLt+TM5z2Mf+9gq767Xvfnd735XBdmz1J5FX1543tWT++s5VM/u7S1vecvynve8p2YB1bG6azPRKr8vMN74xjfW51WW0HMdwtoQEQwhrDdMh3/ktNkDG7iJRGJ0YYkJXUQnO2sjgjJmGr8EiGRpAOtWZ7ZRszjqftcEo0njE5/4xLLvvvvW1zSozT56gxvcoGZ9dHVsWSNZFxJGbIigbJvGvrIRAK8RCPIk+9i6NeoqqWuospEE4kcKCIHugf7tHGSV8MnkmUimNd4dWzdW2R7ns12biXKfffapmScSYRkL53QOXQ2VhaiSupZlaqxJBMkZyZNVcr7W7dE2pM2kPaTpG9/4xsrlEZTJtkRJptAx1TU5Vm6vydISKuH+kEj3RRdJS0Uos8yXbqOt3teEMg0SQXUha+b+t3vuvCRWtpaUqQOvEVx1QXjb+FKvu9cyfTKChM2XBp5DY/zadasb12Uynn4R/MlPfrJyVlhlkPHzfJDGLl0RdE7HkC02+RHhljEE6fMcEkIyqm5lKZXF0hMkWuZRVjaEdSEiGEJYb5x2ybxySJaISCTWKXSrthD9ZGdtRJCoEUHdOIkGCSMruvqRqyY00KiWsZGl2nHHHauQEUVZFY1pQqFh39A4t4+fg0SQIBn7JRsjo9WycG0/gUEiqEuofTX6Zfm654V/a9wrm+vTdRUtIyjjKDvkXPDTOodkTjaubd8YTUaQBMpaqdP2HjklV7pFEmaZsu45Ca9lHdSpzCppIXcyYP5NrhrEiFB5jzirA1lQXWkdp7vt6nDekcYIduu94RrUl+uTuVT/uk8+97nPrWV1vQ3HaHKnziwA74sDkxZ15bh7Hv8eJIKeRaLnOSaEXfpF0D62UaZu12M4vnGMssYyigSa+LeMoG7HnsUQ1oWIYAhhvTBv0fJkAxOJMYqjzp1dFi+d3BNBrI0IysroMqebpEyOfXUL/L//+7+awZJF0VgWZMY4LV00CQBp0X1TF0ZjrFoDfhCDRFADnVCQOZk9s5gaoyYz1ZWRQSJowhfZKN0MlXsQMoq6/b3qVa9ambX64Q9/WDNMrrk/gyaL+MhHPrJmNklYlzWJoGsiw7pWdpc5IBjkU7dT2SeZtVafgnh6T7ZN11wCSriM+yN43W0FQZfR/OIXv1jHCpJc49rI5mi7Myr3IBEEGW8Ttey0005VNImne+5Z0G1W/ZMvkqeMMq8ymyS3TfAD53E/ZTBJGEl2/9RdV1ptN0gEPQejFUFST6gdw/PoWe3Wm3r2TKsvzwURVH7ZWOMRu/cyhOtCRDCEsF64dNbijA1MJMYoLDh/8czJPRHE2oigBr/uhsagaciTCw17IkKKHv/4x9fGcgv/tl4dEdGYNjEIOTRBR1cC+hkkgtBNTxnIF8mSmSSijteEapAIGndozJkuf7Jhg9D1UldBx2uySARdF+nsZo1A9HSRVRbdIbusSQQJifr50pe+tIoQu+5vf/vbtT6NWyNV3fpsdarL7fHHH1+7ZRJwMqMsg7YlwLq46t5o8h4i6d8t47YmRhJB4/8IFzH37DiX8vpp0hVC1URQ3ZlJ9Jvf/GYd72cb987xupMJEWGzccqGynq6JvUno+mLBDQRvP3tb3+dRdAXGrrfKqcvFQbVmzChUcsIEkHXqu4igmFdiQiGEMYcH01nXbagHJKZQhOJMQldrM/s/U5NZkYrgmTGjIkyWcRONogImkBFpsc4MAKlW103SIRuiWZ31IjWINdQX133upFEEN4zZswMmMRGdlIWad99960N9EEiqByygUJ2cBDEhtSSXBOAoIkgMRlJBHU5XduM4EgiqE5Ij4yaa7OUQX99CjLblj2Q4XMPXOugbW3jHjs2IbrRjW5UJ5Qx02j/NQ1ikAjqdqq+TawiPA+66xI/4xGNwzShj6VB1D+ci4zKttqXlKs/k8XIKrayODZptK/jOr7jEWTbjIUIyvIReLKna/OgelNOX3Yof0QwjDURwRDCmOOzyRpoB50REUwkxiL8Lh3X+52azDQRNJX/XnvtNfTqqhAZUmTGyle/+tUrRcnrxk7pxmi5h9VlmWQAZXt0GSRH/WMEu6xOBBuORzBNDEJOiYVG+yAR1KDXFVDDf88991wpJ12M+ZPFMp6RiGBDiyC5NgOmCV1M/kKQRkJ3WNcvQ/uoRz2qrnHY3321i/LbnlhZWsOyFSa5WZPUDBJBQvfhD3+4Zns9M74QIHDqVd2RMXXZFcGGcsveEsa3vvWtNStHePu3Uxfk3D0wLpTs2m8sRFD5jf8zHlHWb03jJW0fEQxjSUQwhDDm+GwyPjALyCcSYxeHn3PtGK7JSBNBSw3IjpCwFkTEun0awbIyuvURptatU+Pd7IskwTIIttXQtq+ulsaiafB7jQAQMsJG8CwvQDBsZ3tZGoKp8e/f/SLoXMblyToSDwLguLqbkjXLVthG9IugsB1hlD00G6SF3B1Dl0OTmugWarydZSuaXGxoEVRHrknmkjC7dt0YCZDtiJzr152SvOgqa7ITMmVMnS6wrkf9kTz1KQPa1h1UD5Zo0IVT10wy59rVueML+7kvuuC6/4NEsGXI1GebNbVlKG2j+6lxfq1rKDk0ptEssbZTHnXmWZD9VAYZS5k4z4vz2sZ53A/nka0le2Mhgu6PDKZnwXPtHK7BsZxXxrUtWO++RwTDWBMRDCGMOT6bjGka1JhNJBLXLYy5ncwQIpO3aOiTFl0SW5gkRQPchCnkhJy1pRVAXIgHUTJWTcP6gx/8YN1Xw9lYNrMstnF5BFKWSGaO6Lz+9a+vQmh7GTvr/JGOJoKWZyBLGuMa6SZX0ZXQ+Uznb7kBAqcxLzPVhNGYPmvBtXUEoTupMhFegtaOQYZkyQifchCd1tA3e6UujuqhXwRJlyyqrBbB6UI0CA3ZUyf9IqjLI4l0PU3SGs5jBlPvO76xbJauUFaTl6gnMm6yFzLouqwxqIuoiW5cj+sgZPYlakQGytC6dSqXrJ1rt4ah4wv7WSNP10lZRvsQSs+He6tLqWOY+EemzjW6DzLDpNgzJKNpjGObNZQQqkPHtZ1nwnWQUc+VyVs8GwTO/XWdyiI7a8ymzJ3rUK/K45nwJYHr6IqgenVNsnxd1CcR9Jz74gOkWv27v47vGtSb83puXav7qp7Vny8XCLQvB7r3MoTrQkQwhDDm+GyyEPagxmwikbhuMdlFUNbFuDSNXI3ibpidklxoMBOf1ujuQlxkrcgKwZKRsS+JIZHWzJNZaWjMG08og0UiiJnQ7VQDXgOdaJA4GRiZPMKpq6BGOPFzbOfQJdJSBXvsscfKc9hO5kcWSQbRv6HxLpNGFhyjW07LCOgqKWPZ7Saou6tJZoyN7BdBk7WYiIU8yGZ2IQ/et0j52972tpWZrIZrMsOmbqqDxkp6zbU6vmtUTnXkmgiUe6FO4KesrK6kBNt2tie2JErWrP8cLTNI+tR7q8+2n+6mxEvmVbll79xLmTjZQ69ZBsI2xmi2/dSVJUMIoewxGVb/1pQkqMpvW3Xv3pJDS4qoc0FwldkXD+2Y7pVnqHWTdW71Z4ZXS5a0ayPUvgzQhdTMq13Uj+sk9p53uJ++oPBse6/VmyCGniHnVL++/PBsyjqTyu69DOG6EBEMIYw5PpsigonE2MZkF0GZGA193eN0e+uG7nOyIoNkpR9d/mSAiIB9zeRpOYCuBHYhfBryREVmxzgx2UkNb1LgfWVSttbwllkjDrKFzkEoCRdxbGjgy2TZxvH6BY6U6V6pfK2chMUx+hv4ujQSJhmx/vcIhfd0vXTMLq7B++rDdZGh7v6uiegS09Ut4+A6yJFyqiNdHMkLme5HJtPyGrazvf10tewvWxf3RhdIXWXtI37xi1/Ubq3uZ0O9q+uWrW3IlBnTaD/1bYIV9U1U3VvZN/9WXuKofLb1jLh+M3J2UVb3oj2L7Rr6v4Boz6trbvXnHF43iZAvJro4j/J7v3VrbpA8ddDqTehSrG7alwLE2XPmdRnR/mchhLUlIhhCGHN8NkUEE4mxjckugiGEEDYsEcEQwpgTEUwkxj4igiGEEMaSiGAIYcyJCCYSYx8RwRBCCGNJRDCEMOZEBBOJsY+IYAghhLEkIhhCGHMigonE2EdEMIQQwlgSEQwhjDkRwURi7CMiGEIIYSyJCIYQxpyIYCIx9hERDCGEMJZEBEMIY05EMJEY+4gIhhBCGEsigiGEMScimEiMfUQExx8WBreYu58Wbw8TBwvtW2Tf4uyrW0x/Q2BheIve//Of/6yL+/djMXzP2K9//eu6IP1JJ51Utz/00EPLkUcemWcvXGcigiGEMScimEiMfUx2EZw/f345+eSTyz777FO++93vlj333HNlHHDAAbXxO2/evKGtxwd77bVXefnLX16+973vlaVLlw69unGZPXt2lYMf/vCHq9Rhi/3226+cd955G11+1haypI79HAnveUY8Rz//+c/r/XHNfv72t78tp59+en3OcMIJJ5S3ve1t5XWve12ZM2dOfW1jsXjx4vLnP/+5vPjFLy6f/exnV3nOvXfIIYeU9773veUJT3hCefazn12+/e1vl7/97W/ljW98Y3nnO99ZpTaE60JEMIQw5kQEE4mxj8kugpdddllt4N7//vcvd7/73ct97nOflfGwhz2svOUtbym/+93vaqN3dTKwIfnc5z5XHvzgB5fPf/7z40aszj333PLxj3+81ttmm222Sj2Kpz71qeUf//jHuJPq1bFo0aJy7LHH1oxZE7l+vH7qqaeWH/3oR1XwHvvYx5b73e9+9Zr9dN2f/OQny4knnli3P+yww8rTn/708uhHP7rMnDmzvraxWLBgQfnZz35WHvGIR5TtttuuzJp17e86aX/Tm95UHv7wh5ctt9yyfPCDH6xZQb8Lz3jGM6o8XnjhhUNbh7B2RARDCGNORDCRGPuY7CKoq9suu+xS7na3u9UG+vve976V8cIXvrBssskm5TnPeU75/e9/P6IMbGi+9KUv1cb7zjvvPG5E8Kyzzirvec97qvy88pWvXKUexWc+85maMSNXEwHdHi+99NLy5je/uV6P56QfInXEEUfU637oQx9aHvnIR9ZMn3+7Zj9f/epX19f++Mc/1n0OP/zw+jzJsnXFa2PgGmUr99hjj5rp637RoevqQx7ykPKGN7yhHHfccXVb759//vk16ytbPtGyu2H8EBEMIYw5EcFEYuxjKojgrrvuWsXqBz/4wdCrK5DpkeW5xz3uUXbcccdywQUXDL2zcRmPInj22WdX+ZEpOuqoo4ZenbgYH3fGGWeUe9/73uV5z3teufjii4feWYHuoscff3x5xzveUTbddNMqTLJlMszdsXMXXXRRlb9p06bVf48nEVwduvI+4AEPqL8b6QIaxpqIYAhhzIkIJhJjH1NFBHUDNUawH5nARz3qUWXbbbetYtAgCnPnzq0Nf13kBFnQuO+O22vjx2xne+/NmDGjCoLtr7766oEyZz+vX3XVVXVbx7efLNQXvvCFmn0aJIKO75iObR/7asjbr39yj1YWXRS7+9nnyiuvrOPE+sshrrjiipXvNZoIyqLq/rgm7Ltw4cJ6rHZ9zu08ztc9NtShbY1FbPvZXtnbtq7PuDt17Zirq992Ta2eZP/s6xjqwvFl+4jgM5/5zPKf//ynbtfur+fGfXjgAx9Yuw+P9kuCkUSwv+ytTLbxrPWj/lsd2NZ+6qhbb/7ba47T7p1rtm/Dc2Ff57G99xzzy1/+crnvfe9bxw6aMMYzpN5ldP234/TfI/XifqibVn73q7/87Tpl2IXj2ce2/ccMk5OIYAhhzPH5ERFMJMY2prIIapSa/EP27V3velft/giNdvuZrMV4qXve8541jKciB+ecc85K6dLQdQzdTk0ectppp5WXvvSltZFNIoy9MiFNt7HsvBroMk4m5SAjjv/85z+/Zp0++tGPVjntF0GN+DPPPLPssMMOtVuffYxVe9WrXlXHdxGH1tBWPt38XvSiF9Xj2U/W07Xa561vfWuVFuXXNdCkIcZRek93R2JEDBprI4LKQDzMfCqTpi6U9UEPelDtTmk8nWO3svq5//77l9e+9rW1G69ui695zWvqOEnjJZuUktpvfetb5VnPelYtp/vx/ve/v9Zjux8gLM7x7ne/u9535zaW7zvf+U6tI9eiHLoL3+AGNyg3uclN6vhR233xi1+s0mfmT/ejSeJoGUkEiZWym5RF91rnevzjH1+vz3i9VhdwvY5j0hZ1ZkymY/74xz9eOQGN63VMYxcdxzN0r3vdqz7Hxj2qA2GyGOdUr/7tWnzJcPvb375e+21ve9ua8fQMGeNpQqBXvOIVte66z57zyaAbD2mcpPK7Rs8oAe2W3++ZetO91H018ZF9vvnNb9YyhMlPRDCEMOb4nIkIJhJjG1Opa6ixT11kNGR7jP/6+te/XrNLIBzGVREYwkFIxJ3vfOdyq1vdqgoVGYQsIGG8wx3uUBvrj3nMY1Zuf8c73rFKhgY9WWnIjJBHjemb3exmtYue7YmYSUYIpHMrd2uMEzaTmmyxxRblFre4RS0XSdh8883L7W53u3KnO92pfOhDH6ryBw13wuPanvjEJ5anPe1pVYqchwDd/OY3r+X99Kc/XYXHtTkemSAIruOggw6qwoomgi95yUvKMcccU18bCfVHPFwP4WjXR4Bcr3GZxLVJDYkwoY9rIxckwr7kQR0QIxlMk5vYV1deZSU/d7nLXWr5ySyIhuUQnNN1+tlkyqQoMsDu3fbbb1/Lc8Mb3rDWRSvj7rvvXr8QII3qjPiszQQ4I4mgiX/Ie7tvwrUoI5l1fXC/PXueDXLq/trWNat7M326t67B2ET12Y7ZRF6Wb/r06fVYP/3pT+uxPBvqxqymT3rSk8pd73rXeu3uu3N4Rt1vzxhR1AW4K4IEkoDbXr23MpFIX2aQ0gaZ9p5jeI7Usy9KCGJEcGoQEQwhjDkRwRVx6iXzypyFy8q8RcvK9KsXDdxmMschvWfgvCsXllkLlpYFS5aXhb2YvWBZOWH63IHbJ1YfU0UEiZUsmLFR4vvf/37NfBAlUmCik5ZV0oC3lpoMjPXXdKUTsisatsRK5k6jlggae6hBrpEsW0fGbG8b2RoN9L333rt2u5MZ/Pe//12e8pSnVOkyHlCj3fYyLp/4xCdqWQkOmdIYtw+RlNUiDqRC5rF1MXQ95InMeK91gSRvBPjGN75x2Wqrreo1OZfza/g7ByF47nOfW8XUsdSDbQnvV77yldrFD00EXT95bPXYwjFb19hvfOMbNVun8U++ZbxcH8FyvQSHlB588MG1ToggSVBPJOMjH/lI7a7o3rkXrl8m1vXJeLkOx1MnyiJ7Ss7JMpk3+YttCbpMqG2NazShi+OqU+J19NFH17omOI5lO5Lumgmg+pblXRt5GUkE//CHP9Q6Vr/um3MRVudWfhLmOtWVLLT7RuKMPbStLKxjqEPdM3/yk5/UenSv2jE9d7Yhi8TMdZo1lBx6xkGqfQHi3rr29izJlMrSEkFfRshq2xbqZOutt673zKRAra7cP12qlbU7/laW0BcT7oH7TT6d03PZzRyGyUtEMIQw5kwmETxi2qxy+qXzy8UzF5cr5y2pUjN74Yrw31fMXVIumbWonHnZ/GHXfEbvtcVLrynLll9TLpu9eJX3JmocdMbM3rWsqIfze5J3xDmzBm7n9Rm9a1687JrSu/yVLOrVx8kXzavbnHTh3Fqn6vDUi1e8lhg5poII7rbbbjU717rBCUJFnmR+yEJ3tksNclmgNrauQTQ0fDXciaR/kx//7dgazzI1rbHrPd0yZUQsvaDxLFv21a9+tTbCP/axj9XxU2175yICb3/72+s+rWsoaSE1Gv5E0zUpY0NDnXA9+clPrmJAXrsZQdkZmbDWENfgJxIycBr3jt2ycxr/unSqH/KgPGgiKPOpkd/qsQVRsC1x0R1UWchB/xg/QuC6ZcO+9rWv1frwvnX5ZJl0PSVTrd69R3AIk3N4r3VZVTe6QbonxNq5iSARI3Eyl+04rsv9aPuqP1JFPmUK3ZuG+v3ABz5Qj0HGWvlHw0gi6B6J7vNEygmnDPC+++5by+T5kQmV/Wv3EZ5P98hPkueZU4eekZaxVE7/LRyrK4Kemy7uOVHzLLZsJHSJ7RdBwidL6EsK0ui4UJe+HHFvdOtt16ZMvhTxZYP707YPU4eIYAhhzPFZPNFFUPnPnrGgXNmTlPmLl5clQ0LT38xY3rvYpb33FvS2IYzd655sInjwmTPLKT2Jc72Yu2hZOfb8OcO2O7wngTKBS3vXbcuZ85dWUSZ+x18wtwrNUefO7sn1olqnjke0+4+TWDWmigjKNGmsGqclSyFjRTyMseo2hBsa1bpFkoXf/OY3tcugxrT9NNxlioiF0KiWWZOd6goaZAKJo7GC1uIznb/xeBrWpvTvx/7GjTlPE0GC49yk5S9/+ctKOegi66LhLZtkrFcTQd0RdZ0kU939nFsmkHh1u63aRobJubrjJpsIKrdr0ZWzG7JtxIdIGJOmG2f3uA3Hl3XSXdFxXJu6JuSyjQSMHDfUh0XbZU9Jq/GLxK+Ff5NdUiPDR5RkD3V3fP3rX19FSKaz/774t8mBBomg85NVQmf/tWEkEYTnyf13PZ4n4xRbxthzqB7Iotddk+sj8DKU3XvnGo3lI4zC9RoTSQC70ro6EfT8qjPlkA1sDBJBzyPp1KWUjHfrn7D6osF1tC8T/H75coBkEv0w9YgIhhDGHJ9vE1UED+zFkdNmlelXL6wCWEWl93+6eF46a3E55/IF5bRL5tUgegTvqnlLa7fHc69YUA7rNdbbsSabCMoGnjB9Tr1WTZire4J3zHmzh21HDmVMPQfq8Pjpc+vzoG7bNmTxgqtWyOLipctrdrF7jMTwmCpdQzWs2zhAWSnd8nTxlEHrZqBA7nSvIz4muiBMJIVUGfMmW0f+NLxtq1Gt4atR3Y+MEikgOKRHg10XVcLWnaW0i4Y04WoiSB4sY0AodR/tNvYbugZq7BvXRh6IA4lz3Rrv3cY+/v73v1dBNEkIyWvYT/dJIuWc/SJImkikeuyGurAvydElVDZx0Eybyq4e1KXjy8Y2EVRPsk5NKEDY1K8spCwTqSFOLdwLUikDKQPoWMpqnJxykDLXryukum/ZqdWJoOshV8THlwhNiEbDIBFUL+qsPU9E+XGPe1z9QkGm2jXohty2Vc4Pf/jD9V5a2sI4VrItG6fcrtGxPcOO5zl+2cteVsVa19CWZR4rEXSfPPe+7FDWbv3LbPuS5QUveEHN/sLz6560jHCYekQEQwhjTu9zbUKKIFEhcrp6rsgAXlOzWdN6gqc749E96THurW1/0BlXVxGS5TIe8D89AZI1a+9Pxq6h7utpl8yv2VIZvkH3+bgL5tQxgepP108C2b+Nujv2vDm1jmRS/Xf/NolVY6qIoAybxm9DtzZT6BMl3TbJg0a4xnPrbmgfokgqbGscm0a+fRyrK4KyIoNEUEOcFLTslyzZNttss1oRJCHdjCBZMouj1+yvkd8PETROTndX2TnX0kRQdq47mQdWJ4K6JJIEk4D0i6BZSMnOSBARAmbbNqFOF2X3uuzedtttV4/vNSJIaAaJIOk2npHQkXmC0ULGVbfYX/3qVyvHM4KEyuQaA0eSiCIZbzOArk4EPRv77LNPLY8ssvs2WvpF0LWpT+NRCZbZUHUHlS0jmW08YxNB2Mc9/8UvflG3lbVVft2MfSnQUE5jM3UT9bzahpDp9ivLPZYiSOzcU114++tf2T1zrXt1RDBEBEMIY462z0QUQWU+94qFVWBcw1U9ifnvxfOu87VMRhEcTRzfE0FZQ9c9Y87iKn2DtkusXUwVESQe/ctHEBJio9Gu4a9bnsa7hm3rzqkLnu5t5IRM6W44SARHygj2i6CsDqmTEZPR6cc5NLg10psI6tpIRjWuNby72csGASEZuuqZGKUrgrqM9nd/va4iuKblI3TPJCNE4sADD6zH60LACA450eXQta1OBO1vzKLMk0lk2pjF0eLekRtZNVk490j9CSIow6UeuiKoPL4MIJ6eG/I5WqHpF0HlNwuorNmnPvWp+r5zeXac32yeZLQrgl0Ivgyv+6T7pfuvfF3Uqe68JukxcYv7bRbVsRJBYqeMfn8GdaPupyuCsqth6hERDCGMOT77JpoIHjI0/s3kJjAZioyX1wdtP5qICEYExzKmsghqQBMQjWdZOtk2DV3d8CwloDuhCWE05gmaddGIlYb1aLuG9ougbnvEwnIGJlWR4dFgdw5dVs3AqUuiyVrarKHOocslaWki1iYekYUhtDJHuq/qktnGlG0MEXR9lkMg0ro3OpbMlbp2HTJyMnSO75q8tjoR9J7xjbpIep8wqSfX7pjqgRwSSmVXH2Tbv53Xa44nA6dMxIvgeN390D3T/VFOrzfJIjDO5V6oV8+OcZjuufO2c3m+dNdVJgwSQRLvfhsH6DnyGkH1b9t5nnwR4dzqw/Ec1/Hbtu6vjLAMp9fJpOt0ba085FU2VoaT0I2VCMo6+v2R1STlpNj53APX4xnpHiMiGCKCIYQxZyKK4FHnzq5j/UBgzGLZ7eZ5XWI0Injo2TPL0b1zH3f+nLqswok9+WxhbJ3upmuqywN7oqVLq22Pnz5n5f6O5zXj8QZ1z9QV1rF1yyRvJw6d3366dx45bfawOnCutp1jN1F2fvt43TjKNlvozJ5QnzxUHqGe2zHNLNrOd1TvXN3z9AcJquUcOnfbTxmcW7lG2k/XXduqZ68d0TtXLevQsXTvXRfh31AxFURQZo3Yycz0YwIXEkgSyILtdSk0CYeZKDXQCQtRkW0zTkp2SkNao11D3H+b0VNXv36MEXQsckTYNKCNZZOJNOZNtzsZMuewv3OQyjYjpMY8QSA7Mon2sQ0RlVEkpxr5sl1eN4uj7Z1HA92EOISzXwTbZDEa9/0iKLvkGs1eqosiHMskJuTR8g0j4dy6h5IRdU4mdVVUVtksWTaSYMIcgmd7YRIf16CbblcEoUzeN27RTJ7qxTqEjqkedKX1b2JCSGTeyJfFzNWr8+riS+p0bSROIK2yl+p0p512qtJECMkeyCRpN16ThFu3TzdUz4fjqnuyTnhNgAOSrHuue06YXJvj2t899wza14yphJjwe48UKpfnUebTNTp+e/YIuDGDXnNcWUJ12J4D26k7X2p4TlyHZ8f26psAdyG27rEuz12Jcx2OYQ3AJoKec+cilMrsixLnU+cWiVd3MucN6wh6hpUtIjg1iQiGEMac3ufphBJBGSvZP8Km7Na6G4vyj0YEjY9rM5OaOKWhHMYpzlm4tG4zkuyQKiIzrSdfcxcuq/s0nNcahmbwJGDd/Vwz0VNG12vCFuds+5kF9aKZi6podfdzPl1nYfxkmyzGuMF5i1ed7W8Qjum89jHOEOrIxDHd87RwzUT2rN62pLL/+lyz7ryub1Dm8eQL59UlK9TtJbMW12NddPWiFRPeDB3KvRk06c14i8kugjI1Mnsa5jIU/Wh8EzGNd8JE0kiP8VgavpaFELrGGWNGnIzFIhctI6gRT3pIQz8ay7I0ZEEjH7JYGvIyXCSknYMQOAfhkIWSrdGYh58yQKSVEFnGwT4Wk9ft0D5mFG2Nd/JEPB3nbW97W83cdNHgl/Vxzd0xcPaTkSIDMoBNEh2LcLr+I444or42EmbHJD9kwmyfyqisJkZRT7KWJLAJGVki08ZNEjh12g9ZkS2UFTNxTLt2wmz5AvdQ2YmHsYfd89pGloswkruGbK+sn3trMhTbyrx1u586L5kitt1jCvuQKRlXWTMQIsLnWmQEYZkIQuz+2kfI1L75zW+u4zcJcFs+wthG3Y+NOXUuQRZlDn1R4T46rufKNbVyt7KYRVWXXFLsmbGdbshmQe3i+XWPdSftfklgPUPC7Djt2YMJbDxH7RqcU9l8AWKcY3eGWHLpGVW3GSM4NYkIhhDGHA3siSSCxISggFwQlHXNBorRiKBxiHUJit42ZIygiEW9/7YfV/FTlq2/TkmSTJdM5vLeNqJ7DP/tuP771J6odfcjTpfPWVKP7fyLlw3fj0CRyO45RxJB3Wrb7Kn9UtmOKxzP2oz2WZMIrpjBdXatuxXlXCHH7Vj+u53HtbimflnuiqBtLuxJoGPJWLbjWOy/X5THY0x2ESRdGsakaiSB0fWuZXesmQYCIJNFzIRGrQySbJfGswY/6dJNz38b79b27aI7oayJxeW7DW7dFr0n86PbaDuHbp3Kq9Gvm16TJRAm8iIjpWFvPxPEyFI1yWzY1jVolLs2+3WR6ZOxkfHsZoTsR57VBYlo7/npvGS6/1yDcBwSaXvip6xETj24du83/LculeqJzKrTQZAbGU+LmqsvxyR3spsti+dYxt65R+rGdjJUBMez0I/6dZ9l4Wzbf59AMN0XsuaY7X75skDdu852n0ikbJvzE+IGgbOt8/hSgCjbzzMjE+wLCCg/qXLfiLdz+W/jPtUbmvD6MsGxbOOnLyS6oqtMnjHX77nt4pn17Dl/qzv4EkS2Ulm7zx48/7Ksnj3Xr3ye0zZba8Nz674YL9qtgzB1iAiGEMYcnzMTSQRlvYwJBEEYtM11idGI4EUzF5fL5y6pS08c28lK6TZpnT0CA7Nw9suKro6yXFghOotXWdePJJ7fEywLtp9y0dyVrx/WO7YsGtwrQqeLZJNfxzBTquOd05O1tp8YSQS7MdoxgmsSQdk7ZVjWO59jOV9bq9Hsrf579kJTtNfD1GuSOe0eo4mg/Wf19ldPjiMDPBayvyFjsotgCCGEDUtEMIQw5kw0ESRMxKXnCFUsBm1zXWI0IkiSjN+TyequsydIDdEb8pwqTmSgvW9GU10/iRnZq8fp7O+/u8dvrxNf3UHdp/mLl9Xxcd337eff/fuJDSmCRFjWz/ku7dUfQV2lnL3/JoQWtlcksmzsX/cYTQS97zzWg/Rs9l/XRIiIYAghhLEkIhhCGIiGvO6IsjLXJfqlZjyHSUNWimBPkAZtc11iNCK4uiBQpG3Iu+qC9m18nZARa9muGbOXrLLv6sLkKa7XcYnRSKI2KDaUCFqzsQqe8wzN4DroOIROPbRuomfNWJExbO83EWzncZyJKIEiIhhCCGEsiQiGEIaxYPGyct4VC4dlVyZrjFcRFBr/TQSNwWszXwozm5rUxftkabTj3MxQapIVhzUWsT/TtrrYUCKoa6qxhs51Vq8eVzerJ/FzPlwyc/EqddTNCBLLidYdtBsRwRBCCGNJRDCEMAzdDGX1JsKU+mMR40EETaAiW3XGpfOrhE+/alG5uCc17kWDeMmUtX1WjBFcVGVp6bJritlHZXEJw+oysu6tSVvsJxy3zSy6puzghhJB+9mfqJ7cGd84Uszq3Tf37+pembpfYDQRNJGOiXm6+0y0iAiGEEIYSyKCIYRhdKXFGCzj1NYuFk2ozEsbI9iyRoO2uS4xGhG0Lp5JTsxoOXvB0jpmj/zo6mi/Ieeq9IugOiaxZNG2BM11mGDlvCsWVLEcNFaTmBE496qdwzmv7InSBT0hNPaQLPbvJzaUCNaZUHvncQxyN/wZWzUcQ6lmL1xa72c7ThNBk8Rc16zseImIYAghhLEkIhhCGEa3wWzCkrWFJ0ykyWJkkEwSA0IkOzcWYxzXJIKESxavbUP+dPU0bk/2yj5mE20u2C+CgqiTSUsgkFjHgW6VZkI9vyd2ZK1fxkwEY4F2EuraiZI9lcHahZZZIFT9Qr+hRJD8DV3KWrE6ESSM3XNMtIgIhhBCGEsigiGEYUw1EdQlkiSAvBCUschork4EySaRIlXk0/u6deoaShBPuGBulSyiN+RdA0VQGN9nuYlTL5lXu3iSyJoh6+1H7FzbIGEju+7TKb3zWZBeZpHAETDi5N9dqRIbOiO4pPeeY3h/NHFmr86PHFqnUEQEQwghhMFEBEMIw5hqIqisZp4kLsouQ2atvXXNCq5OBE2A0iZDsZj9SBO9jEYEu0EKiZhMn8yi45OgQQvS94dJY8iZbJx9lNs4RQLSttlQItgWu5+/eHld47D73tpERDCsDyx4/t///rcuCt4WVbcg9/Tp0+vi40uWXDu2N6w/1LN7MG3atDJ37tyhV8c3l156aTnppJPKjBkzhi0EvzHwzJ5zzjkD68+zrW6PPfbYcvLJJxcL2luw/8wzzywzZ/psGvpwChOWiGAIYRhTTQSFiVfaovJER2ZuXSfLWZ0IWgqCnFjA3li+7nstCFR3+YjRiGA3ZAiNOYTzD5K2QUG8nAt+Erv23oYSQZlNdSOjedol83vHuG73IiI4cdHInDVrVjn99NPLUUcdVQ4//PAaRx99dDnttNPKlVdeudEaoieccEL5wAc+UH70ox/VBj00pr/2ta+VT3ziE1VOxgPz58+vjfYjjjhiZf0J9am8GvYTGVLy3e9+t3zuc58r//nPfzaqmKjLc889t5aj1fcxxxwzrJ5/85vflG233bb85S9/qV8ebEyItGf2Yx/7WC1r+wJDPc6ZM6ccdthh5cMf/nB5+ctfXsvsd/EnP/lJ+dCHPlT++c9/bvTyh3UnIhhCGMZUFEHllaUjDCAgBE1G7rpmBkcjgiRlJBE0YcuMOSPPGrqmIG2yeyBkMn6DtusP0mWMIMixCWnaextKBI3btFg+Lp61qF73dVn/LyI48dAIJTBkT6PzLW95S3nCE55QHvGIR9R40pOeVN74xjfW95qEbWg04h/84AfXBrGMiTJr+L/oRS8qj3zkI2sDekNBMs4777yaoelmmJSJBH7wgx8sD3vYw1bWn3j84x9ftttuu/Kvf/2rZjfHM66J9A/KtJ566qn1WXA9++2330bJsMmayar94Q9/KNtvv3151rOeVR71qEfV5+CpT31qef/731+lafnyFUvc7LzzzuWud71r+da3vrXR657seWYf+MAHln322af+G4sXL67P81ZbbVXudre7lYc//OHl2c9+dv0CwXNzz3ves+y2224b7fcvjB0RwRDCMKaiCArXemnv2pvokEFdRo2/cz39QkNMZA29Z1//3ZWV1Ymg7qDkxHvO4RhtXyLmeCdfNK+WodEvgqSMHNjXf7fX23vG97VF2S/W/XRoMXrXYb9Dz5o1cD9ZyLZshfGGXYH0/oYQQdG6h6qn83vvW0zf+btirq7U+4rrGX6PIoITC/IiC/i3v/2tPPOZzyx3vOMdywMe8IDaqNa4FkTm/ve/f22Y/vSnPx3ac8Py97//vTzkIQ8pO+ywQ5UwEJVdd921yuEll1xSX9sQHHrooeV1r3tdzTS1hjyIx/HHH19e/OIXl5vf/OZVBlsdavjf9ra3Lc94xjPK73//+9rwH69cddVVZc899yzPf/7za7fKJlSQEfzGN75RPv7xj2+UjCAxlaF+5zvfWZ/Je9zjHvW5aPWszn1hQKgWLlxYy7f77rvX7b7zne+scr82BuqS0BFY3T+bSF9++eXlq1/9ai3ne97znlrPyrp06dIqjO9617uq3LZu0WHiEhEMIQxjqoqgIBUz568QEJAei6/LkMlSERHh+kiQsXeX9WRnTk+4zPCpvtqxVieCum2SJZgsxvEJ16Fnz6wZQplAEti2Qb8IntY7xqzea45tcflWNqFrKwlU/sXLltdt2351ltQFS4sZS02u4prbfqf05PPqoeuvAnblqoJmmw0lgtYPbCLrWLKTsrYypcrhHujGev4VC2tZnKu/PBHBiYUMya9+9avaqL7Xve5Vu6WdeOKJqzQ4dUc77rjjys9+9rMNmnnrMkgENxZf+tKXymabbVZ++MMfDhTB17zmNeWVr3zlKmO6Lr744lq39773vWtDX1ZzPKK8Z511VpVAZe0XwY2Jsp1yyik1Q73JJpuU1772teUXv/hFLWN735caBx98cPn+978/LkVwJM4///zaXXTzzTevX8qEyUtEMIQwjKksgkKWadrlC6vMuBZNJz8JECFp4d+9Hyu3IU2jFUHZP68RlFZn7fjtuAt653eMFVsMF0Gy5/gr9l0hS9eWbUWZHJ9kHtWZjOb4C65dN9F2q+63YhkJ/01GZQfbfmJDiqA6IqpVBnundNbeIYeXd+g95bEkRvcYEcGJA9n7xz/+UTOBuqLptui1QQ1/r8lObKzJNsaTCBKL+9znPuWAAw5YZcxWVwRf8YpXlCuuuKL3u+I3xd+ba+pkN8Z+yRiq6/GK+n3pS19avxwYL+KkbnVXff3rX1/ucpe7lC9+8YtVnjyTrY7hvz2jXm9MBBGU3ZbZln03djBMXiKCIYRhTHURFLJkJIfcyUTJ2pGQhv8kF9b9mzV/WbmoJ1uWLRht11ChjizbMHvBinX84IdzWRTe+dV/a1f0i6CukmYHlbHsdiEFEbNAvKUoHKNbLkJBsJxXtrCL8hpX6H1LXPRL3IYUQeF8/+kdj8waM9jqCf7LzKsmxFG/soPuW3f/iODEwUQbshAEy3ivtel2pmGuO5vxcLoKmqDje9/7XnnrW99avvzlL9exchrkbUIX48pe9apX1fjMZz5TJ8EY1D1SY//Xv/51Haeoe9/b3va22iXOeDDd/roi6Ni6033yk58sV199dX2tQdB+97vf1fIRM8f5wQ9+UDNzDeWTPfKe7p7K9JWvfKW84Q1vKNtss00dU9bOpawa6LqEPvShDy03u9nNymMe85jyspe9rOy4447l3//+d5UPk9o4H+HriiBkWl/ykpfUfQ455JChV1egLDJb6tL51RPpUZeyYF2xaTi2MWN77LFHLa99lM+YOOUYJO1mqpTJVL+2f8c73lF++ctf1nrXvXbvvfcuW265Zc246d5KWm1Hoi666KKV5/vCF75QxRbkSjdZ5T3jjDPq6ybwkbEz4YmxpS1r10X5PYOOtfXWW9ftlV3XT1lqGdSDDjqobuvZ9Fzc6U53qhnV9nyNhpFE0DjDvfbaa2VdiI985CNV5mUTu6g3z4rnyXbKussuu9Q665bDpEWykW9+85tX1q+6affPNXvGPLPuq6yx+iHe973vfcutbnWr2n3Yffz2t79dx+763SSJsvHd58Azacym+vPMOZ/jmm20+7usjl375z//+fren/70p1rfupu6p6OtxzA2RARDCMOICK4I8qTxTbjMKkp6ZJxWxIp/y7R5v1+2xKFnzariZjvjDLvvdbexfzu2n85le8cT7b02Rq7tS7CUz9i/tk2LeoyeyBGj/sluHNP9adfVv5/Xvd9/PXXfXoxUnhZea/WlDP3vt1D2es7edrp7DtpGtDGT6rp7nf7bebzeuov272uyn1aW1Z1jIsRkFkGNPw1bUqLhv7azbtpfRoZEkgWN6cc97nG1wU0GjX8yZb9G+/3ud786AYYJO2Rz7nznO1cx1OBuk5EQS/Ly3ve+t25vG9vrgqkroIY3OemKIDEji09+8pNXGSNoMpeddtqpjhm7+93vXo8jlJW0XXDBBXU75/7xj39cNt100zrBiCzegx70oFrWO9zhDrUcZip1PmJgwhqTdtzylrcs17/+9cutb33rej3Pec5zqrhopJO91ignHU0EnYuImmTFNai7hmOb9dL5Hd91Kq+fymZspnFi3eUGHE/9vfrVr64CYdsW9iETxLzto2yyTgTDNbZ6ca3uH9HWJfRTn/pUHSd6oxvdqF5juw/qwf6k4u1vf3t5wQteUA488MB6bMJL4EjMZz/72fLc5z631p391COB9373HjVRN7mL8tpWuBblcc3uHzFVh8YtEhfHJTJrM/vqIBEk/R/96EfreNhWF+rOtZN1Xwy0bK97Rci32GKLVcrqXpE1Muf5leX1XDumenU8xzaZjUlf4Frc5yc+8Yl1e2JN0BzLlws3uMENyu1vf/v6b8+krtuE2LhHGej25QlB/Otf/1rr2rW1Mgn154uGVn7dun1R8OhHP7o+e37axxcZ7nlEcMMSEQwhDCMimEiMv5jMIqghbSkGDXFZwbXJBkLjkSAZ06TBa0yZTNGf//znKkMax7IPJsaQAdF4N/aJcMh+EDyNaFlFGDMni6TLpYYzadIdVGi86rp6k5vcZBUR1LjWaNewbZk+DW0CZHsNbv/tvK5VxoqIKSeREkTjNre5TS3Pm970piqGhE/XQw1lx9l///2rSGm0y07KIBJAUklmjjzyyCo5TWbJGRGzreyWcrpW102Yu5N+KANhlkHUOCc7MnTKLJNEBkiA7rtk0/bKogHv+uwj60YS2j6tvgiy1+1DgFwTUXj3u99dx9apW9f7zW9+s34pYHydjB7pJPW2tZ3jyvJ5ZmRhyc7Tn/70eh0ggmTmf/7nf2p3UvJvP/dcHRFPskxc3B8yY6IZzx5RIpntmskauXduz5bsmnol774Q8GyMlCEdiZFEUPaVoJq8pz1rro3EybK1NStdCzF1D9pz6dpIr/ola54/WTvy6h66D65Xtk/d+GIErt+soZ5Zx/ElgHupzn0pY5yuDLq6VeeuUxbS74VjKY/68MyRevdZhr2V3yQ+bXZd99L51LVn/3a3u10VR9emS7gstntqm7DhiAiGEIYREUwkxl9MZhHUeNUw1qAnKf2NQY11Warf/va3tUueIHm6otm2ZQQ18kmRxqtuZhqpDWJBHMyA2F63H6HR4G1ZJqJCEJ/3vOfVxrbzdKf51w1Qw1Z2qiuCJEvjWaZGQ1y5dEF84QtfWAVRA18ZQAD8m3ARKGXqiiBp8r7zKqtzEGSZGd0FZWAaugSaSIVENbGA/UiwiWJkwhyTEApZIl1KZSo1/Bu6ZJJVmSYCQXK6kmhbGSFjx9QX4VRGUiB7JTtHMlr2xz7qnByQBF0T1YmMGtFV792MmvK7b2RO+dWh/cmK7Fz33qGJoGtq4xztSy6ud73r1bonKQTHfq7H9rKxxMu1KYsuvepI3bZrdm7vkWsZRzOtEi8yRGrUOcl2b7plWhODRFAdym56brpSKRMo66hrZ5Nfvx8yeO5De56cnyiqO3XuGVU2wkhqveZ67K+8reuy1zybvmQgbv4N+7/vfe+r2dP+CZk8+8rUum87lm7LxFi96+LacC73/GlPe1r9nVIOGUEiSK49Q93tw4YnIhhCGEZEMJEYfzEVRFBWRgO9v2GtwSuzYz1B2SEh62cf27aMoAyQBikZad3WumjokhTCZ7zXz3/+8yopGqWWAJCZkT0kksSJKDWp6UJIicHqRFCDXhaGMMh8yIBZ/LyFY2sgWxqD0GokNxH89Kc/XRv1Ddci20PAZPEIW0MWyTlkGftFUEaQCMowanzrJit0bVXXBOfrX//6yllD1Yt14oiiLNCgbnquyX4kVpbVtZIG2bSRFkmX4ZOBVA8yjsqpeyPhJLjuBaFoItLwb2VqIkgiu8/GSCIo6/W///u/dcxdt04ItPqSKVPHnjvZPfXhfrqH/c8N+VRGXzLICKoTokSGjG0jYP3lXh2rmyyGWBFCXSkJnOfQtqTOvVQ2r3v21adM3yCRki0mbO4xufUlSpPGLso9SASJrm7RRJCMdukXQfdTVtrvHrHrPuNCt2T32b0goi0j6FlW/rBxiQiGEIYREUwkxl9MZhE0dowEka9BXUM1IGVtZKmELnlkRSO3K4JkQYaNPPTLpG6fuvvpHqqLI5EhjRqwN77xjetxNYDJhrFpzqGxOwjSIaO2OhHUaJcp0QVOd1XnIRstNKZlFUlJG5/YRJBwdCeccS1//OMfqziSDw39xupEsI3bI2Dk2PWRGK/LbhEKWaPWqCd2ukISCBnW/jqEjJmGv4ypbBtx0dh33TKGg7pJEmxiIZNFSmxDfmW6yD0Rk00i2I6nLrAuImhMoS6H3YlWHNeahE95ylPqc0b61YdMtLpoXWq7kMXPfe5zK0XQ++rmsY99bK0D1zZImEdikAi612TZM0601KcyOudNb3rTOs6TCDo3ade92X3TxdXYQveekLXfG/VL5GXjXJv6tcalLHN3/O1YiKB6Vw8m89FltPuMK78vCKxZKZNJRomgCYhcX2Yk3fhEBEMIw4gIJhLjLyazCJImXcc0bglaNxsGDVTZHA1JQWI0cDXmNY67IigDRhC6DXqNeV1ANfYJnC6DbTZO4qih2kSQDBgfSJ5GylhoNPcvHzFIBDXsiaCyOpesYwvnk32zDqCsVFcEXV+/COqiqqsq8VkbESQR6kS9tYY+lM/4RA1y3QxdN5kggsRqpEwX2ZOd1fgndcTN8WVpScogKfK6cxDvww8/vL6mfCRK+dWN99x//yaJ3nf+dRFBr/WLoMwUoTZ+jQgqgzpw3fbtv2bbGH/XRND76l9WS1ddctWdOGdN9IugLzlkUsmf8XTq1bF1nfWM6rLaRNC5Xb97Y6yo50h21hg/dahu2/W6VvJPAFsG2JcbMtOeTzjeuoqg/QipulDe7jMujAH1xQvRlS1uIqg8/jtsXCKCIYRhRAQTifEXk1kENUBJiIlGNIb33XffYd3muugmqVE7WhE0Vk4WiwQSL41bjWmCSBZk64gZKdAVUHdR2cnW9bQfXUplJFcnghrixu3pdqrx77irY00iaCwd4RlJBAeNEWwiSK4HSY7xbxrxuptq/ItWTyRvkNR5nagQaPdMPRobqAwa+/3ZXGjwOwe5di/6cQwTlJASE+gQaHKlvE0EycdoxwiujQgSWwJKBp2rP6Op66UJUmS4iCAIjXtlRk3CdNpppw3MhA6iXwSdX5dKda5MyuyLAeMTZV/9PnRFsIv9ZT2JlqUs3DtfonS3cw+JuOv2BYhxmeraNmJdRdAYQhJLYD0Pa6Irgm3SmrDxiAiGEIYREUwkxl9MZhGERi3B0iiVXdJNUAZIQ741UP30bw1yDcnRiqBGrsyJrE8TMscy1k62R9ZF9kIDWDk0mmVZbC/j1Rr5zkNaNLxNjrI6EbStiTHajKEa1N3xc6SEiDbhua4iqKurxr3xWMSm0RVB53et3Xp0fFKia6bGPeFxXOPSyKsxjK6je+3kxPncn+23375u7zi6W8qqyqSSDtfS9vE+6dGVkmzbR9kcS5ay3Sc/fQFABHVplKFUTvfSEgQyTrpkOmZjXUXQ+V2je6/8sqqtfHB/zNLpWTNrqPsDZXCdljpR97JuutXavls+dee17jH7RZBYe848n6QQrlvZdKckeG2MoGN7XtznJtxec+98cSFrKZvqPds4fiuPcsgiGi/qetuzsK4i6PejTSCkK3T3GXQ8ZXDu9kxEBMcXEcEQwjAigonE+IvJLoIgWbquGTtH6oieiUQ0umVJ/LRenAlPZGgsWK2BrbFLyIxRGjRG0JIJuscZj6exr+FMIixVoIH/f//3fytFEKTOdPga7CZCIVTOb+ZG2RvnsU9XBJ3DBB7d5SN0v9SN0hIAhEX2hsw4lkbwT3/603o9yj8aEdTFtH+MoG017mXlNNrJs+6G9lFuEqFLrOyS87Z6JG+ky7g9s34qq3AdRM/sqzKjBKddu/qyj0a8rrbOody2IXr2IVga+67TPs5DvBxTJtc+5EC5nVcW1vHVvbGZ7hGxIHRwr9Sdde3cO2P6SKTzrkkEjZPrF0HC3L5EIC3qSvdOYk98ZFaVSfmVl6BZk9DzpswNkuN+um4yKCtGjmQHXY/9ZT+9ZmKc9oUGETR5ShNB98W9c24Sbl/XJWun+7E1FImiY+keLeuqHtzrtq3y+31Rz/7tddLnHrk36tA98ftizJ6fyiJIti8vjJ/1bzQRlKW0rEMX2VHnastHqFNdqD33vvQw66zzC5L4s5/9rGaeiT2UQ0ZZBjYiuPGJCIYQhhERTCTGX0wFEdQQJUAygxqst7jFLWojXLZGNzxhzJ2fGuAkg0QJjU5CRMb6RVAWUNaGqFj/z/4yLRq0BFEGTBZKIx4a7RrRpERDXBnsI3Ooq559yB0RdF6YOEWj2j5NBCHLo/ulY1j8XflbaDyTvyZUGtFeH2myGI1ngtoVQRNuEACTdTi+/1Z/slG66sn4WBicYHbrUHmIhgyZjE1DxknDnuw6pv1s7x7YRxlIRlew/LdxbqRaGSxu392HPP7whz9cuQ+R01XU++346lbZiCv5aFLSMo6OY1If+5BrQkh6jKUjgqQMJgWS5b3hDW84UASNi2zLLzi28xBCEw8pt8lZnIMY+kJChlnXWve9dQ1t2FfXSJlB+3he/ezWs+fYNSmH7WVUmwiqd2VQJvXt2bSPc3u+CJMvI1pG0L1RF+6b+rBtO5fnWRad2HqOCbLnvXvfXY9nsY3BVR6iK5vX1lUEETR+1XnMYNpFNlx2lqi2rKQvH8iuc6i/Vi4/hfUb3SuQP78/7kEmi9n4RARDCMOICCYS4y+mgghCY1SDXQZBY5nYkQ8NR5OlEDaZGhkX24EoaYxqOFsWgIi1Ri38t66Rpt53PN3odONsC86TEuPSZDEasi+67ckAyiDZR9ZHQ1bj3+QcGvDOC41vmRbl6y7voGzkxEyPxrq5DsfSuJdhatsSN+Uxjkv2T7ao4RgydbIxMqb9kqhbn4a/45qJU3bH8WT+lMnrJmNxbmF8pdddi+6q/XUlM0imZejUuX0IhO6ihKR1F+3SzmeGTePF7GPsnQyhBn93H/eNpBI6WU7bui+6Fjpv684I5dFN0v0hubZVB+6/+2zMp3ukGy5sS1Ic12uupaEMJiUiM6SbWME51LdsoIyy540gWcpEBks9kCKZuH6U1TnJt+eoXY9jOJblOWS6GzJkMnykzTPm3ITQvdeN173ynJBt9SlTrU59oWBbrylXe5bcH9evPpuYuWaZZlk9kqwshNbz6ll0HPjp/nhmPQvtdRlE53A9/eP+dFf1e0a82+9fuwbi7T3lamXznLXlUUAyzRAr094y8GHjEREMIQwjIphIjL+YKiLYIDga6hquZEvI7MkmaUB3ZQH+rVug7E7/e3A8+7Xj+SlLo4FKxjRku7KicatBTbra+e3jNQ1u+5CHdi7HcW6vO1cX/7atTF47lrK6vu62pMw2fvYfwzmVhXT0v+c6HK8dt41FdD3K1M7ZQj163bW0xn8/rss2tm37OHdr0A/CPrZp19k9TxfndBz3sh1f3aqj7j1o2N57bVvnsF07n3pp51A3smK2HXR96sZ51X33Pf9tv/Z8uAbnlMkijrpP6oI5CPt6ttz7VsbuNXWfx3aOJoHws3sPnbs948rarhd+dp8l53P93fvSjtdfv/3PG9oz271H7Rz9r8PrytkynA3/7Rlt19DK5vjdsvlv16Nsq3uWwoYhIhhCGEZEMJEYfzHVRDCE8YDMnSyfbr+DZjwNYSITEQwhDCMimEiMv4gIhjD2yHgZb6jLo+63LdNrHJ1xosZY6haqO6PXQ5hMRARDCMOICCYS4y8igiGMPbozmujHuElhHKixesYsWlrD5D+6hg5axy+EiU5EMIQwjIhgIjH+IiIYwthjHF5btsQEQiakMbmK7qDWuzNpii6hbSKWECYTEcEQwjAigonE+IuIYAjrBxOomIH08MMPr7N3mlnUTzOGmtQkhMlKRDCEMIyIYCIx/iIiGEIIYSyJCIYQhjEWInjo2RHBRGIsIyIYQghhLIkIhhCGMRYieMS0WeXAM4Y3ZhOJxHWLw8+JCE4WrMFmLbW23l8IIWwMIoIhhGGMhQgec97siGAiMUZxUO936T/nT72p6y04baF3i1RbEFuY3t9r/QtdTxRMTnLssceWffbZpxx99NHDFvheH5jtknSqR2PeBi203lAeC4+r5+7i7srdFhNv90L4d3fB8xDCxCEiGEIYxjqLYC9OvnBuOfjMdA9NJMYi/C6dctHcFb9gUwAyMn/+/HLSSSeVXXbZpc7e+IpXvKLGm970pjrDoyn/J6J8zJw5s3zyk58sm266adl+++2rRK1vSKAJUCyL8M53vrMcc8wxNSs5CKL4wx/+sLztbW8rZ5999so6vuSSS8q3vvWtVe6FeN3rXlc+//nPlzPOOCMyGMIEIyIYQhjGWIjgeVcsyIQxicQYhd+laVcMbrhPNmSepk2bVrbddtsqS7e61a3K//3f/5XrX//6Nfz3LW5xi/LMZz6z/Pa3vx3aa+JAyvbYY4/y3Oc+t4qVrOf6RgZ1t912Kze5yU3KDW5wg/LRj360XHDBBUPvroqF1Lfbbrtym9vcpi6w3jKvRO+1r31tudnNbrbyXrTw2pOe9KQq51lmIYSJQ0QwhDCMdRVBLFqyvBxxzqxVGrOJROK6hfGBV81f/8KwsSEdRxxxRJWkTTbZpLz4xS8uBxxwQDnllFNqduqcc86pC3v/5Cc/KTvvvHOd4n+ioUsmMbv00kvrzw2xSLkunV/5ylfKrW996yrXD3rQg8p+++03MBtJBN/3vveVO97xjjVz2ETwzDPPrNlAa+394he/qPdC6N5qwfXb3e525V3velddhmFDdHcNIaw7EcEQwjDGQgS1bYxpMrap26BNJBJrF7qFnnTh3LJs+foXho2JboXWbXvjG99YNt988/L1r3+9igZZkiUkTMJ2XpsxY8YG6VY5GVBfu+++e7nzne9c3v3ud5eHP/zh5fWvf30dq9gPkSN2tu0Xwa233roGWW/3Q0bztNNOKy9/+cvLwx72sHrMtk8IYXwTEQwhDGMsRBAXzVxUMxndRm0ikVi7kFn3uzTZMQaNrDzykY8sn/vc58pll1029M7oICXGFf7zn/8s3/nOd+qxhIwiiSGT8FM27sc//nH5zW9+M2ysnDF8//rXv+p7ytRwfGXaf//9y9e+9rXy5S9/ufz85z8v55577ioZMF0/jzvuuNrt0/ll4g455JAqYyCyBMtkMbZrOL5z/+lPf1q571e/+tXyq1/9qpx33nlDW63AZC72d23z5s2r8vW9732v7iNbSsy64/WcW3nvcIc71O3U75Of/ORaNnXRZbQi2FBuE8a85S1vKfe9733re+keGsLEICIYQhjGWIngoqXLy6mXzCuHZKxgInGd4pAzZ5b/XjyvLFwyubvaESldDHU9fMpTnlL++9//Dr0zOsiI7o8E8DnPeU55wAMeUO51r3vVIJYmZSFdtiM2xx9/fHnGM55RJ54hX13OP//88vGPf7y+10SN3B188MHlHe94Rz3efe5zn3rsLbbYosrXhRdeWLcjUd/97nfLS17yknK/+92vbnPve9+7vPSlL62CStpIEtF71rOeVaUMrp9MmRjn6U9/ern//e+/ct/HPvax5dOf/nQ5/fTT67Ygb65zyy23rMcwdu/BD35w3ecRj3hE7dppfF8T1CaCt7/97WuXUJPwvPWtby0vfOEL6zjLrjSuSQRlbB27Yd9TTz21PP7xj6/lyaQxIUwcIoIhhGGMlQji6nlLaxfRLCWRSKxd+J05tve7M2PO5O9mR7R++tOf1glgTFQis7c2EB7LHXzwgx+ss11+6lOfqjOLfuITnyjPfvaza1dI/yZExMakJmTrec97Xp0ls4vuqITPOEXZLVJDgt785jdX0TLrpoyamTKda9ddd63y49i//OUv66QpT3ziE+u5v/SlL9Wf22yzTfnd735Xrr766pqB/MIXvlC7v5JIyFISUN02nXunnXaq++644441c0c4yWPLtBnHR/xue9vbViFUZ5/97Ger8Dq3Y5NH50NXBGUSvf7HP/6xitsHPvCBen2NNYngq171qjoDqe2U+bDDDqvlVEZSSqwJdwhh/BMRDCEMYyxF0LCmC69eVI46d/YqjdxEIjFykEC/M+dftbAsWTb5G9VExUyaT33qU2u3y25XSxAl3SNPPvnkms0Suj+27qPEgzz+4x//qO/JDhI4YwgthfC4xz2uvPKVryxnnXVWFRsZLROmyIj1i6AZSwmZ9wgjcdO184EPfGAVVdkvr5EyWTrnI6GyggRRFs9P16QMfsp2Oq79hIluHvrQh9aun3C95Ez5XZfMoX1lCXXflOUjk60bJwkjtze+8Y1rZo+w2ce4yW9+85v1vRe84AW1bOiKoC6x6oWwkUXb7bnnnivHW65JBB17q622qtsQUGMD73nPe1Yptk2ygSFMHCKCIYRhjKUIYnGvIXv+lQvLkdOyyHwisaZoEnjelQvKgkneJbRBVHTr1C3Sz/6MkjFxMm9kSGZO6P5o9sr+bf2bVBm7R4T23nvvKpiyf20ik7URQcJH5HTR9JouohZS7xceUkrsZOpk50gi6esv3yAR7EJ6iaXM5IknnljF+DGPeUwVLjOnQkbQ/ne/+93reEYZ1YayytrJDLZZVQeJIA4//PDaHdfEMYceemgV0tWJoG6h97jHPeo16goqZAJJ8qtf/erys5/9rNZNG48ZQhjfRARDCMMYaxGErIbMoG6ixj11G76JRGJFmCFUd9ALrlo46ccFdpHB063Q+EDZtP619YiL7JNul0K3zs0226x8+MMfXpk9JB/kiaj94Ac/KB/72MdqBssxSY1ukMb8dUXwRS960RpF0PFl4j7zmc+URz3qUVXIvv3tb5eDDjqodo0kdnBcMkXCnvCEJ9TymnDGTKjO0eRoJBF0zeTRWEIZOt0tCZpspvI7rusDEXzIQx5SHv3oR9f1ALuyqZuqZRzUk4lmMJIIKsuPfvSjuiSE65OBXFNGkFCb0fVvf/tbDWMMia+xk4J8yk6GEMY/EcEQwjDWhwhCN1FjBk1+4bjJDiYSK4IA+p2wTMQVc5dM+qUi+iEksnsmcJHta2PbGiSK/MiI6d6puyTBM6kLUfM+KXvve99bu1E2KdGN0aQt1s8jME0EZc1ktVaXEdRlkgjCOXS7lJ0zZo+A+WlhdsdqMigz599vf/vba6ZMGRxH91RdR5VzkAi2JRhcu/ITTj+VX1dTYwH7RdD+spT+uyuC6kg9yIL++te/rq+NJIJQb8Yxqovf//73Zfr06XXc4EgiqIxebzi345lB1ayk6oRMhhDGPxHBEMIw1pcINmQ6pl+1sBx73uxy6Nkza4bwoDNmRgwTUyY86555z/6hvd+xk3sC6Hdi/iLr5Q39okwhiJaJWV7xildUeZJZW91YM8smmHVT1s++xrsRnVvd6lY1y7fXXnvV45Gcv//973UWz+c///m1ayjpIoTWvDORjCxYF6JJ5GzfnR2zIXtJWpX1rne9a+2uqgtngxiRJ2MaZTdJo+6UZhN1rkEi6PUddtih3OlOd6pdLE3oQijJo6yeazXGsds1VEaQCNpmXUSQnMrsmdnU7KPq5kMf+tCIIii6y0fAMVyvTCihJLXdMoUQxicRwRDCMNa3CDZkPa6at6Sce8WCmgk5+tzZ5d9nzazZkURisgb5O2LaiuzfOZcvKLMXLKvZ8qkOGTI+0Fp0pKRNPNIvFP5tLb+uCMpiPe1pT6tj1nTjbPsQFJkqwmQWUN00bU/2bOtcsmy2g/d0LSWBJJGMwfFs0y2LMYHbbrtt7bqpe6X37N+O1TAOzxg64+uMWewXwVYeM32ahMUYu4bxiaRWN1Z1sj5EEMYk6u6qDk0gY/bVtRFB25xyyim1nITVtt0yhRDGJxHBEMIwNpQIQlNBe2G5RlQvyGEiMRVixTO/4ncgrJAt49ssz3DTm960Thyz7777VunpZgf92/IQuk02EbTwuwydzJsxdm3yFBOpvPjFL67HI4KyXc5DHInV//3f/9WxcU2+TjjhhJrhu81tblNFkAjJIMosWm6hde+ELqQmWpHx0/VTVpJo/vvf/15ljJz9rDto20EiqDyOb3IX2dAmX85j/J3ussrv+vrHCI6VCLa6t/yErqjGF8p2DhJBXUObIMP7ZnMlumYx1W23P8saQhifRARDCMPYkCIYQggNGTDLM5Az8kNGyI7sH5EjZzJwXpfNI1SEidh84xvfqGJnMXnyZH09XRWF9f9MFqNrKMxCKqt4i1vcosoRCXN8WUL/bTIa2xsjaFmKAw88sL6uLMpgW/+9ySab1ElhSBKZs1ahLqckzDZCpo8IOh8hI4LWEZQlbOsIGqdonN5NbnKTKoiWqXDNhMz1kjNjBFtGkPw5rrGE/SIou2iMo/Ja1xDOu9tuu9Wus01a+yF0lq8gmMpxt7vdrQpfE0GiaPIaS0WYxbRdn3ISWHVpCQmymiUkQpgYRARDCMOICIYQNgaEhnjI8P3pT3+qmS0ZNyJFhsgP+fjkJz9ZZ6dsEkQGTVDSllogiSSJUOq2+Za3vKVm+tq6erKIukPut99+ddIZE8qYPIZAyZiZLMU+xx9/fD22DKLxfiZwURbHJ5qyX8RVmcmlDKSsGZFUXl0l/dskLLJkzkt2dYF1HWY3BXEikt1zEFjyZjZVAkY4jcOD7q/2l+3sdoWFbXTv1M30L3/5S31N2XQxlUU15tA4x0E4ljoke0RSfbUZXC3HocuoayLWrq/dExPiuCbbtO1DCOOfiGAIYRgRwRDCxoTYyJzJLhljZ6kGWTnj93S/tGSCTF1XgEiWmT1l8Wx/2GGH1e0ImMlLjGHrCpB9ZRJlCR3XovEye7aXebO9TBrIjWO1sgjlME6wiY/z254YOp7yOqZ/E7FWVtsRS/sT2YbXdfls53Ad/u2alMVxWpdXMml/ouq/u9hG+U1g07poklnncmxlHilj53USrtwmylFfrdyOKytoIh/X1sK1Kodzde9HCGH8ExEMIQwjIhhCCCGEMLmJCIYQhhERDCGEEEKY3EQEQwjDiAiGEEIIIUxuIoIhhGFEBEMIIYQQJjcRwRDCMCKCIYQQQgiTm4hgCGEYEcEQQgghhMlNRDCEMIyIYAghhBDC5CYiGEIYRkQwhBBCCGFyExEMIQwjIhhCCCGEMLmJCIYQhhERDCFsLBYvXlzOOuus8oMf/KDsuuuu5Utf+lLZZZddym9/+9ty0UUXlWXLlpX//ve/5cc//nH5/e9/X2bNWv3fKMc75phj6vH+8Y9/lOXLlw+9s4JLL720Hucb3/hG2Xnnneu5vve975UjjjiizJ49e2irEEKYfEQEQwjDiAiGEDYGpOz73/9+efazn13ufve7lzvf+c7lTne6U/35wAc+sHz+858v06ZNKwcddFB55StfWV70oheVo48+emjvwUyfPr184hOfKE960pOq7BFBcdVVV5W99967vOpVryqbb7552WSTTep5xD3ucY/yrGc9q8rjkiVLho4UQgiTi4hgCGEYEcEQwobmsssuK3vuuWd51KMeVbbYYouy0047lf3337/GXnvtVT70oQ/VDKFsILn78Ic/XAWO3K1O1g499NDy4he/uDznOc+pWT4ZxfPPP7+8//3vLw960IPKYx/72LLddtvVc++3335l3333Lbvvvnt573vfW7761a+WhQsXDh0phBAmFxHBEMIwIoIhhA3NCSecUN785jdXOdPt88ILL6wSJnTRPPvss8sZZ5xRZs6cWbt7HnDAAeUxj3lMed3rXle7jA5i7ty5VRQf97jHlQ9+8IPl6quvrsf94he/WDbddNOy1VZblV//+tfl9NNPr11MnWvBggXl8ssvr8J52mmnVXEMIYTJSEQwhDCMiGAIYUOiq6buni94wQvK05/+9HLxxRcPvTOYa665pvznP/8pb3zjG2v28JBDDhl6Z1XI41vf+tby1Kc+tWb7SKSxhvbR/VSGMF0/QwhTlYhgCGEYEcEQwoaECOrC+dKXvrR2DSWF/ZO69KMr6de+9rXy0Ic+tGb4+oWOLP71r38tz3ve88qb3vSmcsopp5Rzzz23dim9//3vX8VwTecIIYTJTEQwhDCMiGAIYUND0j772c/WrqGvfe1ra+ZOZlA3UFLXz6JFi+osoM94xjNqmGimiy6eJpd58pOfXL785S/XbqFHHXVUnWDmuc99bu0iOui4IYQwVYgIhhCGEREMIWxojMWTtdthhx3Kwx72sPLwhz+8itzf/va3OjkMIezHmMF3vvOddbwfKeyKnSUoSJ+MoMyg8X9/+tOf6njBt7/97XX8YAghTGXGtQj6e75s+TVl0dLlZf7iZWXuomVlzsIVMXvh0kRiwodneV7vuV6wZHlZ3HvOPe/jgYhgCGFjccUVV5Qf/vCHVeLuete71lk9P/e5z9UxgXPmzFmlO+eVV15Zl5u4733vWwWyyaJtzDb6hCc8oWy//fY120gEf/7zn1fJ1D00YwNDCFOdcSmCBHDpsmvK/F4DecacxeXsGfPLf86fUw4/Z1b591kzy0FnrGigJhITPQ45c2Y5+rzZ5eSL5pWzL19Qrpi7uH7xsbGFMCIYQtiYLF26tFxwwQXlO9/5Tl32wVqCL3vZy8pf/vKXVRZ5J3xmG7Xmn7GCloWQWbTNtttuW2cV/dGPflS7iepKauF4GUFLUfh3CCFMZcadCC7vWaAMyblXLChHnzu7HNhpNCcSkz18yXFU77m/eOaisrQngxtLByOCIYTxArmzrt+97nWv8upXv7occ8wxQ++swFg/Wb9b3/rWK6XPIvOygcYatgXnZQB1HzVm8OUvf3mZMWNGfT2EEKYq40oEZQE1QI85ryeAvQZxJDAxFcNzf9AZM8t/zp9drpq3dKNkByOCIYTxgnF/8+bNq4u+kziy598NXT4tH2GcoOUkLBHx9a9/vU46QyCtCQjHsTbg1ltvXbe1TyaLCSFMZcaNCBoDOO2KBbX7JwnsNowTiakYsoOH9iTsgqsW1u6iG5KIYAhhPEHYdt555/LEJz6xLhlx1VVXDb2z4j3dSK1B+JCHPKRODPOa17ymPOUpT6ldSXUzbZDCb33rW+UOd7hDzQqalTRLSIQQpirjQgRJ4DmXD0lgpyGcSCRWiNj5Vy4sC5dsuMZKRDCEsCEhYyTt5JNPLuecc86wiVxk8l71qlfVjOABBxxQs4BdZs2aVQXxNre5TXnlK19Z7ne/+5WPfvSj9VhdSOGpp55axw/e9ra3rbOH/vGPf6yTznSF0IyiZiR13q5IhhDCZGKji+DiZdfURu4R05IJTCRGCl+SXGTcYO/3ZUMQEQwhbEhM8GKNv/e///1lq622KjvuuGPZaaedanzyk5+sC82bGdQkL0SuP4tntlCzim6++eZlk002KXe7292qMJpltJ/58+eXI488smyzzTbljne8Y13AnhB+4hOfqOf71Kc+Vd73vveVN7zhDWWXXXYZJp0hhDBZ2KgiaOiTBqcZQSOBicTq49je78kVczfMdOcRwRDChkT3Tuv+7bbbbuWZz3xm7eK52Wab1TDWzwLwbQkJIteP/S0Y/573vKfKnW6fZhMlmP3Y1oQyJ510Utl1113LlltuWQXynve8Zz3fve997/LoRz+6vOUtb6mzjCYjGEKYrGxUEZy7cFk59eJ5dQr9boM3kUgMD1+WnHnZ/Lrm4PomIhhC2NDIvJ199tnlD3/4Q9lrr73KV77ylRp77LFHHet30UUXrXbtP8J27LHHlj333LMcdNBBtbvo6mjyaNvvfe97tWup85loZp999ilHHHFEfT+EECYrG1UEp1+1oktot7GbSCRGDkuqXDprxYLJ65OIYAghhBDC5GajieDipcvLyRfNzeLwicRahKzgGZfOr78/65OIYAghhBDC5GajieCVc5fU9QK7jdxEIrHmOO6COeXKeet3rGBEMIQQQghhcrPRRPDCqxeVI85Jt9BEYm3jyGmz6+/P+iQiGEIIIYQwudloInj2jAV1sexuAzeRSKw5Dj5zZjlrxvBZ88aSiGAIIYQQwuRmo4lgHR+Y2UITibUO42pPvzQiGEIIIYQQrjsbTQRPmD43awcmEtcxTr1k3tBv0vohIhhCCCGEMLnZaCJoEfluwzaRSIw+IoIhhBBCCGFdiAgmEhMwIoIhhBBCCGFdiAgmEhMwIoIhhBBCCGFdiAgmEhMwIoIhhBBCCGFdiAgmEhMwIoIhhBBCCGFdiAgmEhMwIoIhhBBCCGFdiAgmEhMwIoIhhBBCCGFdiAgmEhMwIoIhhBBCCGFdiAgmEhMwIoIhhBBCCGFdiAgmEhMwIoIhhBBCCGFdiAgmEhMwIoIhhBBCCGFdiAgmEhMwIoIhhBBCCGFdiAgmEhMwIoIhhBBCCGFdiAgmEhMwIoIhhBBCCGFdiAgmEhMwIoIhhBBCCGFdiAgmEhMwIoIhhBBCCGFdiAgmEhMwIoIhhBBCCGFdiAgmEhMwIoIhhBBCCGFdiAh2QoP3xOlzy2m9RvZo4pSL5pUjzpk18FjrMw4+c2Y5YaicJ104t/570HaJyRsRwRBCCCGEsC5EBDtxYk+qZi1YOlTCNbNo6TXlvxfPG3is6xIHnTGzHDltdjnugjn150iCRz7nLFxWy7Bg8bLethteRtdXuGbX5/k4+tzZA7dJRARDCCGEEMK6ERHsRFcEly2/pszvSdbshUtHjCvnLqmZuUHHWts48Iyrq/hcMmtxWbhkebngqoXliBEEjyQqXyvnUZNImI4+b3Y5v3ft8xYtW0VGEqtGRDCEEEIIIawLEcFOdEVw5vylYyZ5ownZwDMvm79S8KZfPbIIapjPmL2kCiMZPXwjdE9dX3H6pfPLvJ6A44retQ3aJhERDCGEEEII60ZEsBMTRQQnc0QERxcRwRBCCCGEsC5EBDsREdz4EREcXUQEQwghhBDCuhAR7EREcONHRHB0EREMIYQQQgjrQkSwE2MlgoecObOcevG8csmsReWqeUvK7N4xTS5z9fwl5aKZi+qSD7ax7THnza7vmQXUmL8VGljK4qXLy9xF105Wc87lC1YeX8P88jm94/Ze97N/jKDJY86/cmF9/+Le+UxEYyZOE9CQK9foPf991oz5qwjnv8+aWZeluGzO4pXnVhczemJghlTvd8/VjUN75VJOE94Yu9jOI67uHePimSuO0b/fCdPnlBlD51MHy69ZUQtLl12zcn916FoHnd9Mo8dfMKcnz4vqNSlv2+/KXv07rzofqez2d79aeY+aNrtuS8xbfSl/9x5s7IgIhhBCCCGEdSEi2ImxEEHCdWFPSGS1iMxQgq/Cb5b0XiMnxMT2looYDRf1jrnyHD1xW7B4eX19QU+czCLa3hPH9uSSuMH1mI308p7QkMsmWfDf9nfsY89bsWSFspsttWUm4b/8e25PVsnRoWcPFyoZzfN68qlcS3vb2v3aI1x77YS3X6jIoXOuDsea3zs22ezuq77PnrGgXudi9d07cecS6zW6D6SaCA9akoKUn90T4rYfOVdGM5c6HlwToezfd2NFRDCEEEIIIawLEcFOrKsIEjSZOMJDKhyLUOjuKMtGRBxXdum03mv2ITbes82lvcZ3EzWZRPt6Txzbqa+1EcFFPfmTmVy8bHnNbp0xVBbn8p7T+SlzeO4VC+t/t31sJ2NIXImQkrmmky8antWTVSMPZFHZCSVpdAyiJuPX6oWUEeC2r4zmyRetWCBfNtH5QRq91sJ5CWfbTz2ce8WCKpF8bVGvLmbMWZHlXHnevutULqLXjiH6RdB9allIcutYZHU8PbMRwRBCCCGEsC5EBDuxriJ4/PQ5NRNISkiM4+lieODQ+7JXjkl6+tf+W5sxgmsjgg5HlHRJPa5X57qJ2oYMkVYL0jsjSZIBs61ytMwZwSNtJJIMKh9R6u9m6biu95SerOmmaZF7+3qPaBHZ6VctquVxjNZltXsMsTZjBJVTmR3PffNv51nlvOfNqSI3s/c+ydb1lCC2bdp2XRFUt7qhnnbJ/CpBbbvxFBHBEEIIIYSwLkQEO9EVQYJFVkjDSNEvYASPlIhuQ3o0sf5E8JoqtYO6RJK1FVnIumntWiljRoz6tyUeMnmQ3evPqo0mjr9g7opxkL3zOda6iKDzG4fYjmW/rtz1hzGA5Fx96Cbbff76RVBmkQSONKZwPITrJ+TrK7pjOSOCIYQQQgiTj4hgJ7oiOBr6u0jKhhENMkdiVicm/bG+RNC4QJm/7vvdmHb5gtplE66922WzG4edM6uKV9uO9A7abnXhns9esELyZCDXRQSVu00sY5KY/rGD/UH2ZEXVrzo7qyfy3fe6Iqjr7mhF95+nDX59MkVEMIQQQghh8hER7ERXBMlRnXWyJyMjRf81mP2SxBAKE5vIqsjEHTRAePpjfYmgrpO6anbf78ZZvXM2oTI+b9A2LWQ54dqUddA2wvWSB91fCZV6Ugb7KA90Mx1UL6MVwYt7ZXUMZT9jNWXphjGXtrefrFd7vV8Evdc/E+tIEREMIYQQQggTkYhgJ7oieJ0mi+nJQ3eyGMJhrJnjWJLgUOMFR5DC9SWCrqP7Xn8QL+MClVW30EHbtFiTCMqAKovrlbGzveyarpsygM7R8N/rIoLqVR3r7nlKpxvj6sJYQWWxn+xge71fBHUj9Vp335FiY4mgbqvGP66v8Cy3c0UEQwghhBAmHxHBTqyrCAqZJF0VW5aNW1iCwGya5EiGbFCX0Ykugof0xOSUi3r11ztfve7eZbgU1+PY5FgMXd46i6CZSXH5nMWjfpZkbK3lCLOittf7RXC03UI3ZmSymBBCCCGEsC5EBDsxFiIoiJ59dR+VCSNGDV0jNeL7M4MTWQSJlGUpHEPpXYJtLDthMhpLMMjayci1+h07ERzeRXekcE9kKBERXD0RwRBCCCGEyU1EsBNjJYLCkhHkjhRan48YESSu0Wa57G4/kUVQFtQ1wXqFjkkeXBPZE8TXPR8rEfQecZu9Fl1D67FH0TU0IhgRDCGEEEKY7EQEOzGWItgNoqExbTmKpctWzCraP5PnRBZBy1A0iXKNxpe1tRO7YY2/sRJBE9uoKxnXbllWF7rm2l4XVVnK9npEcDgRwRBCCCGEyU1EsBPrSwRbyFyZiRT90jWxRXB2fR3nXrFgxPX31OfipSuub11FcNoVC8vCntSRN/K5plk+lUl3UOM1ZQWtE9jeiwgOJyIYQgghhDC5iQh2Yn2LoOUkzHJJOKZ3li8QpOik3vkJEi6ZubjONNrdpsV4FkHHIA7d/YRlNGTxmmyNJILqSHdPGGNoEpr+bQRZ877jqQNLQxC6QduKs3qi5zqN17yor4wRweFEBEMIIYQQJjcRwU6sqwgSCF0ONdK7MkFmjr9gbm1c65YoK6abYndfcejZM2vXRZhllBQNEqHxJoK6grZMn2ybxdqViejJ1LmOS3sSKIPXMp4jiaAF7WUCsaBXLuscDqoDGVTrB7Yxf35a/49Md7c/sXcPZQ+NYXRuk8XYpnusiOBwIoIhhBBCCJObiGAnuiK4qCdYM+YsrnKxurDm3GFD3RJPvmhuFT3ScXlPZto2Jia5at7SniytWEuPpB03wvUTFVkrQeLs6xiO3bYZbyJofcSLZy6uZa5S1juebS2jQQBlQQmu87RunyOJ4KE96SDTpM3xCF6rA1m/7tIbJFNX1JbpU/fdOhP+vagnqU0CCZTyds8ZERxORDCEEEIIYXITEexEVwRHC+EiXvY//oIVk6EQvp53rIRgmCSGsBjPdvz0uQPXEhSyZzN7x7B95xC1O2PbZryJIKEzEYwxeNZPJF2t7P5bWQnheVcsXFm/I4mgkBVUflnGJmd+OC9R7G7r2pXFMhJkc1nbYQjnsZ9lLE6+aLgEiojgcCKCIYQQQgiTm4hgJ4xj02XTWLbRhu6JukbaX9dOmTtZKlLVtiF/Zgm11l6/tA0Kk8rY3n7tGBr+7f1Dz5pVzr9qYX1d1qtfjpRHubwvi9Z9rz9Il3MRTdnNQdu0UH7HtD2Z7b5nllCzh549Y0GZftWilWVXD2detmIhfeVq9Str17+WYgvdPj0flt3o1oHzDhoHaHsi6lq79S4u6JWFKB7Vu7cjiaf9dSFt51rTxDPjISKCIYQQQghhXYgIJhITMCKCIYQQQghhXYgIJhITMCKCIYQQQghhXYgIJhITMCKCIYQQQghhXYgIJhITMCKCIYQQQghhXYgIJhITMCKCIYQQQghhXYgIJhITMCKCIYQQQghhXYgIJhITMCKCIYQQQghhXYgIJhITMCKCIYQQQghhXYgIJhITMCKCIYQQQghhXYgIJhITMCKCIYQQQghhXYgIJhITMCKCIYQQQghhXYgIJhITMCKCIYQQQghhXYgIJhITMCKCIYQQQghhXYgIJhITMCKCIYQQQghhXYgIJhITMCKCIYQQQghhXYgIJhITMCKCIYQQQghhXYgIJhITMCKCIYQQQghhXdhoInj89DnlwDOGN3ATicSaIyIYQgghhBDWhY0mgidMnxsRTCSuQ/i9Of3S+UO/SeuHiGAIIYQQwuRmo4ngaZfMK4ecOXOVBm4ikVhz/PusmeWcyxcM/SatHyKCIYQQQgiTm40mguddubAcfs6sVRq4iURizXHEtFll+lWLhn6T1g8RwRBCCCGEyc1GE8EZcxaXo8+dvUoDN5FIrDmOu2BOuXLukqHfpPVDRDCEEEIIYXKz0URwwZJl5cQLM04wkVib8PuiW/XCJcuHfpPWDxHBEEIIIYTJzUYTQaR7aCKxduH35YKrFpbl1wz9Eq0nIoIhhBBCCJObjSqCcxctKyclK5hIjCr8nvz34nll9oKlQ79B64+IYAghhBDC5GajiiBkN5IVTCRWHwf24shps8olMxet92wgIoIhhBBCCJObjS6Ci5YuL2fNWFAOzlISicSIcejZM8u0KxaUBet5bGAjIhhCCCGEMLnZ6CIIE8ecNWN+uogmEgPC78UZl84v8xYtG/qNWf9EBEMIIYQQJjfjQgQxb/GycuZl81dpACcSUz1kyi0ev6EygY2IYAghhBDC5GbciOA118gMLq9jBnWD6zaGE4mpGATs/CsX1qUi/H5sSCKCIYQQQgiTm3EjgtDYXbLsmnL1/KXl9EvnrdIoTiSmQpgU5tCeeJ09Y36Z2fs9WLx0w0sgIoIhhBBCCJObcSWCDbMizl+8rFwxd0k594oF5ZSL55Vjz5tdDsmEMolJFgedsWIimKPOnV1OvHBunRDGcy8LuCFmBx2JiGAIIYQQwuRmXIpgl0W9BvGchcvK1fOW1MbpJbMSickTl/ae6RlzFpcre8/3rAVLN0o30EFEBEMIIYQQJjfjXgRDCBueiGAIIYQQwuQmIhhCGEZEMIQQQghhchMRDCEMIyIYQgghhDC5iQiGEIYREQwhhBBCmNxEBEMIw4gIhhBCCCFMbiKCIYRhRARDCCGEECY3EcEQwjAigiGEEEIIk5uIYAhhGBHBEEIIIYTJTUQwhDCMiGAIIYQQwuQmIhhCGEZEMIQQQghhchMRDCEMIyIYQgghhDC5iQiGEIYREQwhhBBCmNxEBEMIw4gIhhBCCCFMbiKCIYRhRARDCCGEECY3EcEQwjAigiGEEEIIk5uIYAhhGBHBEEIIIYTJTUQwhDCMiGAIIYQQwuQmIhhCGEZEMIQQQghhchMRDCEMIyIYQgghhDC5iQiGEIYREQwhhBBCmNxEBEMIw4gIhhBCCCFMbiKCIYRhRARDCCGEECY341oEly5dWl75yleWvffeu1x55ZVDr67KOeecUz75yU+WXXfddeiVtWfbbbct3/rWt8pFF1009EoIU5uIYAghhBDC5GZci+DixYvLLW95y7LDDjuMKGnHHXdcec5znlNe+9rXDr2ydjjuPe95z/LUpz61HHXUUUOvhjC1iQiGEEIIIUxuxo0I/vvf/67C140PfvCD5YY3vGHZYostyjve8Y5V3tt3333L5Zdffp1F8Jprrilz584tn//858u97nWvcsc73rF86lOfKueee+7QFhuHyy67rPz0pz+tZXGdhx9+eBXUr3/96+VPf/rT0FYhrF8igiGEEEIIk5txLYKri3URwUWLFpUTTjihfOlLXypPf/rT6/Fe/epXl2c+85lVDE8++eSyYMGCoa03DMTUOT/3uc+VRz3qUeUmN7lJ+X//7//VLqt77bVXfe3DH/7w0NajZ8mSJbX77B577FFOOumkcsABB5R//etfZebMmUNbhDCciGAIIYQQwuRm3HUN1R30ggsuqLKy//77lx/96Eflhz/8Yfn1r39djj766DJjxoyybNmyoa1H3zV0/vz55cILL6zbk8gPfOADZcstt6wy6HXyt9NOO5Wtt966bL/99uUnP/lJFadLL720zJs3b+go6w/jIZVhk002qRnKl73sZeVNb3pT+ec//1kOPvjgWraf//znQ1uvEMc5c+aUv/3tb+W8886r9TaIiy++eKXwHnTQQWXHHXesx3bMhQsXDm0VwqpEBEMIIYQQJjfjSgSJiUydrNhb3vKW8sIXvrA8//nPr/GqV72qvPOd76zZsTPPPHOlDI5GBHW3/Otf/1q7V37sYx+rIvSe97ynZiEJWEOW7M9//nN517veVTOEn/nMZ2om7Re/+EXtRro+ce0mxbn5zW9ey3b++ecPvTMY13/aaaeVRz/60bUr6axZwxvrBPb3v/99Hf9IfskjyfZvE+xs7G6wYfwSEQwhhBBCmNyMKxHUhfGjH/1oedjDHlblRjZr+fLl9T3Zso9//OPlSU96Us1wtVlEiaAunWTwmGOOqXH66aev0rWT8L3//e8vb33rW8uee+5ZZXN1zJ49ux6XBMoQEsdp06YNvbt+kLHcZZddyk1vetOyzz77DBS7Lrp8HnrooeU2t7lN+fGPfzxse/V2/PHHV6Heaqut6jU1XBe5/sMf/lCPE0I/G1IEly2/pixaurzMX7yszFu0rMztxZyFK2L2wqWJxKSK9mx7zhcsWV6f/eXXXDP02xBCCCFsOMaVCOoO+pKXvKR88YtfHHplVcjM61//+io2RA1+PuEJT6gCdY973KOGJSdkDTcWsnVXXHFFzb7psikTNxKETbaRvH7kIx+pYwO/8pWvVPE1BtL+Mnuymk32yNvZZ59dM5VmVd19993r9rqI6i4K+8gwPvKRjyy/+93v6muNSy65pGYFifWaMo9harK+RdBvBAHUEL58zpJy5mXzy3HnzylHnzu7/PvsmeXgs2aWA89Ycf5EYjKF5/qQ3vN9xLRZ5cQL59Zn/+r5S8ui3u/C0t7vRJwwhBDChmJciaAxbLJvumSSna5AEabp06fXLqKvec1rVmb11tQ11MQwpGhdQ8ZutNkzmcw3v/nNNVunfCON3wO5kwE0MUx/vO51r6tZPeMVXaOJbHDqqacO3F5885vfrNsQYVnQJz7xiQOzi9ttt12tS/IdQj/rUwT9Wi/sNXovvHphObLXGO42khOJqRrHnje7nHflgpoZjwyGEELYEIwrETRpCwncdNNNa/dFGa7WNdQSCttss01dSsJYvzZmb00iSHhkC9c1LC/x1a9+dVQTrIwHEfzHP/5RxzkaCzgoI7nffvvVTKoxlyH0s75EcHnvUZQBPO6COeWgZPwSiZVxYC/8Thx17uz6++d3JYQQQlifjCsR1KVSd8rvf//7VWI233zzld09ZbZInXFtukw2uVmTCBpLSCjXNZSLtK2um2djrLuG9ovgaLqGGjeozvwchHozkyjxHpQxDFOb9SGCxkKdd+WKLGAkMJEYHLqOHnbOrPq7snhpbDCEEML6Y1yJIEjU1VdfXUVHNqxNAHPKKadUsSJNLUuINYngRGF1k8X0iyDI4OomizEpzuMf//i6TMQgzjrrrHpMi/bLxIbQZaxFkASec/mCcnivgZuxf4nEmuPQ3u/d2TMW1N+dEEIIYX0w7kRwbSFA1hdc00ygV111Ve0O+b73va/OTromjjzyyNqt0hqGG4KxFsHvfOc75clPfnKV6UE438tf/vJaH5kwJvQzliK4fPk15fwrF9bJMXR/6zZ2E4nEyOGLk3OvWFiWLktmMIQQwtgz7kRQl0rCZjH10cY73vGOmgFbHcbtffrTny6PetSjRpSjLr/97W+rfH34wx8eemX9sj5E8ClPeUrtMjoSut9GBMMgxlIEHevY8+YkE5hIrGX44sRMupfMWjT02xRCCCGMHeNOBAmNmSx33XXXUYWZMR/84AevsWtoRHBVLMXx0pe+NCIYBjJWImgGxJMunFsOPnPmKg3cRCIxuvC743doQe93KYQQQhhLJnzX0NGOEWwieNe73rXK4yCp7IZM4yMe8YgJIYLWXbz00kuH3lkBEXzMYx4z4vIQFsh/7nOfW3bccce6rmAIXcZKBKdftbB2b+s2bBOJxNqF30Hdq0MIIYSxZMqJ4G1ve9vywhe+cFj30v4wo6ZZS6+LCJK6ww47rOy///511tLu5DYjsbYiuHTp0nrtRFDmj7waz2j2UcgSWh7CDKyDMAbSNe688851ncQQuoyFCC5Zdk05YfrccvAZyQYmEusSulX/5/w5ZVnWlAghhDCGTDkR3BBdQ51rtOsINtZWBMmlLODTnva0cvOb37z8z//8zyrrCMoEqpOPfexj9d/9GIf57Gc/u54rhH7GQgRnzl9a9+02aBOJxHWLI86ZVWYvWDr02xVCCCGsO1NOBB/wgAeUfffdd+WyFCPFl7/85fKkJz3pOomgCW8++9nPlmc84xk1Qyd7tyYsVC+LZ6bPP/3pTysXzMdf/vKX8p73vKfsvffeQ69cy0EHHVS22mqr8tjHPrY8+tGPLj//+c/r65bf2GGHHcozn/nMlWsLNojp9ttvX17/+teXAw88cOjVEK5lXUVQ3uKCqxaWf5+VbGAiMRbhd8kMopMd6+76onNNa/b6stTauTNmzBjV+r7jjXadlszqxmiufSqhTmbOnFnOPPPMes9TNyGMLRNeBP/73/+Wrbfeurz3ve8demUwl112WfnqV7+6coH60QSx0nVyIkIkdQs1zpFYdrGGoDUGjQ8899xzh14N4VrGQgT/e/G8TBKTSIxR+F367yXzV/yCTVJ8aWo4hSWerCc8EmTgN7/5TXnDG95Qv7RdsGDB0DsTB5/RPn+1Ybpx6qmnlosuuqj2EhrNsJLJDvn7xS9+UYey6KllfoQQwtgx4UUwjMyJJ55Y3vrWt9Zuqr5Vgw/Q7373u7VL6Z///Of6Wgj9rLMI9kzQtPdZMiKRGJvwu3TktNlDv2GTE8Mddt999/oF5h577DH06nD0oLHdfe5zn/LGN76xXHjhhUPvTBx8/up1dL3rXa9c//rXLze4wQ1q3PjGNy7Pe97zyq9+9au6/vFUz4B5Jr70pS/VoTaf+cxn6r0PIYwdEcFJjC6hP/3pT+vyGsYM+kCZPn16edCDHlQ+97nPTcgPz7BhGAsRzALyicTYxnX5XZxIaPTL8D3sYQ+rM1+vDsNCyKKhExMxc/aHP/yhTvR23/vet/Zqeve7313DUI+73OUudYZzX9oaajKVke099thj6xwJJ5100sovtUMIY0NEcBLTxI/0PfKRj6zrBfqQec1rXlM/RPMHNYzEWIhgxgcmEmMbh0YEV2Ksu+6VE7FbKH73u9/V+Q2M1T/ttNNq9k9YzulrX/taufe9713fNwP5VEY7xr3WRTTdQkMYeyKCkxxjLoyP9E3aokWLyhlnnFHHJaR7RVgdEcFEYvxFRPBaSAEJ7M7KLTNoOSSfdQTCNsbatddWlzls2za5HGmSN8fw+emYtvVzkKDYzvGUr8lMtxxNBC1XZUxgw7a+tH3+859fJ4GT8fRao3ss51eW7vv9+MLX9bSyOncrW7dO2nFtax/R6k5dtHP42V9XI32p7NjO0S1rO1+Xdq5B9dnO18oxiPZ+2985B2F/5bV997ir2yeEyU5EMIQwjIhgIjH+IiJ4LcbYbbfdduUHP/jBSrkhVC996UtrRu0///lPnb3bv425e9e73lW7Y5KFfk455ZTac+blL395lbNXvepV5Rvf+EadtKZBHGSlDjjggCpvW265ZV2CybrExq5ZlqoJEeEwy6XtlOXwww8vH/rQh+q2u+22W5k2bVr5/e9/X881aIyjfz/3uc+tk7r9/e9/H3p1xcQphnm8//3vr+/bXw8fE+eYWbPJGkiXTOO3v/3t8rKXvaxu6/zKc+ihh5a3vOUt9Rr1GoKZ1ffaa686Q/nf/va3Oru67V/0ohfVHkREiSgeffTR9XrVq2O++tWvrvfK7K1dLr/88vLLX/6yvP3tb6/1b1v/TWwdB8qrntSJulefzmeSvrYm8uzZs2tdef+QQw5ZRQZJ3RFHHFE+9alPlZe85CV1fwLtPH/84x+H1cnBBx9cr095//3vf9fnQ9ns8853vrO+35XQEKYCEcEQwjAigonE+IuI4LWQFkMeLPHUMlhmxDapiLF3xt2RA8JkBnATy5AXItKwz8knn1yXn7LG8FOf+tQqEzJxT3ziE6soNFEiVpZrIiptO8dWhoc85CFV9Mz6CTJhXNu97nWvKhpmNyVVtnd9RFBG0L9N5kbCQFpkxn74wx+W+9///lXWlA+kxvJQL3jBC+pkOpaGIlfqyvWaWbPNtOr85JMou64tttiinsuSVmbfNETkdre7XXnHO95RZRGWnHIN97vf/cq2225br1PZSaSJ51y/5aaI62Me85haB8961rNqXT3ucY8rX/jCF6qYQVnJnDKqR9s5r/jIRz5SRZfQkT11b3/vuR7bkz7zGzieWWTJrPvq+puomQPBNRNR99fEO/Z3HPXj37vuumvdv2UhSbz75b1tttmmXp/76Ho22WST8uIXv3jUaz+HMFmYsCLoG7zvfe979VtB3/D54+4DwR+Wtf1Gxx8LM3T5BiuEEBFMJMZjRASvxUQqGvWWQeqKIMG5wx3uULN6hOqf//xnFQCZMxOzfOADH6hZJtJFWN73vveVBz7wgVVQLE9AdvbZZ5/yute9roqF/yYtROjII4+sgkNAHJcYWv/XdoToRz/6Ud1WG0RGkgje7W53q1KqDLJ52ijaLLJcxIyUfP3rX6/ncSzZsZbZIovKSmRk8ZTpCU94Qq0j53c89eDc3iOfzm/4x8c//vEqqKRvv/32q2X961//Wvc1a/jNbnazKoIt89ZEUN1530ydju+86smajbKqpPNjH/tYzbh53zU7B+FUJucnoa7LdXzrW9+qr8syKsf+++9f77M6kJG8853vXI/761//uta97KZjyv6tTgS9T+xlTbX97Gd/bcGvfOUrdU1mUqttp+sn3AOS2O6J87gGrzvWzW9+87qv6w1hqjDuRNA3Mf6I+cX0B9Efin/84x915ix/6Bv+aPjj+9GPfrR2SfBHzxTMxLB9KzVa/LHwB1EXhRBCRDCRGI8REbyWkUTw1re+dc3+kY6WIZNlI1kyR7JAxsq3bo62lZWTeWtjxYgAMZPJ0mWwyZiumcpIdvzbT+cgRsosg+hcZIWU3fOe96yhi6QMVhciRZZudatblc0337zuT9zM6m0CGWJj8hgoj3WQyc2nP/3pOu6fmArvaf889KEPre0mZbV2MJHUfVX7qbWJyK+2FNmRAesXwQ9+8IN1xlIZwZalbGhzkUDi3OYcEI5H7tTjDjvsUF/TBVd5CJ51EeHcyuGa1L26JN/Op66bfKlT77U6HySCjqMrL6nTBuwmAOzjHu25557lTne6U71G/3b+JoLEWZmbIJLSn/3sZ1WOdeeVsbR9CFOBcSWC/jj4Y+yP6dve9rb6C+lbG3+U9Ff3x6+xtiKo+4NvBweFPxa+AfNNX/97vk3yB3Kyo878cfcNpD+8DfdDHZBzf6DD1CAimEiMv4gIXssgEfRlrqzOK17xivq5TQoaxoTp5uizXmaPbHzzm98st7jFLap4ycpZjoJ0OLbjOr6umG0ylyZ+hNPxCN7ee+9dM2KWaTJWjcA0Edxss81qd0PZtO4X2WjLRxAo3Rt1udSt8R73uEfZaaedapfUVn5dTrWDZC4tMaFbrHKK73//+7W9pB3jGpTV645lLN+gMZEnnHBClc/+rqGypa6DNHdx3dpa3tPN1TjDVlfu0yc/+cmy6aab1roiV8RMXeteatujjjqqimW3bea/1Z9rMqZPllVZ1G+TOgwSQTOr6lJK6mUA++tWeZVB/QpjPdVlE8G3vvWttXtuw/aE1UytrsFz1H/MECYr40oEL7jggvLFL36xfksmxe8bGfKni4Jvt3QtaH8g1lYECY5jdMO3Zb61uuENb1hudKMb1T8Q+tF3t/EHoztYe7LiA1gff99I6tLSUPf+OPoDOugDZazwQenbWR9GPvSUwQd12DhEBBOJ8RcRwWvpF0GNeZk+WR2f2/0ZLZ8pZEt3TBOMyGTJbmk3yEqRtpbBa6FrJ0nUNtH2cHyZpu23377Kn26aD3jAA2p31CZwMn9dETTOz7CT/gxTmzW0TRYjk6ZLpbGE2iKkp2XJSCepcm26NSpXf1kFOfNZqh3l2IbPqJd+DK0hYP0iKCMoa6od1UWWk3jpNko4B51bzyp10toJMm7GaKoj8mgIDmlr2Tnh+rThZOhkX02CQ2zdq5ZBHSSCJE77zb3RbujHsbULdZclr67RM6Id45lxnd31Gb1HFo2l1O5znyOCYaowrkRQP3K/3LoLdPEHwbdb/uDr5nDMMcfUPyqEcbQi2MUfCdnHtsYe+fHtmD82pm323lRjJBHUh17feRK+pnodhH18iPbPitbFB4dvLn2I+HCXEX7lK19ZvzHNH+ONQ0QwkRh/ERG8ltWJIPnqF0Fi1i+CMmA3velNa5Zt9913r0NR+kMXTV9KOp4uoESMLJlsxNg00iXrR6x02+wXQdtpo6xJBOE6iA0RlNWUSXMs4/R8DpMa7aNB5ZQNNNGJc/ksJzQye4M+Q03+os0jE7c2Iuj89tFNtf/cfvqCvrWfZOC0JYwRND7Tl+6ysdpZTXChvnzWyyqagEbXWPfPmEbXPkgEyS5hJoLqqL9u/dt+uvyqS9nBlhH0zLjv3Tkh1BERdO0RwTDVGDci6JeOdPhjasB2P/7g6npwxzvesX7z5psp/x6tCPoj4A8U6fBh44+HY/rA8QfMH3h/aP0B9cfLh4QuDlNFCkcSwXVF11LjBNT1INwXg+bNLuabQB8KMoJmO9O1RLnChicimEiMv5hKIii7tTrWVQTJiB4vTTBaBqoLoRBEyJehMl/ETtdSY92cUybPl5fKrGvouohgQ2ZRltLxfJGqK6ehMrpCmiSl/1horzn/LrvsUr8o9wVrf88an7naSXr6DBojOEgEXY/PZKJGptrYui7dMvnv9m/3RlfOn/zkJ/V6zQpK8tDdTltLplJbwcQzyuIeDhJBPcXUm3YD+XQPuthGV09jKvXyMrQlIhjCYMaNCPqlJCJ+Ea3H048/Ir6RMggaa9M11C+0bcif/vy6Keha4Y9Bd5CxP7a6ivhD5Y+3c5Iix/NHZDKzvkTQjFzukzEMg3Dfzfrlw7y7ZpPxArqKGKfZ/YAJG4aIYCIx/mIqiKBJ4kiVLpg+u/ujfRavqwiSB5k2XypbrqC1Bdp5vE8A/SRXxrPZ1vg8/27ndAyyYfKVdc0INuyrS6Wuito0vpgmrbJqJmQhd62cwqQx5Ex5/FsWUzvGtZLW9rqfpEwGUz2ZCGc0IgjLOWgfGKtoPoF2TEHiXHf74lx5fOmu7to2hFY7zbIYvvRXH7bpllvoGaZeZER9KTxIBO0rK9lk1nbda9SusM8NbnCDWm8tAxkRDGE440YE/dHWNWB1IqhbgqwR1kYEyaMupz5cdAFxLOfzR6sreP7ba7od+KNnW3+M/fFr3ScmKxtLBH2YG2Ohy2/3XhiM7wPe9NPdbiRhwxARTCTGX0yVjKDPBFJCsmTAhMW+jZUjLARtXUUQRESPFRPGyLbpSulc5igwkYw2iSUXCI3xe0RMVtB2yuKLZW0LkqLL6KAxgtdFBH0WmmTlfve7X70WXTmdj8DpFSU7KEOpfoiTzOHnP//5OsmJfbVh1IulGbRhiJf9tZtkyVyDuRHWRgTVp7IYI6gc2lGO6bPbvSCIbVF912bhdt1CzbFgOzOVGgvo/MptUh1Dc2xHstW7bd/61rfW7qEyg56HQSII2UO9hm5729vWbqJmHnUM55a9VE++ZNYrqe0TEQxhOBNKBHUb9W0c1kYEfWj4g2I2LX9UfAPVlY5+/AGwj219e2a/9oek4Y+XD45BfzBHwgeOP5YGTPuD5o+Ybq6+ITM+zodP/3nUiQHPxNQfVQuf+qZP9rJhTIFj+5ByPJk03WoGjcvzTaIPGB+GZit7+MMfXvc1dfIgEfTH1T057LDDhl5ZgQ9Gf4jboHTnFT5IfBj5hk7/fR8aN77xjetMbt53fN9oNoiefvztw6jhHvmQbZIYNiwRwURi/MVkF0HtAOP/Tb5Cznx+9IfPXp8v5MA4N585TQR9jsgCkas202fD56TPUt0biQi0A0w8Ytyb7BJ5cA4/7373u9feQ+YkIHGycsYR+hxrk6b4DJWRMgaOYHziE5+oguMLZT2QdO9UFl0Z+0VQ1s7nsHO0Reu7kDmZSp+Zhq+4HvJnCIvjdstKQn1+t898deF6iZ4hNOqTFPpvxzSJjF5R3TGCPrfVrS/MLW3RjzqWXdSGUVe3v/3tV57fcdWBMrpO7QWzb971rnddWU5SbMIbMqb9QMLdQ3XYthHaeSbjMd7ROdW767csiH1bG8lPXUxdo3ZCtzzOpV3hHjhXq3uzz7s+k9L0i6Avn7WHtOs8R+owhKnAuBFBMucbrfUhgusD0z37A+2brNFiAhSDpXV5IH++ufx//+//letd73pVlPxBJIkktCErSf5cp2v3B1a3jNZH3x9C0yP7tsz1Ox7x8o2fP3b+iMIfQhLomzwfGmZJ/Z//+Z+6j319U2dCnn4R9IfaB0hXxvxh9UGh/v3hdT7nFT5w3EcfWIS0vd7Cmkm+IfUB7BtIZfGh1NZLanjftfmgJMNhwxIRTCTGX0x2EfS5JpPmc0dmsD+ImM8WjXhZMpk5mUKfbz4zfI74olMWj2h08WWw9oEvYbvZQg1+2SCvm/SkncewBDLVjkMWCJ05DHRLtJ21Ckmpz2WzZPrZeho5n89sZel+pjd8/hkC4TNfFrEf5SKsP/jBD+qXxHrG2I4gKVurE5k253Y8n80N7SBC47ps45oci/ipM20QbQT7wbINzkOWjKkbhGMae+eL41YHfiqPdkOTNOX0+S1T2LbxpbL6bGMW1SdxdKxW77Kw5gxQJt1L4ZzahN6TqVO3DfVqW1lAsugY7r8vzm3brQ+4duVQtm470fOjfl27tqXnqHueECYz40YE/fEkBLp/Dsqy6XJBePySYrQi6I+HVP+6BnkjfcoJf5h0d/DHfrQQQdLlWyfdH2S8/AFz3b5NI0quSV/6BhH0TZdv6nxj1b6lah80xFL3EX/QfRgQNLOt6SrxuMc9rn6DBn9U/YF0fl0z9Jt3PdYI9E0iCfQN7GhE0B9z2T7fSvr2zFgO5xW+EfWh48PCt3GObdYu1+V9H+JE1geGDyjl8WE36Ns3x/CNJXkNG5aIYCIx/mKyi6DPNY17oke6+sNnvYybzwuf8xrsXeEjF22b/oa8zxyv969TB9t6r53XMWzXzSbBfzuf921HahxLmW3vvba9130RO6gscGz7eF+5B+EaHcN5XLNja4PYr1sn7f1+vOb4rbz2UxbtAu0GXTNbFlEZCJw67ReoLq7L+doxW121thGUU53oVdW2IVr9x2313o6l/tVhtz5s41i2aXLYxbb2affOz5Hq1P6uz3X23xPbe085B+0bwmRl3IggfDMmA+SPUxd/BHQzMBjbLFH6xxMg3ULWJIL69fu2al1DRlJXyHX5A0EEyZO+747lj5c/Rv4o6XJBxnR/8K1c+4NJBHXD8I1Z98PL+8olI+dbMN9+tT+S/pD5ptIAdsLpHL55031C33sCSCJ9SAjdX429NFPXaERQt1PHIq/GWrRv+EBidTFxzzDSGEHld03K4xvVQeiyoxupLq9hwxIRTCTGX0x2EQxjAynTDhBd4fF573PVF8W6SMrGaX+EEKYu40oEZbws40BgdH+Ab5oIj24MRIokGkOg371M08bqGnpdUH5ZzTbzaRcSpxuJLqK+rfONFoigaaBlDrv4460OdLU07sE4CdLUQsZOnRFnHwqETRdUmUPfmvVD/lp319WJoG/MPvShD9WsJtlcE6sTQQJrjKD7PgiCuvPOO1cRDhuWiGAiMf4iIhhGgy9+dY/0Gav3lM9hn+HETy8bXyD76UvYZL9CmNqMKxEkcLoJmryEzBA+oqdbpnFl3muZsrUdI0h+LFrqmKMNXSzNQjVWEMGXvOQlwyZeAVlT9lve8pa1D3vLshFBMmc2rS5EkTD/3//937BxeN3QDVR9kE8iaIxAk8wuJp8xaH1NIujbRFJn/J8s5ppYkwgaPN5dNqKLbiW6nUYENzwRwURi/EVEMIwGk8/4fNVzypfP2go+h0005wv0rbfeun4265UUQpjajCsRhMyYMWTGsOkCKow5MzC8K3hrK4Jm4NKlsR1zTaGbpq6nIy2Efl0gggSq282yoSulbOdNb3rTOptYmwxmJBGUESTIpk42i9igaxBm9VSnupsSQYIrq9cPqSOpaxJB4wmIummZjflbE+sigu5rWysobFgmqwgecc6scvJFc8tpl8wbMf578bzeNvPKf86fUw46IzKbGD8REQyjQRvIGH0Tt/mCV9vjVa96VRVAk6pYUqENJQkhTG3GnQiOlrUVwbXFzGCOP9YiSGosmKvrBhnSf18/fl03fVNnuuU2bTJGEkHi6I+82UGJo4xnG0PYBqmb+cvrxgWYPcwspY4ls9oGRMtEGi9odi/iuyYRlE38zGc+U2c4VTazcLVvFZ3XsXwb2T5kmgjqqtodS9hE0BpHxjcOwnhDM4aaQChsWCarCJK8+YtX3xVq2fJryqKl15Qr5y0pp/S21/g+cMCxEokNHRHBMFp8vmsL+azXu0b4b5/T3QlwQghTm4jgCKxJBGWx9K83hnG0EEFdObfYYovy2c9+tk4rTcos9mrwtvLLtnVnIh1JBMmdLOl973vfOoaQVMn6WfOIfFlz0HhLkggCZzye87/2ta+tM5bq9krwdtlll9p95CY3ucmoJouRCXRO4wyMTzT7p5lEbSPj6BvHti6SaaotuGsGU11ezbZKUAmoDK0xjr657J/BC+TVbKlmbQ0blqkggkuWXVPmLVpWZi9cujLmLFxWFi5ZXmWwbXPWjPm1AT7oeInEhoyIYAghhLEkIjgCaxLB67qOoEyazJuxgP/7v/+7ci0/QuR1XTa6s4OOJIINk6lYGLatC2hcoOPqYmo8QBNB4mh9JrN9mpDGNra1+K5zm5mToI5GBNsCr7KRzqP8jqXrqayjJSTalNRk2cylba3Bto6gbyudx+Kv6nDQWAWCaNH9HXfcceiVsKGYCiI4Y87icuz5c1Z5/+AzZ5ZTLppXLu+9t7QngSCD9ks30cTGjohgCCGEsSQiOAJrEkEzdlrovYnWaCCCxuFZBPdTn/pU2WyzzaoQkjPXIVPWP4PXBz7wgbp0hizfSDiebN8d7nCHKmbE0FgAawT2TwxDBs0c6ty21R1VRtB1EDbX1F3QX5dRx5Nt7KJrp6U5jEO0HhEBtLyHmcis/9eVWestEjrns3zGO97xjqF3Su326d61DGJD1xWT+xBUM52FDctUFcFuXDxzUVk6lBm8at6SctwFI2+bSGyImCoi6O//oBiE133eGI4wqGfJINo+2gqj3SeEECYjEcERWJMIEiHnkWkbLU0EdQfVNdI4P5PC+Onfgz7ovO4Drl8QuyiDbbrHUz6v9x/Th1733K7BB6Jt2zV1Pxi9Z9tB5/eaffrP2//Bajvl627TMDusmc10Le1CimUCLVNhop+wYYkIXl0OOuPq2nUUc3s/T7pw7sDtEokNFZNdBH12+Jwwtv3iiy+uSwi1MMZt0Oek7b72ta/VL0zNfj3oc7Qfx/cFqHVs9cJZm8/xEEKYTExYESQU/pgbo0c0DII+//zzq8iM5oNgTXSPP1Y0ERy0fMRURddRWUFdUH3QN/75z3/WWc4+/elP50N6IxARXBFXzltaJAVNHmPfQdskEhsqJrsI+lJXLxRDFO52t7utEj4nfPFrSEH3y16TjemJYhtr543m899kbR/5yEfqUAXH6/ZgCSGEqcSEFcGJSERwOL7h1U2W9LU1G2UDjTNUV76tDRueiOCKmDl/hQguWLy8jh0ctI3QQD/vioV1bOGsBSsmnrHvjNlLyumXzC+HjKIu1PNpvW3VvX3rBDa9Y10xd0mZftWicsS0WeXEC+fWcntvkJgefs6slRPfWArDeU+/dH5vnyUryzXt8gXD9hNn9La7dNa157a9c9v+mPNmD9ynhfPaX9mvnr9k5f5mXr2wV/Zjz59dM6z9+x169sw6M+slvfPqftvKfnVPwC+eubicMH1uOeTMkevOe+f0yndp77z279a9e3HuFQvWeI+nX7Ww7nNB7+fR586u16pb8NWdejB2dNC+GzomuwiaKM3EZ5tuumkd424YAcnz86lPfWodpvCa17ymHHTQQfWzA75A/P3vf1+XXzKJ22hFUG+fW9ziFhHBEMKUJiK4AYkIDsaYQmsdmeXUBDFmE1VXxj62ZSjChiUieHU5ctrsMm9oW0JAwvq3IQgn9mTlyp4wLVq6YrbR1gxVB/6teynZOHo1MkV4yIdtjUvstmWX9/6xeNk1VcrOu3JhlVKcddn8Ycc5sieLDQJ3bk9OzYTaLZfztO0ti0HiLpm1qNZLPffQdlje+/eiJcvL5b1zW1uxe64W6mWV/VcpeymLl17Tk7Ilw54H6zQS3Lm98pmYx7YNxzBJz1U9IRwkoaTS68TVLK/O27//imVAlldBJMP9x2ihXuEnqZzRe/adu12H465ORjdkTBURlP37whe+UM4+++xy1lln1Z8mLDNJ2/3vf/+y0047rZyQTI8RPXf8u8nhmogIhhDCCiKCG5CI4GB8eFtHUb2YNMZkNbqG6pobNg4RwavL9KsXVUFxLef3RM5i9N33SaBxgzJHtiEjMllnz1hQs3FNcpqQ+PegzGAVmiH5qOLYK99FPVlzDAIjU0UQvUfq2mymaxJBx3Stc4ho71pk7ByTgNn2wJ5M2Z7EOTbhlIE8vyebtpVNI1GtXESpX2Zl0NSjbZYsW16FmEw5z5m98sn0yagpN4lp+8luqo/FvXpxPbZxPvvJil7UK6+snv2O67tHJNA1OO8KAbymjuHsXqM6c8z2/pyeyI8kg00EXTvxnN8TbeJr2RDHOrUX6mrQvhs6poIIWmfX2HBdRLv4UnDfffctD3vYw8q2225blxeCcYWGclindqSx7MYdkkVDSHQrNa5QN9ORRLAd0z7C/l5TBv+2fX/msXseYSbs/vHyIYQw3ogIbkB+97vf1dk5fRsZwnhmqopgk7tpPSkhbyRC90YzhvZ3bTxq2uwqPrYhHLJ13a6MxNH5dFG0DcEjFd1jCF1Ku+ciH47tPQIiY6dbapOy1vxckwg6H5EiZMSrv/zuK2FqEqg+nOew3vmcl3Qc37tuUub6nJskdo9BemXk7E8o1VO775bbIIoyhqf26qHbvVK9KJv9XJesqm6i7X3ZWHVpO+VsrwvHbPJKIv23cnevUZ05Lzls5SOGg7KLTQRJKaEkr0RzvHQH7cZUyQiSve985ztDr16LSeIe9ahH1a6ixgbi8ssvrzNL61HSP/u0pY4svaTb6Dvf+c66Tu+XvvSl2pX0Qx/60EAR1NXULN22a/tY99fM2ZY68tqZZ545bB/nsV3b55Of/GSdMZs8jsW8BSGEsD6ICIYQhjEVRFCj/8KeKJCZFrpSykQ18SFxxLA/k3do798Eq7dZlQzdLcmHrpbd7cgQ+VswtEi9LpLd94koQVFffhovN5KAkCXy1NqUaxJBkuSaupm4FsrleATU4WTDSN+gtRLJnUyZ8zp/V8x0zWx1tbrul/2hnnUZtS8ZHbTNoFA3ziP7qD6JODHsr/cWhJIM2naQyIomgrYxlrJlTMdjTOWMIMnbfvvt65JHJK31GDEu8H3ve18dV6g3SZMucrbXXnvVXjjk8bGPfWx50pOeVJdaeu1rX1vX5+2fLOaqq64qP/rRj8orXvGKuo/llIxVND7x9a9/fZ1J3Ozkxii2YQuyf/vtt195+ctfXvd5whOeUB7/+MfXpY+8pqfLaLushhDChiYiGEIYxlQQwZHQjCRuBIWoDRIzr9cuob1tZZwIx0jdB4kF2bAt+ewezzkIEQmRhVxTnRFXQoM1iaBrOKonSf3bCNJKiuDcumOOJKCkl1BC1kz2rYmX56SJoG1GOkZ/yIK269YNdjST6YiWDYSxkidftOYlPWQW1btnkvD2v99E0DXo1tr//niKqZIRNA7wve99b/nLX/5Sw7JQsm1PfvKTyzbbbFMzbU3eiKC1cYlgWz5CyPoRskc/+tHFDKE///nP62tmqCZo1r299a1vvVIE7eNcBJGIyhjKNFpK6lvf+lbZaqut6sykRPDggw9emekzjME6ucpm7KJePzKHMoLW2JW9NH4x3URDCOORiGAIYRhTQQRJjSycrJLuiWSBmCg7ySNwgzJkglx0u3MO2qZFV7pIY7d7Iglxzjoraa9s3f0GhWzYaCeLkcXrH9fYgsjOlInsbUcY+7uN9odulr1irsjg9a6lSS9xal0vZU9dw6BuqP1BPFt20z2o3WF7kjdSfbcgfm0/4zEHbdMf5NHzDPesv6tpE0HjMJWr+954i6kggrvvvnu5053uVO5yl7vU7B8p23zzzeuaf7pdkkBj8RqDRHDOnDl1tlH77bzzzuWCCy5YKWLe++Mf/1jFTdfQ4447rorg7Nmzywc+8IG6dAWJM3t128d4QV1DZQavd73rrcwI2scM146166671vIbg2h7+yuvcjVxDCGE8UZEMIQwjKkggt0xgjJSuoA2MYNunCNl+k6uYrQio6U74bHnzVltdDNq3S6U3W6h5LJ7jkHRLf+aRFCmrV96WugGSgCdmwCT3kHlbmESlpbBU2+tTtSPZ0U2TX04pvMeP31OLctIzwA5lg1dIdMr6sVxCafxkcYLDuruaawh8VSO7jO6ulAHZlCFcqrD7j1tIqhLsGegu+94i6kigrJ1smm6cT7lKU8pd73rXcuWW25Zx+F1JRCDRPCUU06p+1lqQsavH+P0d9hhh1XGCFrT9vnPf36d1I1s9qPb6Mc//vHyv//7vytF8MQTT6zZxac97Wk1kylzKetINM0JoCvrjW9843LAAQdUAQ0hhPFGRDCEMIxuI1vjU+N7bULWarxMud+NkUSwBUGRHaSClk4wDnCQzFhK4brQL4IkDLo7ylx1zzEo1kYEyc9IInb8BXPrfVpb+kVQqENjBZvUQebw6l49mnnT8zNIpmUAyaBykEjY3wyf512xoEpmf4ZQ3RFSy2mYdbT73urCeEgYN0lqB4mgn/0zlI63mAoiaIygyWL23HPPKnVm3/z6179e7nvf+5Y3v/nNdRxgd3bQQSJ46KGH1kwi2fN+P21B+a4IHn744TWz9573vKcuadSPsYDWvO2KIPEzJpDs3fKWt6xdTQeFcYdmNQ0hhPFGRDCEMIzRZlsmWqxJBAkCISMb1ISkDJIDXRQbHIYgjSYcTzfIdpxrRXDxRhPBtSm/jBrp6xc75zmtJ1i61NpmyOvqz9kLl9VrGySDvixomdha50P7wXXKEHbHHV5XEXQORATHN22M4KBZQ0nYAx/4wNrV0/IPjZFE8JGPfGT5/Oc/P2wmUQxaR5AI6vpp9tFp06YNbXktMoK6jPaLoMlkyKDlKL761a+Wr3zlK6uE65GhXLRoxdjWEEIYT0QEQwjDML7sqHN7jWIZmTWM25pIsSYRFMRD98aWpbpk5uKVyzm0IBbeJz3nXr6w7jPa6EpIE8G2REX3HINi7ERwRddQ12CMJCEbVNZBMdL4P9clg2cMpEltuqI5d9HwsXn9+8nGWkuwjYGENf269UKiZR5lHEf7ZUV3jKb7ZRxjRHB8sjoRNObOrJ8mgPnVr361UqxGEkHjC9/0pjeVk046qW7X5fzzz6/j9/pF0PHNKHrUUUcNbXktlqIwgU1XBJ3nWc96Vs1UtrGGgyITxYQQxisRwRDCMEzIYXbIQY3RiRyjEUFBoNRBzzdqBqp/7NiKyWJWZMlk87rvrU3IoDnH/J4QnjKKGTBXzDK6olG5LiLouo1LdG4ZuzVN7rI2QbKc17hD3WxBBmXzVtdd2JhA75vg5rLevVG39rPUBJmzjS655Nnr6q7/GIOCmBrv2dul3nsy230/Ijh+WJ0ILl26tL73gAc8oK7T19YRHCSCjmMmz3vd617l+9///irj8ywqbzZQE8l0J4uxzxve8IZyn/vcp2byul05/ff+++9fZw29XmeyGEtYKIuZSXVf7Y5fVA7LRrTF6EMIYTwSEQwhDMP4OBmj2QuXXrfoScZEHCPYDTNi6oJIIEhCd305/11n3ey9SUy6+61N1HX4eucgPaNZfkHmkLxhXUSQbJFKyJKph26WbCyCXMrmNVZXnv4gfjJ/kF00XtDrpE6dQcaxO95ypNC9VGbRM01M+9+PCI4fVieCIH9kjXjtsccedXbOQSJIGvfee+8qe7qIWlDe0hB/+tOfanfRxz3uceWOd7xjXUewiaB9fvKTn9TtrQGoG6iun8K6hdYhNGlNd/kI+1gqwthCs43qVuo8RPEPf/hD+cIXvlC+9rWv1TUNlSuEEMYbEcEQwpijzTPaRv+GjLURwcN6MrIiY7eiCygxNJul94jKeUPdDXVTnN6TqjVJ3KAw1nBerzyaiGuatVK3ySZHWBcR1BWTILWsG8Fc3VqI1yUO6tWHzGnjrJ7ojvaZMJFMu1Z1a0kKr6tj8ud+uC+ymmRzpAXlqzjOXlzlmTi6//3bRATHD5dccknZZZddqlRZ768fGTayZkzeS1/60to18+yzz64TvBBBUteEy7EI4BZbbFFnICWXuotaJ3C77bYrL37xi+sEL8cee2wVQcjwmaxG91OT09iHdJq9dMcdd6xdQImgpSSIIGbMmFEXrrfYvPUP7UMmTVbjfN/85jfrMhMRwRDCeCQiGEIYc7R5JroICmPSjFnThJMBbKIm23XC9DlV4kBMpl+9qC5vYAH2tr9xcWTIBCUyft2solBHNSvYMxVSZmymbVsGjPjoomuMG5GR2bId1kUEhaygOsCy3g3z3+3crauo/dWReiDC/SJFys7pXZcytu6bbT/bNslyfa69HVc9Ol5bP7DtVwW1V1/kzXWKU3vbdCVb+UwUQwS9T2LV7dE96WuzjLo292JG7ziyuu7PxTMXDayPiOD4QRfOQw45pHzjG98YOE4P1gTcZ599qiiaQVS2TfaNQHYnkSFeJn356U9/WrOAsnWf/vSn6wyeln2wjwXgTRzTZiG1z3nnnVezfF/84hfrPp/5zGfKvvvuWyXTchA3uclN6njCNkbRPqRTttFagvYRJrdRRllMmcMQQhiPRARDCGNOr220WgnZWLG2IqjhTSaalOim2BZpd33EqU1uQnaMKyQcul0K4wdlFZ3Te+Sk/xwyWiTE8Xv/q3JpIhT7O9/sBcuqyOhu69htEpZ1FUFyRVIdl1TVcy9acW5S6/wmzVG2ub2yuwbl6R7Dtq5fd2BC265bOR3XNamXi2etKmGyqaTWcd2Htp/zqq8mxt7rn01V1pJUKpftlF03ZuMAW7mJpPrW7VZm0X2wHmL3OC0iguMHQma5CFm2/vUCG6TKOD/dSGXa/NsYPv9umb0GSXM8mb4LL7ywCpt9nMdP4mifbrbOfzt326edxwQzJpKRebTmYFfu7GMh+csvv7yKpXAu5cz4wBDCeCYiGEIYc7SrJoMICtkuUqGpaDxgd/kH4yCtNWjs2aKejJCXa5uUK9bFI3FkiTTpqtg9dgtCRl5s5xgNkmNymKvnL6llJ56t/OsqgqJlHK1jSMyIVadNXMvvNfLpGrvXLmTiTDbjGrv7+W+T7Kg3Yua6uxPSEGLCZxvX2PBfjkWGSah6GTTW1GsEWqbUOer5VxyisqLeVnQdJZ39mdhuRARDQ0bSMhGt2yeInKyjzKIxhxacJ4ddeQwhhIlKRDCEMOZoI41HESQE5EGG6IyeSLXs3uqiyZ4sFznpn01Vhur46XNqV0ddFmW/HF/IoJ13xcIqbd1lEAbFMefNqdsZE7di/975evsbG6gbKpHSFbN1VVWm/mMQhXZu3S+7XSpHCuPrCKTjtWxeO4br9Zprc7xD+u6p7qCnXTK/1qltu/uZ7VO3zkHPgXKaAVTduMa2n7pzLMLr2Gsas+j6nIPsdevdMR3bOZxr0L4t1Ll9/FQPg7YZLxERXL8ceeSRtevob3/723LEEUeUo48+unZV/d73vle23HLLstlmm5UDDjigZv9CCGEyEBEMIYw541UEJ3oQHhlCWUOyNGibxOSNiOD6xdhAk9CYBfQFL3hBnVDmuc99bp0wxgQwFo3X/TPdPUMIk4WIYAhhzIkIjn3IjunOSgJHmgEzMbkjIrh+MSbQRDVk8DGPeUyd9dNSE1tvvXWVRF1EQwhhMhERDCGMORHBsQ0SaFbMtpi68XrHr6GraWLyRURw/WMiGWMETRhjohk/LVuRMYEhhMlIRDCEMOZEBEcfljxYESvG63XfI4Des2SC5StMgtK6hY5m/F9ickVEMIQQwlgSEQwhjDkRwdGH2URnzV9aLrhyYZ0UptWbSWrMmmnCGV1BW0LC7JYjLYWQmNwREQwhhDCWRARDCGNORHD0oZtnW6bBRDCkr0WbGIYDLu/9JIHdhdkTUysigiGEEMaSiGAIYcyJCI4+TPpiMXTr+HXX4/ODBFos/cqeLJ7S204XUV1FBx0nMfljsoug8XkWh7cguzX9QgghrF8igiGEMSciOPpQT0efO7suZn789LnlRHHhitA11PqDJorJmMDEZBdBs3Luv//+ZZtttim/+c1vhl4NIYSwvogIhhDGnIhgIjH2MdlF8JJLLim77bZbedjDHla+853vDL0aQghhfRERDCGMORHBRGLsY7KL4KWXXlq+/OUvRwRDCGEDEREMIYw5EcFEYuwjIrgqxhRa5++qq64qV1xxRe1aOnv27Pp6P9YBXLRoUZk5c+bKbe3bvz7g8uXLVzmm7a0jOAivG9Nou7atc4QQwkQhIhhCGHMigonE2EdE8FrI2gknnFB++MMfli996Uvl85//fPniF79Yvv3tb5dTTjllFSEjbBdccEH505/+VI9v25133rn85Cc/KZdddlmVP5ig5sQTTyw//vGP67Fs941vfKMceOCBZf78+XWbhv3+9re/lW9+85vlC1/4Qt32a1/7Wvn9739fzj///GGCGUII45GIYAhhzIkIJhJjHxHBaznttNPKpz71qfKYxzymbL755uXBD35w/bnJJpuU973vfeXMM88sS5YsqUJ21llnla985Svl2c9+9sptH/SgB5UnPvGJVQ5JI1n8xz/+Ud71rneVLbbYom5nm0c84hFl2223LWefffbQmUu5+OKLa/me97znlQc+8IE1bOvnU5/61PK5z31ule1DCGG8EhEMIYw5EcFEYuwjIngthx12WPnYxz5WPvGJT9SZRs0yKpP3jGc8o9zpTncqe++9d+3euXDhwvL973+/CuPTnva0ety2LWG074IFC8qFF15Y3vSmN5X73//+5e1vf3t9/Re/+EX56le/Wj760Y+W//znP/W8S5curZk/oviEJzyh7LTTTuVnP/tZ+eUvf1mziE9+8pPLPe5xj/KRj3ykZi1DCGE8ExEMIYw5EcFEYuwjIngtumoSPT9l/YwLJHR//OMfy21ve9vy/ve/v5x33nl1LODHP/7xKmhf//rX6za21x3UeELh30cffXQVxa222qpKZusuKlN49dVX1/28Nn369PLQhz60PPzhD6/yZ/8G6dRd9ElPelK53/3uV4444oiVxwkhhPFIRDCEMOZEBBOJsY+I4LU0+WsTwJC+//73v+WnP/1puctd7lK7eE6bNq0KnGM9/elPL+9973vLqaeeWjN19nWMhjGEr3nNa8qznvWsOu7QUhbErityuprK/t397nev3T91Ee3HfrvuumvZbLPNyg9+8IOBE9eEEMJ4ISIYQhhzIoKJxNhHRPBaZOJk3Hbffffyhje8oWbpbn3rW5cb3ehG5X/+53/KdtttV84999y67RlnnFF23HHHKme6fu6www51bODll19e3wcpNNHLc57znCqSxhPKIB577LFVCEE6dRXV9XPfffetAtqPcpFR2+gqqitpCCGMVyKCIYQxJyKYSIx9RARXoDuoGT9N9kLuTNryzne+s2y//fa1SyghfPe7371SBGXyzOT561//um5jP1k9GUCTzjRZkz00FtC5X/va19ZxgI985CPrDKNtaQiTzjjnfvvtV5eO6Mcx/vznP9euoWYTjQiGEMYzEcEQwpgTEUwkxj4igis46qijahZQ1k43TstI6Nppwhczf97udrdbRQShi+bcuXPrGL9DDjmkjhu8733vWyeM6Y7zI3vGHp5++ul1whhjBk008/Of/7wKJQHddNNNyy677FLL248so7Lf+973LnvttVe6hoYQxjURwRDCmEMEDztn1sDGbCKRuG4xFURwt912qxOxkKiR+Otf/1q23HLL8vrXv74u0yDrZizfjBkz6gyfN7vZzVZ2DfW6MYGyiG1coP8+9NBD69IQJM9+JoWxjqCftrGfMYDElNR9+tOfrq/pZmr5CRPLyPyRywZRPPzww8uLXvSi8pCHPKQcf/zx9VghhDBeiQiGEMYcbZ+jzp1dDjxjcIM2kUisXfhd8js1mWkZwQc84AHlAx/4QDn44IPLQQcdtDL8+5xzzqk/deskcbb377/85S91KQddOY0TbCKo++bvfve7ujC8WT5ta2ZP3T1l9rbeeus6KyjBk8mz1IRj2c5YP5lHXUQtRwHjBWUT73nPe5YXv/jFZY899qhZyH/961/lRz/6Ud2eBGb5iBDCRCAiGEIYc4jg8RfMKQdFBBOJMYmDzphZTpg+Z+g3bHJixk0ZQYvCm9RF189umMjFZC3G8X33u98tT3nKU6p0ee+5z31ueeUrX1lnC9U1tM0aSgS/973v1Qyidf8cw/YWfn/Zy15WBVMW0FhB8miZCe/ZzkyjZhEldd1upmYe/eAHP1iXiXjc4x5X1y60nbGH/tvahmYwDSGE8U5EMIQw5ugMdcal88shZ2acYCIxFuF3ye/UZMaELLJxhOzNb37zsNhmm23qkgzG+Zn8xfhAwuc9+5jAxXhBs4Lus88+dbye7prHHXdcFcy3ve1tddttt922Zg8PPPDA+j4Io8lk7NvOZ/wg4TResEvrIur8Mpdte//tNVnLEEKYCEQEQwjrhcvnLC6HnZ1xgonEWITfpYtmLhr67ZqctAldZAYvuuiiYWHMHlkkb8bemaHzsssuq++1cX7GC3rNdm3GTj9NCKPrqW0dn/gRui4mirEAvfPYjkgaTzgSuoleccUVK8tne2UKIYSJQkQwhLBeWLb8mnK0cYIDGrWJRGL0YXzgMefNLvMWZwbKEEIIY0dEMISw3vjvxfPSPTSRWMewFMsZl03ubqEhhBA2PBHBEMJ6Y+b8pTWTkdlDE4nrFjLqfoeumrdiLFsIIYQwVkQEQwjrlXOvWJg1BROJ6xjGBp5zecadhRBCGHsigiGE9crSZdeUUy6al6UkEom1DL8zJ100t8xblLGBIYQQxp6IYAhhvTNn4bJy4oVzBzZ2E4nE8CCBfmd0CbUuZwghhDDWRARDCOud5b2G7KwFS8spFyczmEisKVom8MqeBJp9N4QQQlgfRARDCBuE5ddcUzOD0y5fUI6cNntgAziRmOpx+DmzytkzFtQvTiKB4x9rGlpPsK1ZGEIIE4mIYAhhg6GL28Ily8slsxaXky6cWw45K0tLJBLiiJ4A+p248OpFZf7iZVOyOyiZshD8ueeeW84+++xVYvr06WXevHnDFoHf2PzlL38pn/jEJ8qvf/3rKoXjAfV41VVXDaxH4fU5c+aMu7q8rixbtqw+N+edd96wa73gggvG5XMTwnghIhhC2OAsXX5NzXhM7zV6T790Xjn+gjnlyGmz6nppWWoiMdnDkhAHnzmzPvPH9Z79My+bX78cmTV/aVmy9JopOybw6quvLn/4wx/KtttuW1772teuEm9961vLHnvsUU455ZSyYMH4mUX1q1/9annkIx9ZPvOZz9TM4HjgiiuuKD/96U/L2972tmH1KN7+9reXQw45pArSRIHIkdvTTz+9LFq0aOjVFcyePbv8/ve/L+9617uGXes222xTn5v//ve/4+q5CWG8EBEMIWw09HyT/TAW6uKZi8r5Vy6sU+XrGpdITNbwjJ/Xe9Y981fMXVKz5JkQppRLLrmk7LbbbmWTTTYpD37wg8uLXvSi8sIXvrD+JFuPfvSjy4477lj+85//jJvs21e+8pXyiEc8YlyJoCzYRz7ykXLPe96zPPaxj11Zjy1e97rXlX/+859l7ty5Q3uMf5R1n332KR/96EfLpZde2vt9ufYXZsaMGeVLX/pSvd4HPehBK58Z8ahHPao8/OEPL9tvv3054YQTyuLFi4f2CiEgIhhCCCGEjY4G/pe//OWy+eabV5E55phjytFHH11/fv/73y9Pe9rTygMf+MCy++67l8svv3xor43LeBRB3WgJ01Oe8pTyzW9+c2U9tjj++OPLlVdeWbtUTgRI32WXXVZe8pKX1LrWtbUrgp4FIkj6dthhh3LUUUfVaxY/+tGPypOe9KRy73vfu3z961+vxwkhXEtEMIQQQggbHSIoIyiDs9deew29ugLdAffcc88qibqOnnHGGUPvrBAFGUJdBHUfFMaMEbPu2DDbOc6sWbPqe/4t06RLqtB1cKSxZF73vu0c3xg7IqVr6OpE0GvKYh/7Ot+gbKbjK7/3lau7ny6cTdps571uOfrLTASJ9Ate8IKa+RsNjj9//vyVx/VzpLF1smrOa3v72a5t3xW0tp332vuD5NM+6rZdb/f+OL/9dO183OMeVx72sIeVk046aZV6IYJf/OIXyxOf+MTyne98Z5UyGy/5gx/8oGy22Wblne98ZznttNOG3llxXmVsz40yuqZB1wyvtzoS9nN+ZVVmz1a7fvfYtbuf9nN9g+6X8nWfQf89qI7Qnl3HUVeO2a1vOHa7H7ZTxpGyoMroeN1tu9fQ8G/X7ZzK5nj2EyMdO0wcIoIhhBBC2Oi0jKDGvgZ9PyZm0dWRCBorBg1fDdjjjjuu7L///uV73/telci99967/Pvf/67j5VrDW6P7zDPPrNvpXqor6q9+9auabRR///vfy0UXXVS366KxS67+9re/1e0c/2c/+1ktAwEZNEawCcpBBx1Us1L2UTaTyhAZZe42uAnAb37zm1oG9XDggQeWH//4x3Uf4yanTZtWG/7K/K9//avKjWP+4he/KOeff34tYzteE8Ett9yy/PWvf62vrQ7HPeecc8qf//znlcf1849//GOdgKV7bKhDZVK/ymU79f2Pf/xjZd21e6J86sz7f/rTn2pZu/WrnlwvYXW9zr3vvvuWI488sooGcTrssMNqxu++971v2XTTTWudf/e73y0HH3xwFZiWEXzCE55QM6Bd0YLtZJK7ImgbYqOMBxxwQD2vcnrGLr744mHPgHsrE9meAffFfiakkWXdb7/96vPQJP/CCy+s9an+3Q914xzGMrrXIFcE97e//W2tb8f1PKrfrmApq+dYPTiP43imPAcyu3B/SJz79bvf/a7Wt+08pyeffHK9xw3HU7d+Bxyv/c74b/WuPrsyal/ndl9cy7HHHlt/h9r1d5+NMPGICIYQQghho9MVQRN8NDQ0ZTl23XXXmi382Mc+Vhuk0Jg28ckrXvGKcvvb377c4Q53qHGLW9yiZpA0bslC21aD+453vGOdfIZQ3OMe96j/tr1s49e+9rUqgw0NcvJArB7wgAeU293udvX4d7vb3aqQbrXVVnVc2mc/+9mVIkgilK9di+2VTchMKasGP8mB69OAl1l8+tOfXrOMujMaK3nrW9+63Oc+96nXTCqVTx0ox21ve9saxr/JkDYJaSL4/Oc/v4rL6iAPhx9+eHnve99br8/xWh3e6173Ku9+97uruNquQTIe//jH166aykWy7n73u5fXv/71VRpsS2q973XX7bi2+9SnPlVlEK5bWT/3uc/Va7/Tne5Uz3uXu9ylvOpVr6r3Vb24Pvvf4AY3KNe//vXrfzumshGpkUTQ8d1z94FEfvKTn1x5bs8EKTeesF2vOrXdLrvsUkWuHUe96mb6wQ9+sN7rVkeem/e85z21rMpDTpvkEcBnPOMZ9T7uvPPO9csC+zz3uc+t1+y5cj/f/OY313q2v/N7Hglr+6IDxkB+61vfql19PROOo64cXz0rpzIaA6k8jmcbx3S8973vfasIMKkkcp4112C7dkzdr/3uKWMTPOc3wZDr1i37xS9+cX2OH/KQh1QZbPUUJiYRwRBCCCFsdFrX0Ic+9KG1Qa8RL2QvZCO8/uQnP7lmo1rGRiP1hz/8YXn5y19eG/yyK943VoyEyIrJCMGxTDhCrsjGFltsUbubakx/6EMfqttrsLfGNWQ8SNKNb3zjKhrOJQMmY/nMZz6z3OY2t6kN6c9//vMrRZBIGqN3s5vdrE5w841vfKMeU3bmTW96U23Me52kOo8GtwledG281a1uVYXM/rYnJcopE2YMnMY3kdQAb11lXY9zqAs0EXze855X66LVY4vW/U8ceuih5WUve1m5613vWl760pdWyZO1cp3Pfvazyw1veMM66QqRaHXiXrQyOccXvvCFmu2TXZNJkil8zGMeUwXWfZAVJEmOQ1LIhGO5hySJADm3zJT75z6Sv1/+8pc1s0hEZayIpAlhHItIG+vo/SaC5JRE+9LAdXpuzJ6qjpSHnBEw0qS8REh5CKJr9mwQtzvf+c71Gu0PGTWzj6pn+8jc2d74UBlqr7vXvmRoIigD6PlwLHLlfroXRxxxRJVl0kaoiNpb3vKWWk7n93yQMs9jE3v3whhHx3F9Mn7f/va36zbqwv0kd6RSOYikZ0sduTay7p6AALvHvsjwO/DpT3+6/PznP6/1rozqWJ34kkTdQv0STMf2+7H11lvXY8jiEuYmjGFiEhEMIYQQwkaHCJIEWQqNa41+jVKNYKJC9jSwNfIbZEKDXWNVI9+/yQgZeOMb31jlkUQ0CdL98Ja3vGVt8MqWtX10sbP8ALnZaaedasNag52MKQMpal3/HN9P3ehkvYhdywgK3QtJJXEkEV5zDqE7ItklJ695zWtqN8AmgkTmRje6UX1fXdjedRAk10Eq3v/+99eyec9xlY+Iel3XRRBBjX/bkwr12KJl7mxLWj784Q/XLJiGvu6hjtuuz3k0+skp0VTHaFIuO0aKWp2QO/9NREhrEznH9LqsmuORLRlMAkMOZW7dV+WxrXp3T/1b3Tg24SBhBFqW0HZNopWLMHtGlLX73JAd5/BlQHtuZAUJqvpWf56Bds2nnnpqvW+vfOUrq2iqY9k4z4vnz/Il3TrSTZLweabcp64IyuApE1F2rfZp4YsDMi3D69lzTOEefOADH6hl1+XUazKfRNazazyf1xxP3QrlkBl1P8ila2jPteenjSdUXwRUef2OEWPvt3M7piypL1vUta6nUL/bbbddud71rle/NPB70OosEjjxiQiGEEIIYaPTRJDAEBbZFl0rb37zm9dujrrSabj2Nz79WyNXkBdZPFkk0tHGG+qGSQSMrSIHsnyO1bAvYbS99zTICYfF4i1lYT+N5S4awzJRpKiNETS+zGtkhCRpXHdRVg18SziQS1kVr5GTdr3eb/t5z3W7fg108qqs8NO1EgDSSwBs30RQ1kdWjkQI4kHQNOptc+KJJ1a5IUqkoP/6/JvUOkarE8hckTfr9HUn7VEe4kuwCINMk2sxiydhcqx3vOMdNeNJMhzfciD3v//9qxCRU1ID1yHguMpLrGSkBs0aSgRJvGt2rTKnnhvSY3xb97lRhwRencrUGiunjK2sum+6PvLomtWXbBwBbuVreAbce8+r56eJoLGezv3Upz61Hr+LY6r35zznOTWj55zt/O617rO6YZJUz4G68VzI2BFRzxlaHakfzx0J1G2YgMsON1FrQa5lkj3PxJ8cer2LLyTcE7KubFC/ZFRGUB20TGGYHEQEQwghhLDRaWME25g73RFlZTSMZWSM0ZJB60ImNIJ1bZN5IVhEpI3lkhky5kkDnQjqZkcYZGT6G8Ea0LJBGslnnXVWzbLIEpICDfVB9C8foZGvnBrbMoZN2rrIbun6SOycUzmaCJrp04Qw3bLp1kdcjGcjuA3Hth9JeMMb3lDLaz/SRBqMIdOFTwZHXbbQ2CcwJqQhPcZ/kch+HMtkMORRFtHxITOl7OSgZQlBPHSZJMGt7kleC0IpQ6k7ZLtuMvrqV7+61heRMQ7U2MA2CQr6RVCZuvWjDOSbAHpWXCPxdE+axLaxgfYj9Z4P2UOZN2Layui/Zfc8I+TY86dudV01lnIQurfavl8EiR7JVvYuxNQzpVuxuujWkeysLypIrC8wyBrZJ6OeM8fUnVPZPCcNz54uy+65a/MliOded91WJvXkmK7RlxTdOmzY1jIb7q/fJzQRdE89M01Ew+QgIhhCCCGEjU4bIyirYfwbmgRoCBMljds2Fg6kRjaIIBBAmTETwRg/5d8Esl8ENb6NgepHdkgmpImg7JYMlmxQv4A2+kXQfsRDRkomaFBj2zgt3U9li4wNs00TQcIhq9mliSBRkhFqqBtZy0Ei2CaLISQjofvis571rCq73cxew7GUhRToxqo+0ETQdXaFjQgSPFk5QqKLo4xqNz7+8Y/Xe9ykUsaLZJMPde246oWEqEuMVgSNsdSN03u+IPBsKCPJ1G3TtXiPsMnikn7ZL+Po+stISAmYe0iCfcHQytyP588z1S+CJNv99EVFF1k/10Fc1X333O38ng/1IgMp2jhJE7+4Z57ttkg+XJeMn/N6tj0PZNMYWddu/US/N7r4Ek5Z3W4dNpzLlwfuny9j0ETQWE6SHhGcXEQEQwghhLDRaRlBYtO/fITsmoa+jJlsCIHQ3U8m0HgmMzeaIEP3Oo1mmTkTb2jwX1cRJF3kREasjZfqotEss0g0mgiSD2Ihs6NsJKYfxya2GvQmLdEgH40Iyi6tjQiuafkIoiHLSPJMYtKP48vYEXP10GR4JBG0vW6XsnDEwTHVR3+YTKeN14N6JCnO5f7IEOpWSerc534RHNQ1lAgSu/5ZQ4kSUTepDfGVYfvJT35Sj2VyFe87dn8ZyZv7KSNIBNXloDUZyS9xM050kAi6b91ZaGGspGfZdeou23/uFo7VrpPYqmuZW11G3/a2t9V7QBzbOeH6nM/viy6lyuALBBPMGF+ofo1PJXndOmz4koLY+vKDnCMiOLmJCIYQQghho7M6EdTANe5Npo3kyApp2JI8ImZsIWnQuCUCMjlkgpBdVxHUoCZ6psqXOTLpRoMAaNATUOPD2mQxsjJEwznMBqmctm0og/c1tGWZdNtU5o0hgmTHxC7OKwOm/huO41pclwxfd1bSkUTQPgRcvevCSHK6wqe86qgtqi66E5nA+DP3SzbL8hze9557IVPo/stetu0xkgiCZOom6zkgbO6HrpkkzBhNckiyurRnxX1Tl7LLvlDQ7bQtRQLXa9ZVX0ToZtqdLGZ1IqhLp8y161GX6rnhmLrtes11+Lc68aVHe46UzaQusnYE1/G8p2z2tY9wf00G5IsMcqfujXskoeqK5PqdaagHomeiHPevZZMjgpObiGAIIYQQNjqrE0HIMMlukCiTXpAQ66HJEpEqjVwZQQt5kzkyZrzadRVBDV4i5TXjrkgGiZBJk+3TTc94K+uvWQvP9hrkulC2JRlIiGPI0NhXw997jmkJABKk0b4xRFDDn5QqCzkg0+rQ9em+6Hpl04id8pMMjCSCIBakTPdD98T9MbbOMWXUZLPabK22lcUVztfOa0ZP3SZdA2l0TWTEBCvGz3k2CAmpc09XJ4IwTo6UES9dV9W17qnqTRndS+cWjiuz53qdWxl131U/bSkQ8ud94yHJvi8KjOkz7m40IqjeLTVh8h4ZbgLZniv1b7kN5WyCbNkJk7Q4pm08D8Yl6h7qvnteSLrJXeynfH4PZJtlM9WL9xzLs+GZNFZTxtwSE+6P6yGXMr8k3Dbti4GI4OQmIhhCCCGEjY7MRmugDxJBWRHdBTWgjQPUlVAjX2PcxCSyIzJ0ujtq4BMrAudYbdZQ459k8NpEGF00hDX4iWDrBqlMzqlMpI+IWsdPxkSXSl0MnbtlBOFcGu8yTjJJfpIY+zo+sdKQNwYNTQTJD3EYJIL2dV1tnB6aCDpHm8zFsXQrlAlSH6sTQZi4hnyTJOc3rtD1ERSZJMcw+2c3a0UEmzD3iyB0XyQUsnckxDU5pmP7b2JOsIiOTKs6dD7bkF2SSVKIY5M69544E3tCpi7cF/VGVIj9SCJIOu3rHipXe26Uv40VdG7huDJtZKxlgJ2DaOoiK/usjLJmroeI+enLgH4RdN/dt34RhO6tunWqY9fjmO36CaTnqYmg++N587pt1KEvQ3yhoJyujxR7BtSdcyqfLwL821jCNsmQZ9REOrZ1Le67Lx88n/7b82myIyLZ6pFkEkGTxUQEJx//v707f969nOM4PkUOopA0ljQlFJJkayHVJBmiwrGcFEkU0T6klBYqlRZblqLFUrZji9KxyzJj0GIwhJGfQv0DH/O4nPeZq0/395zvOXW+3++Z+/Wcuef+fj/39bmW93XdM+/X/X5f1ydCMIQQQgjzDlEh+mL/kz1Nk3AipD14HFNpfQSCCAinngPv5eHf9vTZ4+QwDhEfIpADK11x8eLFLRI2Rj2iXMQEgQTOsMiIqA0nvG+Dsy+yI4XRSZQVMQMnXgRTZIWYkj7ovnp4+DgN02EedXBIL7rAiXddaiwBUbhPPUQxIUQUusZxF9UiBqWvrgp1iAyxqX2L+kp0iHiKVI0dfxE9dhIdm+lRAtI3iV1ipWxG7BkDIQbRU3WxJdFUZaRwilL17dY8mHsCR1n1E0AEm8ijNSCqOxaCILpFGI1R1JEQJZwJvKrPy7ilgOq/tFKwKbsTsNaOcvqrLqLKKZ5+DBBtts7gusimeRsL+4LAdCiSHxSqfWtFvdZiwRaeE+mz6qO5IfIJT/0zDw6AIej80KCcv0V5CfNKK4V16nvk4fREpfK1z5a49iNIX5597TckMM1dv87Duk+EYAghhBDmHQ48R9peJ0JqEhxUETfOdUVf3OdvAsjL364RifYRqouz7EVcEJw+G0McKK/+3hF2n3Q+n1UbHG9lqr/qU26M9vS1Tm1U93hPGvRX/bU3rIcgcd1rfK+y2tef+ky/2IADP1unve4RXdNXfV7ZmNRddp4J5fStbMbu7NXX6X59164yNfcz1ev+Klu2VNb1mutJKNOvG31wrdZD30fXxuP2P1vWGtAHbVozRJnoJzFd9vbORuasX0tj3K9ctd/3r/C3az6rPpqbcb3WSb9G1av+8VgK5fv5mWm99Lbz+Uz1hXWTCMEQQgghhBAmQGwSVBUhLFwXrRVREyEUPY1ICusaEYIhhBBCCCFMQPqpB6nbnykKKGIoIiflUqqvB8N7fEilE4ewLhEhGEIIIYQQwgTsV7Uvz+Eq3u2Vc0CM/xctWtQOW3H4zDhiGMK6QIRgCCGEEEIIE3Dqp0NdpIB69IjHKDgl1GmjDiRyuIrU0RDWRSIEQwghhBBCmIC9gA5oIfjq0RMexn7rrbe263VITwjrIhGCIYQQQgghhDBlRAiGEEIIIYQQwpQRIRhCCCGEsBpIB/RsNydI1jPvvNsrNn5WXlh7sLNn23l+YFI0Q1h9IgRDCCGEsCBxEqOHd//1r39tx/h7+dtDz2d6ePhcoA8f+chHhm9+85ttnxj+9a9/DV/96leHT3ziE8Ptt9/ers037Odh4X/+859X2M/ff/nLX1Y8IHxdxmMcvvvd7w6f/vSn2/69uRTgHuhunsumvW3ZPAfIhHWBCMEQQgghLCgIGE41J/+cc84ZjjvuuOGoo44a3vWudw3HHnvs8MEPfnD4zne+08TMfPCjH/1o2GGHHYZ3vOMdwx//+Md2zQEiS5YsGZ7xjGcMP//5z9u1uYCY+/vf/z78+9//vkdUjGC+4oorhne+853NdvU6+uijhw9/+MPDz372s+HOO+9cXnphItIq+mo9iMD2Ys/4jOe5z31uE4PE2VygD3fdddfw0Y9+tK3H3rbHHHNMW7PXXXddE4QhLGQiBEMIIYSwYJBaeeONNw7HH398O7L/Wc961vCc5zynOfteO+64Y3u98Y1vHL7//e8vv2tuKSF45JFHrhCCTpF873vfO7zuda8bfvvb37Zrc4H2CY9ly5Y1cdJzyy23DAceeOCw8cYbt/4+73nPa6/tt99+2HbbbYeDDjqo3beQI4Oifvr4nve8pz3KoRd7orEf+tCHhje84Q0tGjtX6aHEKRG6xx57DJttttnwtKc9ra1NtvVDwFOe8pT2rMHLLrtswQvtMN1ECIYQQghhQSDi89Of/nR49atfPWyxxRbDPvvsM3zsYx9rwuuXv/xlexEFrp188snD1772teV3zi2ThKC+/+Mf/xj+9Kc/zel+NRE/ok6qaqWpFoTgm970piZQpLH++te/bo8/+Na3vtUE4hOe8IQWbf3b3/62/I6Fh/RLEUzP7jOe3rYEmZRcaZlzKbi0K9q3++67D7vtttvwuc99rv14wb5s+/a3v33Ycssth+c///nDD3/4w+V3hbDwiBAMIYQQwrwj0iO9kkDx4G7pnyJcnO4xytqD1e/Dkq5HjLnmb+ml9hH6v+rwXoeLiDx69Z+PUQ/hUeW9a/vHP/7xPYRg1T1OXyzU47OqR9lxOXX7TFmfVV+rfN9Pf7t+8cUXN9HhnWjSRomlm2++uQnBl7zkJcM///nPu7X3m9/8Zth///2Hl7/85S2NcYyyZcPZ2KqYdE8fxRsztq/7tV3tE9bGsOmmm7bxWBPVj7KRMVcbrqlTfa75X3l19/VPouqr/lTfa+7cC20TgkTgK17xirZu+zH+4Q9/aGLQjxnnn3/+8qv/Rxu9jfr5moR6+3XjXtdq3P18VL0+96pxV79R9qn26/NJNil7KFdl+3EWfR+9tDtTff3Yvddcjqn+17qvPmtnUt1hzYgQDCGEEMK8Q8RIcZRm9/73v3/51dnBMeQkfvvb324RRdEhh4d8/etfb6LNgTOcSfvoRBW/8pWvDF/4whfa64Ybbmifjx1cdXJWCT2Rxy9+8YvtXdRPRPCZz3zm3YSguu0NtK9ROmMP51c5n1199dWtnl/96lfDf//73xVOrXd16xvx4zN9/fKXv9xe6taGciVE1PPmN7+5pScSS5/5zGfa3kn76YynhODee+/d6q62cNttt7WUyhe96EX3iKwqx57EopRLY//Sl77Uxm2eJokB93DSpcWyu3uMVURMSuckscPJF+X7xje+0eZCW+bNnkDt6799eFIwpbeed955w1VXXdX6wRbuJ8IIWfskwdYObFGfa4SjNaAvbCsi2tu98L95E9lTznitDXV4mPy111473HTTTa1s2f+FL3zh8LKXvaxFAvvx6Zs9mFtvvXVb04U2jE0fli5d2mxkvtRP9IxhZ/P2ve99r/WHfcwJe1o/+vSf//xneen/pyeL/JpbZX7wgx+0udBvfa450l61r87f/e53zU69TZRXtzWobfY0T6LH7A7l/a09fVSGndnbOMf1sbv+W2/KXnPNNS3CP+n7J/VW/7XP1sZgXqSDV/vh3hMhGEIIIYR5h2P72te+dth5552bI786cDI57Ntss82w3377DaeffnrbR7jhhhu2vVqcR87qBz7wgZYO+cAHPnDFa5NNNhne/e53t/RC9RQiaJ/85Cebs9+Xl6560kknDU984hPvJgQ59/pvP+O4/xxY9z3ykY9sdTzoQQ9qEcULLrhguOOOO1oZQoIT/ZjHPGY49dRTW8qmvWbVrvRP/XdADmeeCFDfBhtsMKy33nrD/e9//2HRokXDU5/61CaeiNiVCUE22WuvvYaDDz64OecFG3C87b1jTzasPojUKk8g9XX5m5NvPPbIPfShD11xz+abb972TRI8dY93Y5DOutNOOzWRp+yDH/zgJkwJDieznnLKKW1M97vf/doYH/CAB7T/iS/rhS0c0GLNEDXQj0svvbTVpT/SSkXmqj8EPPsQawURQuDYh2h91JjNBfvZj2ou6geKstFMQlBE0EFC5uyzn/3s8qtDE0fsao9m2ejhD394q0NK6Vgss8MrX/nK4VGPetSK/j/72c8eDj/88BbJtS+x34965plntnUpbdo8Pf7xj28/Epx44olNPBF21tgLXvCCFfV5qUc/CTIYH1Fpn676qhy7vO1tb2ufsZkfXIg6UdFHPOIRrQy7i1DbH0nMF75f1q/1oYyyvgePfvSjW1/NZx+5tL6tBVFr4zFu7ft+SYHu119YcyIEQwghhDCvcBhFekR+iCsO8+rAcSV0nv70pzfH1Z444o4zKiLFaRfp4ZxzZD/1qU814UAwcIK32mqrdupkiTIOsVRLQkhUh/ATFbnyyiubY04sECSThCBHl9NbiFZxYjn/+qSOs88+e9hzzz2bWPU/iAAREuKO4OKsn3baaW3/mVM/n/zkJ7fyIimccBETApNQqYigcREUIorKTBKCXmzx+te/vqU2skFFMH1GABMABPKuu+7aonDaZJ8DDjhgeNzjHtfq007dQ1Q5KIdgIcqk9X7+859vEUoi0D3a0l+Yb/YktMy56Jk2RM+cDnv55Ze3ORAtNA/24hGL5557btsTKVJkrghBc2oOx0KQcDR3xJoxEDqip+ZOm0QWStRZF2zvMwLSvLzvfe9rYtm4iMlVCUG2YHvChQj0o0RFKkEEEkkvfvGLW5+04ZRRa+OlL33pCkFOEFm31pK9kYcddlizpTnQT+ucPawH0byC3dnajwz22bInofb73/9+hWhzj/XOlmymzHbbbdf2M4oOEox+yCAejdmaZnNlzbE1T7QpJ8Lthxbi2mfm0Bp8y1ve0kRpRV7ZxH1sby7POuustj4uueSStj6IYmtNvRUZVbfDospeBD8RK/I4KXoa1owIwRBCCCHMK9LSPH+vnHCOdk+JDWmJXsSKQ0LchxKCHOeHPexhzYEngjiiRAdR5G8pcSJNIiPudY0g4Wz2ok7qISeeQ01AEF3EKWdaitpb3/rWJg4INE4upCj2QlCfCSzRJE4ux1e/1UHAcMq1QVi6TkhIASVGCFAiVnqfPlZ0RoSHcFCvMenTRRdd1Bx7dmMDn1VkxX1OBiUaiNFddtmlCTUixTsBJlKqr3AvgUSIisZIsyXI2IrNRLpEKgkHAkz/RPYIM6KOuCV82avuYXORPSm/RKk+ui5SJ1pI4LimvLbqOZHGQGxo033sLW3UvTWnIkNjIci+bEcIio5KjVQfOxJNhxxySBPVRHbtaSNq2d18mHvzox73SaEk0tjePSghyEZEOCHHnsSUCKd3gtaPA+ZVX60JgokAJ/L0XRvGS5QR/uxUa52YIsZF0cyj/pc9lRehVF8vBAks/VEX0c2e1kjZka2cwivaZoxs7p2AI9SJLfV7rAghqR6pm/qprO+ecfjbvFt7yliT0pFd13dC0roqcUzA+l6ok5Crto3H95SIFO0joNkVhCDRaC36nJ3cY75qvYZ7T4RgCCGEEOYVjqaTQDl+IkBjIcgB5GwSGsp4SRkjHpUtIcjx5+hKXezT0sB5VI5Trj6OJSdU9ImQOvTQQ9v/xFClhBJ65ZgW7icutNWLx7EQVM7+JmJSXwlODjZxZb+f/xcvXtz6KxLUC0GpfxzysgNHnrPO8RdBIYKLj3/84y3SIt2xIppFCcGNNtqotUMIeokKEUMie5x0ggdsKOrF+RZVY8Pe6dYfjvy+++7bImHGx6kX4dRvfSEExvcQV6961aua0BeNJE4IV6KFuBL5E+VxX70KfTI3yhIz7FSsTAiuv/76TbSyVdXHjsSL8TvMxbjNr+iXyJuotPv79okV0S3ifBwRJAQJVBE6AtA8sJ3+sk0JcnYUKXvsYx/bnjtI5NRa8M4W5sXcWn/WQ0UJRfP6MVtX1o+1Y231qaGEoDZEnvv0ZO3bO+h7o87rr7/+bu1bd8SrHyVc06bIo/pFA81xzYuxe9cP9hIxdcCTPlUkv8rBHLE1wWit+e71mBORQDa0rvyw4n42sl7U7++qL9y3RAiGEEIIYV7hQHISiS/RPNGGHs7xGWec0YSWqILUzyc96UnDCSec0BxSTiLBINLF0SUsXO/hgBJrRII9eASS0zTV9ZCHPKSlDRKCHFeOP9FJ2IzrwU9+8pMmylYmBIkAqX8cc0KTo8+x5Yx79z/RILIjQtQLQRGfEmcFASZVjrNurAUBSIDYbzcWgsYjImlfFeFpX6CX/XpSE6Xc2btIZOov8SnaaWyib5Ocb2UIVWMQ/RMVPOKII1pKLpHHsR/DHiKa+qFtYyVSX/Oa17RoqUNrRCdFn4y7b9dYzQ0h6J7ZCkH7CicJD5FZcyDdUt+lborSmTeiqsRb4X82FtG0bqB/hKB6CCg/SJgfqY4EmnRJKb3VV31gM4JcNFX0rV8LhKnxmQ9RPHYVYRUJI8LGiEoTm2MhKDVUOqd1yzYFca5vopo+r3a91xgIWuuL6BJ9FFH0/fC5dW481rjvIqEGa4FNfG9rb24dKFNl/OBizbKLyHFdL9jSmP1YYh5FOF3TD/0xN3VIT7jviRAMIYQQwrwiGmR/EsdPVIRj2EOMEVwiByIthBoBMUkIckiV7cWEiAgBRBRxeh1uoR1OpkgV8VVCUNucW055iYsxTkVclRAkiAgEDjbHX3qjKMv4RfhWpKuE4IUXXngPG4hyEm2rKwS16z7pp+WEsxkh49EG7CFNU50EtPIEmz71NiyIJ3YnEJzSSkixHdsTj2MhBVE5IkF6qlRLGC87ighK1xSdFBESXdSPavveCEHR3opSFfaZieQRgmxCtNj7yA5Ey1j4r0wImleHtph794keEtXGoU7CGISV9WZurT1tj9cBMW3e1cGuxit66f8xKxOC0pwJ4P4+YlLd9g+KzBGl4/b9AODHAXVbJ8ZHDIpg6jv7+BGGkK4IoTWufVF8YzI2P6BIMWUTtpMyykZLlixp0cdJmCNroP8BghAUDfQjgbUY1g4RgiGEEEKYVzh+Dtwg1KQw2j/XO/xjRLcIh0lCkDPap1XC/yJSIi+cVCltRIgUPM6uQ0VKCHJyiRZRDgJgUj9EtUR2VhUR5DRzvvXzF7/4RUvX81n/IqREQGcrBAnXSULQ+0xCsD8spkcUSwSVk84B57RLh5X2Kp1zLIpApBEShLIIj31j0nZFm0SyxtFcsBFBQbxLSyzUb5zEvYggwUBg+ruETC8EVyc1lBAUxRoLQWuHwCghyP4EjlRM62EsZEXziNNJqaHjw2IgoilF01pjF/9XRJCNiDTzMl4L/lfWHjjrS6qqU0zH6wD2HhJnMwlBaaiigIW/pTtbs+bBPE9ai+ayUqqtFXNpr5850xfpr6KE1mLNs3GrXySPvZ0ASjT7bliP5s8PLuxkb2L/vYT/tet7a+7ZxrVeCPoBIKwdIgRDCCGEMO9w6h0cImokoiJNkCM5FiMcTw6n1LbZCkF1uS5lj5jg5HoRDfYmSsMrIUg4iORxeqWPcpj7ujjrIkMOOuHsznRYjH4RCKIcHGF7qPrTDn3O6a5UuzUVgtIAOf/2mY3TCEsIjiOCIDgIJmLG2NlFm6I7xiZS6Z6yv3vZxuE5oj4cfmMn2Mybg02IBWMvUeQe4xOFI0ZFySpaSxz1aYaiS1JpiUV2L7uWEFT/OHXzvhCC5tOPBB7jUHav+TZf0oDNKxuPD4uZJAThHuvN3DsJ1jjZiF2JyXHk0ZiU0R47qM+eOeMSRevFtTkmJtVNCI5PDZ0kBN0vymjerB9CsF8LxqN969Hf+mN+ql1lrRepqlJLfWfYvtZvjV3ffQ9E9v04oR3z4RAYKdjSuwnvfk2JlvrhR79FRX3ueoTg3BAhGEIIIYQFAcfRQRcEHeEgisERdniFSJS0NU62SAvBNVshKEVP1IvIkLqnLg408cA5dnx9CUFo0/41+/c4rxxz9xAixIJ6nHK4qlNDOc8OyrB/TiRGeqo2vEQIObveOdJrKgRFHTndUvtE5PSHCNB+CUHpgPYh+r/s6MAbdRG8RCSHnCNPsDoB02Ey+iEd133GLnoqeuYeEUjChWgwP67Ztyl6xn51j3YIO5Fej09wD4GhrwQAuymrT1JV7Z0k0ghwSCs13549Z5+baJh0TnXcF0KQmDEPRBo7iRaLDOqTiKnTP0WM7eecrRDUL2JJBM66M1/6zK7Sbolt815zIfoqCljrxvyJXFuXfmxgK+XY05hETp2OO1shCOvCWEQlHY7DjsboJSLrMBl1mR/91R8RPO3qpzlma3NM1BqjKLL7HDCjHmXsEWRH3wPfZ99PItQPN7431oCxK+9+KeHWoX77u+YrQnBuiBAMIYQQwoKAEOH0EV+EiHS8imJIhXPghJTEOq6fUBkLQXsEx0LQZ5xkn4ukqIvQ45yqT9qh4/o5p+BEExPEi9RO6XCieuqu9Dh7/1aWGloQVkQtR5dw0K5yopMin8QuMbWmQpBDz2HmoIvEcPYJB3Wypcid8RHIxlB2JJ6kgBLT7FWI8Bk7EexF6LjH3kTRWveJapVQA8FLYBu7OtnHGN0jEuQ+0cISzRUlUhdRoyy7+J+tnFRZKYoEqiikQ3dEycyB1EvtlxDUBoGHXgjOZo8g4aWeem6etaUNY2ZPc2Te2Hc2qaGFtWQfn3knnMyHk1iljLKR+a+50B8nb0pXhrrMqz2Ixu1dOfZkHy+2mkkIihiOhaDInXXh+2TtWkvsrg/myzwQ08oR/74PRLn1VntpfSf1k1A25+ZFarH1px71+dv3luATVYQ1Jd3Xd5Y41p7yIsTmzjrzmIw+GhshODdECIYQQghhQcHJtP+MqOE0c9BFMpy+yfmWombfEmecI895tM+Ik8uBHadz+ps4E60iVtTH+eQ4E1iccpHIXhBJwSMuCIE6bZETS2SIcBCQ0iel+UHkhDDk5Ir+9IjkECxEhrq89MMhLSU+Of+iMK4TDOP9fqJsImv2mhlrQQhz4IkWUSuigYMuYiYio41qs17EAmGoHDHUw57EALtoj/PvHrYnkAi+cQqqe0SSCBn2N2d1D6GkHXNVEPzKsq1om7KEFtHKBsRcYe4IBM/YE1lVljhgU2LHdWMRmQLxQewZowimtdTjmYGidISptFBoQ/88XF3Ejh21RfSIcBE1RKi9o1WeeCTOHHJCjFW6Y6FdY7HvTXuV5qyf+ltj0ZY17WCY8ZyLtKnfWlWWoPYjiSirCDZRKQpXiEI61MV6IKDHWBNEK4Fa3ykva44da/2Lwooqqsvn9X0hlkUHzZ859+OH76g9gcqZSyKVALeGlCmIQd8b4/GDjPJsYH34AWRcXpSU3axf6zisHSIEQwghhLAgIY5EdAiDeokUjZ1ucM6V5YBP+hzq43CqR1mOp7Ku1f6oHv8TONX2+B6Odd3jWu2ZGtcDznM/FuVEiXr8T8iUo92j78Y2Uz99xvnv73dd+WqzfxnXuI0x1Z/+nklj69F2f09vozHqq7JsM2nchfEra4xsx97Kqt/YfQ7X1KNO/R/X51rN9/izmtfejgSMU2qJJemwhXuVXdl66+urMtXnGrd3fZmpjt6eNU6pzkQo0e8wl0LZVdnR9fG8lj17xmtHvdru66263K+MOo1tZW3346nyk9CWeiet93DfESEYQgghhAUL57F/rYxVfY5Jdc2m3tW9ZxKT6hmzJvUWM9Vd1/vXbFnb96xJ2dkw23KT6NuRoikCKU1SdHBNmNSXamPSZ2P6csRgf6gOodozm/pQda6q/GzKzaZMz+qWD2uPCMEQQgghhDCViDZJdZVuaQ+jCBe8S7uU7iuFUSqmlMm5QDTMAS4OF5KCWhExos+BKtKP7Q+UcptoWbg3RAiGEEIIIYSphOCzT9PBQfaYOrTG3kDvTr+019CBKE7QrPTTtY10SCeoOpjIPkoHq9gvZ2+hfab22HnkhUcthHBviBAMIYQQQghTSUUEHRTjlEoHnzj0xCE5hKHTTT16Y9LhK2sLgrMOdXHSplM49YkIJFCdNDt+uH4Ia0KEYAghhBBCmFoclOKRHE6qlXrp1FEnWS5btqwdxuJQm7nE3jmHqDgZVSRSX/Rp6dKlTSD26aIh3BsiBEMIIYQQQghhyogQDCGEEEIIIYQpI0IwhBBCCCGEEKaMCMEQQgghhBBCmDIiBEMIIYQQQghhyogQDCGEEEIIIYQpI0IwhBBCCCGEEKaMCMEQQgghhBBCmDIiBEMIIYQQQghhyogQDCGEEEIIIYQpI0IwhBBCCCGEEKaMCMEQQgghhBBCmDIiBEMIIYQQQghhyogQDCGEEEIIIYQpI0IwhBBCCCGEEKaMCMEQQgghhBBCmDIiBEMIIYQQQghhyogQDCGEEEIIIYQpI0IwhBBCCCGEEKaMCMEQQgghhBBCmDIiBEMIIYQQQghhyogQDCGEEEIIIYQpI0IwhBBCCCGEEKaMCMEQQgghhBBCmDIiBEMIIYQQQghhyogQDCGEEEIIIYQpI0IwhBBCCCGEEKaMCMEQQgghhBBCmDIiBEMIIYQQQghhyogQDCGEEEIIIYQpI0IwhBBCCCGEEKaMCMEQQgghhBBCmDIiBEMIIYQQQghhqhiG/wFHBhffS0YmEQAAAABJRU5ErkJggg==" alt="2.PNG"></p>
<p>사이킷런에서 비지도 학습인 차원 축소, 클러스터링, 피처 추출등을 구현한 클래스의 대부분은 fit()과 transform()을 적용한다. 비지도학습과 피처 추출에서 fit()은 지도학습에서의 학습과 같은 의미가 아닌 <strong>입력 데이터의 형태에 맞춰 데이터를 변환하기 위한 사전 작업</strong>이라고 말할수 있다.</p>
<p><strong>사이킷런의 주요 모듈</strong><br>다음은 사이킷런의 주요 모듈을 요약한 것이다. 자주 쓰이는 핵심 모듈 위주로 정리한 것이다.</p>
<p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAfkAAALJCAYAAAC3JFrUAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAAEnQAABJ0Ad5mH3gAAIa1SURBVHhe7b1Nbhw5E657F3RmWktvwTNrAW2gdWZ3KAi4gGBYa7iQAavhyTmTO5dREM6o52qooT3kjSAZZJAMZmaVJDsov88Hfp2Vyf+i+JBZ5az/awEAAADAuwSSBwAAAN4pWfL/43/8DwQEBAQEBITJg6aSPAAe+Z//83+mIwD8gHEJPALJg+nAZAo8gnEJPALJg+nAZAo8gnEJPALJg+nAZAo8gnEJPALJg+nAZAo8gnEJPALJg+nAZAo8gnEJPALJg+n4uZPp43L314fl+j69BGDAceNya1xh3IHX4STJ33/+sFx8e0yviPvr5cOH6yWOxzg4P3xog1wH4GWMJ9Ptscdjd3g9jGN1/jOfxWQL9rEl+XreVOMK4w68ISdLvhqUIUDy4OewJflqYvz3brk4SvLx+PHbRRUHky3YYl3y98s1j6W/7pZHJfUieYw78Da8meSrnT4Ar8hLJL8KJlvwAjbvMH2+i/9l0euxinEH3pCTJL9OGtBqoMZwsdz9m6IA8AI2J9Nu7MUJ1F6cptDssELAbVNwBKNxWd+mjyK/vm8lj3EH3oajJb81Uf6vrYk05QPAqazfFo2EHVE33tYXAQC8hNVxGe4o9WMPEgdvzct28rwCNcUdP3+KA5iPsYsHr8dbS75byGJxCnbwUslj3IG34ATJx4ly/TN3SB68HaPJtP08sw4s8h1jt1u44rYp2Me25FfuGGHcgTfiZMnbk6gAyYO3Y89O3mY0duvxiR0VOIVTdvJ6wYlxB96CEyQPwK/ldMkD8HZgXAKPQPJgOjCZAo9gXAKPQPJgOjCZAo9gXAKPQPJgOjCZAo9gXAKPQPJgOjCZAo9gXAKPQPJgOjCZAo9gXAKPQPJgOjCZAo9gXAKPQPJgOjCZAo9gXAKPDCX/f3383wgICAgICAiTBw128sA92DEBj2BcAo/gdj2YDkymwCMYl8AjkDyYDkymwCMYl8AjkDyYDkymwCMYl8AjkDyYDkymwCMYl8AjkDyYDkymwCMYl8AjryD5++V69XePt64LFO9z+UV6AEZgMgUewbj0z/3ni+Xu3/TCYOu6cP/5moz19jx+u95VnzVOkvz95w/Lhw8flotvrO4k8fvr5ZpbTf/laxKu75vrgo4XFgFJ8jpejlM6NHfuv3fLhaTnkBYIe98kMC+7J1MeI+3CkcZUHLdvzd7FLXgvbI3LesJuNjVrcx3HXZvrmrlQ5s8yFz4ud3/pa4M5uYXyvea/lRyvqUfKT8+5j98uquvhb02Xo+qaz1l/k7k/xDPxXD6Wum0h+aS/xVhX6fu+X+vrgo4X2xrfGx2v9LFVR3GmziP0RVVOQvVXHDOU92eZS5o6p5D70uB4yeeKq8K6AdNUyhpQ+g0LpA7L8VI6PlSdUUne6CA94MD75F1Ivv17OIn0NwNccLrkN+Y6TZ5/+box16lxla/nMS/lpP+2Y7D9e5GyjHhasCIwjmPuPHN6FqGav6XNuX4C1a9a7LTtIJo62BQPFclyXrrv6zoPJd/8LbeS58VN1++qjuZ7xX1QlZNQ/W1K3kqzwtGSrxuTGtoNmFQpOmcvAgj1huU3gCsv8ao3Ub9ZapCENy2Gazpf3kSOAN4rw8lUjYkwttQfURhjaXzliULGJ4+fEI0nofg676jCOKQQ8jyEcXinx2tDHst/XVSTmJST65Vexzzob0heG2m6uoU4TV3btoOfzpbk64leTdZhjMl7Zsx1aixwkHlUy6SaC2m8xtfx+r45m+B89JiWejXxwm5dximh27UueS0oPX7bMct/D6nt/Eryp3xyvKbNEobtSXWIeel6SJ25TM6jvx7q0/1dpn5M8fJ7xUh7pf/4VF5kUKCFzrW0vSonEdJL/3B9ypigi2WuyKH0v8UbSZ4rIo22rhP0untjucFGB4UBYUne6KD6Dwm8R0aTqR6bj/SHkceIHmv5OI23QBpf9Md9nyerlBfnkcdy/MOLZfCx+sNmKO/8R8vp0sRwfy+x0t8CHem6UoScj4zfauKktlSTp/obkb+Bru3gp7Mu+Th2ypzHcyRP0DSG6H1cnesEPb6IPEaq9AW5Xs/ZSQzWnKzGU4BfJ5GUsZrqEK7F+lVjlcrSAgrtzeU0f3Py90PXaxcQXI7OI50Lx6pefSh1qdqT6hDrquvB59qFU32dzhbJJ7L7Urxtyat6Cbp+Cu5DafMv2cmXilNh0rHtgOFjanR+Y60BRa/rNzZVPsdL6fhQdQYkD9Ym0ziJqTHCO2r9hy/jjq9Vk0NMo1fc8odaxlkt9nasVeJux6+Uk87VcaMAYrmSZzqX4lc7AQq5DepvoGo7+OmsSj6Mu3s1ftJ8J8drcx2lDe97M9/l8deN5XheXy9zNueZystzbaIZTzmdxON60PVW5FUeAdUeJpej/n50WXS9dsGAvfEyRY7l74f7pOl7dlV6HftMX2ea9hCt5PXfc9/v6pym7e9ALOsuvfem5FXfS1jrl+MlT0iHxYxTB8gbyRUPHcJvaOowfV2g17qS4Qt63GAdL8dJA4NPacmr9N3ABu+WrduiNAriZJbHYhqD4ZJMFOpcQv+h5uPqD1FNUkQ31ni85riSvy6nHJuTQnMcSG04qPiZqm4ClQHR/xKG45LHhR4DnWiItbmuYvT+NvkRZSzxuI3zpDlnC82cGj6m4vg6Xv6bEmSep8MmfQx0jURVpZfzMs4pf1NSFFckGdI1/bUmtoz0a6pz7BPpq1J3/nvk/OrrAve5ahNdayXf9zGh6t8u0vPdkKqcti9J8J3kj+ckydfUA+aeBkapDHXaPTXUGlAdqcM24tkDv9BNkuDdMZpM9R9S+EPTf0T8x57GYf4j5HPtH52kpz+2MA6rP0T+I9STcJkgotzLH3oIeWKRczSRyB+rlJ3qJGkuKP/wR63yiX8P+pyMcZl8LpabqxJ/1+QHXp3huMwf12i0IGysuS4sDul8/x5v51eo5+whNPaPkvyIrXLoujVmw99VKisc68XNIM0WlcTJTbrej/Q+3ZmS7+klbyD9N6KaWwgqVz4uFOqdPJWX/satMOrjV5f8kK3rwkY8SB5s7+QB+PkcNy73ikSO40QuYovS0ws6WwD2XLpzzrYkz9Dr7TIUW+XQ9VrYcUEbzrEIqYx4nduY5vemDnvrYu/UW7bfm8hGvGMlP0QkfxqvIHkAfi6QPPAIxiXwCCQPpgOTKfAIxiXwCCQPpgOTKfAIxiXwCCQPpgOTKfAIxiXwCCQPpgOTKfAIxiXwCCQPpgOTKfAIxiXwCCQPpgOTKfAIxiXwCCQPpgOTKfAIxiXwyKrkedAiICAgICAgzBmwkwfTwQMXAG9gXAKPQPJgOjCZAo9gXAKPQPJgOjCZAo9gXAKPQPJgOjCZAo9gXAKPQPJgOjCZAo9gXAKPQPJgOjCZAo9gXAKPQPJgOjCZAo/sH5d7f6/8N6f5vfX4W/DpxWvys8r5RUDyYDogeeCR4bi8v14+fPgQw193y6NIns5fi1tynGu6mk59TscsIUnPIQmpk5EuJ4SYvo33+O2ilEuU61Qvnd4oh9P219pFy+Ny95fKh8LFt0eKfB3/m8h56ThM214OlH/X3rac0Ldcp9KHuk8k/5zPqeXkflXlWHlR0P3MtG0O16tFxvYC8P5zXYYeMxaQPJgOSB54ZE3yWm55IqfzUQL0OglKT/iV5I2Jv5dRZCjxbhEQz9v5FNmU66Weci7+1xKTnFPXdD90fdLUW0mzk3OG5Ps59ZuiyLfUN7zKdY75ZsFulqMpZVaSt8jvr83jt+tYzjGSp7jXTb9ZfamB5MF0QPLAI3skn3dhPJGLBKqJ25CIElGQE52Px5aM4q4zlJfT1fGGiwAWjCqn28lrGaW6x2uWmOScuqb6gevQiknXS4tWjis5B14u+X3lKFQf2JIvZbb9XKPqrvuV00v/q6D7ivOtrnd9XwPJg+mA5IFHTt7J0yS/KXljIteCEkQsWqJtvHyNyo+ikOtKyIqcvpF8Sdums0RFban6IS5GqjgqD1u+5VxkJPmYX+jbXE97p76vnEIdX8qxxSzBFL3ui9yvW/mcUA4ByYPpgOSBR06WPL9+hdv1WuyM3A6u43HZdyTYsgst11O9Gsp1vQCJUonXrHSGxCnU9Ss7XX0c4DY3adr26vpo2h02563rcHw5EWmz0O3k1YIiBKMvAxxPX2vf37V8VH11qNrUAMmD6YDkgUfWJF9PyK3kdRwtXyV5lX7vZ+lCJem/0jHn2Xy+HtJW5VCgvHQ5WZhV2jeQPNdF3Wpn+vYaZYT6KvmqdkZUHwR2lJP6vxXpejlWm1J9277itPlcX58qn1SXqs1G3TSQPJiO3ZJ/ul3Orw7pReLH5XL+9Tm9AMvyvNx+PFsuf6SX4GT2Lz6TFLXkDbqdYkMvPcYSbiR/0UsgYdxxNcx8CmvX4zWrTBbaoP4DUcUgZfWy28tLJb+XPZLXi5ohnFb6b5BPJfnB+zsCkgfTAckDjxx9h+mtJG/Ic72cscSZ0yXf10PuAGxzfDuErt+6OynpfODtyul27CNaca/lw3H1NStOAyQPpmM4mbLUz86WMwpB5Eryhys6z8da8nTMcTnEnWzc1cZzl0tISXlcfqUQ8vxnub26pZDitAsIFTfXgThcXS63X89znqEuugyqB6eTuue6SFkfb5dn1TZdrpVXfG20ifPpXnMe58vtU0xb6pHOEc+h7imNOg9qjpY8AD8BSB5Mx2gyZRnJbefnJyX5Ruzx+LBcZllGoT4/HZaDElvIK8g1CTQJsshTziequJS/krouX+oYFwV0nsWchBvyUDK2ysp143S5DVT3H1abUh8w1Cfda/qPlnyuh+4nVbdQX2ACyQOPQPJgOtYm07izTUIMwjwn8ardp8iLr+XdaUlTdsZJsFqKjWyzHIUubryu49W7YgocPwuVkcWBLkuJlknx9aImMGhTkDe9znGb11ryuR65LaoenC63D7RA8sAjkDyYju3JNImSRRXkqCSZRdaIk6juBOid/EmSF1k38bRIhVauoV66rLpceyfP9G3S8AImi56Q19uSbxYNwASSBx6B5MF0jCZTvQsPstLSZSGyALXI+FyKH2TJ8SU97cL3Sj7szpv0HESo7WKgu1ug65HvOtRlVXnn+mzkxfH069T+6nXIY6/kOUD0I/ZL/vRvdANwLJA8mA63O6ZqQXAEWq7eoDbpz+G7jwhAZjguu293J8mrb9fL09OqgIUAeAUgeTAdkPzPpN7J+63nr2d9XD6qp7P1ku/R8QE4HUgeTIdbyYPfml2Sl139QPLlwSe4pQ9eB0geTAckDzyyPi75oSvy8JR6J89i727VS4DowQuB5MF0QPLAI6vjkoX+WXbu1u369glx60+hA2AvkDyYDkgeeGQ4LvPzyFnk6TGwjeTbXzjLC4H0CoBTgeTBdEDywCOjcXn/Tcv6cbm/v9snefWLdACcCiQPpgOSBx45elzidj34CUDyYDogeeARjEvgEUgeTAcmU+ARjEvgEUgeTAcmU+ARjEvgEUgeTAcmU+ARjEvgEUgeTAcmU+ARjEvgEUgeTAcmU+ARjEvgEUgeTAcmU+ARjEvgEUgeTAcmU+ARjEvgEUgeTAdPpggICAgI2wGSB9PBAxcAb2BcAo9A8mA6MJkCj2BcAo9A8mA6MJkCj2BcAo9A8mA6MJkCj2BcAo9A8mA6MJkCj2BcAo9A8mA6MJkCj2BcAo9A8mA6MJkCj2BcrnP/Gb+R/yuA5MF0YDIFHlkbl/efPywfPnAQ0d0v13/dLY/hqr6uwzXFCleXa33+czqbpfm43P2lrufQX78OSVPZ99fpdcGsh6qnkONVbUjl/Xu3XLR5UJ1ryRt1XisnhNQfXb3rvuTrF9/SK6rLtRwPafqXQkiv82G4XanvA9X1Po+cTwulq+LpPANNe9pyA9x/Mj7WgeTBdEDywCPDcdlI5yJM4CKF0UQ9msQpXSf5wuO3i16WuXyRR/ovnW8l30P1+DzKj6iEruqjzktcEXYo0yib694vOkof5OOc1lgocN809duWPJFFWvq36jddBoVwXpdjxMnxNPn9L/TtlvdJta+VPLfrr4s+fwNIHkwHJA88crTkm8m+Rku+EYgp+SiEUA6XoYSrJRJFmcrOshQMaaryMm17ctxSpq6bHLeLkvaugSWsdckLTV8O61dCnZ7geEPJJ3KcRHW9eY9S6NrEeTTvO78/VTzOl9773FdVuamclEdY1OnFlQEkD6YDkgceWRuXRWgyITdiShSpkXDzDlqJRyHSjBN9EUsVqIxDJflyvpX8Vj66rlU++VyRjS35mOb6f9vijaGkY3ZJvhWnyJfPm2VwqMspIt2QfCinCD1eL6+t0C0oKN8qTrNwiK95wZXqqCSv+6Ogx0oPJA+mA5IHHlkdl+3EzqERJ1MmcS32dclvQpKIt6xZRpw//ZfLpjp1AjqCdlHQCbE5X9XX6g9jR7pH8o/faNdLIZ+j61Vd9rBD8lz+nVowVddVe3Wo68H938eRcH3fvs8k72/0WtfNSCdh9F5C8mA6IHngkVeV/L/8iaxgTO406de75eZ6CCLIchs+SofyG0j+mIVDuYXMqJ1norszoONb/WFKXl83JK9Ey3HDeXWuOh5hCprKUmm5LfFYtXMr79XrnI+8PzVh0aL7Qfe1OtYLoDUgeTAduyX/dLucXx3Si8SPy+X863N68do8L7cfz5azs/Pl9imdcszh6tR6xnZe/kgvQWBL8qOd1jG3yTW15FtBjiVCsV8ueUpbf1bfSL5bBDTy2hLkGrne7c6XznyL7cp5v7CckJbaUn95j9rKO2ydt7lQaHfymiMkrzH6dQtIHkyHX8kflsuPt6TAY6A0bR1/EsdJnsR+dWzbfi+O3skPv1m/j/07+ZY1ye/Pp12ctHkdv5Nfk6LCqHeFlu+gnNX0gs7HQl8fSL5eCGnWJG8s+mSxd/RiApIHEzKcTFnqZ7yTPosiV5I/XNF5PtaSp2OOyyHuSmUnzuGS9EtQHpdfKYQ8/wmiu+W8OE4j51AGn0+iz68lLxZ6eC1xmvJ03XLdo1xDmW2alQVFV3bbNyGOSN7Ksz73/+X8YhozrdFnfP532fHvXnwC8BOB5MF0jCbT56/nWSjPT6QqEWUj9nisd9BRpM9Ph+WQdrY5ryBHkXQUWlkQyHlB7eSpnCy3ID06+4PyT6eyJKWOzEjyuUyu12XZfesyKvQdBVoWUF/oXfjhKtZb6mDlye0vdeF+0XnqtKXPc511n+n2vXMgeeCR4yXf3f6It4zaz3L4loO+JTL6rGccr/mySbrtsfszI/BuWZtM4w5WCeYjyUp/Ri4iDSKSHWhJU3bASayVpGqxl92sUETI8it5U1DCjueOkbwuU+VJIcdvSe2L16leKk27G7fyXGtbeJXT6ngpTtVndbr3DCQPPHLSTj6LmYQvgs7y3bkI2B0vUL5gAcmD7cmUpUZiZNkEwSjRZJH28qnuBOid/AmSr4Sd0PHzsc5fH1P6sjAoZVY75x1w/PAxQ3fXodTByjOm0/Wv+8tMK/Wv+qzv5/cKJA888vqST2AnD96K0WSqd6RBUK00WTZavnxOdrAiJ0n/cb/kWXRRyK0IS31CXqq8c8onCp/ShHOcl9rpX12mjxPqMuPrFEffodCodlifycvn7mXRYeXZniuvuS12WlVW7jNI/tfRf/sc+KL131vwppIP3/jLO/ax5O149gCF5AF2TAq9UEmh3ZWDn8PauOR5bm3Dsv9b7fpb2XGOzPOx/uZ1KKO5nmhfd3Nqd5eVy6O81D/n0/WVb3bX+XA9dR4UVv45YKhre739Jjm1+zo82a1xA8XT/8St+vfjzbURdf+ndlA/6G/Pb+XTv4ft+9f0CbX3PrwXdF6eWNe2OQX9fh3LG0me34S76p8I2HJei9e8kQk7H/A7AckDj4zHZS0w+XfQ63PZ6J9YUV4sDxGxIfE4d9LcPLiu5+8oEbse9b/XVm1o5FekpPJRbhD6eipSXbrruY7cFyLD5IZ8rQSu1ymS1+T0TTtX87Gu6/QaOi/tjH2iJG+h4p/C0ZLnSrUdywOpDFgenOmYB0AaGP2A3orHg3mtHPC7AskDj/yMnXyQwjeaL0MeUXZZnjyPSlrreoDn3XQ9S9KeU7lORVIyH8cFRiX5UBbHr/Np22QKjwjxch7lONRV7XCvv93HunN+Eke1WfJvJX/czjgtIMLhaDFj59O5MbejIUg7vQ8hD9XODNUjObF+/47npJ28hbzB1iP57qiC7QDYG69l6zp4/0DywCPHjks9l+2Z13iyz9IJAiLxkUhMyQd59JJnifKxzsssO+QfpRrTFumEV1ngRag5n4EQY9BlRdF1AqOy4619VSbnyXd99U6eoyqhS/lSt2N+CEeohCqS32wP1ce8FkPbPt33sTyR/HH57OXVJT9izyBmXisf8H6B5IFHRuOSJ/Ju0iZ53am5rEhTh34nTzFNEawJQMRVCYwYfWxQ76bluhJuoIhWqPJhUXf1bObuPYuBnA/3hZJhKJsXCZbkrX7bRss3IJJPtO9jFXfQlv594TbQTj7t3ON7Iu1KtH3X9POxvJrkAfhZQPLAI5vjkkSgP7c1NyyNWHo2JL8pmw05J1qhhdBKvr2uJb5H8hWUX5W/omuTykddk36rJL/Zn0xsSxdPp+Vy1vpts98jcicl9A/l10me82n6IcZJL04AkgfTAckDj5wqeVOoEjrxrchwQC2JXvL7aMvt86mkt0uumnG7+p35ehuOl/wAnTZJWdNJfqtfKQ5/JJ25v08/Xbst+ZPbQEDyYDogeeCRV9nJb0KCsxYDK4LpJG+k394pthK28qklX1+LYSyrNn8Fi6/KZ73fWsnr8iXs2hk3C4R2MVa1patjClviD+B2PQAVkDzwCMYl8AgkD6YDkynwCMYl8AgkD6YDkynwCMYl8AgkD6YDkynwCMYl8AgkD6YDkynwCMYl8AgkD6YDkynwCMYl8AgkD6YDkynwCMYl8AgkD6YDkynwCMYl8AgkD6aDJ1MEBAQEhO0AyYPp4IELgDcwLoFHIHkwHZhMgUcwLoFHIHkwHZhMgUcwLoFHIHkwHZhMgUcwLoFHIHkwHZhMgUcwLoFHIHkwHZhMgUcwLoFHIHkwHZhMgUcwLoFHIHkwHZhMgUcwLoFHIHkwHZhMgUfcjsv/vi83fz+lF78nT3/fLN//Sy888BPfE0geTAckDzwyHJc0oX/644/lDw5/fl+elofl5ssDiefTcnNIcTJPy/c/byhGD8cPeegQ8qup4lE5IpRadFxOjPNJZKPFo+ss+RAPXz6VPA43ff2H8tooj/vELK/0Rd9+rovVX6UsDlzH2HY6/0X1F9U/xinpdXkPX0oesSw6yf2S6hbR9U5pN/ollJFe6zGwrzyiOafrbAHJg+mA5IFH1iRfi29N8iwNJVLF3t1ol28WSklfy6UIJdfTkgtxquQ3y0t90h5bAusXK22c/lwveSpDFkiqrbXkjfeh6Ze2P8ICZtAvZdFEZajXXb9odHlVHinQNUgevDsgeeCRPTv5T3+T2NLkbEk+TvQs+n7i5vjVBJ/yaSn5lh0ty0fLsRKDSInqWUlelXND8eNxLTVLZpbkN8ujq28i+dwOrreSfFV2OV9LvrT/D8rvRu4OqD5vJR+ubfRLKCO9rhc/x5Unx1YfaSB5MB2QPPDIHslzuDlEiVkTfJEDi57PlQm8ltuYbvGQhXKk5JVchFZqVTmMtLX5GGGzPG5vLm9d8txX+Za/FnqmP9ft5KuyR5JXbRW6fpH3SbV5rV/kPU3l12NgR3mH7+U9pAVjrHPMtyszAcmD6YDkgUf2j8te8nvg+FESOvQS3CN5Uy4pXqCVS2KP5Is8C5vlpT5pjzvJhzQP6rb7iZLnMl7hdr2J1S872Fcet62WurUQ0kDyYDogeeCR1XHJk3Un6PHu6yXUiwESB+3+WslrWez64l26Td9KvlxPcVJZPRvlsXR1XqbkOU56HerHx73QdVkhkMwfOskTuf4lfS15lQcHrhOXK9Lt+ogD9QGVZb6vZnwt6+ZaW17X3xSobd8hefDegOSBR44dl2V32wiuCRynFncTmlvjHSSKXvIGKd4a5m5TsyOPzK7ybAFHHpaHgyX5nn4nb7O1K66kO+KInby+w2Gyo7ytOkPyYDogeeCR0yX/c5hd8jb7JB+B5BlIHrhn92T6dLucXzV/QT8ul/Ovz+nFa/O83H48W87OzpfbnfPcu4H69ezjLfXA7wsWn8AjkDyYDr+SPyyXR4uO0rR1/Gm8tOxfWXd/QPLAI5A8mI7hZMpSP+Od9FkUuZL84YrO87GWPO8+U/zLH3xCduIcLklhBOVx+ZVCyPOf5fbqlkKK0wgulMHnk+jza8mLpRheS5ymPF23XHeKI2W2adYWFG1fqJ3289fz2BZddtXOFF+nZ6r+auqu+lqXnfuI0nL+8fz7vNMByQOPQPJgOkaTKcsrypqOn0hMIh4tz3ysd6FRpM9PB5JVOiN5BWGJpKPYyoJAzguUp4iXpZbqwnlcBnFS/unU4SqJTstR1zOf12VyvS6LIHUZFak96dXhKtYztIlFK+Xpsqt2UhqqazpKbeIFSnO9TZ/rW+LlfqS65kWJbuc7ApIHHoHkwXSsTaZx95wkw+L5SLtWvXMUwQSppd1mCDFN2X0nsWqRNQLLos4UybPcSt4UsgDl3DGS12WqPCnYsmQh63hSz1rUVdlVOwl+Lem5Te11xkxf+iAgbTLb9r6A5IFHIHkwHduTaZIZy0R2od0uspERUd0J0Dv5LKT9kq+kltDxzZ28Pqb0ZWFQytR1HFOnEUKZPwblVe3UfSPH9F+9QGDM9IP6QvIA/BIgeTAdo8lU73KDULRMWJosKy0bPie7VY7H8SX9x/2SD7v2EKdeOHR3BVR555RP2V3zOc6L809pri7TxwmtsFWctc+2VVu43f/kOjb1lXz+Ty3eUvdzOl8WLvEchbX0umw5p/u96tP3AyQPPALJg+nAZKrQ4k1he6cP3gKMS+ARSB5MByZT4BGMy9ekPL8evAxIHkwHJlPgkfG45Key6UfRxqfG9U8qa+NRYNFZTz1rzx3Sb5knuuegc7Aef2s9D52Czmv0SN32SW1Ve7qn2W1IW9cj1DPFX3l63Hp5Rl8aP+bzOwDJg+mA5IFH1iXfC8Z+HCnJLUhOPYK1Enr/nPsg5FMln7HrqBHZ19IdyPQEyev65/inSr4rn+jK+D2A5MF0QPLAIy/fyTNbkk+0514qeUof4nQiLnWX/HPeKm4pL7XJkny4XodcZ1X/Kv+h5JtFiSH17g5E17bfA0geTAckDzyyOS4bEb1Y8iFekWcnSSt0okvp5XySfRYrl6PTVyH9WI1ul9RV0jV1tEIoq1mkhHpxPkPJxzy7eu4t7zcCkgfTAckDj4zGpfmZNsnI/h1wEtQOyfMC4bv+BbNKkmX33ZapNZoFPwg67/66+jU6XbaSfHe7fI0jJR9/vU36iji2vN8ISB5MByQPPHLsuOx38pZ0SaaHWvIsuChElnmSbSfJFiVEk+b2t2ZL8sTm7XpjoVLRlBEEPpB8WDRJXpyv+R0AXacUVtv/foHkwXRA8sAjq+NyS3Jr6LSdzEjOf0cZHr+T16xLfn0BYdDW8+j2jyT/tDzID0xo9pRnLBh+ByB5MB2QPPDIpuRb6VLYJZ09gtwj+dV/QnbsTr7+Z3YdlnSPzYPZK+a2PAI7+QgkD6YDkgcewbgEHoHkwXRgMgUewbgEHoHkwXRgMgUewbgEHoHkwXRgMgUewbgEHoHkwXRgMgUewbgEHoHkwXRgMgUewbgEHoHkwXRgMgUewbgEHoHkwXRgMgUewbgEHoHkwXRgMgUewbgEHoHkwXRgMgUewbgEHlmVPA9aBAQEBAQEhDkDdvJgOnjgAuANjEvgEUgeTAcmU+ARjEvgEUgeTAcmU+ARjEvgEUgeTAcmU+ARjEvgEUgeTAcmU+ARjEvgEUgeTAcmU+ARjEvgEUgeTAcmU+ARjEvgEUgeTAcmU+ARjEvgEUgeTAcmU+ARjEvgEUgeTAcmU+CRrXH59PfN8v2/9GJ5WG6+PKRjhl7/8cfyRwifYrz/vi83fz/Fy5rDTYpXh5tDuq54+vuTeZ7z/pTT3lDpbf2Ep+X7n305D19SHZmqPnKe08V8A22due072vcpXa/KM+oUwp/f6UqKQe0u10p/fsp9Psgj1z/y8KW9rtqU0e9dClwOtUPqz1R1SvV4+NLmV4+L7j0Z9dkKkDyYDkgeeGQ4Ljsp88TeTuZKxjKRb07oLBdLOgWWlBaNwOelPC6b49iSr5F61tIthPOUT2lnohFewGwftSnLmkUcyxHZmguWgE5Xk9tVSb4Q5GultepntSNTv6d13Lp+Ivcsea5bNUYoUF6QPPgtgeSBR37aTj7A8UWotuiLvNvdfLPLTuWsSV6uidwr6VaLmFT3tox2ocNtN9unZaglL/mOqCUaxN3WiUVa9XnczQcR87VmF8/U+aR6D2ne02ZBUOWV4tU7eTUG0nVL8t1igMJ48QPJgwmB5IFHtsal3j2XCZ0neTW550DCORgSHEzyOQ0LIQlVC6bfCXOZVDbnl+QoEtLphFY2lXSpvEoyWehaYJFKalS2uYhRCwKpiy15kvQXEXstWHPBwm3N8mzkrUPoD+s9KcGUanhvVPuU5M3yqC66P3SdOT6XUb0ne957A0geTAckDzyyOi6DYEiGebdZSykiu9+4w+TJ2xJugfJQu1cLkYVNnd4UoxJuDpTmeyP56npol7SFsYV58/foToUVf0PyzYJhS/K7GEh11J9B2NQX+bqSfEbO5T4rktcLmSJ5ox1HAsmD6YDkgUfG45KllSZzntyDaOhcJRwWLi8CyqTfiiuTBaGDvZMzJW/Ki2QyFEpb11pIgU5oWvIDRu3j8rr6bUn+Kf43ITvgKv2hlrzc3ahD3f97Jc/lSfs5XxG57pPwXtBCQJ87+nZ918/bQPJgOiB54JHRuKwncoJk8/0/S/Lru/I1tAxseaXQyFoz3jX2ku/YIZ/tz9WF9fJObV+Qtrre16dZmDTxhxiLlafDAwVD8oO7ACMgefBbAskDjxw3Lg3JW9LaKf7XuK27KnmjbpWwSD7t9Xb3bcrZbN+O8k6hk3xfxp6d/OpCQmNIvsuLwlq7LMkfmwckD6YDkgcewbgEHoHkwXRgMgUewbgEHoHkwXRgMgUewbgEHoHkwXRgMgUewbgEHoHkwXRgMgUewbgEHoHkwXRgMgUewbgEHoHkwXRgMgUewbgEHoHkwXRgMgUewbgEHoHkwXRgMgUewbgEHoHkwXRgMgUeWRuXm09Xe6unvIHfHkgeTAckDzyyLvnm+fXtc9IHkj/2OeUAtEDyYDogeeCRl0kegLcBkgfTAckDj6xLvt+ll9v1gx+nSQG37MFLgOTBdEDywCO7xuXop0IHv3gGwYOXAsmD6YDkgUcgeeARSB5MByQPPDIal6PfEQ9Bfk+9+a1zAF4LSB5MByQPPPKicTnYyf8B8YMXcrzk76+XDx8+qHCx3P1Lpz/H/wqP3y6W6/v0gth//X651vl/jpHa9Mz95xTnr7vlMZ1ruf98TTkm/r1brr+VmDk9hYtw/nG5+9zmRef+KvFyMMps2wTeht2T6dPtcn7V3O/8cbmcf31OL96Sw3L58Xb5GSWdBPXDmVG/56/ny1nbZ2AXWHwCj5y0k88yI+GL1LKEtxYBOxcJERL+UPJ0LYnWSst1rMrhuFrydHyR8i5ytyRf4DzjYsCGFw1r18HrAMm/MtQnlz/SMTgZSB545PUln2h3tfuvH7OT5106ifmbyqjh5Tv5SFg00DXe1VsilwVA2y7w+gwnU5b62dlyRiGIXEn+cEXn+VhLnnezKX6U3PNy+zG+Pju7JE0TlMflVwohz3+W26tbCimOseMN5fC1j+fLuUi+rZd5ziibz12RgOU8lZfzD2XzdVWfvKiw22H2jTq/1j9y7kD1ueXdPp+fZRHzk4DkgUfeVPJBhnnnvvd62b1resnLYqCXf4FvtavrvHvnNLSr/19K8BIuaLHQSl4vBLrA9Uz11+KXNJD92zCaTPlWs+xIn5+UyBpxxWPaaQdRMlGWz08k0fTF55xXkKCSLsmzLAjkfIKlKHlyuiDBlHc8GyR5CGnPl1sp68dh+UfVPde7ikf1JbHGOHws+cg5zjse636QvIZ9w691fN0/SuKHq1gPLqMIv7QBQPLAJ28keRY173rLLnr/9X2Sr+KTtO+tHTTV74J28vm2fLOTZ6Qt/N8gbuOzdiG3e8DWdfA6rE2mcaerdq+8oz5TMhKJ8bW0S40hpsk7ZRGqkmGUbhF7K7lKllmSUc6lHE5TC5Sp85LrujzruK4Pl89ts/PiMrj8FH9L8lW7Sxydd91eAMkDjxwt+SxDHUiiRbpq98w75+5z863rskMf5R/h3XIWqsono85xncNOu5E853nNIYvZul3P9W3qk+rUAsn/HLYnUxYrySxIPolWpDrYqTJaWvn4CMlXO/mcf50mwufqtJUwc5k6rXVc5722ky+ovknnq/ibO3lIfgQkDzxy0k7eQiT8+O26kjGL9Y7Et/f6iO46SzxL93q559vmWfS9rB+/0W7fkPxamWPK3YbN2/ng1RlNpnoX3u1GWcAsrSyxdE522ByP40t6knAQWCXJVqpReCy7/Bl5/iycgkhS5Ts+p9O2Mh8dN+VV9azz2uwbvrbSP3IOkh8DyQOPvLrkR7z19V10kjfEvPoZv2B/pAB+DphMBS1+8KvBuAQeeTXJA/CzwGQqQPKewLgEHoHkwXRgMgUeGY/L5lfm5FG2ieqxt80T7sK1jafe8S/c6efhP3z5tHz/Lx736WNd5Jn47a/jWc/Kt35Bz+0z9c3HA+v+j7/81/ZZSJfel7q99s8Bm48qbt5X/p2CNo7db/avENZxn5bvX8ojkG+47pT/nvcBkgfTAckDj6xKXgmABSGCaSX89PdNFrRM7N+VtC34t+pv1O/V15K/IXkpUfEP5Pz5KctBx92C6/rz5a7ktgdD8lUbRZBNPN22Kn4QdS/6+n2ysOpdj4OCJXn9HtYLik/8fkPy4D0DyQOP7JV8EG2QPImAhDuUhdqxVbvOhiClQ5FWK/nvFIrA+PUrSZ4l8zeVS+K5OXBbrIWGLFSSpJRYy45Z0inZdbvqlB/LOcUpfcJlSzpaQDWS53r3/afrO6p7hOvQyrQVbwzNYkDV1byesSTf3p1J57ltalxA8uBdAskDj+yVfJFGLZeWsltsFgkNIqX2v0wUM6UP4ov/1bIuEqWwUgZjSb6kGYkyCrhrL6fNMn5YHvg61bNPr9tO6dXumBcsHL+SeJVvIUtZXcvtaXb1uv+YVvI5Lyukulb92gajfpHBeBCpE2FM8IIu5VW9HwMgeTAdkDzwyKrkzUmez9dCKUQ55jTDeEpKSVa95JMcaNfN57PciFZoa+h0ARJqvZseSb4/3+UV0G2W9FryTT+mOFUbGmF36EWAxKVzui5tnxzTR4X2/Uuhq1vbpjqEeqk6hwUHvY/YyYN3DSQPPLJ3J68Ju0I98f/3FOM1suJ4/S3niJYQy5w/ny+v1W411UEL9hiBdWKuJK/zYsHpY0P+WrYJXZdyrPuuzkuo+saQ/MNBvVb9EPILHyXUeVZ9wvW03jvOxxJyZq/kI93OvykzX+f0VDYkD941kDzwyCmSZ1hSZXInwYTPtxtpBKmQjCyJaSlxWWrX34mZ2JK8lYbpzjeSD0IM7aB4eaExkHw4Tm2mEPLN6f8IXxYs6VUcLdfcp81uuOofvrNR8m0/Fw99PxKqEV/o+y0uGEo+dbtXEWlr1gQOyYP3DiQPPPIzxuVIwK/HEXL6rWkWFhTq96UsTuowWjQ08VYWhZA8ePdA8sAjGJfAI5A8mA5MpsAjGJfAI5A8mA5MpsAjGJfAI5A8mA5MpsAjGJfAI5A8mA5MpsAjGJfAI5A8mA5MpsAjGJfAI5A8mA5MpsAjGJfAI5A8mA5MpsAjGJfAI5A8mA5MpsAjGJfAI5A8mA5MpsAjW+OyfVqd9UjZyEP61bhI+TW6hH60qzxBbfD0s+5JajpNxfaT7uq8Ut2pXP1o266umwzKrdpIgeLchMfG1n0TaOOG6208I91vAiQPpgOSBx55ueT7R6GyQFtxVulE7luPOKXrox9HifBjWkeLjh75mdcXSz4JunoGvsB1Dv3AZVHfjCSvkUe+dvEgeQGSB+6B5IFH9ko+/ChKkNfb7+QDLMs/+dfWKL4pOl5ccF22d/MRVb9O8tZvrdt5hrjpGe36OCJSJ4K4H8oCaEXW0sem5Lt6DRYX7wxIHkwHJA88sjUuw+1uJZ7h7XqWuBKeiDMIqRK8BJKoIXlbuCmk/OMteF0PkeFY9kWkhJL8nvIELrcTbGiblEv1kDR8PvwkLL9u5a1Rafg4lM35ybEdhoujdwIkD6YDkgceWR2XQVT8s6dFqCPJ8/kbDkk+9i1wEVoR2Kqsmh13z75dPIu8yqfLl/PRErXauFO6lHc8x/WSnT2lNSXPeer6G/FyfikMFwvvC0geTAckDzwyHJftDjWJz5J8kSjLMl43b9d/oZ273h2TwCzJm1+8a3bVkS3JRzF3C4VG8tUuP0DpzPKEjXJD3+n6233W33loJd/Xo6/r+wSSB9MByQOPjMbl04H3oZqn5cmSPAktfmlMIAH+TWlNyY91vI7siJko7moBoMIuATaS50XFa0o+f8Ev08p7RBOP+wySD0DywD2QPPDIseNydLu+Zf2Ldyrskp+W/CvQSD5KW9drq43rkt+zk7cxFgO4XR+A5IF7IHngEYxL4BFIHkwHJlPgEYxL4BFIHkwHJlPgEYxL4BFIHkwHJlPgEYxL4BFIHkwHJlPgEYxL4BFIHkwHJlPgEYxL4BFIHkwHJlPgEYxL4BFIHkwHJlPgEYxL4BFIHkwHJlPgEYxL4BFIHkwHJlPgEYxL4JFVyfOgRUBAQEBAQJgzYCcPpoMHLgDewLgEHoHkwXRgMgUewbgEHoHkwXRgMgUewbgEHoHkwXRgMgUewbgEHoHkwXRgMgUewbgEHoHkwXRgMgUewbgEHoHkwXRgMgUe+d3G5eO3i+X6Pr1Y7pfrv+6Wx/Tq5/Iry/YPJA+mA5IHHhmPy8fl7q8Py4cPbbgmPRUev10vd/+mFyyuz/oqQ+d0+nT9/vNFSjcqR64T/94tF831i2+9HlngOU5TTnUthev7XrR740VWzjd5fGjj3V/HxQb9N8fhOlNbr6ltdb9G6gVK4f6zKkfnJegyUrD7ry1Tt69pU/c+thh9oNJtAcmD6YDkgUeOG5cs5CT5Thx8nib2tUmc0lz8JRLt5RAEa0jz/nO9sKjqkdFCKsLSEioCl3N1GuGYeNyeTpgicEUr6Mdvd7H+bdwVybPMLTn3/VOzr/90u1Uw2k055vd5LHkDXqxB8uC9AskDj5ws+cT2Tl6o09ZyoHQklCCwsGtvxRGvF/nYYqkk1UlIi1qOJV/dpq143I5Yhgg3l6va3u6uazlTHt9S3L2S5wUSned82wWEtZOvy5P6S9jRfxJ0X+jzkDwANZA88Mj6uBQxq/9+lkk/UktHy1BhiFvkYIpFAgnmsHFd18ViU/JdHhvxQluMuoRAZT1sXOe6JJEHSN5y/Zp23PxfFnSWvFw3FhDS72s7+a3+rdu+BvWBIehjJZ/bvQEkD6YDkgceOU7yzUQfdma0E9VSbEQQJMO3tDvJ7JeDtVNtBTXahd4pCZU4ck7JW7EZL+2qNcfIrtql793Jr7Cnf7bYXgy8guSNfhsByYPpgOSBR46S/L8kn3SFZmzatacdJO82gwBsEVjnazlwXrVYtPiOklizW6zK6SRD5a5KflAet7e9Xi1auL+a67n9dE3fDany4j4+XvKb8GJM14VCe8u/ounDSP8ecZuOWtzQ+NkLJA+mA5IHHhmNy62d3f9qbxHf067539Mk34uiFuExIjlO8ntpFgNGPlvlZGmbAhVI7tSPWvLmAkdCkKxxXkLq863+FY7q54Sk4fGyuTCTYI6RGkgeTAckDzzyuuNyRfLdZK+F0l/fFsZASBuS7/NpFismveT7fHR9xjv5vTv0X7GTt/t5vX9i/3J79/TjfiB5MB2QPPAIxiXwCCQPpgOTKfAIxiXwCCQPpgOTKfAIxiXwCCQPpgOTKfAIxiXwyCtIfvQFEQDeBkymwCMYl8Ajx0tefxsyfEsySZ7O528Z5jjlW4L5SULttxPTAsH6Jwf9tyL1NzObb5Gu5BNoyo3/LEN/k7HJL4f2m47q257d4sbIg+LodtTfuox56+v1P7dJbVHfcq3Tl+vWIw7Nb3hKPJVnwf7nIN7YPZk+3S7nV4f0IvHjcjn/+pxe+OP56/ly+SO9aHlp3a3+2AuVffbxdvHbc7+e8bjcnqvMv1X9LXRF/0+smjmP5l/9z842y+nmuH4e4/yGc3hK39aj/6eDfJ3yznNV88351F791DlrviyuadKr6/0/u7P/GaP049b1TNd2Drrduu/a+rZw3PU+k3TcD3Ye25wk+boD05uWG0KvZXAq+VQDJL/JBUvOZsebA78MHCufFs63/LhDGVBCGFhGHQO5/nqBoJG6lH6w/wlHSV9L3jimMovkjfYN+1T1Wwr5vVN5FiD5X40vyR+Wy1MXBb8h+8alPVftmbcEnr/0hJ/T8jzQ/L1zvPW8R/MY0cwr7SNfcz1yuX05MoeVOpf215T5spa8cVxJ3qi7IflCKcdCnKP7t8KcNwtVX0s9cn0b+Py3uo/r95bqmhdQv0jyWSJcSWlI1QlFGpXk0wAMnUnn47E1QNYkzx2gzht/ODb6TW4GSapbaAe1x1oAlDbdL3fmm035h7qUcrS4a/HG/I+TvEpPdb8e3lXguFb9E+ZgLe+XZ4aTKUvs7Gw5oxBkqKR2uKLzfKxFybvTFD+K9Xm5/Rhfn51dkuIIyuPyK4WQ5z/L7dUthRSnFSDHvaL8c54kyXyc4qgycz10vT8WyYc6h/OpLmuSb9vOtO1T/dG3vT3X9MUgrS6L+ynW4Xy5fYqnfydetpPfmrcKIoIyPzZp1RzN1OWoeuRgzxMhfyXEaj7huZLmijvaLK21YV3yPP+qeqSyTLHrY3FNm56uSz+PJC99Vl8v+Zhu0zT+khDrw+lUH7BDOH2ub01sD5dd2lj6iSnXuD5WHnt4/Z18JQ+qpCX5tuOIYwZ5RA+Wwmo+3OnVgK47sX3jctDl5DdZ56ORetF/s+TLwNKDVtDXtyVvtG/Yp007OMgfbW5HCdf35f3yzGgy1bvg5yeSj4iJBKRlFI/1LpWERvJ+fjqQzNIZySvIM0k2LQJiGXws5xMcV25pBxEWOccFAZWpbnkfrliGnE+RIos95M/SFPlSvpdcZ92Ohq7tZvtE1MY1PpfbSTFogVItCnTarg10wG1UbR/V8z2zKvmNueoY+Ya4Kr92TuD5RM/RdTkr86NGCaqVb5ivsvx5Do0CsvI2JZ/bZs830hcxz9IH+VhcE8o2+oiu144i0nwn5+Ocm+przIUlqDbtiiftoyB9lOurKWOi9E19HMrLfe9J8vxaiyQ15FjJxzdBd6IKIf/SSZrRIA4Dx/xxB2OQDFGDKrWjL0/qVfpBi9uWvCF2fUxlHSt5e9JIQcWt49l9542126Jx95tkFaR7TpJWO0sREF9Lu9G8W6XLZfds7H5ZiErsWXCCKUV1XOUlYq6lKbLm/5a6UeB0G/Ls2q7T83ldj9E1jT6n06p4eXGh62bl9RvwEsnvxph36nyiOO8qOeq4+u9dQjMnyXwi804u85i5MmJKPvfFSPK67sbxsZLn+ndtlpD6htIMr72UXF9FW17qk9p79fv2UyWvK3d9n9403ZAcx3iDug6PDVkd8DLYKqjcKh8KYSAek08zSAZ1K3B8dY7bmQcsYQ4mGuSV5JvrtBC4r8RuvMmq3l16Lp/L1fXI6Pb1Ew2X1S3YZIHmmDXJR9KulGUTBKpEmmVUy5WpdsNyXAnrhZJvyrR28qY0hQ3JR1LbjfaN6hGRdIrdbaADSH5d8vpvNv3drs5VFmF+UX/PxtxZjvl6jGuWQ3NXt9tNmBssmhf0wqH1QAx1OdaC4uIbzWV5HuL5qYkT+qWUU+eRzmfX9OlDm1baRhftOW5HeyJ2nQPm/E950PzeCrp+T8p7VRZDNT9X8h1UQW5k7ngb/cZZWAPRHJwbSBqzs5QsI1qC+lhIbdPowRAETX8QbZwGvTu32Lre17vhNSWfJxG/jCZTvQsPwtGyIQmF28laRnxOdrMcj+NLevlsvBKWLfmw65b0End0rMo060FBFhrdXQVd94au7YzVPqMe8aME6xyLn19TO//PdhvycdVnvw/bi88aPVfVYlBBCemRNlT1LPC4PFL6PE92c3D83tCx5Yw4ZQ7vMebUhq1ytlzD10+R/DiNwoi3OX+39aXXbX+zxDlfp5JPtA1pOE3yxmDckFDMxxI2Ya60VLzu+p5Bu41XyTN1H79Oe9+aYyfT90ZYVIiMRcL6jgL4JZwq+Zeylc/rlXP8HN7jV/JlHizByqfz0kZ7Nuv7xrye5AH4Sfzukgc+wbgEHoHkwXRgMgUewbgEHoHkwXRgMgUewbgEHoHkwXRgMgUewbgEHoHkwXRgMgUe2T8uB1/+AuANgOTBdEDywCPDcdl8czs8W4Qlr751bf5LIiwEwCsAyYPpgOSBR9bH5aN6ulsv+R4dH4DTgeTBdEDywCO7JC+7+oHky8NQcEsfvA6QPJgOSB54ZH1ckrTzg7fqnfxLn0QHwBqQPJgOSB54ZHVcstA/y87dul3PT6fUgp/j6ZPAP5A8mA5IHnhkOC75sdNhR84iZ3n3ku+fTY7b9eB1gOTBdEDywCOjcXn/Tcv6cbm/v9sn+Ql+LAr4B5IH0wHJA48cPS5xux78BCB5MB2QPPAIxiXwCCQPpgOTKfAIxiXwCCQPpgOTKfAIxiXwCCQPpgOTKfAIxiXwCCQPpgOTKfAIxiXwCCQPpgOTKfAIxiXwCCQPpgOTKfAIxiXwCCQPpgOTKfAIxiXwyKrkedAiICAgICAgzBmwkwfTwQMXAG9gXAKPQPJgOjCZAo9gXAKPQPJgOjCZAo9gXAKPQPJgOjCZAo9gXAKPQPJgOjCZAo9gXAKPQPJgOjCZAo9gXAKPQPJgOjCZAo9gXAKPQPJgOjCZAo9gXAKPQPJgOjCZAo/8knF5uFk+/f2UXizLw5eb5SEd/+48/X2zfP8vvfiNgeTBdEDywCNr47IXzsNy8+f3Jev5v+/Lpz/+WP5oghZ4BcndiltLnsrQcVJ5qwsBsx6fct2f/v5knH9avv9p5Tk435bxJcZ4+FLKadt3c+CTo3KIJk/ui9jn1Acpf6bUv+RTlVvlE+OY712KE+tFUH3z8eC91GXm9qX3hOt1c6D2fVFjgtH9wHEp7xseE7q8DSB5MB2QPPDIuuS1HNWkna6PxDCUPKMFkEU5kCCLaTOOzWhHXM6P5MsyVAIVuK1KvEIlW00W2orkiVifIspO8qpcfj+kb3W5D1+KuCVO2/4SX9VnIN0o7/QiU9LJ9fjfUvcM5VuNAUge/A5A8sAjL5a8Ib4xWnjleChwlf+a5M16ru3kSYDxuM8zypBF31zjuuQ8SKpUn5yfyFQvYDhOEJpuc4+W/HeSdcxPSV6LsaqDlNvkn4RaSz7mL+9bvmZKl+PSQkG/zwGqk5zLZUDyAGQgeeCRF43LRnw5rIj/6e/vSUgkh7/jkRY470qt/FYXAm38HKIIo4xS/EwtRym3xGPR87kUh8sx2lXt5BvJx/zqclq05FmL8fWK5NP5+g5CWpTw9SRiWdhE2dYiHkue80n5hn7V9a4lL32+V/Ilfjq3ASQPpgOSBx4ZjUt7d5xCmOxFgnYYT+aSrkjKEngr5rWdvCVXXYc9kt9ECVZTy9bihZJX5ep29OUqCRNZ5IkSX9VHSz70YVvP+F6VMuN1rkf8SAA7eQAykDzwyK5xKZO0hZbfxiTOUuhkTGL6/hMkX1/jvE6QfJVHlGYvW85Xxzvii4jUFw+t5IlcfyXxXG5XLw6URyN5ETZfz/16hHQDub5F9iPJ6/rc/A3Jg98ASB54ZG1cbu9SiRXJt6IecdROXpcnULlDkQ7Rki8CtMJaG9o+stq8WveGbic/YOu9aXfyJur9Mj8mkTCoy1DyLdRuSB68eyB54JF1yRsTfntLdyj5/Tvl1V16QuKYCwcqt6/nxrf8j6jfGsfs5PcueijXny75U4DkAVBA8sAjGJfAI5A8mA5MpsAjGJfAI5A8mA5MpsAjGJfAI5A8mA5MpsAjGJfAI5A8mA5MpsAjGJfAI5A8mA5MpsAjGJfAI5A8mA5MpsAjGJfAI5A8mA5MpsAjGJfAI5A8mA5MpsAjW+OyfoDLQ/178m9A9ZCXHU+I+53Y/zCd+YHkwXRA8sAjo3HJQtFPbeNwc7AlXz8ZT55M1zxxTT2VTp4Ap4XeP12Prh3WJN/WpXlKnDxlbQdbT/Zrr8f6W0+l00/RK0+CqxYu1A/9E/Csp+81T84Lz7U3njBnPu1v/Wl4MwDJg+mA5IFH1sZlEb1IYyT5/vGwteTrdCI9kafsTotMU36rO3nJU8nwRMlnWJg7FhW5nqbk0zUOTfzQTlPKHPo+DFB86Z+4KGgkb7DrkbbOgeTBdEDywCPjcanFLMf0X0NIryJ5LXQ55v9yeSqtwML79GfzUUIj+f7X2VZ+bIblS+V85zoZog91pjaV5+E35QW0gMvxaTv5RJB8WTwMJa/6rypvUiB5MB2QPPDI0ZI3hGtLPt4FyFJUO1g5V8loJHlTuDoPkbKS7kDwMdQCLHcrjCBtpmO9m+7KyxQZl/RNO1U/cNiSfFjMVIsCJfkj2jkbkDyYDkgeeGRtXBYBijCOkXx/y7gVarurzrfBV2/XPywPIV2Ub87PqFdbXl5wnMR2eZlG5F251K76o4SR5FnotJNPUu8kn3jddvoAkgfTAckDj6yOS3VrOTKSfBFMCBTnoZU8C7tKy2Lrd9WV+E3JR7pb0m1djbSj29hd/VMbqpZvlZfp+6hLy3VryzM+k+d6hf7gRQO1xZR8uqYZtXMmIHkwHZA88Mhxkt9Pt5M/RfJD+rSd7F4kv3a3vKM8oWunLfnNnTzF+a774vCwfN8t+f4uyWxA8mA6IHngkS3J17tNe8dpYX7Du8mvFVF72zmERpiFt7xd30qeed3b9Xt28jZ93U5vp1+Ol/z99fLhwwcVrpd7Ov347Xq5+5cO/r1bLvT1z3z1frkO/wXg5eyW/NPtcn7VzH4/Lpfzr8/phW+ev54vZ239gVuw+AQeOWkn//jtYrkmZ0exk8CD0C+i5DUk/Otvj3QAyYPX431J/nm5vbql/wezA8kDj7yp5CUeJA9ek+FkylI/O1vOKASRK8kfrug8H2vJ0zHH5XD5g0+QcD/G12dnl0tISXlcfqUQ8vwnCPmW8+I47QKC415R/jnPQ0hX8ieaMkO9wuvz5fYpCj/k/5HEv1LXki7VE/xyIHngkaMlz+Kub9d/UMJPkQIk9r/uFt7Hh+MQN97aB+AljCZTvr0tMn0mYWbJN7KMxyTgLOm0m346LIf0EVzOKywcRKRxERDL4ONGsByX5czHQcrpOh+Hsowy+ZykCXnWC4JcVyXzw4//t0rzPP/Hhu8CSB545KSd/DYsdS107OTB67E2mcYdbhJikO45SZp3yeFyEWeQt+yGS5qyQ06ylYVCoBb74Urly+i41rFZZiv5kn9V11yHRMorL17ALweSBx45WfL3n+vdfPyCnez02x07JA9ej+3JNO18WYRBoEqkenec5Rqp7gTonfxrSd4osz43kDzHUTt5DdcTovcBJA88cprk76+Xi/CFukJ/u14DyYPXYzSZ6l14twMmYa59zh1up3N8Sf9xv+RZtDm9xB0dt2WGPOPryx8jycfjnO7//n/UHQFb/uDnA8kDj7z6Tt4GkgevByZT4BGMS+CRkyUPwK8CkynwyEvHpfXcegBeCiQPpgOSBx558bjsHtFaCE9isx796pD6+fV7H387gJ94N3oi3tq1o2iewLf7iXnr5Efw8lP5fuF7B8mD6YDkgUdePi6tR8Ay8fz3SX4spXq+PAvuVUT8ltQ/hMMLqtd4nC0kD8CJQPLAI2vjsuxu4y6RX0eRkMDVD7aYt+xlh08710o+LI+0+8zn1bPe5VfX8jUtGzp/83dMz/H0M9vzc/Cb/Gv59b8QJ9Q/IlPicdv4h2F0H8QyVZvb+jd1Hl5TdS13PGRx1J5vWZF8WybT9Atj9R8kD8CJQPLAI8NxyUJVgoi35FnuUXr5GlELMlJ+oEbLqF4cPB0e6AxdV8J8oHNc9kjy5VY3pZSnQOU4g/wljW5TQ9UGLieVWRY2hNknRv1VnXW+T/9R/KquJR0LN+bN50W6dZwaLjctBDjkfqHz0l+cPtxlsfrF6j9VX3XuVwDJg+mA5IFHRuNS7/JCkAlfCVDod/JRVCW9CEYJV7BkQmWMJJ/PM1yXqn5G/oQsONa+JFh/Jq+krSVt9YlV/+pc6gupV77W1DW3rRa7tYCKlPRcd734yHcHQuC87H7p+0+VZ7XrJwLJg+mA5IFH1nbylVADURYP1U5edouKRhDlVnK9o4xQnvrWN6PTs4jUsSn/fGzlT9B1vs1fdrg9I5lW50d9slZ/gc+xaKu6lnT1Tv44ydd10OcFo190HdUxJA/AiUDywCNr41Lvbm8OLAp9Gzkdkwzqb9freAkWhkgoHKd8RUbdjjLmEV/fFDlXklVxSGCVsCQvJbtut9sIbJfkibpP0sm2/jl/VUeJr8vWdc314TS95MsiQKhlHu4yDPuT6PrF7r9e8tai4e2B5MF0QPLAIy8dl3P8O/lanL0wvVPX/6di3r14eyB5MB2QPPDIux+XaVc7l9QBJA+mA5IHHsG4BB6B5MF0YDIFHsG4BB6B5MF0YDIFHsG4BB6B5MF0YDIFHsG4BB6B5MF0YDIFHsG4BB6B5MF0YDIFHsG4BB6B5MF0YDIFHsG4BB6B5MF0YDIFHsG4BB6B5MF0YDIFHhmPS34eenrsaQj6iWv141q7J6Lpx6puPRI1PG61eZqbfvQr076u6mY9jratewx4IM48QPJgOiB54JFVyTfPRheZ8/Pbtdir58Kz4LXY6fWaXMMvxFGoFgqrkmeBa7HT62oBwFjnwExA8mA6IHngkeMlz5Id77wr4W/ylH7Bri5rVfK8iNgUONex38l3dxyAWyB5MB2QPPDIquS1JLOEGyEzLOF0bvRrbiGO5CWSpnPyC3ZVug3Jr8vaFrwE3LKfA0geTAckDzyyZydf785ZoqOdfPysfq9Iw8+jKgFnea9IPqTR18C7BJIH0wHJA4/skXwr9qM+k/+Pf7ncovn5VBb5oDz9fYB4Td8toPyNOwdcJ72A2PwCIHAFJA+mA5IHHtkn+bSDzq+P+Xb9p3xLXtPvyJu7AFUeraBZ9OkaCf9T+Fxf0d4JYCg/3KqfB0geTAckDzzyXsdl+1EAdvJzAcmD6YDkgUcwLoFHIHkwHZhMgUcwLoFHIHkwHZhMgUcwLoFHIHkwHZhMgUcwLoFHIHkwHZhMgUcwLoFHIHkwHZhMgUcwLoFHIHkwHZhMgUcwLoFHIHkwHZhMgUcwLoFHIHkwHZhMgUcwLoFHIHkwHZhMgUcwLoFHViXPgxYBAQEBAQFhzoCdPJgOHrgAeAPjEngEkgfTgckUeATjEngEkgfTgckUeATjEngEkgfTgckUeATjEngEkgfTgckUeATjEngEkgfTgckUeATjEngEkgfTgckUeATjEngEkgfTgckUeATjEngEkgfTgckUeATjcj9Pf39abg7pBXhTIHkwHZhMgUfWxuXDlz+WP/7g8Gn5/l84s9z8+X15ClcFOvflIR33IuTXMQ8VujyYp+X7nxLnhnIl/vu+fFJ5h9dtXhQ+/S25UV30tZT24YvUn2ni5JDKzOj6UKA6P4S20fkvpf6lj+q4ffvAMUDyYDogeeCR4bg83NTyDOISQSohUjwtNZG6pH36+0YJlqklKVSLA86TBS1SX5NmVU9NWXzUki8EQetFhAXlL/WKdbTrX9i6DvYAyYPpgOSBR46WfCVclj4JX+24T93JDyW/Q8LH7eSZGC+Ux2V1u3hFkHzZ1Y8kX+rf9hE4BUgeTAckDzyyNi7Xb9cnwYdjIolZy9oUvIROhEWkw9v1Ap2/EbEHCcfDUCcjvpa8eXtdgpGW21DuStQ7+ePaB44BkgfTAckDj6yPSy1eDis73gHHfGZtSnND8vXHAduSPw4WOi0ilNT7nXzbR6eWBTSQPJgOSB54ZG1c6l1sgHfWjaA7MTfX31zy/1Wl1bfrU/pa8q2UUzDqxHXPt/QpH0vyOU5G3+0ApwLJg+mA5IFHXiZ5Q2jV7XOW4P6dbf8lPY0hcBVq0db0km/vSBjtoLZ+13keHpbveyV/wh0PUAPJg+mA5IFHtsZlvRPv5XXSTn5wS9vcyb/CrvjUnbxNLfk+v/2LGjAGkgfTAckDj2BcAo9A8mA6MJkCj2BcAo9A8mA6MJkCj2BcAo9A8mA6MJkCj2BcAo9A8mA6MJkCj2BcAo9A8mA6MJkCj2BcAo9A8mA6MJkCj2BcAo9A8mA6MJkCj2BcAo9A8mA6MJkCj7gdl+rRteD3A5IH07F7Mn26Xc6vmmd0/rhczr8+pxcz87zcfrxcuieQvpv2zcdwXPIjbPXT4JJ04/PbY5T6CXXpSW+cznjevPk0Owr6F96qc7k861G31qNpI8c/NU8el1vXv/vhm5RXfoStfnyv7isKsU2UZvDc/a5/mkcBF4zH7SbMJwla5Qn8/P02PgXdnnxevd9r7YzlNe+FykfnXT0eeQeQPJgOSJ6B5L2xPS7TY1zTpF9L3hCwJTFBTfbD59RLnFyeFY+lO3o07iDfAbk9Uu/0X51PeSyuElojvyDEihXJa1G2QtSEuPZi5uGLff4Y9HtZSAsLaVPTzv691ZLXi5K6ryB58O4ZTqYs9bOz5YxCEJ2S/OGKzvOxliAdc1wOlz/4BIszvj47SwKlPC6/Ugh5/rPcXt1SSHHaBQQRyhmkN8v5eEuvYryq7hzn6nK5lHhUVs47lMt5nC/nbT6r7QNvyTE7eXktk34lVJrIs7ROkHy1kyeBcVkc15J2lC6LvheduZMf1YeoBJ4WM7Xk03k+5Fdy/iWSb8/rvDJJkhzf2M1bO/ljRfr2kj+9bpA8mI7RZPr89TzL7PmJRCeSb8QXjw/LZZY0C5Uk+UQiTX8/Oa8gX9kxRzkXUTc7aco7yzTIPdVBCT/UR/4bzj1TTqn8eIZkzvGjxG9DfaiuWdR83F7nNOn6WvvSK/A2rEm+Eld6rcXQCZUFYIrAiJtCkX4jnFxekbyIrcRj0fO5IntrUbBGvUtP9aL6v4rkm7oFuH/C+TqUNkk91J0KSaNk/zo7edVX1J5cn8Gizn5vteQJnY/EpXOQPHj3rN0WjbtdJVXe7Z4VGWYJBvmmXXAIMU3ZiSdpaiEHsaa8icOVypfghUHJj0IrdBav2nHnMvi8Thfqq8uyjuu6cNmhXRvtA2/Hrp08T/R/95I3MUWg0bu9wh7J78FeTKwIUWTN9Ra5VZJvFwIprz2S7/pBxG+H1X5tMD+TN/p1jdIuohWxtKlp56bkLSB58DuwJvlI2u0GybNUa7nmna6cS1R3AvROfqfkS96KkeQTcQde5xuxxK6P27oYO/mmLPC2rI9LJeQ06bcy7iRsikAzlnwRFsnncJrkj6eIN8u+kXwXh2nll65LuDlQmtV+2CHHN4X6Vvcrtaeqf1rUbbXz098PXTu6uwyQPPgdGE2meheed7MiWN45s/S0iPmc7HRl1y3pP+6XfNjBpzjrdwKSeHW5ImJVdjy3Q/JUfk6j2jlsH3hTdkt+wC4JG3IoQe0mNZSmlnwRrRVYRPYuPoXUjmr3amFK3kDLz+REye/oK/s3+lMIZbbv23bfdaT+325n347X+CgBkgfTsb2TB+Dnsyl5Qwp6V2aKdWNhcAyvu5Nv5WfwqyX/Gpywc+54keSb8cBhtS96IHkwHZA88AjGJfAIJA+mA5Mp8AjGJfAIJA+mA5Mp8AjGJfAIJA+mA5Mp8AjGJfAIJA+mA5Mp8AjGJfAIJA+mA5Mp8AjGJfAIJA+mA5Mp8AjGJfAIJA+mA5Mp8AjGJfAIJA+mA5Mp8AjGJfAIJA+mA5Mp8MjauNz9NLv0lLiK4ZPSBk+dax/nGvIbPRXOfhLf+pPZwExA8mA6IHngkXXJ73ykbCvoNemGuIa45TGqFSuSP/IxqWAuIHkwHZA88MjRO3lLrrt38knaHL/dzTcLhZj2uJ38i5/XDtwAyYPpgOSBR37OTp5lzefUL8BJGpG9kcfNwZK8LXgJuGX/PoDkwXRA8sAjR+/ku1vtbynd0U4evHcgeTAdkDzwyOuOy5dKWXb8JYxuwXc/Z7r1E7JgKiB5MB2QPPDIaFzau/gUhkIdSH5wOz+Gcgufy2x3/g9fBvnt/jY/mBFIHkwHJA88MutOvluEYCf/roDkwXRA8sAjGJfAI5A8mA5MpsAjGJfAI5A8mA5MpsAjGJfAI5A8mA5MpsAjGJfAI5A8mA5MpsAjGJfAI5A8mA5MpsAjGJfAI5A8mA5MpsAjGJfAI5A8mA5MpsAjGJfAI5A8mA5MpsAjGJfAI5A8mA5MpsAjGJfAI5A8mA6eTBEQEBAQtgMkD6aDBy4A3sC4BB6B5MF0YDIFHsG4BB6B5MF0YDIFHsG4BB6B5MF0YDIFHsG4BB6B5MF0YDIFHsG4BB6B5MF0YDIFHsG4BB6B5MF0YDIFHpluXN5fLxffHtMLevn5erlPx+DX8fjtYrnOb8Tjcvf5jv7/dCB5MB2QPPDIeFzSRP3Xh+XDhzZcLHf/xhj3n63rtnRrCXDamA+ft9LL9QzJvY73Ici+SL6tbzz/+O26zkdo8ot14zwkXVsvrk+53lL3xaBsKjP3wb93y7VarAh1PqkPKO7F57bUwfvzV5TrMfVv3xvBfH9T/kJfDvcllZMlf79c6+sp/dbiDJIH0wHJA4/sGpcsxE4yFmMJZpFkuRaJF1FwWpFCI3lGiznVx5ZFkcxQ8pos377+dfpx+wolTls2tzPfhRj0qV7c5PSm5C2o7zoJb9efZa7vjgiW5K14WvTxeun/Gqrf6vtWgOTBdEDywCOb45IFQ+K414JKmDs9cyfPkz6lV6IqMlNiUrvbbidfCaoc27IoeQ4lrxcMFF4q+bovYpw6LdeJFhMhfcznumsj57NX8pyHLjOFYyVP/cDvK9e/3c2b72+Tf2yXnJNjKseSvGoHJA/eHZA88Mh4XNKE3UzqsmMTGfQitmCxpHhq9zqS/AVN/qOd/OM3WmzEo+XuWy0LvZvUQrrfKXkO4TZzI0EWXVncGJJMWNKSOl18+1/UJrnO/SptU32TqMWarg0lb9dFM6y/tL9aeEk/pBM7uaf3JZZwT+8LH1E5SfLmQoHKhOTBuwOSBx4ZjUtTmhKSlM0JvNnJdzvp+7vwut6xSlklrb2ASAsPtQDoZJF2pkJX/iqNOMOdBRJX3pWOxWpL3ip7n5wD0pZTd/JH1L/Ffm9TaOvC9QvXJG8qp9nJ83usFw+QPHh3QPLAI7vGZSNOTZm81Y68g641Ymgl3kmliW8uOqi8u1eRfF+/cE6kFSTGx2uS7+tm30XYFq0p2GHf9gKNHFf/Ho5r1KN7H604/L5A8uA3Y7fkn26X86tDepH4cbmcf31OL16ZXN7zcvvxbLn8EU+7xuojYe0a6DhV8qZ0JXRCsiSqCDvOJsWujwIMWQwkb4swwmXx5+PrErpf7tXt/LX8hLLAYOEa/ZTCKJ/Qx4N+29ppb9V/K73NxvuYoXIgefC74V/yEwHJvxov3cnvw5ZcPek313fJZFsWUbTWDrbsVKUeUoexvCUfK7+efXcRWkpfhT6nvo99sm/Rs86+eo85XfItkDx4dwwnU5bS2dlyRiGIXEnqcEXn+VhLno45Loe464478HjucgkpKY/LrxRCnv8st1e3FFKcVoBVeefL7VM8J+lLOcTRZVsLE0pzdblcSjoqO7QzHQe4TiFPdU6X9fG8iLytk2oP2Gb34hOAnwgkD6ZjNJk+fz3PEn1+UpJvxB6PD8ullh7J+/mJJMli5jOSV5Bkkm6SY5GynE8oKWrJ5/T5+illW3AdUjmcp8g5HHO6uo6SL/+3WugM61TaA7aB5IFHIHkwHWuTadzJKqnyTvVMREiI5INA0242hJgm74RFmJXoamlmkQsqbiX5nJ5E+jHJ8+iyLXR9rONUXjhHpLZX9ZYyrDptlg80kDzwCCQPpmN7Mk07WZZUkJySnd7JawES1Z0AvZvOonslyZ9UtsWW5Ov6Sr7837yTz2X0ddouH2ggeeARSB5Mx2gy1TvhvFsXSfFtaZZYlnw6JztXjsfxJf3H/ZJnaeb0Ke665Imjy7bYkjyh8i2fyVM95Jw+b9VptXygeTXJN1/O2/piFQBrQPJgOn7LHZMWcAqy8wc+WBuX5j+Ta/95XP72dwks+yL58i32GOL57pvn+YEqOt59/TQ8db18I77/1nn5pr5cU/nwK/Ofj0l9m38JkL5Nrssx+4VCWeS0baZg/bNCfZ0Dl6UXS1WbU/3o+vjb/+8HSB5MB26LAo+sS37nPwHTos9SFGlqyj+tWs9b/qlXLWch/JvrbyLARvKtJEN6EapVJ0bKa6F0huQzQcLGeUPE7b8TL5QyAqr+VZmSp5H3ewSSB9MByQOPHL2T70SpBVmObckXaduSl0WAxFNy3ruTH0neWCwUdBukzBQGkg99E/LktHoXH2nvFrTXC1TeQPLYyRcgeeAeSB54ZDQuR7ekQ2iE+Xo/HCOSF5ScSXjVU/GyAPud9NbteqH6SCGX20g3kSXfLDbqQHEeNq63bQ75ST0IkbxZDsWD5CF54BNIHnjkdcal7H6LxLqdvN6hEu1O3vqc/IIWD1rylfTSefM2OpWl89HxNaWOWuwbkt+DVb4leCLUQYu76adQn1B36eO1J/K9HyB5MB2QPPDI6rg0dpOtYEY79ZN+OEbLLiCCS3B9GgG/iuT/5ZvuQpFpDlRmVY6VPwUt6v56X0/uO+kTXuSEY91Pob3UJ7ruXR+9TyB5MB2QPPDI2rjsBdreTh9z7E4+sCUwQ/ImK/kc8zGEppW8bgvD+WrJt9c7qC3tj/I83t9TaCW/o73vEEgeTAckDzyyPi77Xe3eXWQn+YaR5HVZMah8WHrddaNOW/mcwMt38mtfvlO0kjfykS8DvmcgeTAdkDzwCMYl8AgkD6YDkynwCMYl8AgkD6YDkynwCMYl8AgkD6YDkynwCMYl8AgkD6YDkynwCMYl8AgkD6YDkynwCMYl8AgkD6YDkynwCMYl8AgkD6YDkynwCMYl8AgkD6aDJ1MEBAQEhO0AyYPp4IELgDcwLoFHIHkwHZhMgUcwLoFHIHkwHZhMgUcwLoFHIHkwHZhMgUcwLoFHIHkwHZhMgUcwLoFHIHkwHZhMgUcwLoFHIHkwHZhMgUcwLoFHIHkwHZhMgUcwLoFHIHkwHZhMgUfcjsv/vi+fvjykF78nT39/Wm4O6cVvBiQPpgOSBx4ZjkuS7M3fT+mF8LDcVOJ9Wr7/+cfyxx9/LJ8kbpPu4cun5ft/6UWA09xQTjVFaKmMJPmnv29U+q3yKC1d4+t1aOvAlLz++CPVp11Y8OsuL1V2W15KW7d5VKe2D3R9KPz5fXkIfULnv3ynq5GHLyqOilt6/H0AyYPpgOSBR14ieb3TzGJr0plS6gS3T/J7ysscbpSMe6pdMsUNghapr0lzmG/pm35hEwl9ofrPhPKXesU61pLv2bo+J5A8mA5IHnjkJZJ/+KJkLXLqJN8Kj3estuS7xUAj+T3lCVzuJ6McYSj5HRI+bifPxHihPC7LWORkQrvKrn4k+VJ/yhs7eQB+PZA88MjRkg9Ci5J6bclnaSnhHi/5KMicl3mrnikiHd6uF3QZUm4g1bVBt9m+k5GCkZb7QRYRsU+K5Pm1mQ+HdyZ6SB5MByQPPPKSnXwRsxJbk+6n3q7nNJ3Uo8ytW+ymNA3x6jLq7whsS/44WOi0iFBS73fyenHC4dSyfAPJg+mA5IFH1iTffuns5tBKrQgnS1RL9wgq4Q4k/5rlMSdJ/j9dFvWHkb6WfCvlFIydNy+IwiKGb+mHtveSz3EyVAfcrgfg1wPJA48cNy7tnWuFFqIlUQlbYjIlb/CC8tbzNgSuQi3aml7y7Z0LQ8zUju86T1pQfd8r+bXP+CcFkgfTAckDj7yl5F+DYyR/LOai4BV2xafu5G1qyff54XY9AC7YPZk+3S7nV8024cflcv71Ob34VTwvtx/Plssf6eUpWG0DvxQsPoFHIHkwHfNL/hWA5N0ByQOPQPJgOoaTKYvv7Gw5oxBErkR4uKLzfKwlT8ccl0PcVccddjx3uYSUlMflVwohz3+W26tbCilOK1kVt+RJqHKk7MPV+XL7dKC4qRzi+et5TNPVy4Db9vG8bi/TlsWvcz25fbG80B8hXip/T5lgFUgeeASSB9MxmkyzJPn4SUmeBKYlGI9JsFp+JO9nku4hfWCX8+I8soj1bfYizIyOK2UThx8Si8r8SOXwUZC8KkfqYNUrvaoIkpdrZbHQl6XqSWkuk/izyNM5qQ8T+g4cDSQPPALJg+lYm0zjDlWJNux2i8Cy5Pla2rnqHW3Z4SaZczxjJ8xoMQaquEXoVVmN5HMa+m8QsI4bQimvoiqL66Xya8oSqT9/vQxxeGFR8qcQ8uE8VBpwNJA88AgkD6ZjezJNO1sWXpCWEq7eyTdCq+4E6J38iySvyynHJW3M8zYJ2KqXSVtWWAzYZYXj8DFDep37wCD3GTgWSB54BJIH0zGaTPUuPO/WRYQktrBL1YLjc3o3y/ElPe2M90o+7IwlvRZvkmWp1zldl3NlgRDSa7G29bKgsi6pjhJPFidWWQyXocVe37FIu/gmL3AckDzwCCQPpuN3m0zDIiAJOIbmDsIOursO4NWB5IFHIHkwHZhMjyDdnRjengevBsblXoyn1IE3A5IH04HJFHhkOC75sbL6CW3hSXfGE+/4OetdvIbql9vWngi3/mtw4Ql16nx8tnt60dTD+kEaof7RnFimfrKe9SS88Nz+geSrX8dj2vq3fSShayM/olb3TV1mWy/z0brt+xbCfE/Fg+TBdEDywCO7xmV+dGwjeRZKI75KvMKK5Kv4IsdWkgkW8Q0J1UxLZRSx88JhLLZOykT7+NwiVMlnTfKtVCm09ddtavqjwP3LbZS0veRLOrpmPbM+v1dzA8mD6YDkgUf2jMsil32Sb3fR7bldkmdRmguIIr4qbSd5Q4CJbclrucoxS5UF3qd9VclTWQ+5XboeTXubaxnpuybY5fkFkgfTAckDj2yPSy0TQ3QkrEoordyIIEF1/iW361lW7X8DTT3WpGZLPu7c40KhaXM4Hkh1D20fSeja2JZVlyl1jMG4UzEQ/DC+YyB5MB2QPPDI+rgkyVQ7V3qdxcTXLJnEkCWbhK3FXkveYEPyIsOy4yWqnTyXMf5c/jVv15u7eAldG9bvMOgyYlvrMkv7mbg4WlvMzAwkD6YDkgceGY3LKLlWSCQdQ75DeYWdpZwvUmolX8uL2JQ8wbe8+fNr9VpLneOOJd/IOCwYaslLfXW8TaFynVbj7Jd8iBs+nx9JPrajL6+vt3V3xTuQPJgOSB545LhxeZzkn2gn2mr26b+nbckPqONFmeXX7S3xLMt6NzzC2snXdWLpWvnYbS9Q+bpeTajLaOrKbVKvuU5Veuu9aBY7TNu2GYDkwXRA8sAjbyn5EZbkK3lx2CHmXRjSs+hFGBcRuk72QqSPF8Jr1f8EujsV2MkD8PZA8sAjGJfAI5A8mA5MpsAjGJfAI5A8mA5MpsAjGJfAIydI/n65/nCx3P2bXhKP3y6WD3/dLY90fP+5vlZxf718+PAhhItvHLuJ/+/dcpGu6yBxQzn5fErHaT7fh+s6/yrIdfAuwGQKPIJxCTxymuT/uliuk3hJvcsdifpiU/KcLsYJaSgPjnf/OYr4euRhErdIXvP47bqXfIYXIit5gqnBZAo8gnEJPHKa5D/fkdiTsEmy198onCz5UfyEkvzmTp4Ju/lrKo3zxi7+PYLJFHgE4xJ45MSd/N1yn3bScUddBP6i2/VCWDjE65xGduR5967Jko+797IIqAN29e8HTKbAIxiXwCMnS/6R/8s7+m9JsHskb4p4XfJa7OuSB78LmEyBRzAugUdeIHkWuuyQ31Dy/8b/MvXt+pT2XkmehV9dj8H6TB/MCyZT4BGMS+CRF0m+UIu/CLYVOMUb7rqtBUAJw9vteidv7eoHX9wD84LJFHgE4xJ45ATJvwRb5C/6vLyVvJE/JP++4MkUAQEBAWE7/GTJA/ByeOAC4A2MS+ARSB5MByZT4BGMS+ARSB5MByZT4BGMS+ARSB5MByZT4BGMS+ARSB5MByZT4BGMS+ARSB5MByZT4BGMS+ARSB5MByZT4BGMS+ARSB5MByZT4BGMS+ARSB5MByZT4JHZxuXT35+WP/74o4QvD+nKZPz3ffk0a91/ApA8mA5IHnhkPC4flhstUwqf/n5aHr7c0BWhj8Ph5pAus8iqazHtw5dPy/f/YhRNFvif35eneGb5/kWOCUOMnCaXpznc5HK53kxdLuX9p65bCrnsdkGR0lZ1GOQhcRVdXoeYz9PfNyVu118xmO1750DyYDogeeCRzXHZiLWWvAHJtZVSFnGWmCF5XQ7lEcXcSJ4XFUrCzMOXIvGCjscijuVx3HVp9vkLWcZNfwhB4oO0Bcpf0qZ8KslbGP35OwDJg+mA5IFHtsZlK68tyXfSSjLTu2hzJ1/JTGTYSp7IC4U1YY8kb5RbUUt+eyfPxN18WGiEuq2VEduV8x1KvtQjL5B+MyB5MB2QPPDI6rjkW94sNP5vkk6RPIkoC7APLKZ6gRBlyOdfJHmilm8pr4LrnK7Zt+sFXYaUGzEFrCRv1SMHtVjIcFo5n/IpZWz35+8EJA+mA5IHHhmOS5EZ/fdGdqn0evN2fUctTsaUrZTHkKD72/VFgv3teQtLmhuSl7YmtiR/LJzfDfVfyDPl05WhFichnFjW7EDyYDogeeCR0bg0d6m0C/1eSb4XeM9e2aoy9e7f2MmH839uLTZOkfxTVVbfB5Q+fWFOkM/562DUjeUd0nG96LoleT7X3AHgOvxuu3gGkgfTAckDjxw7Lrtv1++RfBNn+7NxYSD5tNtd39Gv182WcwprbWp28n1brAUInftbn3lYHv7eL/l9dy7eF5A8mA5IHnjkxZLflKQV5zTJ5511yr99XWPX7cW74k7yfRnmTn4AbtfbQPJgOnZPpk+3y/lVMxP9uFzOvz6nF7+Kw3J5dr7cjjYVVh2ttgBXYPEJPALJg+mYX/IbQPJTAskDj0DyYDqGkymL8OxsOaMQJKnEeLii83ysBUrHHJfD5Q8+8bzcfoyvz84uab9NUB6XXymEPP9Zbq9uKaQ4rXRV3JJnPF/VK5ST8g+7+pQfB6njFYVwLu34OY+P500+hGqDPsf14LiX/7stG7wVkDzwCCQPpmM0mT5/Pc9ifX4ioYnkG7HHY5JrljRJl+T9/HRYDukWes4rCFqEHBcBZUEg5xM6rpQtefM54kDyPqi0us7xGsHi/pjSSH2D5CUfXhhwXPpvPsfp04JApe/6BLwZkDzwCCQPpmNtMg07di3asPtVn39raaYdbgwxTUwfQ5a8XgwosWepClVcEXCzUw91UfnkRQefK5LOixLJs6sHxf0/+pwSuk5PVH0C3gxIHngEkgfTsT2Zpp0ui1FEKzveLMB6F8xUu169k3+R5Os0kVrysgDIYt6UfGpfLiOdVTt5LfmIpAFvBSQPPALJg+kYTaZ6Fx4kp8XIMmUhagEqwYbPwjm+pKed8l7J84Igp9ciFgGrfONt9JJPqI9c43K5brqOkif995LqJPFkMaLboNslx12fgDcDkgcegeTBdLynyTR/Dh+od+ZgLiB54BFIHkzHu5pM9d0E3E6fGkgeeASSB9OByRR4ZDwuH6qfXQ1Pdhs8fY2v6Uev6ke9hqfSqXT2s9jf6Ol0YFogeTAdkDzwyB7Jbz0/nR91y7+uJiqvJa9+eS28Hkj+N318K7CB5MF0QPLAI5uSz7+eNiZIXf06W7uTvznUC4a9O/nf8YdZQASSB9MByQOPrEo+yJZ28fpX1fgHWkTEjdTb/zIi9fa/BVvwEvoFAfgdgOTBdEDywCPrkk+y3tjNZ6mnX2izJB/yo938g7mTB6AGkgfTAckDj6xKvvriXRF3Sy31lc/gabHA1yzJdz/ZqsoGvx+QPJgOSB54ZK/k4216+3fS6wWAugNA1Lfnn5bvfxq34JvfaA/wggA7/t8WSB5MByQPPOJlXPJiADt5IEDyYDogeeARjEvgEUgeTAcmU+ARjEvgEUgeTAcmU+ARjEvgEUgeTAcmU+ARjEvgEUgeTAcmU+ARjEvgEUgeTAcmU+ARjEvgEUgeTAcmU+ARjEvgEUgeTAcmU+ARjEvgEUgeTAcmU+ARjEvgEUgeTAcmU+ARjEvgEUgeTAdPpggICAgI2wGSB9PBAxcAb2BcAo9A8mA6MJkCj2BcAo9A8mA6MJkCj2BcAo9A8mA6MJkCj2BcAo9A8mA6MJkCj2BcAo9A8mA6MJkCj2BcAo9A8mA6MJkCj2BczsP954vl7t/04p0DyYPp2D2ZPt0u51eH9CLx43I5//qcXgDwemyNy8dvF8v1fXpBFNE8Lnd/fVg+fGhDERGn7a7/dUcpFf/eLdff4pn7z9fLPf3v+vN9KZeuX+j0dC3GbYTXxLvIedbxxu0p3H+WfOQa1UnX+/46l6OD5FvSc+A28cnrXCezXyjI9QiVqa9Tu2NdY//EKFY9+vbMCCQPpgOSBx45XfKR8pqk/7kW+OO3623hVHIeSF6kpujkrBYLWqhtvE3Jq7Sh7CB3EW4StkGbrxAXLnyg8hVC25vyG3QfxroqyRvs6vMJgOTBdAwnU5b62dlyRiGIXEn+cEXn+VhLno45LofLH3ziebn9GF+fnV0uISXlcfmVQsjzn+X26pZCitMuIJryn7+eqwXFYbn8eLs8U5mXV1Lu+XL7Q9LQ8VOKCqZkr+T5v9ZOcV3yxo61FZSSc94Bt5JX6a9JmvG4kWMjeYlfx+M6XiRxR3ZLvr0D0aAlv7WTZ0LfhDzjHZFuAZCI0pZFBtfVkDzXM53r2jMpkDyYjtFkylKNsqbjJyX5RuzxmKSbJU1yJ3k/Px2WQxJtziuIOwk/LQLKgkDOR7ryRex8guXO13hhkcrl+Gfper0gADOyJfkgLCWVSuqbt+s97eRZlOk4lBnla0lx83a9QOWI2Osdt7HjlzpV7W2DXZeyeODrSfJH5jMbkDyYjrXJNOzY1S78/CPJU++SRfJq16137jF9DFnyejGgxH646nffVfnhdYxzoN17OGcuOJpjMCWrkk+C1SK0pNjuUgVzJ79yyzvyNpJ//HbXlEvtohOj9nT13pC8Fvuq5I+C+4LKSO2PdU2ST7R9fHwZPoHkwXRs7ZhIqcslizZInnfK9Y46ylSdS1Q7cb2TP0LykVQ+H4bybstdA0j+3TIcl2GnKLIisaTd4TGS381gV5olX50f1KOJ10o+wHGaBcPrSP5xeVR51LfrOVA/6j6y8qcgC4YI3ymJdWORc9pO8pzPnvZMCCQPpmM0mepdeBCmFjRJNNwab8Qq8cMtdLW75zsAeyUfbrtTnK78GCt8ni+LB0j+/TIal4+0za2lFkUmErF36SmQEA8b17eWBHknP2CvzKp4huRNsrwNBguSGFbq1Ei+XRR17b2/q/Li94O/T7At+XaxMCeQPJiO7Z08AD+fY8flz9opvpnkDTl35bA8u3hbHzNs0Ei+z3+vnHG7Ph0B4AtIHngE4xJ4BJIH04HJFHgE4xJ4BJIH04HJFHgE4xJ4BJIH04HJFHgE4xJ4BJIH04HJFHgE4xJ4BJIH04HJFHgE4xJ4BJIH04HJFHgE4xJ4BJIH04HJFHgE4xJ4BJIH08GTKQICAgLCdoDkwXTwwAXAGxiXwCOQPJgOTKbAIxiXwCOQPJgOTKbAIxiXwCOQPJgOTKbAIxiXwCOQPJgOTKbAIxiXwCOQPJgOTKbAIxiXwCOQPJgOTKbAIxiXfvhZv9U/A5A8mI7dk+nT7XJ+dUgvEj8ul/Ovz+kFAK/H6ri8v14+fPgQw+f7eOrz9RKPIo/frpWY7pfrFK9A5ySPKp8ktH/vlgt9/UPMv8438vjtQsVL4a+75TFc3ShHcf9Z4sk1Spvz0dd1KO2ur6fz1FcX32IOZj0pyPVIX99YV9WHuv9z+D0WApA8mA6/kn9ebq9u6f/B78h4XLbii3I5XvKKSui1rFiM15w0x+llZonfptSjk7yScSgrtFGEW7et8Ljc/WVfy/2h8xVCW9alrNvUSd5gfx/MDSQPpmM4mbLUz86WMwpB5Eryhys6z8da8nTMcTlc/uATJOmP8fXZ2eUSUlIel18phDz/CRK/5bw4TrOACGWEtOfL7RO/vlxuv56nvFTeH2UhYJ0Ds7Jf8h+ChFvJ1xJdFxTHvfjLkDzL8PNdJdLdO/lcnoi6Pr9b8qqtPbXkt3byTKhryJPTtrv4Qmyn1H0g+dA/g/a8UyB5MB2jyfSZhBplTcdPSvKN2OPxYbnMkk478KfDciA5hzOSV1g4JOEnKZcFgZwXKE8la5a+lPv8lYSf8uY6cB7WOTAvq3eY1O1ikVQt+VZgIqt6ISDnw049IbIqMgxnKZ6ctyS/tos15EhYUty6XS+UtlI7aRGSW9ksdAIi+bB7l/zbYNdF+iXWNbXjyHzeG5A8mI61yTTupssu/PzjOUk67qwDIvkg77SLDiGmKbvxJHNZKARqsR+uVL6BVvLlus6XA9fBOgfmZVXyBpXggtju1S7XEK1aKKyJqt2l6wWBYO7k84Jiv+TNOq1Kvs57VfJHwfle57wrySfaNh9fxpxA8mA6tidTki1LO0iepavkq3fyzS3y6k6A3sm/guR13oJ1DszLaFyaQiUR3ongWJTdDtwWbbze5KUlr25HR8afgY8xyqA8X0Xy//JN90J9u54DxdOSt/KnUC9cuI2xbtzXnLaTPOfT9KfZnncIJA+mYzSZ6p1x3q2LoEnu4XPvLPl0TnbSHE/t7vkOwF7Js6zj5/Pxdj6n57T1IqBck8/s7XNgVk7dyd/f1/KJrEjePJ/oZFYEKNi7+BQMQQsjyVt3CphTy+E8teTbHTfnW5V5Twsm3T7qz7tdkrfvcrw3IHkwHcdOpgD8DE6VvM2K5A1palm1cn0tke3eyQ+/Wb+TRvJ9/nvbVPdh2y/t4uG9AsmD6YDkgUcwLoFHIHkwHZhMgUcwLoFHIHkwHZhMgUcwLoFHIHkwHZhMgUcwLoFHIHkwHZhMgUcwLoFHIHkwHZhMgUcwLoFHIHkwHZhMgUcwLoFHIHkwHZhMgUcwLoFHIHkwHTyZIiAgICBsB0geTAcPXAC8gXEJPALJg+nAZAo8gnEJPALJg+nAZAo8gnEJPALJg+nAZAo8gnEJPALJg+nAZAo8gnEJPALJg+nAZAo8gnEJPALJg+nAZAo8gnEJPALJg+nAZAo8gnEJPALJg+nAZAo8gnH5vnn6+9Nyc0gvJgKSB9OByRR4ZDwuH5abLw/l+M/vyxMffbmhV8zT8v3PP5Y//lCB4/z3ffmU00XJVHFCkDxqHr608SjkvJryUn2WprzM4Wb59HeIUUPndf4xDuc9aJeUM8iv1PnT8v2/cCb3VcToJ50vUbc71aMqb5BHLlPo6/4QJE/nv6g6NX0QQ5vXrwWSB9MByQOPnC55oRFIJ/mbF8hD10GjyhxIPiwudsuf5WgtPFQ5Vjp9jusR+ojqHKRpL2Qi7UKgkPvXrGdq1yBthtLK7j3u5Jv3yOBl79PrA8mD6YDkgUdWJa93em8u+aY8CTkvFrE6T/X5LjvgVuapDiw4S8w5D9q93nyROw3Srr6ckMMxkt+ScBNneyfPxHqFc1zW2s47SL60Yyh59V49UD9A8gC8AEgeeOTlO3mWsxJEJ/l9t+vNW/USQn4srT5dVV6QH8VXApXyszAtWVd5FznmwPmZ6XS9pQ9Gktei1X3LeRjtUuXZfZiCUZZe3PBxJXnpIzP4ET0kD6YDkgceeankw0798H25EQE2ks8MJFkw5MohS0yLWDEqb8Sm5AeM6k/nx3XW1KLN/UVsSf44uBzayaeyOskn2oXDaWW9HZA8mA5IHnhkVfJKAiKuSkgsuCTYIA0+flPJN9fWyhvRSZnbw3k3ojXjGXTxKGxK/qm63t/FoLKa/rLvdPR14njh83iuF/WLKfl0TYPb9QC8EEgeeOTYcVkkX99yDhwelgcl3Xa3WIVOhIZo93ByeZq27HLnItPsvjMkzLV/orb9McSATvKthI3+ojp+13Wh9+P7bsmnxYETIHkwHZA88Mjpkh9w7M46w9IyRDjaQQsnl6cxhMki3FOPLt5K3GPoJN+WcUw5jeQJ3K4H4JXZPZk+3S7nV82S+sflcv71Ob14ew5X58utr7958EZg8Qk8AsmD6XjPkn/+eolFwaRA8sAjkDyYjuFkylI/O1vOKASRK8kfrug8H2vJ0zHH5XD5g088L7cf4+uzs8slpKQ8Lr9SCHn+s9xe3VJIcdoFBBHKUemz5HW5uV6qvI+3y7OqT4zbXKczhytaBHw9z/kDP0DywCOQPJiO0WT6TPKLsqbjJyX5Ruzx+LBcZkmTTEnez0+H5ZB20TmvsHAQoUbplgVBI1qWdM6T8qJ4q5LPsudzz0niZedf7eopPZfLi4icD3AFJA88AsmD6VibTONOuuzCzz+ek6TVLXORbZB32iWHIDvvci5LXi8GlNjbW/F6kSGs7+QJXhhIWYTOU9eFA6dvywR+gOSBRyB5MB3bkynt0lnaQfJ8m5tep9vdRbbqXKK6E6B38jslH4Sd40ZynFbsXbwoep3n6qIBuAOSBx6B5MF0jCZTvfPNu3Ut1vS5d95R8znZKXM8ji/pP+6XPMtYpN3eCShSjrf6Y1mX8aMCXX5acIS8QhzOT6VJdyMgeb9A8sAjkDyYDkymwCMYl8AjkDyYDkymwCOr4/JVHjQDfjmjp/U5BpIH0wHJA4+sjcvqEbbqMa/h6Wum/Pun1sUnqdWPwDUfP7vy2Fn9tDfOb/g8dp1fCvKo1u6JcaE840l3irqe9WNl2zZUj4TlxZFca9pVpWv6kOvYPnmuPWc/Y775nYEUdJ02n1ToDEgeTAckDzwyHpdazEXyLKn1R6BK3Jg+S81cFAj9Y1dNSOQsLs6zk7wQBGsLLYue65dFvC75LMoQvwi2usZtzfk0C4dUZyb0RbXY0b+1z/31vVt0sJxvlKCHkl/tX0LVYwYgeTAdkDzwyHBcVrd4k7h5t7wlE4mbxLNP8pJmnZwXhV7yLNckcM5PZJ6uBklLHbLwGiE31CKPr2WBU1+r68+LiV6oXJYl6ES1gEnniCD1Q6n7MTv5ajFWvZ/+geTBdEDywCO7JR/EQZKrpKjkouWqg5xPktKi7oKWsoHcco4iVJIPu2wjvxCSFBvJf6K8jtrJE/xaS74rQyF3DYpo1xcU+Xa6richUm//W7AFLyHXv8nXO5A8mA5IHnhkv+STXHbt5neQpUN5b8g9wGL+m+qU5aUkv5Pqdn04c/xOXl7X1zgfa/ce40XRqz7saEVd6pSlnvrL3snvoHo//QPJg+mA5IFHxuOSxJNlXot4UzQspEpadfyQPt+C3iH5ZmERBdtKPoq2lElBpYn0capb2g2VyEObinxryXObRPJPywPVLaPqzmmqOv3HX/0jwgKmpNG3+3Vf82f4/Pm81fd5ASOh7VMqw1qEeAWSB9MByQOPrI3L0bfrW+HVWLtjvWCoxWVRX6/TMqbkG1Ey9RfbiE50uq79YiNIOYuzrnN9jYLU8b8Hqpe6tpYnf0Z/oHLbHT4vDFK6ri+suwFpl1/RtBXfrgfgjYHkgUdWx6Uljz1s7uT1tTZOL9sWU/J09KKdvLFImIlu0dEsyma6Vc9A8mA6IHngEYxL4BFIHkwHJlPgEYxL4BFIHkwHJlPgEYxL4BFIHkwHJlPgEYxL4BFIHkwHJlPgEYxL4BFIHkwHJlPgEYxL4BFIHkwHJlPgEYxL4BFIHkwHJlPgEYxL4BFIHkwHJlPgEYxL4BFIHkwHJlPgEYxL4JFVyfOgRUBAQEBAQJgzYCcPpoMHLgDewLgEHoHkwXRgMgUewbgEHoHkwXRgMgUewbgEHoHkwXRgMgUewbgEHoHkwXRgMgUewbgEHoHkwXRgMgUewbgEHoHkwXRgMgUewbgEHoHkwXRgMgUeGY7L++vlw4cPKlwsd//S6c/xv8Ljt4vl+j69INrrwjje/XKty/kcI1n5cB51nSj8dbc8putVnXM+11SC4t+75SJd4/gX32LqXF7X7g9Ub6pjLudxufurvW6UQ9x//pDzD3DZqR/b/qjRZaR8db2NOoQgdQzllPOxDpxG6tj0eQ51G7j+9XWq+72uR2G9PccDyYPpgOSBR9bGZZ64SXwygY9lGM93ct4bL0DyWZX8tZFG0CIu6U3JV/Wp68VouUVB1nlrpI9syV+QEEs6rv/FX7GcNSlW17j/uE+k3oN6DOtI6a47yRdCWw1pC6UvUtpqsSFw2dTWZpHwEiB5MB2QPPDIyZJPtLKy5b0WjwSRhBrCquSNnXwWSyv5wQ5bS4ra1e3kq3xIjEHSUsdeYrLwsCVP1/LCJOZ1l8pp+0NTXdOSX5FxVW9ZEKTQ7+SZ2KZQTliI9fWvypRjyZvLCuma9yC8tsfAMUDyYDogeeCR15B8kEie8O0JfhyPxGDIyypHpNUFkVvOu9wmP2UnX+KoOqoFBFPkHuPIjleLO1y7TztpypP/K/ErkXewkKVeqf4i2QpZhDCqH1NZNUXy+k5FF3QZukw5Vud253MCkDyYDkgeeORlkmex0O5U7RBbOUfW4u2T/KlYO2wLKc8S18U3kuZA8oK9k5c8eUd/F67LuXXJx+ttPXpxKslrsW9I/hhKf6S0WvxWntX104HkwXRA8sAjo3E5kkyRL0/w6Zgn9iTBXs5b8Ujyq+UoOH0TV0Rp1pfKujMl3y8szPIyFF8kr+4WbJWT8+Q0qTw59/qSf4z/ZYx+uvh2r4TM70l9PQRpo8Lcred6DPLp6nk8kDyYDkgeeOTYcVkkVe9kWSx3NLe3stwbr8W63p9Tkhtg7+TtuwcZdediL2s7eY2cayXeltf1m+K02+QsZC35tr5qIbMGLyAqyTf5qOtbC5k1IHkwHZA88Mipkh+xdV04LZ9+178lkaHkm3yqvKzduvXFNIVdzumM7kxsSnhIK/km7735d5I38gnXrYXEfiB5MB2QPPAIxiXwCCQPpgOTKfAIxiXwCCQPpgOTKfAIxiXwCCQPpgOTKfAIxiXwCCQPpgOTKfDI/nG589vXALwCJ0q+/Sag/vZm823LNJjLNzyb6+nbhcPr6ZuYe79pCt4/uyfTp9vl/OqQXiR+XC7nX5/Ti5/B83L78Wy5/JFe7oLTXC6Hk9KCX8XauJR/qhWfHpckXz0YR895EvpvVG9+U5y/sd1clyfWCVYe1Q/HpG/E87Ph4zWa7/mf1xn/HM6st6pPKau0pZvLq2+ZE1ROW+cI1dGK17b5L6pn+OeAKb7RJ+yddaeo/rBIfcQh11XXm8rsH6Jj074fuu3ttdD/bX9tcJLk+Y2t32zdIXSsKiUVtju0vGmV5I3OheSBMJfkT0EkD2ZiOC7zhK/mRp7jDGkGeaxM4Gv/5pvp//mZ/c+vijwG867U+f6Orq9Jvp2XU1w+VDLi8oYuaKWlZanhvtELHx0vXOP28HWpQ/FLuU4hOye+7t4DJsU3r3G+1qJF1ye/5xtwOc37rfunlnwqs+2vDV5P8rnRqmMVpeIcVypNIXf43uvgd2c4mbLUz86WMwpB5Eryhys6z8da8nTMcTnE3XLcOcdzSbKUx+VXCiHPf5bbq1sKKU67gFBxcx2Iw9X5cvs0vm7XI5Y/Spt3911a8KsYjUueqGW+jBKmOc6SPE/4dP6O5TOYxNudXQxFON38Wd1lFVL51bGkS3mlusVFxYk7eR2fxZTjNHUSaak4veRTPTmOyj/GqxcW1+GJdKms7I/SR7JQGjkltCmnK8cF7quSX84n14eo2ltC23/hPW/y1341F3XSXzs5SfKhU/ferqfADa8kblSwup4HYAGSB8JoMn3+ep5F9/ykJN+IPR4flsssaZIqyfv56bAcWKh8RvLiPET41e1zY7ddxaX807EWdX/dqIfK20wr7TLTgl/FqZK3xZ2Cmgu34h125lPPsXJcz7sil1wmX9PSJjbrreMrMXVzOYtOL1TodSX5IEx1XfLK8VTdwzVeKKU2iWtCGaluo3pQCvaabmOA06q+Caj8cl2lPqG+qawu9B5r+1G3ve9j6gfVl3s4WvKbbyxHMhupJG5UEJIHexnu5ImwY9cy/HhOckyiZETyQZpxBxyDSLWcy5LXIlVizwIWurjxeiXq9vr/seoxkHxOS3L/yIsSKy34VQzHJc2Hu2/Xv5DVnbWizOPWvFuO46JkvJPPiOA0SkZcnqSt53IWK5Wh01t5hXhNuyjoNPGcqi+3I7tG9X0OjVOOlHOHWe8VdpZnvqdvKXmhvFHSoYotybfXqMKV5Jvr3HGQPBDWJB9Ju2SWIMtQpMiX9E5eziWqOwF6J3+S5FMd+Ggk6nC9r4cuZ1XyZlrwq1gblzJRRwkkiZIUivjqOS8EQ87W3KrFuzufLMU+XrlFLJIcSV6Jl9KGjxlUPiGGLCbUucodaufMdQ/9Y8pSlaVCFa/rG+WMVSdZpPcovTKhPPPn7py/Fu8xwldxrdvzfiV/RCWYLYlD8kAYTaZ6F5536yJG/uyahdjcus+7YI7H8SU97bL3Sp4XBG16DrJgqERtXO/qsVvyRJcW/Cq2F59CL/keY14l+nmwjrd7nuzK7qXWzfHD+uo62PXW5Hz/pc1dU9fHfymlKUjKd+P57f2XDmWR0h7vYVvyYQGT4sTFjCrfbMMAFdeW/Mvc9xN38sZnHYqthry0oeD9sH8y/clUIjbYug6m5mdIPqRdmVfNXZ+1Y6Wyu3hDqaW6DOur6zqqd2FzLjcFSfkaO/lqR9t5R5fT9xuHcf+vST7WJdQxlRnry2WkMq3+pWCWp9q7eye/sQDRnCx5AH4VkDzwiNtxCX5rIHkwHZhMgUcwLoFHIHkwHZhMgUcwLoFHIHkwHZhMgUcwLoFHIHkwHZhMgUf8jcu1L4+B3wVIHkwHJA88Mh6XzTe70zfCy7fM229+x3+Kpb+FXh5e08fLNN/ozj86o74V/9JvaoP5gOTBdEDywCP7xiWJ15K8IdqjJR/Q/4Stl3zP9j95A3MDyYPpgOSBR15tJ99dP0HysqsfSJ7zi+fsBQZ4P0DyYDogeeCRVcnrh7Yk9u7kbcGn0KXjBYPIP+WbJH9cPuC9AMmD6YDkgUfeSvLx2BCztZNnoX+WnXst+Uj75LhSBnifQPJgOiB54JFVyVdypkDSryTfXOfHnGrJ97CsG8nzI1bDYoGvcdpe8rxYKMJn7AUGeD9A8mA6IHngkWPH5brE6+t7dvL337SsaZFwf7dP8uZn++C9AMmD6YDkgUd+vuTX02dwu/63BpIH0wHJA49gXAKPQPJgOjCZAo9gXAKPQPJgOjCZAo9gXAKPQPJgOjCZAo9gXAKPQPJgOjCZAo9gXAKPQPJgOjCZAo9gXAKPQPJgOjCZAo9gXAKPQPJgOjCZAo9gXAKPrEoeAQEBAQEBYe6gyZIHAAAAwPsCkgcAAADeKZA8AAAA8C5Zlv8fDZah7cllWHYAAAAASUVORK5CYII=" alt="3.PNG"></p>
<p>일반적으로 머신러닝 모델을 구축하는 주요 프로세스는 피처의 가공, 변경, 추출을 수행하는 피처 처리(feature processing), ML 알고리즘 학습/예측 수행, 그리고 모델 평가의 단계를 반복적으로 수행하는 것이다. 사이킷런 패키지는 머신러닝 모델을 구축하는 주요 프로세스를 지원하기 위해 매우 편리하고 다양하며 유연한 모듈을 지원한다.</p>
<p>사이킷런에 내장된 데이터 세트는 일반적으로 딕셔너리 형태로 돼 있다.<br>    # 딕셔너리란 사전형 데이터를 의미하며, key와 value를 1대1로 대응시킨 형태를 말한다.</p>
<p>키는 보통 data, target, target_name, feature_names, DESCR로 구성돼 있다. 개별 키가 가리키는 의미는 다음과 같다.</p>
<ul>
<li>data는 피처의 데이터 세트를 가리킨다.</li>
<li>target은 분류 시 레이블 값, 회귀일 때는 숫자 결과값 데이터 세트이다.</li>
<li>target_names는 개별 레이블의 이름을 나타낸다.</li>
<li>feature_names는 피처의 이름을 나타낸다.</li>
<li>DESCR은 데이터 세트에 대한 설명과 각 피처의 설명을 나타낸다.</li>
</ul>
<p>data, target은 넘파이 배열 타입, target_names, feature_names는 넘파이 배열 또는 파이썬 리스트 타입, DESCR은 스트링 타입이다. 피처의 데이터 값을 반환받기 위해서는 내장 데이터 세트 API를 호출한 뒤에 그 key값을 지정하면 된다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"></span><br><span class="line">iris_data = load_iris()</span><br><span class="line">print(<span class="built_in">type</span>(iris_data))</span><br></pre></td></tr></table></figure>

<pre><code>&lt;class &#39;sklearn.utils.Bunch&#39;&gt;</code></pre>
<p>load_iris() API의 반환 결과는 sklearn, utils, bunch 클래스이다. 데이터 세트에 내장돼 있는 대부분의 데이터 세트는 이와 같이 딕셔너리 형태의 값을 반환한다. 딕셔너리 형태이므로 load_iries() 데이터 세트의 Key 값을 확인해 보자.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">keys = iris_data.keys()</span><br><span class="line">print(<span class="string">&#x27;붓꽃 데이터 세트의 키들:&#x27;</span>, keys)</span><br></pre></td></tr></table></figure>

<pre><code>붓꽃 데이터 세트의 키들: dict_keys([&#39;data&#39;, &#39;target&#39;, &#39;target_names&#39;, &#39;DESCR&#39;, &#39;feature_names&#39;, &#39;filename&#39;])</code></pre>
<p>데이터 키는 피처들의 데이터 값을 가리킨다. 데이터 세트가 딕셔너리 형태이기 때문에 피처 데이터 값을 추출하기 위해서는 데이터 세트.data를 이용하면 된다.<br><br>load_iris()가 반환하는 객체의 키인 feature_names, target_names, data, taget이 가르키는 값을 다음 예제 코드에 출력했다. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">&#x27;\n feature_names 의 type:&#x27;</span>,<span class="built_in">type</span>(iris_data.feature_names)) </span><br><span class="line">print(<span class="string">&#x27; feature_names 의 shape:&#x27;</span>,<span class="built_in">len</span>(iris_data.feature_names)) </span><br><span class="line">print(iris_data.feature_names) </span><br><span class="line"></span><br><span class="line">print(<span class="string">&#x27;\n target_names 의 type:&#x27;</span>,<span class="built_in">type</span>(iris_data.target_names)) </span><br><span class="line">print(<span class="string">&#x27; feature_names 의 shape:&#x27;</span>,<span class="built_in">len</span>(iris_data.target_names)) </span><br><span class="line">print(iris_data.target_names) </span><br><span class="line"></span><br><span class="line">print(<span class="string">&#x27;\n data 의 type:&#x27;</span>,<span class="built_in">type</span>(iris_data.data)) </span><br><span class="line">print(<span class="string">&#x27; data 의 shape:&#x27;</span>,iris_data.data.shape) </span><br><span class="line">print(iris_data[<span class="string">&#x27;data&#x27;</span>]) </span><br><span class="line"></span><br><span class="line">print(<span class="string">&#x27;\n target 의 type:&#x27;</span>,<span class="built_in">type</span>(iris_data.target)) </span><br><span class="line">print(<span class="string">&#x27; target 의 shape:&#x27;</span>,iris_data.target.shape) </span><br><span class="line">print(iris_data.target)</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<pre><code> feature_names 의 type: &lt;class &#39;list&#39;&gt;
 feature_names 의 shape: 4
[&#39;sepal length (cm)&#39;, &#39;sepal width (cm)&#39;, &#39;petal length (cm)&#39;, &#39;petal width (cm)&#39;]

 target_names 의 type: &lt;class &#39;numpy.ndarray&#39;&gt;
 feature_names 의 shape: 3
[&#39;setosa&#39; &#39;versicolor&#39; &#39;virginica&#39;]

 data 의 type: &lt;class &#39;numpy.ndarray&#39;&gt;
 data 의 shape: (150, 4)
[[5.1 3.5 1.4 0.2]
 [4.9 3.  1.4 0.2]
 [4.7 3.2 1.3 0.2]
 [4.6 3.1 1.5 0.2]
 [5.  3.6 1.4 0.2]
 [5.4 3.9 1.7 0.4]
 [4.6 3.4 1.4 0.3]
 [5.  3.4 1.5 0.2]
 [4.4 2.9 1.4 0.2]
 [4.9 3.1 1.5 0.1]
 [5.4 3.7 1.5 0.2]
 [4.8 3.4 1.6 0.2]
 [4.8 3.  1.4 0.1]
 [4.3 3.  1.1 0.1]
 [5.8 4.  1.2 0.2]
 [5.7 4.4 1.5 0.4]
 [5.4 3.9 1.3 0.4]
 [5.1 3.5 1.4 0.3]
 [5.7 3.8 1.7 0.3]
 [5.1 3.8 1.5 0.3]
 [5.4 3.4 1.7 0.2]
 [5.1 3.7 1.5 0.4]
 [4.6 3.6 1.  0.2]
 [5.1 3.3 1.7 0.5]
 [4.8 3.4 1.9 0.2]
 [5.  3.  1.6 0.2]
 [5.  3.4 1.6 0.4]
 [5.2 3.5 1.5 0.2]
 [5.2 3.4 1.4 0.2]
 [4.7 3.2 1.6 0.2]
 [4.8 3.1 1.6 0.2]
 [5.4 3.4 1.5 0.4]
 [5.2 4.1 1.5 0.1]
 [5.5 4.2 1.4 0.2]
 [4.9 3.1 1.5 0.2]
 [5.  3.2 1.2 0.2]
 [5.5 3.5 1.3 0.2]
 [4.9 3.6 1.4 0.1]
 [4.4 3.  1.3 0.2]
 [5.1 3.4 1.5 0.2]
 [5.  3.5 1.3 0.3]
 [4.5 2.3 1.3 0.3]
 [4.4 3.2 1.3 0.2]
 [5.  3.5 1.6 0.6]
 [5.1 3.8 1.9 0.4]
 [4.8 3.  1.4 0.3]
 [5.1 3.8 1.6 0.2]
 [4.6 3.2 1.4 0.2]
 [5.3 3.7 1.5 0.2]
 [5.  3.3 1.4 0.2]
 [7.  3.2 4.7 1.4]
 [6.4 3.2 4.5 1.5]
 [6.9 3.1 4.9 1.5]
 [5.5 2.3 4.  1.3]
 [6.5 2.8 4.6 1.5]
 [5.7 2.8 4.5 1.3]
 [6.3 3.3 4.7 1.6]
 [4.9 2.4 3.3 1. ]
 [6.6 2.9 4.6 1.3]
 [5.2 2.7 3.9 1.4]
 [5.  2.  3.5 1. ]
 [5.9 3.  4.2 1.5]
 [6.  2.2 4.  1. ]
 [6.1 2.9 4.7 1.4]
 [5.6 2.9 3.6 1.3]
 [6.7 3.1 4.4 1.4]
 [5.6 3.  4.5 1.5]
 [5.8 2.7 4.1 1. ]
 [6.2 2.2 4.5 1.5]
 [5.6 2.5 3.9 1.1]
 [5.9 3.2 4.8 1.8]
 [6.1 2.8 4.  1.3]
 [6.3 2.5 4.9 1.5]
 [6.1 2.8 4.7 1.2]
 [6.4 2.9 4.3 1.3]
 [6.6 3.  4.4 1.4]
 [6.8 2.8 4.8 1.4]
 [6.7 3.  5.  1.7]
 [6.  2.9 4.5 1.5]
 [5.7 2.6 3.5 1. ]
 [5.5 2.4 3.8 1.1]
 [5.5 2.4 3.7 1. ]
 [5.8 2.7 3.9 1.2]
 [6.  2.7 5.1 1.6]
 [5.4 3.  4.5 1.5]
 [6.  3.4 4.5 1.6]
 [6.7 3.1 4.7 1.5]
 [6.3 2.3 4.4 1.3]
 [5.6 3.  4.1 1.3]
 [5.5 2.5 4.  1.3]
 [5.5 2.6 4.4 1.2]
 [6.1 3.  4.6 1.4]
 [5.8 2.6 4.  1.2]
 [5.  2.3 3.3 1. ]
 [5.6 2.7 4.2 1.3]
 [5.7 3.  4.2 1.2]
 [5.7 2.9 4.2 1.3]
 [6.2 2.9 4.3 1.3]
 [5.1 2.5 3.  1.1]
 [5.7 2.8 4.1 1.3]
 [6.3 3.3 6.  2.5]
 [5.8 2.7 5.1 1.9]
 [7.1 3.  5.9 2.1]
 [6.3 2.9 5.6 1.8]
 [6.5 3.  5.8 2.2]
 [7.6 3.  6.6 2.1]
 [4.9 2.5 4.5 1.7]
 [7.3 2.9 6.3 1.8]
 [6.7 2.5 5.8 1.8]
 [7.2 3.6 6.1 2.5]
 [6.5 3.2 5.1 2. ]
 [6.4 2.7 5.3 1.9]
 [6.8 3.  5.5 2.1]
 [5.7 2.5 5.  2. ]
 [5.8 2.8 5.1 2.4]
 [6.4 3.2 5.3 2.3]
 [6.5 3.  5.5 1.8]
 [7.7 3.8 6.7 2.2]
 [7.7 2.6 6.9 2.3]
 [6.  2.2 5.  1.5]
 [6.9 3.2 5.7 2.3]
 [5.6 2.8 4.9 2. ]
 [7.7 2.8 6.7 2. ]
 [6.3 2.7 4.9 1.8]
 [6.7 3.3 5.7 2.1]
 [7.2 3.2 6.  1.8]
 [6.2 2.8 4.8 1.8]
 [6.1 3.  4.9 1.8]
 [6.4 2.8 5.6 2.1]
 [7.2 3.  5.8 1.6]
 [7.4 2.8 6.1 1.9]
 [7.9 3.8 6.4 2. ]
 [6.4 2.8 5.6 2.2]
 [6.3 2.8 5.1 1.5]
 [6.1 2.6 5.6 1.4]
 [7.7 3.  6.1 2.3]
 [6.3 3.4 5.6 2.4]
 [6.4 3.1 5.5 1.8]
 [6.  3.  4.8 1.8]
 [6.9 3.1 5.4 2.1]
 [6.7 3.1 5.6 2.4]
 [6.9 3.1 5.1 2.3]
 [5.8 2.7 5.1 1.9]
 [6.8 3.2 5.9 2.3]
 [6.7 3.3 5.7 2.5]
 [6.7 3.  5.2 2.3]
 [6.3 2.5 5.  1.9]
 [6.5 3.  5.2 2. ]
 [6.2 3.4 5.4 2.3]
 [5.9 3.  5.1 1.8]]

 target 의 type: &lt;class &#39;numpy.ndarray&#39;&gt;
 target 의 shape: (150,)
[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2
 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
 2 2]</code></pre>
<h2 id="Moder-Selection-모듈-소개"><a href="#Moder-Selection-모듈-소개" class="headerlink" title="Moder Selection 모듈 소개"></a>Moder Selection 모듈 소개</h2><h3 id="학습-테스트-데이터-세트-분리-train-test-split"><a href="#학습-테스트-데이터-세트-분리-train-test-split" class="headerlink" title="학습/테스트 데이터 세트 분리 - train_test_split()"></a>학습/테스트 데이터 세트 분리 - train_test_split()</h3><p>먼저 테스트 데이터 세트를 이용하지 않고 학습 데이터 세트로만 학습하고 예측하면 무엇이 문제인지 살펴보자. 다음 예제는 학습과 예측을 동일한 데이터 세트로 수행한 결과이다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line">iris=load_iris()</span><br><span class="line">dt_clf=DecisionTreeClassifier()</span><br><span class="line">train_data=iris.data</span><br><span class="line">train_label=iris.target</span><br><span class="line">dt_clf.fit(train_data,train_label)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 학습 데이터로 예측 하기</span></span><br><span class="line">pred=dt_clf.predict(train_data)</span><br><span class="line">print(<span class="string">&#x27;예측 정확도:&#x27;</span>,accuracy_score(train_label,pred))</span><br></pre></td></tr></table></figure>

<pre><code>예측 정확도: 1.0</code></pre>
<p>정확도가 100%가 나왔다. 이러한 결과가 나온 이유는 이미 학습한 데이터를 기반으로 예측을 하였기 때문이다. 따라서 예측을 수행하는 데이터 세트는 학습용 데이터 세트가 아닌 다른 테스트 데이터 세트여야 한다.</p>
<p>사이킷런에서는 train_test_split()를 사용해서 데이터 세트를 쉽게 분리할 수 있다. train_test_split(): 첫 번째 파라미터: 피처 데이터 세트, 두 번째 파라미터: 레이블 데이터 세트 그리고 선택적으로 다음 파라미터를 입력 받는다.</p>
<ul>
<li>test_size: 전체 데이터에서 테스트 데이터 세트 크기를 얼마로 샘플링할 것인가를 결정한다.(디폴트가 0.25, 25%이다)</li>
<li>train_size: 전체 데이터에서 학습용 데이터 세트 크기를 얼마로 샘플링할 것인가를 결정한다.</li>
<li>shuffle: 데이터를 분리하기 전에 데이터를 미리 섞을지를 결정한다. (디폴트는 True입니다) 데이터를 분산시켜서 좀 더 효율적인 학습 및 테스트 데이터 세트를 만들어 준다.</li>
<li>random_state: 호출할 때마다 동일한 학습/테스트용 데이터 세트를 생성하기 위해 주어지는 난수 값이다. 이 파라미터를 지정하지 않으면 수행할 때마다 다른 학습/테스트 용으로 데이터를 생성하게 된다.</li>
</ul>
<p>붓꽃 데이터를 학습 데이터를 70%, 데스트 데이터를 30%로 분리하는 예제를 해보겠다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">df_clf=DecisionTreeClassifier()</span><br><span class="line">iris_data=load_iris()</span><br><span class="line"></span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(iris_data.data, iris_data.target,</span><br><span class="line">                                                    test_size=<span class="number">0.3</span>, random_state=<span class="number">121</span>)</span><br></pre></td></tr></table></figure>

<p>학습 데이터를 기반으로 DecisionTreeClassifier를 학습하고 이 모델을 예측 정확도를 측정해 보자.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">df_clf.fit(x_train,y_train)</span><br><span class="line">pred=df_clf.predict(x_test)</span><br><span class="line">print(<span class="string">&#x27;예측 정확도:&#123;0:.4f&#125;&#x27;</span>.<span class="built_in">format</span>(accuracy_score(y_test,pred)))</span><br></pre></td></tr></table></figure>

<pre><code>예측 정확도:0.9556</code></pre>
<p>테스트 데이터로 예측을 수행한 결과 정확도가 약95.56%이다. 붓꽃 데이터는 150개의 데이터로 데이터 양이 크지 않아 전체의 30% 정도인 테스트 데이터는 45개 정도밖에 되지 않으므로 이를 통해 알고리즘의 예측성을 판단하기에는 그리 적절하지 않다. 학습을 위한 데이터의 양을 일정 수준 이상으로 보장하는 것도 중요하지만, 학습된 모델에 대해 다양한 데이터를 기반으로 예측 성능을 평가해보는 것도 매우 중요하다.</p>
<h3 id="교차-검증"><a href="#교차-검증" class="headerlink" title="교차 검증"></a>교차 검증</h3><p>알고리즘을 학습시키는 학습 데이터와 이에 대한 예측 성능을 평가하기 위한 별도의 테스트용 데이터가 필요하다. 하지만 이 방법 역시 과적합에 취약하다는 약점을 가질 수 있다. <strong>과적합은 모델이 학습 데이터에만 과도하게 최적화 되어서 실제 예측을 다른 데이터로 시도했을 경우 예측 성능이 과도하게 떨어지는 현상</strong>을 말한다.이러한 문제를 개선하기 위해서 사용하는 교차검증에 대해서 배워보도록 하겠다.</p>
<p>교차 검증은 쉽게 말하자면 예를들어 시험을 보기 위해서 예비 모의고사를 여러 번 풀어보는 것이라고 생각하면 된다. 교차 검증은 데이터의 편중을 막기 위해서 별도의 여러개의 세트로 구성된 학습 데이터 세트와 검증 데이터 세트에서 학습과 평가를 수행하는 것이다. </p>
<p>대부분의 ML 모델에서 성능 평가는 교차 검증을 기반으로 1차 평가를 한 뒤에 최종적으로는 테스트 데이터세트에 적용해 평가하는 방식으로 진행된다. ML에서 사용되는 데이터 세트를 세분화 하면 학습, 검증, 테스트 3가지로 나눌 수 있다. 검증 데이터 세트는 최종 평가가 이루어지기 전에 학습된 모델을 다양하게 평가하는게 사용이 된다. </p>
<p><strong>K 폴드 교차 검증</strong></p>
<p>K 폴드 교차 검증은 먼저 K개의 데이터 폴드 세트를 만들은 후 K번만큼 각 폴드 세트에 학습과 평가를 반복적으로 수행하는 방법이다.</p>
<p>5폴드 교차 검증을 수행한다. 5개의 폴드된 데이터 세트를 학습과 검증을 위한 데이터 세트로 변경하면서 5번 평가를  수행한 뒤, 5개의 평가를 평균한 결과를 가지고 예측성능 평가를 한다.</p>
<p><img src="data:image/png;base64,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" alt="4.PNG"></p>
<p>총 5개의 폴드 세트에 5번의 학습과 검증 평가가 반복적으로 수행된다. 학습 데이터와 검증 데이터를 점진적으로 변경하면서 마지막 5번째까지 학습과 검증을 수행하게 되는 것이다. 5개의 예측 평가를 구하게 되면 최종적으로 평균을 취해서 결과가 반영이 된다.</p>
<p>사이킷런에서는 K 폴드 교차 검증 프로세스를 위해서 KFold와 StrarifiedKFold 클래스를 제공한다. 먼저 KFold 클래스를 이용해서 붓꽃 데이터 세트를 교차 검증및 예측 정확도를 알아보도록 하겠다. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> KFold</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">iris=load_iris()</span><br><span class="line">features=iris.data</span><br><span class="line">label=iris.target</span><br><span class="line">dt_clf=DecisionTreeClassifier(random_state=<span class="number">156</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5개의 세트르 분리하는 KFold 객체와 폴드 세트별 정확도를 담을 리스트 객체 생성하기</span></span><br><span class="line">Kfold=KFold(n_splits=<span class="number">5</span>)</span><br><span class="line">cv_accuracy=[]</span><br><span class="line">print(<span class="string">&#x27;붓꽃 데이터 크기:&#x27;</span>,features.shape[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>

<pre><code>붓꽃 데이터 크기: 150</code></pre>
<p>KFold로 KFord 객체를 생성했으니 이제 생성된 KFold 개체의 split()을 호출해 전체 붓꽃 데이터를 5개의 폴드 데이터 세트로 분리한다. 전체 데이터 세트는 150개이다. 따라서 학습용 데이터 세트는 120개, 검증 테스트 데이터 세트는 30개로 분할하게 된다. 다음으로는 5개의 폴드 세트를 생성하는 KFold 객체의 split()을 호출하여 교차 검증 수행 마다 학습과 검증을 반복해 예측 정확도를 측정하도록 해보겠다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">n_iter=<span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># KFold객체의 split( ) 호출하면 폴드 별 학습용, 검증용 테스트의 로우 인덱스를 array로 반환  </span></span><br><span class="line"><span class="keyword">for</span> train_index, test_index <span class="keyword">in</span> Kfold.split(features):</span><br><span class="line">    <span class="comment"># kfold.split( )으로 반환된 인덱스를 이용하여 학습용, 검증용 테스트 데이터 추출</span></span><br><span class="line">    x_train, x_test = features[train_index], features[test_index]</span><br><span class="line">    y_train, y_test = label[train_index], label[test_index]</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#학습 및 예측 </span></span><br><span class="line">    dt_clf.fit(x_train, y_train)</span><br><span class="line">    pred = dt_clf.predict(x_test)</span><br><span class="line">    n_iter += <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 반복 시 마다 정확도 측정 </span></span><br><span class="line">    accuracy = np.<span class="built_in">round</span>(accuracy_score(y_test,pred),<span class="number">4</span>)</span><br><span class="line">    train_size=x_train.shape[<span class="number">0</span>]</span><br><span class="line">    test_size=x_test.shape[<span class="number">0</span>]</span><br><span class="line">    print(<span class="string">&#x27;\n#&#123;0&#125; 교차 검증 정확도 :&#123;1&#125;, 학습 데이터 크기: &#123;2&#125;, 검증 데이터 크기 &#123;3&#125;&#x27;</span>.<span class="built_in">format</span>(n_iter,accuracy, train_size, test_size))</span><br><span class="line">    print(<span class="string">&#x27;#&#123;0&#125; 검증 세트 인덱스:&#123;1&#125;&#x27;</span>.<span class="built_in">format</span>(n_iter, test_index))</span><br><span class="line">    cv_accuracy.append(accuracy)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 개별 iteration별 정확도를 합하여 평균 정확도 계산 </span></span><br><span class="line">    print(<span class="string">&#x27;\n## 평균 검증 정확도:&#x27;</span>,np.mean(cv_accuracy))</span><br></pre></td></tr></table></figure>


<pre><code>#1 교차 검증 정확도 :1.0, 학습 데이터 크기: 120, 검증 데이터 크기 30
#1 검증 세트 인덱스:[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29]

## 평균 검증 정확도: 1.0

#2 교차 검증 정확도 :0.9667, 학습 데이터 크기: 120, 검증 데이터 크기 30
#2 검증 세트 인덱스:[30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53
 54 55 56 57 58 59]

## 평균 검증 정확도: 0.98335

#3 교차 검증 정확도 :0.8667, 학습 데이터 크기: 120, 검증 데이터 크기 30
#3 검증 세트 인덱스:[60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83
 84 85 86 87 88 89]

## 평균 검증 정확도: 0.9444666666666667

#4 교차 검증 정확도 :0.9333, 학습 데이터 크기: 120, 검증 데이터 크기 30
#4 검증 세트 인덱스:[ 90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119]

## 평균 검증 정확도: 0.941675

#5 교차 검증 정확도 :0.7333, 학습 데이터 크기: 120, 검증 데이터 크기 30
#5 검증 세트 인덱스:[120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137
 138 139 140 141 142 143 144 145 146 147 148 149]

## 평균 검증 정확도: 0.9</code></pre>
<p>5번 교차 검증 결과 평균 검증 정확도는 0.9이다. 그리고 교차 검증 시마다 검증 세트의 인덱스가 달라짐을 알 수 있다.</p>
<p><strong>Stratified K 폴드</strong></p>
<p>Startified K폴드는 <strong>불균형한 분포도를 가진 레이블 데이터 집합을 위한 K폴드 방식</strong>이다. 불균형한 분포도를 가진 레이블 데이터 집합은 특정 레이블 값이 특이하게 많거나 매우 적어서 값의 분포가 한쪽으로 치우치는 것을 말한다.</p>
<p>Startifide K폴드는 K폴드가 레이블 데이터 집합이 원본 데이터 집합의 레이블 분포를 학습 및 테스트 세트에 제대로 분배하지 못하는 경우의 문제를 해결해 준다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">iris=load_iris()</span><br><span class="line">iris_df=pd.DataFrame(data=iris_data,columns=iris.feature_names)</span><br><span class="line">iris_df[<span class="string">&#x27;label&#x27;</span>]=iris.target</span><br><span class="line">iris_df[<span class="string">&#x27;label&#x27;</span>].value_counts()</span><br></pre></td></tr></table></figure>




<pre><code>2    50
1    50
0    50
Name: label, dtype: int64</code></pre>
<p>iris 원본 데이터는 50:50:50의 비율로 구성되어 있는 것을 확인 하였다. 이슈가 발생하는 현상으 도출하기 위해 3개의 폴드 세트를 KFold로 생성하고, 각 교차 검증 시마다 생성되는 학습/검증 레이블 데이터 값의 분포도를 확인해 보자.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">kfold = KFold(n_splits=<span class="number">3</span>)</span><br><span class="line"><span class="comment"># kfold.split(X)는 폴드 세트를 3번 반복할 때마다 달라지는 학습/테스트 용 데이터 로우 인덱스 번호 반환. </span></span><br><span class="line">n_iter = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> train_index, test_index <span class="keyword">in</span> kfold.split(iris_df):</span><br><span class="line">    n_iter+=<span class="number">1</span></span><br><span class="line">    label_train = iris_df[<span class="string">&#x27;label&#x27;</span>].iloc[train_index]</span><br><span class="line">    label_test = iris_df[<span class="string">&#x27;label&#x27;</span>].iloc[test_index]</span><br><span class="line">    print(<span class="string">&#x27;\n##교차 검증: &#123;0&#125;&#x27;</span>.<span class="built_in">format</span>(n_iter))</span><br><span class="line">    print(<span class="string">&#x27;학습 레이블 데이터 분포: \n&#x27;</span>, label_train.value_counts())</span><br><span class="line">    print(<span class="string">&#x27;검증 레이블 데이터 분포: \n&#x27;</span>, label_test.value_counts())</span><br></pre></td></tr></table></figure>


<pre><code>##교차 검증: 1
학습 레이블 데이터 분포: 
 2    50
1    50
Name: label, dtype: int64
검증 레이블 데이터 분포: 
 0    50
Name: label, dtype: int64

##교차 검증: 2
학습 레이블 데이터 분포: 
 2    50
0    50
Name: label, dtype: int64
검증 레이블 데이터 분포: 
 1    50
Name: label, dtype: int64

##교차 검증: 3
학습 레이블 데이터 분포: 
 1    50
0    50
Name: label, dtype: int64
검증 레이블 데이터 분포: 
 2    50
Name: label, dtype: int64</code></pre>
<p>StatifiedKFold는 KFold로 분할 된 레이블 데이터 세트가 전체 레이블 값의 분포도를 반영하지 못하는 문제를 해결해 준다. StatifiedKFold는 레이블 데이터 분포도에 따라 학습/검증 데이터를 나누기 때문에 split() 메서드에 인자로 피처 데이터 세트뿐만 아니라 레이블 데이터 세트도 반드시 필요하다. 포드 세트는 3개로 설정 하자.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> StratifiedKFold</span><br><span class="line"></span><br><span class="line">skf = StratifiedKFold(n_splits=<span class="number">3</span>)</span><br><span class="line">n_iter = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> train_index, test_index <span class="keyword">in</span> skf.split(iris_df, iris_df[<span class="string">&#x27;label&#x27;</span>]):</span><br><span class="line">    n_iter += <span class="number">1</span></span><br><span class="line">    label_train = iris_df[<span class="string">&#x27;label&#x27;</span>].iloc[train_index]</span><br><span class="line">    label_test = iris_df[<span class="string">&#x27;label&#x27;</span>].iloc[test_index]</span><br><span class="line">    print(<span class="string">&#x27;\n## 교차검증: &#123;0&#125;&#x27;</span>.<span class="built_in">format</span>(n_iter))</span><br><span class="line">    print(<span class="string">&#x27;학습 레이블 데이터 분포: \n&#x27;</span>, label_train.value_counts())</span><br><span class="line">    print(<span class="string">&#x27;검증 레이블 데이터 분포: \n&#x27;</span>, label_test.value_counts())</span><br></pre></td></tr></table></figure>


<pre><code>## 교차검증: 1
학습 레이블 데이터 분포: 
 2    34
1    33
0    33
Name: label, dtype: int64
검증 레이블 데이터 분포: 
 1    17
0    17
2    16
Name: label, dtype: int64

## 교차검증: 2
학습 레이블 데이터 분포: 
 1    34
2    33
0    33
Name: label, dtype: int64
검증 레이블 데이터 분포: 
 2    17
0    17
1    16
Name: label, dtype: int64

## 교차검증: 3
학습 레이블 데이터 분포: 
 0    34
2    33
1    33
Name: label, dtype: int64
검증 레이블 데이터 분포: 
 2    17
1    17
0    16
Name: label, dtype: int64</code></pre>
<p>출력 결과를 보면 학습 레이블과 검증 레이블 데이터 값의 분포도가 동일하게 할당됐음을 알 수 있다. 첫 번째 교차 검증에서 학습 레이블은 0, 1, 2 값이 각각 33개로, 레이블별로 동일하게 할당됐고 검증 레이블 역시 0, 1, 2 값이 각각 17개로 레이블 별로 동일하게 할당된걸 알 수 있다.</p>
<p>다음은 StratifiedKFold를 이용해 데이터를 분리한 것이다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">dt_clf = DecisionTreeClassifier(random_state=<span class="number">156</span>) </span><br><span class="line">skfold = StratifiedKFold(n_splits=<span class="number">3</span>) </span><br><span class="line">n_iter=<span class="number">0</span> </span><br><span class="line">cv_accuracy=[] </span><br><span class="line"></span><br><span class="line"><span class="comment"># StratifiedKFold의 split( ) 호출시 반드시 레이블 데이터 셋도 추가 입력 필요 </span></span><br><span class="line"><span class="keyword">for</span> train_index, test_index <span class="keyword">in</span> skfold.split(features, label): </span><br><span class="line">  <span class="comment"># split( )으로 반환된 인덱스를 이용하여 학습용, 검증용 테스트 데이터 추출 </span></span><br><span class="line">  X_train, X_test = features[train_index], features[test_index] </span><br><span class="line">  y_train, y_test = label[train_index], label[test_index] </span><br><span class="line">  <span class="comment">#학습 및 예측 </span></span><br><span class="line">  dt_clf.fit(X_train , y_train) </span><br><span class="line">  pred = dt_clf.predict(X_test) </span><br><span class="line">  </span><br><span class="line">  <span class="comment"># 반복 시 마다 정확도 측정 </span></span><br><span class="line">  n_iter += <span class="number">1</span> </span><br><span class="line">  accuracy = np.<span class="built_in">round</span>(accuracy_score(y_test,pred), <span class="number">4</span>) </span><br><span class="line">  train_size = X_train.shape[<span class="number">0</span>] </span><br><span class="line">  test_size = X_test.shape[<span class="number">0</span>] </span><br><span class="line">  print(<span class="string">&#x27;\n#&#123;0&#125; 교차 검증 정확도 :&#123;1&#125;, 학습 데이터 크기: &#123;2&#125;, 검증 데이터 크기: &#123;3&#125;&#x27;</span> .<span class="built_in">format</span>(n_iter, accuracy, train_size, test_size)) </span><br><span class="line">  print(<span class="string">&#x27;#&#123;0&#125; 검증 세트 인덱스:&#123;1&#125;&#x27;</span>.<span class="built_in">format</span>(n_iter,test_index)) </span><br><span class="line">  cv_accuracy.append(accuracy) </span><br><span class="line"></span><br><span class="line">  <span class="comment"># 교차 검증별 정확도 및 평균 정확도 계산 </span></span><br><span class="line">  print(<span class="string">&#x27;\n## 교차 검증별 정확도:&#x27;</span>, np.<span class="built_in">round</span>(cv_accuracy, <span class="number">4</span>)) </span><br><span class="line">  print(<span class="string">&#x27;## 평균 검증 정확도:&#x27;</span>, np.mean(cv_accuracy))</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<pre><code>#1 교차 검증 정확도 :0.98, 학습 데이터 크기: 100, 검증 데이터 크기: 50
#1 검증 세트 인덱스:[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  50
  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66 100 101
 102 103 104 105 106 107 108 109 110 111 112 113 114 115]

## 교차 검증별 정확도: [0.98]
## 평균 검증 정확도: 0.98

#2 교차 검증 정확도 :0.94, 학습 데이터 크기: 100, 검증 데이터 크기: 50
#2 검증 세트 인덱스:[ 17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  67
  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82 116 117 118
 119 120 121 122 123 124 125 126 127 128 129 130 131 132]

## 교차 검증별 정확도: [0.98 0.94]
## 평균 검증 정확도: 0.96

#3 교차 검증 정확도 :0.98, 학습 데이터 크기: 100, 검증 데이터 크기: 50
#3 검증 세트 인덱스:[ 34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  83  84
  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 133 134 135
 136 137 138 139 140 141 142 143 144 145 146 147 148 149]

## 교차 검증별 정확도: [0.98 0.94 0.98]
## 평균 검증 정확도: 0.9666666666666667</code></pre>
<p>3개의 Stratified K 폴드로 교차 검증한 결과 평균 검증 정확도가 약 96.66%로 측정되었다. Stratified K 폴드의 경우 원본 데이터의 레이블 분포도 특성을 반영한 학습 및 검증 데이터 세트를 만들 수 있으므로 왜곡된 레이블 데이터 세트에서는 반드시 Stratified K 폴드를 이용해 교차 검증해야 한다. 사실, 일반적으로 분류(Classification)에서의 교차 검증은 K 폴드가 아니라 Stratified K 폴드로 분할돼야 한다.</p>
<p><strong>회귀(Regression)에서는 Stratified K 폴드가 지원되지 않는다.</strong> 이유는 간단하다. 회귀의 결정값은 이산값 형태의 레이블이 아니라 연속된 숫자값이기 때문에 결정값별로 분포를 정하는 의미가 없기 때문이다.</p>
<p><strong>교차 검증을 보다 간편하게 - cross_val_score( )</strong></p>
<p>사이킷런은 교차 검증을 좀 더 편리하게 수행할 수 있게 해주는 API를 제공한다. 대표적인 것이 cross_val_score( )이다. KFold로 데이터를 학습하고 예측하는 코드를 보면 먼저 ① 폴드 세트를 설정하고 ② for 루프에서 반복으로 학습 및 테스트 데이터의 인덱스를 추출한 뒤 ③ 반복적으로 학습과 예측을 수행하고 예측 성능을 반환한다.</p>
<p>cross_val_score( )는 일련의 과정을 한꺼번에 수행해주는 API 이다. 다음은 cross_val_score( ) API의 선언 형태이다.</p>
<pre><code>cross_val_score(estimator, X, y=None, scoring=None, cv=None, n_jobs=1, verbose=0, fit_params=None, pre_dispatch=&#39;2*n_jobs&#39;)</code></pre>
<p>이 중 estimator, X, y, scoring, cv가 주요 파라미터이다</p>
<ul>
<li>esmitator :  사이킷런의 분류 알고리즘 클래스인 Classifier 또는 회귀 알고리즘 클래스인 Regressor를 의미</li>
<li>X : 피처 데이터 세트</li>
<li>y : 레이블 데이터 세트</li>
<li>scoring : 예측 성능 평가 지표를 기술</li>
<li>cv : 교차 검증 폴드 수</li>
</ul>
<p>cross_val_score( ) 수행 후 반환 값은 scoring 파라미터로 지정된 성능 지표 측정값을 배열 형태로 반환한다. cross_val_score( )는 classifier가 입력되면 Stratified K 폴드 방식으로 레이블값의 분포에 따라 학습/테스트 세트를 분할한다.(회귀인 경우에는 Stratified K 폴드 방식으로 분할할 수 없으므로 K 폴드 방식으로 분할한다.)</p>
<p>다음 코드에서 cross_val_score()의 자세한 사용법을 살펴보자. 교차 검증 폴드 수는 3, 성능평가 지표는 정확도인 accuracy로 하자.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier </span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score , cross_validate </span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris </span><br><span class="line"></span><br><span class="line">iris_data = load_iris() </span><br><span class="line">dt_clf = DecisionTreeClassifier(random_state=<span class="number">156</span>) </span><br><span class="line"></span><br><span class="line">data = iris_data.data </span><br><span class="line">label = iris_data.target </span><br><span class="line"></span><br><span class="line"><span class="comment"># 성능 지표는 정확도(accuracy) , 교차 검증 세트 는 3개 </span></span><br><span class="line">scores = cross_val_score(dt_clf , data , label , scoring=<span class="string">&#x27;accuracy&#x27;</span>,cv=<span class="number">3</span>) </span><br><span class="line">print(<span class="string">&#x27;교차 검증별 정확도:&#x27;</span>,np.<span class="built_in">round</span>(scores, <span class="number">4</span>)) </span><br><span class="line">print(<span class="string">&#x27;평균 검증 정확도:&#x27;</span>, np.<span class="built_in">round</span>(np.mean(scores), <span class="number">4</span>))</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>교차 검증별 정확도: [0.98 0.94 0.98]
평균 검증 정확도: 0.9667</code></pre>
<p>cross_val_score( )는 cv로 지정된 횟수만큼 scoring 파라미터로 지정된 평가 지표로 평가 결과값을 배열로 반환한다. 그리고 일반적으로 이를 평균하여 평가 수치로 사용한다. cross_val_score( ) API는 내부에서 Estimator를 학습(fit), 예측(predict), 평가(evaluation)시켜주므로 간단하게 교차 검증을 수행할 수 있다.</p>
<p>cross_val_score( )와 앞 예제의 StratifiedKFold의 수행 결과를 비교해 보면 각 교차 검증별 정확도와 평균 검증 정확도가 모두 동일함을 알 수 있다. 이는 cross_val_score( )가 내부적으로 StratifiedKFold를 이용하기 때문이다.</p>
<p><strong>GridSearchCV - 교차 검증과 최적 하이퍼 파라미터 튜닝을 한 번에</strong></p>
<p>사이킷런은 GridSearchCV API를 이용해 Classifier나 Regressor와 같은 알고리즘에 사용되는 하이퍼 파라미터를 순차적으로 입력하면서 편리하게 최적의 파라미터를 도출할 수 있는 방안을 제공한다. (Grid는 격자라는 뜻으로, 촘촘하게 파라미터를 입력하면서 테스트를 하는 방식이다.) 예를 들어 결정 트리 알고리즘의 여러 하이퍼 파라미터를 순차적으로 변경하면서 최고 성능을 가지는 파라미터 조합을 찾고자 한다면 다음과 같이 파라미터의 집합을 만들고 이를 순차적으로 적용하면서 최적화를 수행할 수 있다.</p>
<pre><code>parameters = &#123;&#39;max_depth&#39;:[1, 2, 3], &#39;min_samples_split&#39;:[2,3]&#125;</code></pre>
<p>총 6회에 걸쳐 파라미터를 순차적으로 바꿔 실행하면서 최적의 파라미터와 수행 결과를 도출할 수 있다.</p>
<p><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAmkAAAFSCAYAAACpJEghAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAAEnQAABJ0Ad5mH3gAAB/zSURBVHhe7d2Pbxtneifw/EH7dwkGiACEIZwRXWAZJdwKwVlB2aA8dwWvVTfa1koRpTXTrtONsqncgG4qrJdtlFsf01R7lptTmgMR8CDwIDw35AzF0W85taOX0ucDPNjg5YhLWe+885133hm+FgAAJEdIAwBI0Gs/+clPQimllFJKnV8dxUwaAECChDQAgAQJaQAACRLSAAASJKQBACRISAMASJCQBgCQICENACBBQhoAQIKENACABAlpAAAJEtIAABIkpAEAJEhIAwBI0GSEtJ3t2Hy6ERtfbkTnWTf6RfML+34rOoP3edqJre+LNgCABKUf0nqb8fDtakxNTRU1HQufbR8Iar3YeHcmf70yH6vP+9HfbEbtSv4zs+9vDrfvft6I6vA9qtH4vJv/KPzo+tF91slPOjazvrxbNP9n7XRjszM4menE5vNe3ubEhImU7SObxT7y9Xb0XtY+kqj+t5vRyX7XjaebsV3suqnb/5mP+HsdNR7xwtIOaf3taN0pwle5KnOx8mX5j14KaVdmY+lRO9qfLMZMsb2QRlL6W/HwraIvv7kUGy9p/CqfmEzfWY9uNlDq80yknc1YqRX7yLWFaF/ortuPznuz+e86NRNLX0xCoDnwmZ/89tDf66jxiBeXbkjb7UXng7moDDtBVpXZmH9ruugUWU03Yu3ZaD6tFNKOqOlbjai/VYvZa5WizQGLcySkwcmEtMQJaT+WRENaP7YeNWJ62AGKTrDejf7OVqw1SkHt+lK0vxtsf3JIq96YHYe9YTlgXTz96D3vFNPv2f92Dl5OLL++Gds7wx/ab3Rp8Mtsu+e97CcGjvm53mid5Nmn8vvfbebv316LpetFXyyHtH43torPPXzfb0cnIePLo+PKXv9m7weHr5dnj6u3VmL9y834KtuP8pBWibn3W/nPdjaje9TvDy9bdrK9/XWxL2Z9e7R/7e0z5dfLlzUHV1HuzcfczVrM3X4Yne/H243rxS+j9Z4Xl+hKdXCdc++b0WW87P2/K72ytzY6H1/2lhF0i23Kr++NH5nsdxzv10XtW35wfEjb+yxZdTpbhy779rujMWtcR213pN1+bBeXKMdVjDt749u4Bu87+lUPfebfbJb+Xn8Tv/mfR49Hk3IpNyVJhrRuezlmiwQ+WIPW+GRr3OG/70TzrdGM2OCP/zA2e+WQNh3z95Zj+c5ccXAS0i6F3e1Ye6e8drFUV2tRe3PcZ/KajvqHG/nZXXZAWP95ada2qMrNpVj/JhvIHo+CzlTU3u9Er9+Ldqm/LZzWl7I++7BxzEnEMKRlIau9EvNXD78+e6+VDWylmbcDVZ1bjva/bx7z+nT89P139j77/pqJxsebF36tD+es246Fa0f1v6zvXq/FbPVA+9W5WHmSnVgdnEn7Xye8z1vZPlAOU0cZXJl5MH9oHx/WWw9ja/Dj3Y1Yebs0CVDU7J212MpOanpPs+PSgdfyqsRMFk4O7meV2lK0nmdv/F0r6gd/z2FVhvv31s4RIa23GWt3Rm2lujofzU6edAbHyVrlwOuDqtajNZy8OMHOVqyWJzxKNVge1G0vliZJyjUaN06YSfsvfxR/+gej7cs1HYttKe1FJRjSxgsQh+l984i7Ofel/MEZyckzadakXQInhbRjKxs0nvxHbH18zOCdVeXt1dj6P9kB42bRdrURq+sPY/710usnzUrtdmP97vF9cxDS2v+6eswgPqhKzP/tevzNMSFtUJX/9l68N3fUayeFtEHtP2uHl+6EkHZsXa3H2u++OnNIG1T1nbXYPiGn9Z+v7u2zh2oQ0rJjSuv20aFlULUPOvHtsSHt+Kpk773578eFtLxq7/+P+Jdy4Pnnb2Pj50cEtFG9sRjt/10akw7WGUJat3TiebBODmmDmhmOm0LajyO5kFa+jn3Wmnn31/FrIe1y2xfSKlG7vRQr7y3HQm08g1a50Yil91Zi6XZtL5RNL/wy/mpvZnYm6u8uZz+3GPPT+euDm1RWn2UnDp8vjAetUf+8UouVp6cMOt31WNh7r1os/Hw5lu/V9y4DTL35Z/Hob8chce8z/nT8Gaf+4M/j/iiEXcnOZLP32PcZr/x+3P/rv943ezz1Zj2WPngYrV/+8V5bpbYQywd+/5l3N8KwyStTDmlZ3x3uX/cXY640azzzzlIsv5/tq3Pj/bf+8ZP4y+NC2lHv8/p8rA1mrY7RK4WOmTur0S4mAfYmAr5tRWMUpKbnYzHbT5bfzfbT0b5eW4nfflEKaaNt7s2Xwkx+FWf/vlmL5j//wzikXZsf7t/73vv6X8Snf14KPP/061h8o3ht8J53i/fcC6nTsfD3n4zfczBj1x7/PhuDy50nBNbBRMjm+6P/v+xY+Iv18c8WlzvL/17Vtxazv8+BcfPOP8Zvlo8Jaddux8d/1zxyPGp/c+IH4wgXJKS147cfLwyvh9cOVbYzfZpfLhXSLrBySBucSX6bN29/NvqbV2J+dNk8G5BHA1z1nV/FV1+1Y+29Rsy/nQekleyA0bgxCm6zsTwIYjtb8fDW/kum03dPXww7PIMvLknM3GvnlxcHNw6MQlcW0n51f3zWPNPIwmU2IK78vBGzo0sZ/3Uh/my0/d4i6myg/bBW/Fz+Gc9840Dp97egl1eqHNLmisuKu71o3y1mrQYh5uv8wN17Mg4Gc81/Oj6kHfc+m/n7HKX35fL4xKg6Uxwb5mL+zkq0NrP9u9+NzS9asXJnPuYbeShZuZ8dU/bC1UJ8/utxSJt9r5OPJb1OLN8othksXRiuMytfCsz2zfW/L403a7E9HAOy8apejCfX/iQ+WioFnsfj7QeXfwdhcDAmLd4azfRlIfYXn8SfjIJgtm/P1vLj3Vx9MZrr2Th34j5dHjuyk7drs/m/x9x8NN5bi053f0ib+0UxbpZOOAfj5j8dG9LyMcqNAy9Hepc7u51YvTdO7JWb2dl/eWepzsXC/UGnHddqe7A4fLQ48/jFpELaBXYwpBXT/eO/eTawfZr1k0FjaY3IYLD57T+Ub1I5WEVIG7zXvksEs7HSOf6gMLIvpI1mrfbd3bk/pB1Zx4S0rV/MFdu8YEgr/f4GT16pckgbrf3a98ikcbjaFwxOCmmnvM+Rvt+I5dH7HazqfKz+7tvSOtMj6mBIK67O7LsLde8moPJM1TEhrTxenRTSjqxsLPvVV/H43nGfdzoWHm8PPsix+pvjJRuH6uZKfPH5USFt/DcQ0n48Sd44UF6gOdwZeqUdobYSm0etATrDgUdIu8B+aEir/1Us762fmN47I53ZGySLkDY48927nJrXYAb3tIX3LxrSqm/m///lml/4YHy5c3oh1ouQ9oNn0oQ0fiyphLRM/9uNePhufe+Ky/iRTFPxe3/xV/GHo31+NNNWmx2fvL2kkFapF2vnzjqTdiUbk4rPu1dz9Vh+ku3LO1ux/uFizM8V7dfHa+oGa2VPWqM30O2sxXIjvyNz8PMze6FtJv70wzt7v3vtw+J3NZN2LiYjpA0evXFvftiR5u+tFTvoAULa5fZDQ9ofvh9/WjwOo3JzOVpfDNZltGP1p6MBLw9AvfJ6lFFV5vYu1Rzru9KatKvzsfxp+8AjOLKQ9sH40kPt52v71ssMZ4affVW6e7Ma8/fXYv2zZjTeLNoGB6jscwhpJCehkHbQ4BLo6CkCb9z+s/j94r9ns2PMcB8s76cvKaRNvV6LxY/Xo/XR4vjOzGwM+HjvRoEs8PzjZ+MxY7oezc/L48HBR2GUZGPN3lq24yYzjpV95l+M1sZW4533S2twry/Ew8frsfrz0hMTfnrSmjQh7WVKPqRNXR2tHzi6hs/QGewYpQPPVOXon/Ew2wvsh4a0P/rb+JsDM2T7Kxtk/yULSaP1aK/PR/PRyt7gU22sxe82mlHPzrpnrx+oG7VofLgef3/7hPfPBvZft1cOB8BSzdx7FM0T7u4cXJ4YLqsR0kjNjxbSfi+W/vKPo3bjiP2wNh8rT74Yh4hDNbiD+lfxs9H7H1UvK6QdUYMb3/6lHHgGzxxrnDBmDL/6sDv+3Y+o6u1fxSc/mzv8bzGoW0vxjxt/tzfDf6iyv8nKL382DmmHKhtLHj8//u5OIe2lSj+knVZFh9gX0k4tIe3C+cFr0rKQ9dXDI59RltdsvPtgPPU/vMS50431O+OZtrsP/vuxA9pg++637Vi+Ob60sq8GA/v3veh8WD/+PU4KaW8uxNogoWWENJLzI4a0ez+7nv/3ocr6/qPjQ9r0O6vZSc7BB6gfqFcU0qbfaUbn+8PPSet/04rFvZuXDtRpIS0bE1r/tjm+OelgZT//d+3jQlol5j7YiG9/M/5b7K9qzL2fjWn9E56TJqS9VEmGtP6ztVi4dXgm7Kiau5118sGO8f1GrLwzd+Q2h2quEc193/3J5Cs9lb98C/rgWwSGlwlKTwbvD24yKS4djJ42vve08AP1dDN+92+lbxwYdZvS+37161/G8t2FWPjpgbqzGCufFYtuB5/v0NO9s///r7PgWAxee19YvK8OXO6szu9d/jj0xdODLzQuPfF8qPz7j55wvvf7l7aDV2G39I0CpWdejp/qX/r2i/K3eHzX2/+F3f/vtPf5Kv71s5VYvHPUfrgUq193j97/DjyHc/itIAe2GW43+Az/t/T5yt8GUvqch/blwZhRek7aeElFtn3pmw56zzZi/fNWVu3YPLSfHqjicmf52wgOvjZ4PmP7w8XD/xaDevdhbPzH4NtNDvzs4PcqvsWkHJhn7o6WYAz+LuN/rf2fef+/w3BcOmo84oUlGdKAkvKNBqOZY2AyHJi5H97dmbh9s5qjuzs5F0IapE5Ig8klpPGfIKRB6nZ7sfmomT8X8KP20V8OD6Rp8KiMX+TP9Hz4eDK+L7f/TTsefjD4zM1YK74rlPMhpAEAJEhIAwBIkJAGAJAgIQ0AIEFCGgBAgoQ0AIAECWkAAAkS0gAAEiSkAQAkSEgDAEiQkAYAkCAhDQAgQUIaAECChDQAgAS99sYbb4RSSimllDrfOkhIU0oppZRKoA5yuRMAIEFCGgBAgoQ0AIAECWkAAAkS0gAAEiSkAQAkSEgDAEiQkAYAkCAhDQAgQUIaAECChDQAgAQJaQAACRLSAAASJKQBACRISAMASJCQBgCQoMkPac+bUbsyFVNTWd1eLxohEfonXEq9p82ov1HJ9/1hVaJ6czFa3xYbwBlMdkjb3YpmbbQDOAiSGP0TLqfyydnBuroQ7Z1iOzjFhbjcufVgNu/8DoIkSP+Ey6Vzfzrf5996GNv9vK3/3Vo0qnlQqz8qGuEUQhq8YvonXC7rt/Mw1nhcNBRG7bMPtooWOJmQBq+Y/gnEbieWruUhbeFJ0QanENLgFdM/4ZLb3YrVW9V8HLi6GBu7RTucQkiDV0z/hEus14nl0Q1E0wvR6hbtcAZCGrxi+idcUs9WY/5qHtAqN5ej0yva4YyENHjF9E+4fLYfL8RM8RiOmXvt6LrEyQ8gpMErpn/C5dL9ZD7f5yuzsfh4u2iFFyekwSumf8LlsrfPH1mz0XxebAinENLgFdM/4XIR0nhZLkRIAwC4aIQ0AIAECWkAAAkS0gAAEiSkAQAkSEgDAEiQkAYAkCAhDQAgQUIaAECChDQAgAQJaQAACRLSAAASJKQBACRISAMASJCQBgCQICENACBBQhoAQIKENACABL1248aNUEoppZRS51sHmUkDAEiQkAYAkCAhDQAgQUIaAECChDQAgAQJaQAACRLSAAASJKQBACRISAMASJCQBgCQICENACBBQhoAQIKENACABAlpAAAJEtIAABIkpAEAJGhyQ9rudrTu1qJamYqpqaJen4n6g070ik3g3D1vRu1K0T9vrxeNwEXX/fJhNG5WozI6PmVVuVaP5lNHKM5uQkNaP9p3quNwdqBqH24V28E52t2KZq3UN4U0uBT6TxaiWuz3lela1BtzMXu1ko8DV2rRfFZsCKeYzJC2sxb14Q4wE4tPunnbbj8235/Nd4Jry9HJW+HcbT0o+qWQBpfC+u08oM1/Wp4168Xarbx96k67aIOTTWZIe96M2UFHf3Ml9s2ZjdqnGuFwSCqENCB2NmPlehHSjAWc0YW6caD36Xy+A9SasV20wXkT0uDyGs2qlav2UXEFCE5xYUJar70YM8UC7bmP7QCkQ0iDy+tQSLuxElu7xYtwigsR0rYf1YtFmpWoPdiMftEOKRDS4LLrR6/TjLnq4DhVjYUnjlKczYSHtF60783kB8Ar09F45CIn6RHSgIHO/WljAS9kckNarxPNW8VjOK7Ox8NNZyakSUiDy6XbaUXrcSs63xUNha0PikmF6839N73BMSY0pHVi6WrW0bPOPv12MzqeDUjChDS4XDbu5RMI1dut6BbzB/1na1EvjlvVext5I5xiQkPaejQGB73jysGQhAhpcMl0lmPm4HFpVIOH2T4vtoNTCGnwiglpcPl0v2xG/VrxLQPDqkT15mK0vrE0h7Ob8BsHAAAuJiENACBBQhoAQIKENACABAlpAAAJEtIAABIkpAEAJEhIAwBIkJAGAJAgIQ0AIEFCGgBAgoQ0AIAECWkAAAkS0gAAEiSkAQAkSEgDAEiQkAYAkCAhDQAgQa/duHEjlFJKKaXU+dZBZtIAABIkpAEAJEhIAwBIkJAGAJAgIQ0AIEFCGgBAgoQ0AIAECWkAAAkS0gAAEiSkAQAkSEgDAEiQkAYAkCAhDQAgQUIaAECChDQAgAQJaQAACZrckNbfjta7czHz+lRMTRV1pRKzd1uxvVtsA+dlN+ufd2tRrZT65+szUX/QiV6xCXBx9Z42o/5GZbz/T1WienMxWt8WG8AZTGhI24pmrej4lWrMvtWI+s3pqFzJ26q31x0IOUf9aN+plgbn/VX7cKvYDriQnjejVhyPDtXVhWjvFNvBKSYzpGU7wOygs19bik551uzpUkwPd4RaNL8p2uDHtrMW9WE/nInFJ928bbcfm+/P5oP0teXo5K3ABdS5P53v6289jO1+3tb/bi0a1cG4MBX1R0UjnOICrUnrR/dRI6rDg+NsNJ8XzfBjG51EvLkS++bMRu1TjVgvmoCLZ/12HsYaj4uGwqh99oHZdM5m8kPa3oGvVNXFaFuXRmJ6n87n/bPWjO2iDbgkdjuxdC0/Ri08KdrgFBcwpFWj8XlxiQkS0WsvxkyxRmXuY/0TLpXdrVi9VaxTvboYGyYROKOLc7lzZzvW787kO0Gtuf8yE5yj7Uf14jJ8JWoPNsNqFLhEep1YHt3oNr0QLedovIALtCYts7dg25o0UtCL9r3ixOHKdDQeucgJl8qz1Zi/mge0ys3l6HjsAC9oMkPazla0H7ei1d7aPyux24rGMKRNRcPKbM5Tdvbc3Lu8MR8PN82fwWWy/Xhhb4nDzL12dF3i5AeYzJDWXY25wcHvSi1Wvh7d39yNjfu1/KA4NRerppQ5N51YKs6ep99uOnuGS6b7SXGTUGU2Fh+bQeeHm9DLnb1Yq+cHwaOqeqdt3Q/naH1vRvfIum2aFy6yrQfFMxGPLMtxOLvJXZM2+Fqou7N73zIwrOHX7myYVuacCWlwmQlpvCwX68YBAIALQkgDAEiQkAYAkCAhDQAgQUIaAECChDQAgAQJaQAACRLSAAASJKQBACRISAMASJCQBgCQICENACBBQhoAQIKENACABAlpAAAJEtIAABIkpAEAJEhIAwBI0Gs3btwIpZRSSil1vnWQmTQAgAQJaQAACRLSAAASJKQBACRISAMASJCQBgCQICENACBBQhoAQIKENACABAlpAAAJEtIAABIkpAEAJEhIAwBIkJAGAJAgIQ0AIEFCGgBAgiY/pO22Y7E6FVNTo2rEevESnJvd7WjdrUW1Uuqbr89E/UEnesUmwMXV/fJhNG5Wo7J3bJqKyrV6NJ8aATi7iQ9p3Y9q+Q5wZbQjCGmct36071T3BuaDVftwq9gOuIj6TxaiWuzvlela1BtzMXu1ko8BV2rRfFZsCKeY7JBWmkWbu7cYs8OdQkjjnO2sRX3YF2di8Uk3b9vtx+b7s/kgfW05OnkrcAGt386PS/OflmfNerF2K2+futMu2uBkEx3S9mbR3sgOes+aQhppeF70xTdXYt+c2ahdH4XLZ2czVq4P9v+sbhsBOJvJDWmlWbT6o+xsxQGQxPU+nc8H6Foztos24GIbzaqVq/ZRMcMOp5jYkLZvFm03axDSSFivvRgzxbrJuY8N0HBZHAppN1Zia3DMgjOYzJC2uxGLV/MOX/+sn7cJaSRq+1G9WERcidqDzSh6LHBp9KPXacbc8OpPNRaeGAU4m4kMad2P5/IzkuvN8ZofIY3k9KJ9bybvq1emo/HIRU64zDr3p/PxwJo0zmgiQ9pR1/gPVTnAwY+t14nmreIxHFfn4+GmM2e4LLqdVrQet6LzXdFQ2PqgOGlzfOKMhDR46TqxVFyOn367GR3ProRLZeNefoJWvd2KbnF+1n+2FvViXKje28gb4RQTe+PAIS53koz1aAz74jHlUgdcbJ3lmDlq3x/U4GG2z4vt4BRCGrx0Qhpcdt0vm1G/VnzLwLAqUb25GK1vLH3g7C5OSAMAuECENACABAlpAAAJEtIAABIkpAEAJEhIAwBIkJAGAJAgIQ0AIEFCGgBAgoQ0AIAECWkAAAkS0gAAEiSkAQAkSEgDAEiQkAYAkCAhDQAgQUIaAECChDQAgAS9duPGjVBKKaWUUudbB5lJAwBIkJAGAJAgIQ0AIEFCGgBAgoQ0AIAECWkAAAkS0gAAEiSkAQAkSEgDAEiQkAYAkCAhDQAgQUIaAECChDQAgAQJaQAACRLSAAASJKQBACRoQkPaejSmpmLquLq9XmwH56S/Ha1352Lm9VK/vFKJ2but2N4ttgEurN7TZtTfqJSOTZWo3lyM1rfFBnAGQhq8dFvRrBV9sVKN2bcaUb85HZUreVs165+9YkvgAnrejFqxvx+qqwvR3im2g1NMfEhryGOkJhugZwf989pSdMqzZk+XYnrYb2vR/KZoAy6czv3pPJC99TC2+3lb/7u1aFTz41b9UdEIpxDS4EfRj+6jRlSH/XY2ms+LZuDCWb9dHJ8eFw2FUfvsg62iBU428SGtcrUaldF/T9di8bPt7HAIiRjNqpWruhht69LgctntxNK1fAxYeFK0wSku5Jq0mrMUUnEopFWj8Xm3eBG4FHa3YvVWNR8Dri7GhpM0zmhCQ9pmrN5ZiIWsmuvdfOZstx+bH87ll5Ou1KNlYSYp2dmO9bsz+SBda4bTCLgkep1YHt1INL0QLedovIAJDWnHyXaG4XTydCx3iiZIxc5a1IezadakwaXwbDXmr+YBrXJzOTpu6+YFTWZIG11CenPlwIzEKKRVY/GLogl+bDtb0X7cilZ7a//6yN2WG17gkth+vBAzxWM4Zu61o+sSJz/AZIa0nVbUh52/GvVPixsF+t3YuF8b7hAud3KuuqsxN+yHtVj5uohp5f45NRerLnnAhdX9ZD7f1yuzsfh4u2iFFzexlzu3HowOeIdr5j3XOjlPvVirH903B1W903YHMlxgWw9mj9z387LcgbOb4DVp/dj+bCnmSl+7MXgER+Ojjqe5c/4GXwt1d3bvWwaG9fpM1B9suOwBF5yQxstywW4cAAC4GIQ0AIAECWkAAAkS0gAAEiSkAQAkSEgDAEiQkAYAkCAhDQAgQUIaAECChDQAgAQJaQAACRLSAAASJKQBACRISAMASJCQBgCQICENACBBQhoAQIKENACABL1248aNUEoppZRS51sHmUkDAEiQkAYAkCAhDQAgQUIaAECChDQAgAQJaQAACRLSAAASJKQBACRISAMASJCQBgCQICENACBBQhoAQIKENACABAlpAAAJEtIAABIkpAEAJGiCQ1ovOh81Yvb1qZiaKupKJWbqzej0ik3gvPS3o/XuXMwc6J+zd1uxvVtsA1xY3S8fRuNmNSqj/T+ryrV6NJ86QHF2ExvSth7Uxge/g1VrxlaxHfz4tqJZK/pipRqzbzWifnM6Klfyturt9ewUA7io+k8WolocjyrTtag35mL2aiUfE67Uovms2BBOMZkhbacV9eEBbyYWn3SLxsyzlZgd7hjVWPyiaIMf2/Nm3g+vLUWnPGv2dCmmh/0zG6S/KdqAC2f99mA/n4r5T8unY71Yu5W3T91pF21wsskMaaOD4FQj1oumXD963W50s+r1iyZIQj+6jxrF2fVsNJ8XzcDlsLMZK9eLkHZ7/5ELjjPhIa0SlfKan8p0zL3Xjq41P6Rir6+WqroYbX0ULoXRrFq5ah+VrgDBCSY8pB1d1TvtMJFGEg711Wo0PjdAw2VxKKTdWIktJ2mc0YSHtOyA99n4gNdbdzmJRO1sx/rdmXyQdmMLXDL96HWaMVfNj1sLT0wjcDYTvyatte+MpBPL1wbtU9FwyZ/U7KxF3UkEXFqd+9P5iZo1aZyRkAYv285WtB+3otXe2n/ZfbcVjWG/1T/hIut2sv0/GwM63xUNha0Pitn062bTOZvJDGl7j+CYirmPtotGlztJRHc15gb98EotVr4uYlq/Gxv3R8/2m4tVy9Lgwtq4Vx3u69XbreiOhoBna1G/mh+3qvc28kY4xWSGtEznveKM5Ihy4wDnqxdr9aP75qD0T7jgOssxc8S+P6zBw2xNInBGExvSjvxaKI/gIBWDr4W6O7v3LQPDen0m6g829E+4BLpfNqN+rfiWgWFVonpzMVrfOEXj7CY4pAEAXFxCGgBAgoQ0AIAECWkAAAkS0gAAEiSkAQAkSEgDAEiQkAYAkCAhDQAgQUIaAECChDQAgAQJaQAACRLSAAASJKQBACRISAMASJCQBgCQICENACBBQhoAQIJe+8lPfhJKKaWUUur86ihm0gAAEiSkAQAkSEgDAEiQkAYAkCAhDQAgORH/HwhiaVwPG7KfAAAAAElFTkSuQmCC" alt="5.PNG"></p>
<p>GridSearchCV는 사용자가 튜닝하고자 하는 여러 종류의 하이퍼 파라미터를 다양하게 테스트하면서 최적의 파라미터를 편리하게 찾게 해주지만 동시에 <strong>순차적으로 파라미터를 테스트하므로 수행시간이 상대적으로 오래 걸리는 것에 유념</strong>해야 한다.</p>
<p>위의 경우 순차적으로 6회에 걸쳐 하이퍼 파라미터를 변경하면서 교차 검증 데이터 세트에 수행 성능을 측정한다. CV가 3회라면 개별 파라미터 조합마다 3개의 폴딩 세트를 3회에 걸쳐 학습/평가해 평균값으로 성능을 측정한다. 6개의 파라미터 조합이라면 총 CV 3회 * 6개 파라미터 조합 = 18회의 학습/평가가 이뤄진다.</p>
<p>GridSerchCV클래스의 생성자로 들어가는 주요 파라미터는 다음과 같다.</p>
<ul>
<li>estimator : classifier, regressor, pipeline이 사용될 수 있다.</li>
<li>param_grid : key + 리스트 값을 가지는 딕셔너리가 주어진다. estimator의 튜닝을 위해 파라미터명과 사용될 여러 파라미터 값을 지정한다.</li>
<li>scoring : 예측 성능을 측정할 평가 방법을 지정한다. 보통은 사이킷런의 성능 평가 지표를 지정하는 문자열(예 : 정확도의 경우 ‘accuracy’)로 지정하나 별도의 성능 평가 지표 함수도 지정할 수 있다.</li>
<li>cv : 교차 검증을 위해 분할되는 학습 / 테스트 세트의 갯수를 지정한다.</li>
<li>refit : 디폴트가 True이며 True로 생성 시 가장 최적의 하이퍼 파라미터를 찾은 뒤 입력된 esitmator 객체를 해당 하이퍼 파라미터로 재학습 시킨다.</li>
</ul>
<p>다음은 결정 트리 알고리즘의 여러 가지 최적화 파라미터를 순차적으로 적용해 붓꽃 데이터를 예측 분석하는 데 GridSearchCV를 이용한다. 테스트할 하이퍼 파라미터 세트는 딕셔너리 형태로 하이퍼 파라미터의 명칭은 문자열 Key 값으로, 하이퍼 파라미터의 값은 리스트 형으로 설정한다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris </span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier </span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV </span><br><span class="line"></span><br><span class="line"><span class="comment"># 데이터를 로딩하고 학습데이타와 테스트 데이터 분리 </span></span><br><span class="line">iris = load_iris() </span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(iris_data.data, iris_data.target, test_size=<span class="number">0.2</span>, random_state=<span class="number">121</span>) </span><br><span class="line">dtree = DecisionTreeClassifier() </span><br><span class="line"></span><br><span class="line"><span class="comment">### parameter 들을 dictionary 형태로 설정 </span></span><br><span class="line">parameters = &#123;<span class="string">&#x27;max_depth&#x27;</span>:[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], <span class="string">&#x27;min_samples_split&#x27;</span>:[<span class="number">2</span>,<span class="number">3</span>]&#125;</span><br></pre></td></tr></table></figure>

<p>학습 데이터 세트를 GridSearchCV 객체의 fit(학습 데이터 세트) 메서드에 인자로 입력한다. </p>
<p>GridSearchCV 객체의 fit(학습 데이터 세트) 메서드를 수행하면 학습 데이터를 cv에 기술된 폴딩 세트로 분할해 param_grid에 기술된 하이퍼 파라미터를 순차적으로 변경하면서 학습/평가를 수행하고 그 결과를 cv_result 속성에 기록한다. cv_result 는 gridsearchcv의 결과 세트로서 딕셔너리 형태로 key값과 리스트 형태의 value값을 가진다. cv_result를 Pandas의 DataFrame으로 변환하면 내용을 좀 더 쉽게 볼 수 있다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd </span><br><span class="line"></span><br><span class="line"><span class="comment"># param_grid의 하이퍼 파라미터들을 3개의 train, test set fold 로 나누어서 테스트 수행 설정. </span></span><br><span class="line"><span class="comment">### refit=True 가 default 임. True이면 가장 좋은 파라미터 설정으로 재 학습 시킴. </span></span><br><span class="line">grid_dtree = GridSearchCV(dtree, param_grid=parameters, cv=<span class="number">3</span>, refit=<span class="literal">True</span>, return_train_score=<span class="literal">True</span>) </span><br><span class="line"></span><br><span class="line"><span class="comment"># 붓꽃 Train 데이터로 param_grid의 하이퍼 파라미터들을 순차적으로 학습/평가 . </span></span><br><span class="line">grid_dtree.fit(X_train, y_train) </span><br><span class="line"></span><br><span class="line"><span class="comment"># GridSearchCV 결과를 추출해 DataFrame으로 변경</span></span><br><span class="line">scores_df = pd.DataFrame(grid_dtree.cv_results_) </span><br><span class="line">scores_df[[<span class="string">&#x27;params&#x27;</span>, <span class="string">&#x27;mean_test_score&#x27;</span>, <span class="string">&#x27;rank_test_score&#x27;</span>, </span><br><span class="line">           <span class="string">&#x27;split0_test_score&#x27;</span>, <span class="string">&#x27;split1_test_score&#x27;</span>, <span class="string">&#x27;split2_test_score&#x27;</span>]]</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>params</th>
      <th>mean_test_score</th>
      <th>rank_test_score</th>
      <th>split0_test_score</th>
      <th>split1_test_score</th>
      <th>split2_test_score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>{'max_depth': 1, 'min_samples_split': 2}</td>
      <td>0.700000</td>
      <td>5</td>
      <td>0.700</td>
      <td>0.7</td>
      <td>0.70</td>
    </tr>
    <tr>
      <th>1</th>
      <td>{'max_depth': 1, 'min_samples_split': 3}</td>
      <td>0.700000</td>
      <td>5</td>
      <td>0.700</td>
      <td>0.7</td>
      <td>0.70</td>
    </tr>
    <tr>
      <th>2</th>
      <td>{'max_depth': 2, 'min_samples_split': 2}</td>
      <td>0.958333</td>
      <td>3</td>
      <td>0.925</td>
      <td>1.0</td>
      <td>0.95</td>
    </tr>
    <tr>
      <th>3</th>
      <td>{'max_depth': 2, 'min_samples_split': 3}</td>
      <td>0.958333</td>
      <td>3</td>
      <td>0.925</td>
      <td>1.0</td>
      <td>0.95</td>
    </tr>
    <tr>
      <th>4</th>
      <td>{'max_depth': 3, 'min_samples_split': 2}</td>
      <td>0.975000</td>
      <td>1</td>
      <td>0.975</td>
      <td>1.0</td>
      <td>0.95</td>
    </tr>
    <tr>
      <th>5</th>
      <td>{'max_depth': 3, 'min_samples_split': 3}</td>
      <td>0.975000</td>
      <td>1</td>
      <td>0.975</td>
      <td>1.0</td>
      <td>0.95</td>
    </tr>
  </tbody>
</table>
</div>



<p>위의 결과에서 총 6개의 결과를 볼 수 있으며, 이는 하이퍼 파라미터 max_depth와 min_samples_split을 순차적으로 총 6번 변경하면서 학습 및 평가를 수행했음을 나타낸다. 맨 마지막에서 두 번째 행을 보면 평가한 결과 예측 성능이 1위라는 의미이다. split0_test_score, split1_test_score, split2_test_score는 CV가 3인 경우, 즉 3개의 폴딩 세트에서 각각 테스트한 성능 수치이다. mean_test_score는 이 세 개 성능 수치를 평균화한 것이다.</p>
<p>주요 칼럼별 의미는 다음과 같이 정리할 수 있다.</p>
<ul>
<li>params 칼럼에는 수행할 때마다 적용된 개별 하이퍼 파라미터 값을 나타낸다.</li>
<li>rank_test_score는 하이퍼 파라미터별로 성능이 좋은 score 순위를 나타낸다.</li>
<li>mean_test_score는 개별 하이퍼 파라미터별로 CV의 폴딩 테스트 세트에 대해 총 수행한 평가 평균값이다.</li>
</ul>
<p>GridSearchCV 객체의 fit( )을 수행하면 최고 성능을 나타낸 하이퍼 파라미터 값과 그때의 평가 결과 값이 각각 best_params, best_score_속성에 기록된다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">&#x27;GridSearchCV 최적 파라미터:&#x27;</span>, grid_dtree.best_params_) </span><br><span class="line">print(<span class="string">&#x27;GridSearchCV 최고 정확도: &#123;0:.4f&#125;&#x27;</span>.<span class="built_in">format</span>(grid_dtree.best_score_))</span><br></pre></td></tr></table></figure>

<pre><code>GridSearchCV 최적 파라미터: &#123;&#39;max_depth&#39;: 3, &#39;min_samples_split&#39;: 2&#125;
GridSearchCV 최고 정확도: 0.9750</code></pre>
<p>GridSearchCV 객체의 생성 파라미터로 refit = True가 디폴트이다. refit = True이면 GridSearchCV가 최적 성능을 나타내는 하이퍼 파라미터로 Estimator를 학습해 best_estimator_로 저장한다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GridSearchCV의 refit으로 이미 학습이 된 estimator 반환 </span></span><br><span class="line">estimator = grid_dtree.best_estimator_ </span><br><span class="line"></span><br><span class="line"><span class="comment"># GridSearchCV의 best_estimator_는 이미 최적 하이퍼 파라미터로 학습이 됨 </span></span><br><span class="line">pred = estimator.predict(X_test) </span><br><span class="line">print(<span class="string">&#x27;테스트 데이터 세트 정확도: &#123;0:.4f&#125;&#x27;</span>.<span class="built_in">format</span>(accuracy_score(y_test,pred)))</span><br></pre></td></tr></table></figure>

<pre><code>테스트 데이터 세트 정확도: 0.9667</code></pre>
<p>일반적으로 학습 데이터를 GridSearchCV를 이용해 최적 하이퍼 파라미터 튜닝을 수행한 뒤에 별도의 테스트 세트에서 이를 평가하는 것이 일반적인 머신러닝 모델 적용 방법이다.</p>
<h2 id="데이터-전처리"><a href="#데이터-전처리" class="headerlink" title="데이터 전처리"></a>데이터 전처리</h2><p>데이터 전처리(Data Preprocessing)은 ML 알고리즘만큼 중요하다. ML 알고리즘은 데이터에 기반하고 있기 때문에 어떤 데이터를 입력으로 가지느냐에 따라 결과도 크게 달라질 수 있다.(Garbage In, Garbage Out). 사이킷런의 ML 알고리즘을 적용하기 전에 데이터에 대해 미리 처리해야 할 기본 사항이 있다.</p>
<p>결손값, 즉 NaN, Null 값은 허용되지 않는다. 따라서 이러한 Null 값은 고정된 다른 값으로 변환해야 한다. 피처 값 중 Null 값이 얼마 되지 않는다면 피처의 평균값 등으로 간단히 대체할 수 있다. 하지만 Null 값이 대부분이라면 오히려 해당 피처는 드롭하는 것이 더 좋다. </p>
<p>사이킷런의 머신러닝 알고리즘은 문자열 값을 입력 값으로 허용하지 않는다. 그래서 <strong>모든 문자열 값은 인코딩돼서 숫자형으로 변환</strong>해야 한다. 문자열 피처는 일반적으로 카테고리형 피처와 텍스트형 피처를 의미한다. </p>
<p><strong>데이터 인코딩</strong></p>
<p>머신러닝을 위한 대표적인 인코딩 방식은 레이블 인코딩(Label encoding)과 원-핫 인코딩(One Hot encoding)이 있다. 레이블 인코딩은 카테고리 피처를 코드형 숫자 값으로 변환한다. 주의해야 할 점은 ‘01’, ‘02’와 같은 코드 값 역시 문자열이므로 1, 2와 같은 숫자형 값으로 변환돼야 한다.</p>
<p><strong>레이블 인코딩</strong></p>
<p>사이킷런의 레이블 인코딩(Label encoding)은 LabelEncoder 클래스를 구현한다. LabelEncoder를 객체로 생성한 후 fit( )과 transform( )을 호출해 레이블 인코딩을 수행한다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder </span><br><span class="line"></span><br><span class="line">items=[<span class="string">&#x27;TV&#x27;</span>,<span class="string">&#x27;냉장고&#x27;</span>,<span class="string">&#x27;전자렌지&#x27;</span>,<span class="string">&#x27;컴퓨터&#x27;</span>,<span class="string">&#x27;선풍기&#x27;</span>,<span class="string">&#x27;선풍기&#x27;</span>,<span class="string">&#x27;믹서&#x27;</span>,<span class="string">&#x27;믹서&#x27;</span>] </span><br><span class="line"></span><br><span class="line"><span class="comment"># LabelEncoder를 객체로 생성한 후 , fit( ) 과 transform( ) 으로 label 인코딩 수행. </span></span><br><span class="line">encoder = LabelEncoder() </span><br><span class="line">encoder.fit(items) </span><br><span class="line">labels = encoder.transform(items) </span><br><span class="line">print(<span class="string">&#x27;인코딩 변환값:&#x27;</span>,labels)</span><br></pre></td></tr></table></figure>

<pre><code>인코딩 변환값: [0 1 4 5 3 3 2 2]</code></pre>
<p>문자열 값이 어떤 숫자 값으로 인코딩됐는지 직관적으로 알고 싶을 경우에는 LabelEncoder 객체의 classes_ 속성값으로 확인한다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">&#x27;인코딩 클래스:&#x27;</span>,encoder.classes_)</span><br></pre></td></tr></table></figure>

<pre><code>인코딩 클래스: [&#39;TV&#39; &#39;냉장고&#39; &#39;믹서&#39; &#39;선풍기&#39; &#39;전자렌지&#39; &#39;컴퓨터&#39;]</code></pre>
<p>classes_ 속성은 0번부터 순서대로 변환된 인코딩 값에 대한 원본값을 가지고 있다. inverse_transform( )을 통해 인코딩된 값을 다시 디코딩할 수 있다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">&#x27;디코딩 원본 값:&#x27;</span>,encoder.inverse_transform([<span class="number">4</span>, <span class="number">5</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>]))</span><br></pre></td></tr></table></figure>

<pre><code>디코딩 원본 값: [&#39;전자렌지&#39; &#39;컴퓨터&#39; &#39;믹서&#39; &#39;TV&#39; &#39;냉장고&#39; &#39;냉장고&#39; &#39;선풍기&#39; &#39;선풍기&#39;]</code></pre>
<p>레이블 인코딩은 간단하게 문자열 값을 숫자형 카테고리 값으로 변환한다. 하지만 레이블 인코딩이 일괄적인 숫자 값으로 변환이 되면서 몇몇 ML 알고리즘에는 이를 적용할 경우 예측 성능이 떨어지는 경우가 발생할 수 있다. 이는 숫자 값의 경우 크고 작음에 대한 특성이 작용하기 때문이다. 즉, 냉장고가 1, 믹서가 2로 변환되면, 1보다 2가 더 큰 값이므로 특정 ML 알고리즘에서 <strong>가중치가 더 부여되거나 더 중요하게 인식할 가능성이 발생</strong>합니다. 하지만 냉장고와 믹서의 숫자 변환 값은 단순 코드이지 숫자 값에 따른 순서나 중요도로 인식돼서는 않된다. 이러한 특성 때문에 레이블 인코딩은 선형 회귀와 같은 ML 알고리즘에는 적용하지 않아야 한다. 트리 계열의 ML 알고리즘은 숫자의 이러한 특성을 반영하지 않으므로 레이블 인코딩도 별 문제가 없다.</p>
<pre><code># 원-핫 인코딩(One-Hot Encoding)은 레이블 인코딩의 이러한 문제점을 해결하기 위한 인코딩 방식이다.</code></pre>
<p><strong>원-핫 인코딩(one-hot incoding)</strong></p>
<p>원-핫 인코딩은 피처 값의 유형에 따라 새로운 피처를 추가해 고유 값에 대항하는 칼럼에만 1을 표시하고 나머지 칼럼에는 0을 표시하는 방식이다. 즉, 행 형태로 돼 있는 피처의 고유 값을 열 형태로 차원을 변환한 뒤, 고유 값에 해당하는 칼럼에만 1을 표시하고 나머지 칼럼에는 0을 표시한다.</p>
<p>원-핫 인코딩은 사이킷런에서 OneHotEncoder 클래스로 쉽게 변환이 가능하다. 단, LabelEncoder와 다르게 약간 주의할 점이 있다. 첫 번쨰는 OneHotEncoder로 변환하기 전에 모든 문자열 값이 숫자형 값으로 변환돼야 한다는 것이며, 두 번째는 입력 값으로 2차원 데이터가 필요하다는 점이다. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> OneHotEncoder </span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"></span><br><span class="line">items=[<span class="string">&#x27;TV&#x27;</span>,<span class="string">&#x27;냉장고&#x27;</span>,<span class="string">&#x27;전자렌지&#x27;</span>,<span class="string">&#x27;컴퓨터&#x27;</span>,<span class="string">&#x27;선풍기&#x27;</span>,<span class="string">&#x27;선풍기&#x27;</span>,<span class="string">&#x27;믹서&#x27;</span>,<span class="string">&#x27;믹서&#x27;</span>] </span><br><span class="line"></span><br><span class="line"><span class="comment"># 먼저 숫자값으로 변환을 위해 LabelEncoder로 변환합니다. </span></span><br><span class="line">encoder = LabelEncoder() </span><br><span class="line">encoder.fit(items) </span><br><span class="line">labels = encoder.transform(items) </span><br><span class="line"><span class="comment"># 2차원 데이터로 변환합니다. </span></span><br><span class="line">labels = labels.reshape(<span class="number">-1</span>,<span class="number">1</span>) </span><br><span class="line"></span><br><span class="line"><span class="comment"># 원-핫 인코딩을 적용합니다. </span></span><br><span class="line">oh_encoder = OneHotEncoder() </span><br><span class="line">oh_encoder.fit(labels) </span><br><span class="line">oh_labels = oh_encoder.transform(labels) </span><br><span class="line">print(<span class="string">&#x27;원-핫 인코딩 데이터&#x27;</span>) </span><br><span class="line">print(oh_labels.toarray()) </span><br><span class="line">print(<span class="string">&#x27;원-핫 인코딩 데이터 차원&#x27;</span>) </span><br><span class="line">print(oh_labels.shape)</span><br></pre></td></tr></table></figure>

<pre><code>원-핫 인코딩 데이터
[[1. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 0. 1.]
 [0. 0. 0. 1. 0. 0.]
 [0. 0. 0. 1. 0. 0.]
 [0. 0. 1. 0. 0. 0.]
 [0. 0. 1. 0. 0. 0.]]
원-핫 인코딩 데이터 차원
(8, 6)</code></pre>
<p>8개의 레코드와 1개의 칼럼을 가진 원본 데이터가 8개의 레코드와 6개의 칼럼을 가진 데이터로 변환 됐다. TV가 0, 냉장고 1, 믹서 2, 선풍기 3, 전자레인지 4, 컴퓨터 5로 인코딩됐으므로 첫 번째 칼럼이 TV, …, 마지막 여섯 번째 칼럼이 컴퓨터를 나타낸다. 따라서 원본 데이터의 첫 번째 레코드가 TV이므로 변환된 데이터의 첫 번째 레코드의 첫 번째 칼럼이 1이고, 나머지 칼럼은 모두 0이 된다. 이어 두 번째 레코드가 냉장고 이므로 변환된 데이터의 두 번쩨 레코드의 냉장고에 해당하는 두 번째 칼럼이 1이고, 나머지 칼럼은 모두 0이 된다.</p>
<ul>
<li>원핫인코딩: <a target="_blank" rel="noopener" href="https://wikidocs.net/22647">https://wikidocs.net/22647</a></li>
</ul>
<p>판다스에는 원-핫 인코딩을 더 쉽게 지원하는 API가 있다. get_dummies( )를 이용하면 된다. 사이킷런의 OneHotEncoder와 다르게 문자열 카테고리 값을 숫자 형으로 변환할 필요 없이 바로 변환할 수 있다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd </span><br><span class="line"></span><br><span class="line">df = pd.DataFrame(&#123;<span class="string">&#x27;item&#x27;</span>:[<span class="string">&#x27;TV&#x27;</span>,<span class="string">&#x27;냉장고&#x27;</span>,<span class="string">&#x27;전자렌지&#x27;</span>,<span class="string">&#x27;컴퓨터&#x27;</span>,<span class="string">&#x27;선풍기&#x27;</span>,<span class="string">&#x27;선풍기&#x27;</span>,<span class="string">&#x27;믹서&#x27;</span>,<span class="string">&#x27;믹서&#x27;</span>] &#125;) </span><br><span class="line">pd.get_dummies(df)</span><br><span class="line"></span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>item_TV</th>
      <th>item_냉장고</th>
      <th>item_믹서</th>
      <th>item_선풍기</th>
      <th>item_전자렌지</th>
      <th>item_컴퓨터</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>



<p>get_dummies()를 이용하면 숫자형 값으로 변환 없이도 바로 변환이 가능하다.</p>
<p><strong>피처 스케일링과 정규화</strong></p>
<p>서로 다른 변수의 값 범위를 일정한 수준으로 맞추는 작업을 피처 스케일링이라 한다. 대표적으로 표준화와 정규화가 있다.</p>
<p>표준화는 데이터의 피처 각각이 평균이 0이고 분산이1인 가우시안 정규 분포를 가진 값으로 변환하는 것을 의미한다.</p>
<p><img src="data:image/png;base64,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" alt="8.PNG"></p>
<p>일반적으로 정규화는 서로 다른 피처의 크기를 통일하기 위해 크기를 변환해주는 개념이다. 예를 들어 피처 A는 거리를 나타내는 변수로서 값이 0 ~ 100KM로 주어지고 피처 B는 금액을 나타내는 속성으로 0 ~ 1,000,000,000원으로 주어진다면 이 변수를 모두 동잏한 크기 단위로 비교하기 위해 값을 모두 최소 0 ~ 최대 1의 값으로 변환하는 것이다. 즉, 개별 데이터의 크기를 모두 똑같은 단위로 변경하는 것이다.</p>
<p><img src="data:image/png;base64,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" alt="9.PNG"></p>
<p>그런데 사이킷런에서 제공하는 Normalizer 모듈은 일반 정규화는 약간의 차이가 있다. 사이킷런에서의 Normalizer 모듈은 선형대수에서의 정규화 개념이 적용됐으며, 개별 벡터의 크기를 맞추기 위해 변환하는 것을 의미한다. 즉, 개별 벡터를 모든 피처 벡터의 크기로 나눠준다. 3개의 피처 x, y, z가 있으면 새로운 데이터는 다음과 같다.</p>
<p><img src="data:image/png;base64,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" alt="10.PNG"></p>
<p><strong>StandardScaler</strong></p>
<p>StandardScaler는 표준화를 지원하기 위한 클래스이다. 즉, 개별 피처를 평균이 0이고, 분산이 1인 값으로 변환해준다. 이렇게 가우시안 정규 분포를 가질 수 있도록 데이터를 변환한다. 특히 사이킷런에서 구현한 RBF 커널을 이용하는 SVM(Support Vector Machine), 선형 회귀(Linear Regression), 로지스틱 회귀(Logistic Regression)은 데이터가 가우시안 분포를 가진다는 가정하에 구현됐기에 사전에 표준화를 적용하는 건 성능 향상에 중요한 요소가 될 수 있다.<br>    # 가우시안분포 = 정규분포</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="comment"># 붓꽃 데이터 세트를 로딩하고 DataFrame으로 변환합니다.</span></span><br><span class="line">iris = load_iris()</span><br><span class="line">iris_data = iris.data</span><br><span class="line">iris_df = pd.DataFrame(data = iris_data, columns = iris.feature_names)</span><br><span class="line"></span><br><span class="line">print(<span class="string">&#x27;feature 들의 평균 값&#x27;</span>)</span><br><span class="line">print(iris_df.mean())</span><br><span class="line">print(<span class="string">&quot;/nfeature 들의 분산 값&quot;</span>)</span><br><span class="line">print(iris_df.var())</span><br></pre></td></tr></table></figure>

<pre><code>feature 들의 평균 값
sepal length (cm)    5.843333
sepal width (cm)     3.057333
petal length (cm)    3.758000
petal width (cm)     1.199333
dtype: float64
/nfeature 들의 분산 값
sepal length (cm)    0.685694
sepal width (cm)     0.189979
petal length (cm)    3.116278
petal width (cm)     0.581006
dtype: float64</code></pre>
<p>다음으로 StandardScaler을 사용해서 표준화를 진행해보자.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"></span><br><span class="line"><span class="comment"># StandardScaler객체 생성</span></span><br><span class="line">scaler=StandardScaler()</span><br><span class="line"><span class="comment"># StandardScaler로 데이터 세트 변화, fit()과 transform()호출</span></span><br><span class="line">scaler.fit(iris_df)</span><br><span class="line">iris_scaled=scaler.transform(iris_df)</span><br><span class="line"></span><br><span class="line"><span class="comment"># transform() 시 스케일 변화된 데이터 세트가 NumPy ndarray로 변환돼 이를 DataFrame으로 변환</span></span><br><span class="line">iris_df_scaled=pd.DataFrame(data=iris_scaled, columns=iris.feature_names)</span><br><span class="line">print(<span class="string">&#x27;feature 평균&#x27;</span>)</span><br><span class="line">print(iris_df_scaled.mean())</span><br><span class="line">print(<span class="string">&#x27;\nfeature 분산&#x27;</span>)</span><br><span class="line">print(iris_df_scaled.var())</span><br></pre></td></tr></table></figure>

<pre><code>feature 평균
sepal length (cm)   -1.690315e-15
sepal width (cm)    -1.842970e-15
petal length (cm)   -1.698641e-15
petal width (cm)    -1.409243e-15
dtype: float64

feature 분산
sepal length (cm)    1.006711
sepal width (cm)     1.006711
petal length (cm)    1.006711
petal width (cm)     1.006711
dtype: float64</code></pre>
<p>모든 칼럼 값의 평균이 0에 아주 가까운 값으로 그리고 분산은 1에 아주 가까운 값으로 변환됐음을 확인할수 있다.</p>
<p><strong>MinMaxScaler</strong></p>
<p>MinMaxScaler는 데이터값을 0과 1사이의 범위 값으로 변환한다.(음수 값이 있으면 -1에서 1값으로 변환한다.) 데이터의 분포가 가우시안 분포가 아닐 경우에 Min, Max Scale을 적용해 볼 수 있다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler</span><br><span class="line"></span><br><span class="line"><span class="comment"># MinMaxScaler객체 생성</span></span><br><span class="line">scaler=MinMaxScaler()</span><br><span class="line"><span class="comment"># MinMaxScaler로 데이터 세트 변환, fit()과 transform()호출</span></span><br><span class="line">scaler.fit(iris_df)</span><br><span class="line">iris_scaled=scaler.transform(iris_df)</span><br><span class="line"></span><br><span class="line"><span class="comment"># transform() 시 스케일 변환된 데이터 세트가 NumPy ndarry로 변환돼 이를 DataFrame으로 변환</span></span><br><span class="line">iris_df_scaled = pd.DataFrame(data=iris_scaled, columns=iris.feature_names)</span><br><span class="line">print(<span class="string">&#x27;feature 최솟값&#x27;</span>)</span><br><span class="line">print(iris_df_scaled.<span class="built_in">min</span>())</span><br><span class="line">print(<span class="string">&#x27;/\fearure 최댓값&#x27;</span>)</span><br><span class="line">print(iris_df_scaled.<span class="built_in">max</span>())</span><br></pre></td></tr></table></figure>

<pre><code>feature 최솟값
sepal length (cm)    0.0
sepal width (cm)     0.0
petal length (cm)    0.0
petal width (cm)     0.0
dtype: float64
/earure 최댓값
sepal length (cm)    1.0
sepal width (cm)     1.0
petal length (cm)    1.0
petal width (cm)     1.0
dtype: float64</code></pre>
<p>모든 피처에 0에서 1 사이의 값으로 변환되는 스케일링이 적용됐음을 알 수 있다.</p>
<p><strong>학습 데이터와 테스트 데이터의 스케일링 변환시 유의점</strong></p>
<p>StandadScaler나 MinMaxScaler 같은 Scaler 객체를 이용해 데이터의 스케일링 변환시 fit(), transform(), fit_transform() 메소드를 이용한다. 그리고 한번에 적용하는 기능을 수행해야 한다.</p>
<ul>
<li>fit(): 데이터 변환을 위한 기준 정보설정을 적용</li>
<li>trancsform(): 설정된 정보를 이용해 데이터를 변환</li>
<li>fit_transform(): fit(), transform()을 한번에 적용</li>
</ul>
<p>데이터 전처리 시 fit( ), transform( ) 함수를 사용하는데 이 때 학습용 데이터에는 fit과 transform을 사용하되 테스트 데이터에는 fit을 사용하지 않도록 해야한다. 여기서 fit( )은 데이터 변환을 위한 기준 정보를 설정하며 transform( )은 설정된 정보를 이용해 데이터를 반환한다. 그런데 테스트 데이터에서 다시 fit( )을 사용해버리면 학습 데이터와는 또 다른 새로운 스케일링 기준 정보를 만들게 되어 올바른 예측 결과를 도출하지 못할 수도 있다.</p>
<p>학습 데이터와 테스트 데이터의 fit(), transform(), fit_transform()을 이용해 스케일링 변환 시 유의 할점은 아래와 같다.</p>
<ul>
<li>가능하다면 전체 데이터의 스케일링 변환을 적용한 뒤 학습과 테스트 데이터로 분리</li>
<li>1이 여의치 않다면 테스트 데이터 변환시에는 fit()이나 fit_transform() 을 사용하지 않고 학습데이터로 이미 fit( )된 Scaler 객체를 통해 transfrom()으로 변한</li>
</ul>
<h2 id="사이킷런으로-수행하는-타이타닉-생존자-예측"><a href="#사이킷런으로-수행하는-타이타닉-생존자-예측" class="headerlink" title="사이킷런으로 수행하는 타이타닉 생존자 예측"></a>사이킷런으로 수행하는 타이타닉 생존자 예측</h2><p>캐글에서 제공하는 타이타닉 탑승자 데이터를 기반으로 생존자 예측을 사이킷런으로 수행행 보겠다.</p>
<p>타이타닉 탑승자 데이터에 대해 개략적으로 살펴보자.</p>
<ul>
<li>Passengerid : 탑승자 번호</li>
<li>survived : 생존 여부 0 : 사망 / 1 : 생존</li>
<li>pclass : 티켓의 선실 등급</li>
<li>sex : 성별</li>
<li>name :이름</li>
<li>Age : 나이</li>
<li>sibsp : 같이 탑승한 형제자매 또는 배우자 인원수</li>
<li>parch : 같이 탑승한 부모님 또는 어린이 인원수</li>
<li>ticket : 티켓 번호</li>
<li>fare : 요금</li>
<li>cabin : 선실 번호</li>
<li>embarked : 중간 정착 항구 C = Cherbourg, Q = Queenstown, S = Southampton</li>
</ul>
<p>탑승자 파일을 판다스의 read_csv()를 이용해 DataFrame으로 로딩하자. 예제를 실행 할때는 맷플롯립과 시본을 이용해 시각화 하자</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Mount Google Drive</span></span><br><span class="line"><span class="keyword">from</span> google.colab <span class="keyword">import</span> drive <span class="comment"># import drive from google colab</span></span><br><span class="line"></span><br><span class="line">ROOT = <span class="string">&quot;/content/drive&quot;</span>     <span class="comment"># default location for the drive</span></span><br><span class="line">print(ROOT)                 <span class="comment"># print content of ROOT (Optional)</span></span><br><span class="line">drive.mount(ROOT)           <span class="comment"># we mount the google drive at /content/drive</span></span><br></pre></td></tr></table></figure>

<pre><code>/content/drive
Mounted at /content/drive</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> os.path <span class="keyword">import</span> join</span><br><span class="line"></span><br><span class="line">MY_GOOGLE_DRIVE_PATH = <span class="string">&#x27;My Drive/Colab Notebooks/pymldg-rev/2장&#x27;</span> <span class="comment"># 주소만 수정하면됨</span></span><br><span class="line">PROJECT_PATH = join(ROOT, MY_GOOGLE_DRIVE_PATH)</span><br><span class="line">print(PROJECT_PATH)</span><br></pre></td></tr></table></figure>

<pre><code>/content/drive/My Drive/Colab Notebooks/pymldg-rev/2장</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">%cd <span class="string">&quot;&#123;PROJECT_PATH&#125;&quot;</span></span><br></pre></td></tr></table></figure>

<pre><code>/content/drive/My Drive/Colab Notebooks/pymldg-rev/2장</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 모듈 불러오기</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="comment"># Excel 데이터 데이터프레임으로 변환</span></span><br><span class="line">titanic_df = pd.read_csv(<span class="string">&quot;titanic_train.csv&quot;</span>)</span><br><span class="line">titanic_df.head(<span class="number">3</span>)</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>PassengerId</th>
      <th>Survived</th>
      <th>Pclass</th>
      <th>Name</th>
      <th>Sex</th>
      <th>Age</th>
      <th>SibSp</th>
      <th>Parch</th>
      <th>Ticket</th>
      <th>Fare</th>
      <th>Cabin</th>
      <th>Embarked</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>0</td>
      <td>3</td>
      <td>Braund, Mr. Owen Harris</td>
      <td>male</td>
      <td>22.0</td>
      <td>1</td>
      <td>0</td>
      <td>A/5 21171</td>
      <td>7.2500</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>
      <td>female</td>
      <td>38.0</td>
      <td>1</td>
      <td>0</td>
      <td>PC 17599</td>
      <td>71.2833</td>
      <td>C85</td>
      <td>C</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>1</td>
      <td>3</td>
      <td>Heikkinen, Miss. Laina</td>
      <td>female</td>
      <td>26.0</td>
      <td>0</td>
      <td>0</td>
      <td>STON/O2. 3101282</td>
      <td>7.9250</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
  </tbody>
</table>
</div>



<p>로딩된 데이터 칼럼 타입을 확인해 보겠습니다. DataFrame의 info() 메서드를 통해 쉽게 확인이 가능 합다</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">&#x27;/n ### 학습 데이터 정보 ### /n&#x27;</span>)</span><br><span class="line">print(titanic_df.info()) <span class="comment"># 데이터를 불러오고 가장 먼저 해야하는 작업</span></span><br></pre></td></tr></table></figure>

<pre><code>/n ### 학습 데이터 정보 ### /n
&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
RangeIndex: 891 entries, 0 to 890
Data columns (total 12 columns):
 #   Column       Non-Null Count  Dtype  
---  ------       --------------  -----  
 0   PassengerId  891 non-null    int64  
 1   Survived     891 non-null    int64  
 2   Pclass       891 non-null    int64  
 3   Name         891 non-null    object 
 4   Sex          891 non-null    object 
 5   Age          714 non-null    float64
 6   SibSp        891 non-null    int64  
 7   Parch        891 non-null    int64  
 8   Ticket       891 non-null    object 
 9   Fare         891 non-null    float64
 10  Cabin        204 non-null    object 
 11  Embarked     889 non-null    object 
dtypes: float64(2), int64(5), object(5)
memory usage: 83.7+ KB
None</code></pre>
<p>판다스는 넘파이 기반으로 만들어졌고 넘파이의 String타입이 길이 제한이 있어 이에 대한 구분을 위해 object 타입으로 명기한다. age, cabin, embarked 칼럼은 각각 714개, 204개, 889개의 not Null 값을 가지고 있고, 각각 177개, 608개, 2개의 Null을 가지고 있다.</p>
<p>사이킷런 머신러닝 알고리즘은 Null 값을 허용하지 않으므로 Null 값을 어떻게 처리할지 결정해야 한다. DataFrame의 fillna() 함수를 사용해 간단하게 Null 값을 평균 또는 고정 값으로 변경한다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">titanic_df[<span class="string">&#x27;Age&#x27;</span>].fillna(titanic_df[<span class="string">&#x27;Age&#x27;</span>].mean(),inplace=<span class="literal">True</span>)</span><br><span class="line">titanic_df[<span class="string">&#x27;Cabin&#x27;</span>].fillna(<span class="string">&#x27;N&#x27;</span>,inplace = <span class="literal">True</span>)</span><br><span class="line">titanic_df[<span class="string">&#x27;Embarked&#x27;</span>].fillna(<span class="string">&#x27;N&#x27;</span>,inplace = <span class="literal">True</span>)</span><br><span class="line">print(<span class="string">&#x27;데이터 셋 Null 값 개수&#x27;</span>,titanic_df.isnull().<span class="built_in">sum</span>().<span class="built_in">sum</span>())</span><br></pre></td></tr></table></figure>

<pre><code>데이터 셋 Null 값 개수 0</code></pre>
<p>현재 남아있는 문자열 피처는 Sex, Cabin, Embarked 이다. 먼저 피처들의 값 분류를 살펴보자.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">&#x27; Sex 값 분포 :\n&#x27;</span>,titanic_df[<span class="string">&#x27;Sex&#x27;</span>].value_counts()) <span class="comment"># value_counts: 각 값의 개수 세기</span></span><br><span class="line">print(<span class="string">&#x27;\n Cabin 값 분포 :\n&#x27;</span>,titanic_df[<span class="string">&#x27;Cabin&#x27;</span>].value_counts())</span><br><span class="line">print(<span class="string">&#x27;\n Embarked 값 분포 :\n&#x27;</span>,titanic_df[<span class="string">&#x27;Embarked&#x27;</span>].value_counts())</span><br></pre></td></tr></table></figure>

<pre><code> Sex 값 분포 :
 male      577
female    314
Name: Sex, dtype: int64

 Cabin 값 분포 :
 N              687
C23 C25 C27      4
B96 B98          4
G6               4
F2               3
              ... 
C32              1
C148             1
D6               1
E46              1
C103             1
Name: Cabin, Length: 148, dtype: int64

 Embarked 값 분포 :
 S    644
C    168
Q     77
N      2
Name: Embarked, dtype: int64</code></pre>
<p>Sex, Embarked 값은 문제 없어 보인다. cabin의 경우 N이 가장 많은 것도 특이하지만 , 속성값이 제대로 정리되지 않은 것 같다. cabin의 경우 선실 번호 중 선실 등급을 나타내는 알파벳이 중요해 보인다. 일등식의 경우 삼등실에 투숙한 사람보다 살아날 확률이 높다. Cabin 속성을의 앞 문자만 추출하자.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">titanic_df[<span class="string">&#x27;Cabin&#x27;</span>]=titanic_df[<span class="string">&#x27;Cabin&#x27;</span>].<span class="built_in">str</span>[:<span class="number">1</span>]</span><br><span class="line">titanic_df[<span class="string">&#x27;Cabin&#x27;</span>].head(<span class="number">3</span>)</span><br></pre></td></tr></table></figure>




<pre><code>0    N
1    C
2    N
Name: Cabin, dtype: object</code></pre>
<p>머신러닝 알고리즘을 적용해 예측을 수행하지 전에 데이터를 먼저 탐색해 보겠다.<br>어떤 유형의 승객이 생존확률이 높았는지 보자</p>
<ul>
<li>성별이 생존 확률에 어떤 영향을 미쳤는지, 성별에 따른 생존자 수를 비교해 보자.</li>
<li>부자와 가난한 사람의 생존확률 , 일등실, 이등실, 삼등실에 따라 생종확률을 살펴보자.</li>
<li>나이에 따른 생존확률</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">titanic_df.groupby([<span class="string">&#x27;Sex&#x27;</span>,<span class="string">&#x27;Survived&#x27;</span>])[<span class="string">&#x27;Survived&#x27;</span>].count() <span class="comment"># male: 남성, female: 여성</span></span><br></pre></td></tr></table></figure>




<pre><code>Sex     Survived
female  0            81
        1           233
male    0           468
        1           109
Name: Survived, dtype: int64</code></pre>
<p>Survied 칼럼은 레이블로서 결정 클래스 값이다. Survied 0은 사망, 1은 생존을 의미한다. 남자가 여자에 비해 사망자가 더 많았다. 시각화는 시본 패키지를 이용해 할 것이다. 시본은 기본적으로 맷플롯립에 기반하지만, 좀더 새련된 비주얼과 쉬운 API, 판다스의 연동 등으로 데이터 분석을 위한 시각화로 애용 되는 패키지다.</p>
<p>Seaborn 패키지를 이용해 시각화를 진행</p>
<ul>
<li>barplot() 을이용</li>
<li>X축에 ‘Sex’칼럼</li>
<li>Y축에 ‘Survived’ 칼럼</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sns.barplot(x=<span class="string">&#x27;Sex&#x27;</span>,y=<span class="string">&#x27;Survived&#x27;</span>,data=titanic_df) <span class="comment"># 여성의 생존율이 높은걸 확인</span></span><br></pre></td></tr></table></figure>




<pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x7fe6a0d67be0&gt;</code></pre>
<p><img src="/images/%EC%82%AC%EC%9D%B4%ED%82%B7%EB%9F%B0%EC%9C%BC%EB%A1%9C_%EC%8B%9C%EC%9E%91%ED%95%98%EB%8A%94_%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D/output_120_1.png" alt="png"></p>
<p>부자와 가난한 사람 간의 생존 확률은 어떻게 다를지 확인하자. 객실 등급별 생존 확률을 살펴보자</p>
<p>앞에 barplot() 함수에 x 좌표에 ‘Pclass’를 , 그리고 hue파라미터를 추가해 hue=’Sex’와 같이 입력하면 된다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sns.barplot(x=<span class="string">&#x27;Pclass&#x27;</span>,y=<span class="string">&#x27;Survived&#x27;</span>,hue = <span class="string">&#x27;Sex&#x27;</span>,data=titanic_df) <span class="comment"># 높은 객실의 승객 생존율이 높은걸 확인</span></span><br></pre></td></tr></table></figure>




<pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x7fe6a0d18278&gt;</code></pre>
<p><img src="/images/%EC%82%AC%EC%9D%B4%ED%82%B7%EB%9F%B0%EC%9C%BC%EB%A1%9C_%EC%8B%9C%EC%9E%91%ED%95%98%EB%8A%94_%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D/output_122_1.png" alt="png"></p>
<p>여성의 경우 일, 이등실에 따른 생존 확률의 차이가 크지 않으나 삼등실의 경우 차이가 크다. 남성의 경우 일등실의 생존 확률이 월등히 높다는 것을 볼수 있다. </p>
<p>이번에는 나이에 따른 생존 확률을 알아보자</p>
<ul>
<li>0~5: baby</li>
<li>6~12: child</li>
<li>13~18:teenager</li>
<li>19~25: student</li>
<li>26~35: yuong adult</li>
<li>36~60: adult</li>
<li>61이상: elderly</li>
<li>-1이하 오류값: unknown</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 입력 age에 따라 구분값을 반환하는 함수 설정. DataFrame의 apply lambda식에 사용. </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_category</span>(<span class="params">age</span>):</span></span><br><span class="line">    cat = <span class="string">&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">if</span> age &lt;= <span class="number">-1</span>: cat = <span class="string">&#x27;Unknown&#x27;</span></span><br><span class="line">    <span class="keyword">elif</span> age &lt;= <span class="number">5</span>: cat = <span class="string">&#x27;Baby&#x27;</span></span><br><span class="line">    <span class="keyword">elif</span> age &lt;= <span class="number">12</span>: cat = <span class="string">&#x27;Child&#x27;</span></span><br><span class="line">    <span class="keyword">elif</span> age &lt;= <span class="number">18</span>: cat = <span class="string">&#x27;Teenager&#x27;</span></span><br><span class="line">    <span class="keyword">elif</span> age &lt;= <span class="number">25</span>: cat = <span class="string">&#x27;Student&#x27;</span></span><br><span class="line">    <span class="keyword">elif</span> age &lt;= <span class="number">35</span>: cat = <span class="string">&#x27;Young Adult&#x27;</span></span><br><span class="line">    <span class="keyword">elif</span> age &lt;= <span class="number">60</span>: cat = <span class="string">&#x27;Adult&#x27;</span></span><br><span class="line">    <span class="keyword">else</span> : cat = <span class="string">&#x27;Elderly&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> cat</span><br><span class="line"></span><br><span class="line"><span class="comment">#막대 그래프의 크기 figure를 설정</span></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>,<span class="number">6</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">#x축의 값을 순차적으로 표시하기 위한 설정</span></span><br><span class="line">group_names = [<span class="string">&#x27;Unknown&#x27;</span>, <span class="string">&#x27;Baby&#x27;</span>, <span class="string">&#x27;Child&#x27;</span>, <span class="string">&#x27;Teenager&#x27;</span>, <span class="string">&#x27;Student&#x27;</span>, <span class="string">&#x27;Young Adult&#x27;</span>, <span class="string">&#x27;Adult&#x27;</span>, <span class="string">&#x27;Elderly&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">#lambda 식에 위에서 생성한 get_category() 함수를 반환값으로 지정.</span></span><br><span class="line"><span class="comment">#get_category(X)는 입력값으로 &#x27;Age&#x27; 칼럼 값을 받아서 해당하는 cat 반환</span></span><br><span class="line">titanic_df[<span class="string">&#x27;Age_cat&#x27;</span>] =titanic_df[<span class="string">&#x27;Age&#x27;</span>].apply(<span class="keyword">lambda</span> x : get_category(x))</span><br><span class="line">sns.barplot(x=<span class="string">&#x27;Age_cat&#x27;</span>, y=<span class="string">&#x27;Survived&#x27;</span>,hue =<span class="string">&#x27;Sex&#x27;</span>, data=titanic_df,order=group_names)</span><br><span class="line">titanic_df.drop(<span class="string">&#x27;Age_cat&#x27;</span>,axis=<span class="number">1</span>,inplace=<span class="literal">True</span>) <span class="comment"># axis: 열의 값을 설정</span></span><br></pre></td></tr></table></figure>



<p><img src="/images/%EC%82%AC%EC%9D%B4%ED%82%B7%EB%9F%B0%EC%9C%BC%EB%A1%9C_%EC%8B%9C%EC%9E%91%ED%95%98%EB%8A%94_%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D/output_124_0.png" alt="png"></p>
<p>아기의 경우 생존이 비교적 높고, 아이의 경우 생존률이 낮은 것을 확인했다. 분석결과를 기초로 Sex, Age, PClass 등이 생존을 좌우하는 것을 확인할 수 있었다.</p>
<p>이제 남아있는 문자열 카테고리 피처를 숫자형 카테고리 피처로 변환한다. 인코딩은 사이킷런의 LabelEncoder 클래스를 이용해 레이블 인코딩을 적용한다. LabelEncoder 객체는 카테고리 값의 유형 수에 따라 0 ~ (카테고리 유형 수-1)까지의 숫자 값으로 변환한다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">encode_features</span>(<span class="params">dataDF</span>):</span></span><br><span class="line">    features = [<span class="string">&#x27;Cabin&#x27;</span>,<span class="string">&#x27;Sex&#x27;</span>,<span class="string">&#x27;Embarked&#x27;</span>]</span><br><span class="line">    <span class="keyword">for</span> feature <span class="keyword">in</span> features:</span><br><span class="line">        le = preprocessing.LabelEncoder()</span><br><span class="line">        le = le.fit(dataDF[feature])</span><br><span class="line">        dataDF[feature] = le.transform(dataDF[feature])</span><br><span class="line">    <span class="keyword">return</span> dataDF</span><br><span class="line"></span><br><span class="line">titanic_df = encode_features(titanic_df)</span><br><span class="line">titanic_df.head()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>PassengerId</th>
      <th>Survived</th>
      <th>Pclass</th>
      <th>Name</th>
      <th>Sex</th>
      <th>Age</th>
      <th>SibSp</th>
      <th>Parch</th>
      <th>Ticket</th>
      <th>Fare</th>
      <th>Cabin</th>
      <th>Embarked</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>0</td>
      <td>3</td>
      <td>Braund, Mr. Owen Harris</td>
      <td>1</td>
      <td>22.0</td>
      <td>1</td>
      <td>0</td>
      <td>A/5 21171</td>
      <td>7.2500</td>
      <td>7</td>
      <td>3</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>
      <td>0</td>
      <td>38.0</td>
      <td>1</td>
      <td>0</td>
      <td>PC 17599</td>
      <td>71.2833</td>
      <td>2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>1</td>
      <td>3</td>
      <td>Heikkinen, Miss. Laina</td>
      <td>0</td>
      <td>26.0</td>
      <td>0</td>
      <td>0</td>
      <td>STON/O2. 3101282</td>
      <td>7.9250</td>
      <td>7</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>1</td>
      <td>1</td>
      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>
      <td>0</td>
      <td>35.0</td>
      <td>1</td>
      <td>0</td>
      <td>113803</td>
      <td>53.1000</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>0</td>
      <td>3</td>
      <td>Allen, Mr. William Henry</td>
      <td>1</td>
      <td>35.0</td>
      <td>0</td>
      <td>0</td>
      <td>373450</td>
      <td>8.0500</td>
      <td>7</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
</div>



<p>Sex, Cabin, Embarked 속성이 숫자형으로 바뀐 것을 알 수 있다.</p>
<p>지금까지 피처를 가공한 내역을 정리하고 이를 함수로 만들어 쉽게 재사용할 수 있도록 만들겠다. 데이터의 전처리를 전체적으로 호출하는 함수는 transform_features() 이며 Null 처리, 포매팅, 인코딩을 수행하는 내부 함수로 구성했다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Null 처리 함수</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fillna</span>(<span class="params">df</span>):</span></span><br><span class="line">    df[<span class="string">&#x27;Age&#x27;</span>].fillna(df[<span class="string">&#x27;Age&#x27;</span>].mean(),inplace = <span class="literal">True</span>)</span><br><span class="line">    df[<span class="string">&#x27;Cabin&#x27;</span>].fillna(<span class="string">&#x27;N&#x27;</span>,inplace=<span class="literal">True</span>)</span><br><span class="line">    df[<span class="string">&#x27;Embarked&#x27;</span>].fillna(<span class="string">&#x27;N&#x27;</span>, inplace = <span class="literal">True</span>)</span><br><span class="line">    df[<span class="string">&#x27;Fare&#x27;</span>].fillna(<span class="number">0</span>,inplace = <span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> df</span><br><span class="line"></span><br><span class="line"><span class="comment">#머신러닝 알고리즘에 불필요한 속성 제거</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">drop_features</span>(<span class="params">df</span>):</span></span><br><span class="line">    df.drop([<span class="string">&#x27;PassengerId&#x27;</span>, <span class="string">&#x27;Name&#x27;</span>, <span class="string">&#x27;Ticket&#x27;</span>],axis=<span class="number">1</span>,inplace=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> df</span><br><span class="line"></span><br><span class="line"><span class="comment">#레이블 인코딩 수행</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">format_features</span>(<span class="params">df</span>):</span></span><br><span class="line">    df[<span class="string">&#x27;Cabin&#x27;</span>] = df[<span class="string">&#x27;Cabin&#x27;</span>].<span class="built_in">str</span>[:<span class="number">1</span>]</span><br><span class="line">    features = [<span class="string">&#x27;Cabin&#x27;</span>,<span class="string">&#x27;Sex&#x27;</span>,<span class="string">&#x27;Embarked&#x27;</span>]</span><br><span class="line">    <span class="keyword">for</span> feature <span class="keyword">in</span> features:</span><br><span class="line">        le = LabelEncoder()</span><br><span class="line">        le = le.fit(df[feature])</span><br><span class="line">        df[feature] = le.transform(df[feature])</span><br><span class="line">    <span class="keyword">return</span> df</span><br><span class="line"></span><br><span class="line"><span class="comment"># 앞에서 설정한 데이터 전처리 함수 호출</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">transform_features</span>(<span class="params">df</span>):</span></span><br><span class="line">    df = fillna(df)</span><br><span class="line">    df = drop_features(df)</span><br><span class="line">    df= format_features(df)</span><br><span class="line">    <span class="keyword">return</span> df</span><br></pre></td></tr></table></figure>

<p>원본데이터를 가공하기 위해 원본 csv 파일을 다시 로딩</p>
<ul>
<li>Survived 속성만 별도 분리해 클래스 결정값 데이터 세트로 만들기</li>
<li>Survived 속성을 드롭해 피처 데이터 세트 만들기</li>
<li>생성된 데이터 세트에 transform_features()를 적용해 데이터 가공</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#원본 데이터 재로딩하고, 피처 데이터 세트와 레이블 데이터 세트 추출.</span></span><br><span class="line">titanic_df = pd.read_csv(<span class="string">&quot;titanic_train.csv&quot;</span>)</span><br><span class="line">y_titanic_df=titanic_df[<span class="string">&#x27;Survived&#x27;</span>]</span><br><span class="line">X_titanic_df=titanic_df.drop(<span class="string">&#x27;Survived&#x27;</span>,axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">X_titanic_df = transform_features(X_titanic_df)</span><br></pre></td></tr></table></figure>

<p>train_test_split() API를 이용해 별도의 테스트 데이터 세트 추출, 세트 크기는 전체의 20% 설정</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, X_test, y_train, y_test=train_test_split(X_titanic_df, y_titanic_df, \</span><br><span class="line">                                                  test_size=<span class="number">0.2</span>, random_state=<span class="number">11</span>)</span><br></pre></td></tr></table></figure>

<p>ML 알고리즘 결정 트리, 랜덤 포레스트, 로지스틱 회귀를 이용해 타이타닉 생존자를 예측해 보자</p>
<ul>
<li>결정 트리를 위해 DecisionTreeClassifie 클래스 제공</li>
<li>랜덤 포레스트를 위해 RandomForestClassifier 클래스 제공</li>
<li>로지스틱 회귀를 위해 LogisticRegression 클래스 제공</li>
</ul>
<p>위의 사이킷런 클래스를 이용해 train_test_split()으로 분리한 데이터와 테스트 데이터를 기반으로 머신러닝 모델을 학습(fit)하고 예측(predict) 한다. 예측 성능 평가는 정확도로 하기위해 accuracy_score() API를 사용한다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line"><span class="comment">#결정트리, Random Forest, 로지스틱 회귀를 위한 사이킷런 Classifier 클래스 생성</span></span><br><span class="line">dt_clf = DecisionTreeClassifier(random_state=<span class="number">11</span>)</span><br><span class="line">rf_clf = RandomForestClassifier(random_state=<span class="number">11</span>)</span><br><span class="line">lr_clf = LogisticRegression()</span><br><span class="line"></span><br><span class="line"><span class="comment">#DecisionTreeClassifier 학습/예측/평가</span></span><br><span class="line">dt_clf.fit(X_train, y_train)</span><br><span class="line">dt_pred = dt_clf.predict(X_test)</span><br><span class="line">print(<span class="string">&#x27;DecisionTreeClassifier 정확도: &#123;0:.4f&#125;&#x27;</span>.<span class="built_in">format</span>(accuracy_score(y_test, dt_pred)))</span><br><span class="line"></span><br><span class="line"><span class="comment">#RandomForestClassifier 학습/예측/평가</span></span><br><span class="line">rf_clf.fit(X_train, y_train)</span><br><span class="line">rf_pred = rf_clf.predict(X_test)</span><br><span class="line">print(<span class="string">&#x27;RandomForestClassifier 정확도:&#123;0:.4f&#125;&#x27;</span>.<span class="built_in">format</span>(accuracy_score(y_test, rf_pred)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># LogisticRegression 학습/예측/평가</span></span><br><span class="line">lr_clf.fit(X_train , y_train)</span><br><span class="line">lr_pred = lr_clf.predict(X_test)</span><br><span class="line">print(<span class="string">&#x27;LogisticRegression 정확도: &#123;0:.4f&#125;&#x27;</span>.<span class="built_in">format</span>(accuracy_score(y_test, lr_pred)))</span><br></pre></td></tr></table></figure>

<pre><code>DecisionTreeClassifier 정확도: 0.7877
RandomForestClassifier 정확도:0.8547
LogisticRegression 정확도: 0.8492


/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)</code></pre>
<p>최적화 작업을 수행하지 않았고, 데이터 양도 충분하지 않기 때문에 어떤 알고리즘이 가장 성능이 좋은지 평가할 수는 없다. 교차 검증을 위해 KFord 클래스(폴드 개수는 5개로 설정), cross_val_score, GridSearchCV클래스 모두 사용 한다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> KFold</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">exec_kfold</span>(<span class="params">clf, folds=<span class="number">5</span></span>):</span></span><br><span class="line">    <span class="comment"># 폴드 세트를 5개인 KFold객체를 생성, 폴드 수만큼 예측결과 저장을 위한  리스트 객체 생성.</span></span><br><span class="line">    kfold = KFold(n_splits=folds)</span><br><span class="line">    scores = []</span><br><span class="line"></span><br><span class="line">    <span class="comment"># KFold 교차 검증 수행. </span></span><br><span class="line">    <span class="keyword">for</span> iter_count , (train_index, test_index) <span class="keyword">in</span> <span class="built_in">enumerate</span>(kfold.split(X_titanic_df)):</span><br><span class="line">        <span class="comment"># X_titanic_df 데이터에서 교차 검증별로 학습과 검증 데이터를 가리키는 index 생성</span></span><br><span class="line">        X_train, X_test = X_titanic_df.values[train_index], X_titanic_df.values[test_index]</span><br><span class="line">        y_train, y_test = y_titanic_df.values[train_index], y_titanic_df.values[test_index]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Classifier 학습, 예측, 정확도 계산 </span></span><br><span class="line">        clf.fit(X_train, y_train) </span><br><span class="line">        predictions = clf.predict(X_test)</span><br><span class="line">        accuracy = accuracy_score(y_test, predictions)</span><br><span class="line">        scores.append(accuracy)</span><br><span class="line">        print(<span class="string">&quot;교차 검증 &#123;0&#125; 정확도: &#123;1:.4f&#125;&quot;</span>.<span class="built_in">format</span>(iter_count, accuracy))     </span><br><span class="line"></span><br><span class="line">    <span class="comment"># 5개 fold에서의 평균 정확도 계산. </span></span><br><span class="line">    mean_score = np.mean(scores)</span><br><span class="line">    print(<span class="string">&quot;평균 정확도: &#123;0:.4f&#125;&quot;</span>.<span class="built_in">format</span>(mean_score)) </span><br><span class="line"><span class="comment"># exec_kfold 호출</span></span><br><span class="line">exec_kfold(dt_clf , folds=<span class="number">5</span>) </span><br></pre></td></tr></table></figure>

<pre><code>교차 검증 0 정확도: 0.7542
교차 검증 1 정확도: 0.7809
교차 검증 2 정확도: 0.7865
교차 검증 3 정확도: 0.7697
교차 검증 4 정확도: 0.8202
평균 정확도: 0.7823</code></pre>
<p>평균 정확도는 약 78.23% 이다. 교차 검증을 cross_val_score() API를 이용해 수행한다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"></span><br><span class="line">scores = cross_val_score(dt_clf, X_titanic_df , y_titanic_df , cv=<span class="number">5</span>)</span><br><span class="line"><span class="keyword">for</span> iter_count,accuracy <span class="keyword">in</span> <span class="built_in">enumerate</span>(scores):</span><br><span class="line">    print(<span class="string">&quot;교차 검증 &#123;0&#125; 정확도: &#123;1:.4f&#125;&quot;</span>.<span class="built_in">format</span>(iter_count, accuracy))</span><br><span class="line"></span><br><span class="line">print(<span class="string">&quot;평균 정확도: &#123;0:.4f&#125;&quot;</span>.<span class="built_in">format</span>(np.mean(scores)))</span><br></pre></td></tr></table></figure>

<pre><code>교차 검증 0 정확도: 0.7430
교차 검증 1 정확도: 0.7753
교차 검증 2 정확도: 0.7921
교차 검증 3 정확도: 0.7865
교차 검증 4 정확도: 0.8427
평균 정확도: 0.7879</code></pre>
<p>방금 전의 K폴드의 평균 정확도가 약간 다른 이유는 cross_val_score가 stratifiedKFord를 이용해 폴드 세트를 분할하기 때문이다.</p>
<p>GridSearchCV를 이용해 DecisionTreeClassifier의 최적 하이퍼 파라미터를 찾고 예측 성능을 측정해 본다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"></span><br><span class="line">parameters = &#123;<span class="string">&#x27;max_depth&#x27;</span>:[<span class="number">2</span>,<span class="number">3</span>,<span class="number">5</span>,<span class="number">10</span>],</span><br><span class="line">             <span class="string">&#x27;min_samples_split&#x27;</span>:[<span class="number">2</span>,<span class="number">3</span>,<span class="number">5</span>], <span class="string">&#x27;min_samples_leaf&#x27;</span>:[<span class="number">1</span>,<span class="number">5</span>,<span class="number">8</span>]&#125;</span><br><span class="line"></span><br><span class="line">grid_dclf = GridSearchCV(dt_clf , param_grid=parameters , scoring=<span class="string">&#x27;accuracy&#x27;</span> , cv=<span class="number">5</span>)</span><br><span class="line">grid_dclf.fit(X_train , y_train)</span><br><span class="line"></span><br><span class="line">print(<span class="string">&#x27;GridSearchCV 최적 하이퍼 파라미터 :&#x27;</span>,grid_dclf.best_params_)</span><br><span class="line">print(<span class="string">&#x27;GridSearchCV 최고 정확도: &#123;0:.4f&#125;&#x27;</span>.<span class="built_in">format</span>(grid_dclf.best_score_))</span><br><span class="line">best_dclf = grid_dclf.best_estimator_</span><br><span class="line"></span><br><span class="line"><span class="comment"># GridSearchCV의 최적 하이퍼 파라미터로 학습된 Estimator로 예측 및 평가 수행. </span></span><br><span class="line">dpredictions = best_dclf.predict(X_test)</span><br><span class="line">accuracy = accuracy_score(y_test , dpredictions)</span><br><span class="line">print(<span class="string">&#x27;테스트 세트에서의 DecisionTreeClassifier 정확도 : &#123;0:.4f&#125;&#x27;</span>.<span class="built_in">format</span>(accuracy))</span><br></pre></td></tr></table></figure>

<pre><code>GridSearchCV 최적 하이퍼 파라미터 : &#123;&#39;max_depth&#39;: 3, &#39;min_samples_leaf&#39;: 5, &#39;min_samples_split&#39;: 2&#125;
GridSearchCV 최고 정확도: 0.7992
테스트 세트에서의 DecisionTreeClassifier 정확도 : 0.8715</code></pre>
<p>하이퍼 파라미터인 max_depth=3, min_samples_leaf=1, min_samples_split=2로 DecisionTreeClassifier를 학습시킨 뒤 예측 정확도가 약 87.15%로 향상됐다. 일반적으로 하이퍼 파라미터를 듀닝하더라고 이 정도 수준으로 증가하기는 어렵다. 테스트용 데이터 세트가 작기 때문에 수치상으로 예측 성능이 많이 증가 한것처럼 보인다.</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-12-01T01:06:14.981Z" title="2020-12-01T01:06:14.981Z">2020-12-01</time></span><span class="level-item">Updated&nbsp;<time dateTime="2020-12-01T01:10:02.464Z" title="2020-12-01T01:10:02.464Z">2020-12-01</time></span><span class="level-item"><a class="link-muted" href="/categories/study/">study</a></span><span class="level-item">43 minutes read (About 6497 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/12/01/study/kaggle_titanic-eda-simple-model-0-80622_mviola/">캐글 Titanic EDA + Simple Model [0.80622] 분석</a></h1><div class="content"><h1 id="Titanic-EDA-Simple-Model-0-80622"><a href="#Titanic-EDA-Simple-Model-0-80622" class="headerlink" title="Titanic EDA + Simple Model [0.80622]"></a>Titanic EDA + Simple Model [0.80622]</h1><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p><a target="_blank" rel="noopener" href="https://www.kaggle.com/mviola/titanic-eda-simple-model-0-80622">https://www.kaggle.com/mviola/titanic-eda-simple-model-0-80622</a><br>이 노트북은 Titanic : Machine Learning from Disaster 대회에 대한 저의 첫 번째 접근 방식을 다룹니다 ( 자세한 내용은 <a target="_blank" rel="noopener" href="https://www.kaggle.com/c/titanic">여기</a> 에서 확인 ).<br>초심자로서 제 목표는 기본 기능 엔지니어링과 간단한 모델로 80 % 이상의 점수를 얻는 것이었고 결국 큰 만족과 노력으로 그것을 만들었습니다.<br>지나치게 복잡하지 않고 똑같이하고 싶은 분들을 위해이 과정을 안내하고 어떤 식 으로든 도움을 드리고자합니다.<br>시작하자!  </p>
<h2 id="Importing-packages-and-data"><a href="#Importing-packages-and-data" class="headerlink" title="Importing packages and data"></a>Importing packages and data</h2><p>표준 모듈을로드하고 데이터를 살펴 보는 것으로 시작합니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd </span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">pd.plotting.register_matplotlib_converters()</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line">sns.set_style(<span class="string">&#x27;dark&#x27;</span>)</span><br><span class="line"><span class="keyword">from</span> sklearn.impute <span class="keyword">import</span> SimpleImputer</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> OneHotEncoder</span><br><span class="line"><span class="keyword">from</span> sklearn.compose <span class="keyword">import</span> ColumnTransformer</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line">print(<span class="string">&#x27;Setup complete&#x27;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Setup complete</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!pip install kaggle</span><br></pre></td></tr></table></figure>

<pre><code>Requirement already satisfied: kaggle in /usr/local/lib/python3.6/dist-packages (1.5.9)
Requirement already satisfied: six&gt;=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.15.0)
Requirement already satisfied: slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (0.0.1)
Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.8.1)
Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.0.1)
Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.41.1)
Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.23.0)
Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle) (2020.11.8)
Requirement already satisfied: urllib3 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.24.3)
Requirement already satisfied: text-unidecode&gt;=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify-&gt;kaggle) (1.3)
Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;kaggle) (3.0.4)
Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;kaggle) (2.10)</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> google.colab <span class="keyword">import</span> files</span><br><span class="line">uploaded = files.upload()</span><br><span class="line"><span class="keyword">for</span> fn <span class="keyword">in</span> uploaded.keys():</span><br><span class="line">  print(<span class="string">&#x27;uploaded file &quot;&#123;name&#125;&quot; with length &#123;length&#125; bytes&#x27;</span>.<span class="built_in">format</span>(</span><br><span class="line">      name=fn, length=<span class="built_in">len</span>(uploaded[fn])))</span><br><span class="line">  </span><br><span class="line"><span class="comment"># kaggle.json을 아래 폴더로 옮긴 뒤, file을 사용할 수 있도록 권한을 부여한다. </span></span><br><span class="line">!mkdir -p ~/.kaggle/ &amp;&amp; mv kaggle.json ~/.kaggle/ &amp;&amp; chmod <span class="number">600</span> ~/.kaggle/kaggle.json</span><br></pre></td></tr></table></figure>



<p><input type="file" id="files-b92572c2-d7dd-44f7-bcbc-baf683840bd7" name="files[]" multiple disabled
   style="border:none" /><br><output id="result-b92572c2-d7dd-44f7-bcbc-baf683840bd7"><br> Upload widget is only available when the cell has been executed in the<br> current browser session. Please rerun this cell to enable.<br> </output><br> <script src="/nbextensions/google.colab/files.js"></script> </p>
<pre><code>Saving kaggle.json to kaggle.json
uploaded file &quot;kaggle.json&quot; with length 63 bytes</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ls <span class="number">-1</span>ha ~/.kaggle/kaggle.json</span><br></pre></td></tr></table></figure>

<pre><code>/root/.kaggle/kaggle.json</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> google.colab <span class="keyword">import</span> drive <span class="comment"># 패키지 불러오기 </span></span><br><span class="line"><span class="keyword">from</span> os.path <span class="keyword">import</span> join  </span><br><span class="line"></span><br><span class="line"><span class="comment"># 구글 드라이브 마운트</span></span><br><span class="line">ROOT = <span class="string">&quot;/content/drive&quot;</span>     <span class="comment"># 드라이브 기본 경로</span></span><br><span class="line">print(ROOT)                 <span class="comment"># print content of ROOT (Optional)</span></span><br><span class="line">drive.mount(ROOT)           <span class="comment"># 드라이브 기본 경로 </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 프로젝트 파일 생성 및 다운받을 경로 이동</span></span><br><span class="line">MY_GOOGLE_DRIVE_PATH = <span class="string">&#x27;My Drive/Colab Notebooks/python_basic/kaggle_titanic-eda-simple-model-0-80622_mviola/data&#x27;</span></span><br><span class="line">PROJECT_PATH = join(ROOT, MY_GOOGLE_DRIVE_PATH)</span><br><span class="line">print(PROJECT_PATH)</span><br></pre></td></tr></table></figure>

<pre><code>/content/drive
Mounted at /content/drive
/content/drive/My Drive/Colab Notebooks/python_basic/kaggle_titanic-eda-simple-model-0-80622_mviola/data</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">%cd <span class="string">&quot;&#123;PROJECT_PATH&#125;&quot;</span></span><br></pre></td></tr></table></figure>

<pre><code>/content/drive/My Drive/Colab Notebooks/python_basic/kaggle_titanic-eda-simple-model-0-80622_mviola/data</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!kaggle competitions <span class="built_in">list</span></span><br></pre></td></tr></table></figure>

<pre><code>Warning: Looks like you&#39;re using an outdated API Version, please consider updating (server 1.5.9 / client 1.5.4)
ref                                            deadline             category            reward  teamCount  userHasEntered  
---------------------------------------------  -------------------  ---------------  ---------  ---------  --------------  
contradictory-my-dear-watson                   2030-07-01 23:59:00  Getting Started     Prizes         82           False  
gan-getting-started                            2030-07-01 23:59:00  Getting Started     Prizes        177           False  
tpu-getting-started                            2030-06-03 23:59:00  Getting Started  Knowledge        269           False  
digit-recognizer                               2030-01-01 00:00:00  Getting Started  Knowledge       2401           False  
titanic                                        2030-01-01 00:00:00  Getting Started  Knowledge      18309            True  
house-prices-advanced-regression-techniques    2030-01-01 00:00:00  Getting Started  Knowledge       4648            True  
connectx                                       2030-01-01 00:00:00  Getting Started  Knowledge        425           False  
nlp-getting-started                            2030-01-01 00:00:00  Getting Started  Knowledge       1270           False  
competitive-data-science-predict-future-sales  2022-12-31 23:59:00  Playground           Kudos       9679           False  
jane-street-market-prediction                  2021-02-22 23:59:00  Featured          $100,000        519           False  
cassava-leaf-disease-classification            2021-02-18 23:59:00  Research           $18,000        726            True  
rfcx-species-audio-detection                   2021-02-17 23:59:00  Research           $15,000        239           False  
rock-paper-scissors                            2021-02-01 23:59:00  Playground          Prizes        932           False  
hubmap-kidney-segmentation                     2021-02-01 23:59:00  Research           $60,000        155           False  
riiid-test-answer-prediction                   2021-01-07 23:59:00  Featured          $100,000       2149           False  
kaggle-survey-2020                             2021-01-06 23:59:00  Analytics          $30,000          0           False  
nfl-big-data-bowl-2021                         2021-01-05 23:59:00  Analytics         $100,000          0           False  
nfl-impact-detection                           2021-01-04 23:59:00  Featured           $75,000         56           False  
halite-iv-playground-edition                   2020-12-31 23:59:00  Playground       Knowledge         53           False  
predict-volcanic-eruptions-ingv-oe             2020-12-28 23:59:00  Playground            Swag        345           False  </code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!kaggle competitions download -c titanic</span><br></pre></td></tr></table></figure>

<pre><code>Warning: Looks like you&#39;re using an outdated API Version, please consider updating (server 1.5.9 / client 1.5.4)
Downloading train.csv to /content/drive/My Drive/Colab Notebooks/python_basic/kaggle_titanic-eda-simple-model-0-80622_mviola/data
  0% 0.00/59.8k [00:00&lt;?, ?B/s]
100% 59.8k/59.8k [00:00&lt;00:00, 7.92MB/s]
Downloading test.csv to /content/drive/My Drive/Colab Notebooks/python_basic/kaggle_titanic-eda-simple-model-0-80622_mviola/data
  0% 0.00/28.0k [00:00&lt;?, ?B/s]
100% 28.0k/28.0k [00:00&lt;00:00, 3.90MB/s]
Downloading gender_submission.csv to /content/drive/My Drive/Colab Notebooks/python_basic/kaggle_titanic-eda-simple-model-0-80622_mviola/data
  0% 0.00/3.18k [00:00&lt;?, ?B/s]
100% 3.18k/3.18k [00:00&lt;00:00, 451kB/s]</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!ls</span><br></pre></td></tr></table></figure>

<pre><code>gender_submission.csv  test.csv  train.csv</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Load and display train data</span></span><br><span class="line">train_data = pd.read_csv(<span class="string">&#x27;train.csv&#x27;</span>)</span><br><span class="line">train_data.head()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>PassengerId</th>
      <th>Survived</th>
      <th>Pclass</th>
      <th>Name</th>
      <th>Sex</th>
      <th>Age</th>
      <th>SibSp</th>
      <th>Parch</th>
      <th>Ticket</th>
      <th>Fare</th>
      <th>Cabin</th>
      <th>Embarked</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>0</td>
      <td>3</td>
      <td>Braund, Mr. Owen Harris</td>
      <td>male</td>
      <td>22.0</td>
      <td>1</td>
      <td>0</td>
      <td>A/5 21171</td>
      <td>7.2500</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>
      <td>female</td>
      <td>38.0</td>
      <td>1</td>
      <td>0</td>
      <td>PC 17599</td>
      <td>71.2833</td>
      <td>C85</td>
      <td>C</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>1</td>
      <td>3</td>
      <td>Heikkinen, Miss. Laina</td>
      <td>female</td>
      <td>26.0</td>
      <td>0</td>
      <td>0</td>
      <td>STON/O2. 3101282</td>
      <td>7.9250</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>1</td>
      <td>1</td>
      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>
      <td>female</td>
      <td>35.0</td>
      <td>1</td>
      <td>0</td>
      <td>113803</td>
      <td>53.1000</td>
      <td>C123</td>
      <td>S</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>0</td>
      <td>3</td>
      <td>Allen, Mr. William Henry</td>
      <td>male</td>
      <td>35.0</td>
      <td>0</td>
      <td>0</td>
      <td>373450</td>
      <td>8.0500</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
  </tbody>
</table>
</div>



<p>그런 다음 훈련 및 테스트 데이터 모두에서 결 측값을 확인합니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_data.info()</span><br></pre></td></tr></table></figure>

<pre><code>&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
RangeIndex: 891 entries, 0 to 890
Data columns (total 12 columns):
 #   Column       Non-Null Count  Dtype  
---  ------       --------------  -----  
 0   PassengerId  891 non-null    int64  
 1   Survived     891 non-null    int64  
 2   Pclass       891 non-null    int64  
 3   Name         891 non-null    object 
 4   Sex          891 non-null    object 
 5   Age          714 non-null    float64
 6   SibSp        891 non-null    int64  
 7   Parch        891 non-null    int64  
 8   Ticket       891 non-null    object 
 9   Fare         891 non-null    float64
 10  Cabin        204 non-null    object 
 11  Embarked     889 non-null    object 
dtypes: float64(2), int64(5), object(5)
memory usage: 83.7+ KB</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Load and display test data</span></span><br><span class="line">test_data = pd.read_csv(<span class="string">&#x27;test.csv&#x27;</span>)</span><br><span class="line">test_data.head()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>PassengerId</th>
      <th>Pclass</th>
      <th>Name</th>
      <th>Sex</th>
      <th>Age</th>
      <th>SibSp</th>
      <th>Parch</th>
      <th>Ticket</th>
      <th>Fare</th>
      <th>Cabin</th>
      <th>Embarked</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>892</td>
      <td>3</td>
      <td>Kelly, Mr. James</td>
      <td>male</td>
      <td>34.5</td>
      <td>0</td>
      <td>0</td>
      <td>330911</td>
      <td>7.8292</td>
      <td>NaN</td>
      <td>Q</td>
    </tr>
    <tr>
      <th>1</th>
      <td>893</td>
      <td>3</td>
      <td>Wilkes, Mrs. James (Ellen Needs)</td>
      <td>female</td>
      <td>47.0</td>
      <td>1</td>
      <td>0</td>
      <td>363272</td>
      <td>7.0000</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
    <tr>
      <th>2</th>
      <td>894</td>
      <td>2</td>
      <td>Myles, Mr. Thomas Francis</td>
      <td>male</td>
      <td>62.0</td>
      <td>0</td>
      <td>0</td>
      <td>240276</td>
      <td>9.6875</td>
      <td>NaN</td>
      <td>Q</td>
    </tr>
    <tr>
      <th>3</th>
      <td>895</td>
      <td>3</td>
      <td>Wirz, Mr. Albert</td>
      <td>male</td>
      <td>27.0</td>
      <td>0</td>
      <td>0</td>
      <td>315154</td>
      <td>8.6625</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
    <tr>
      <th>4</th>
      <td>896</td>
      <td>3</td>
      <td>Hirvonen, Mrs. Alexander (Helga E Lindqvist)</td>
      <td>female</td>
      <td>22.0</td>
      <td>1</td>
      <td>1</td>
      <td>3101298</td>
      <td>12.2875</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">test_data.info()</span><br></pre></td></tr></table></figure>

<pre><code>&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
RangeIndex: 418 entries, 0 to 417
Data columns (total 11 columns):
 #   Column       Non-Null Count  Dtype  
---  ------       --------------  -----  
 0   PassengerId  418 non-null    int64  
 1   Pclass       418 non-null    int64  
 2   Name         418 non-null    object 
 3   Sex          418 non-null    object 
 4   Age          332 non-null    float64
 5   SibSp        418 non-null    int64  
 6   Parch        418 non-null    int64  
 7   Ticket       418 non-null    object 
 8   Fare         417 non-null    float64
 9   Cabin        91 non-null     object 
 10  Embarked     418 non-null    object 
dtypes: float64(2), int64(4), object(5)
memory usage: 36.0+ KB</code></pre>
<p>Age, Cabin및의 값 Embarked은 기차 데이터에서 누락되고 Age, Fare및의 값 Cabin은 테스트 데이터에서 누락되었습니다.<br>필요한 경우 나중에 처리합니다.<br>마지막으로, 우리가 무엇을 예측할 것인지에 대한 아이디어를 얻기 위해 목표에 집중합니다. 얼마나 많은 승객이 살아남 았는지 봅시다.  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_data[<span class="string">&#x27;Survived&#x27;</span>].value_counts(normalize=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>




<pre><code>0    0.616162
1    0.383838
Name: Survived, dtype: float64</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">g = sns.countplot(y=train_data[<span class="string">&#x27;Survived&#x27;</span>]).set_title(<span class="string">&#x27;Survivors and deads count&#x27;</span>)</span><br></pre></td></tr></table></figure>



<p><img src="/images/kaggle_titanic-eda-simple-model-0-80622_mviola/output_21_0.png" alt="png"></p>
<p>훈련 데이터에서 승객의 약 38.4 %만이 재난에서 살아 남았습니다. 이것은 우리가 염두에 두어야 할 중요한 가치입니다.</p>
<h2 id="Feature-analysis-and-creation"><a href="#Feature-analysis-and-creation" class="headerlink" title="Feature analysis and creation"></a>Feature analysis and creation</h2><p>이 섹션의 목표는 모델링 부분에서보다 정확한 기능 선택을 수행하기 위해 데이터에 대한 일반적인 이해를 얻는 것입니다.<br>따라서 승객의 생존 여부를 예측하는 데있어 그 중요성을 결정하기 위해 한 번에 하나의 기능을 탐색 할 것입니다.</p>
<h3 id="Sex"><a href="#Sex" class="headerlink" title="Sex"></a>Sex</h3><p>승객의 약 65 %는 남성이고 나머지 35 %는 여성이었습니다.<br>여기서 주목해야 할 중요한 점은 여성의 생존율이 남성의 생존율의 4 배라는 점이며 이것이 Sex가장 유익한 특징 중 하나입니다.<br>성별 제출 자체 점수가 0.76555 인 것은 아닙니다!  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">fig, axarr = plt.subplots(<span class="number">1</span>, <span class="number">2</span>, figsize=(<span class="number">12</span>,<span class="number">6</span>))</span><br><span class="line">a = sns.countplot(train_data[<span class="string">&#x27;Sex&#x27;</span>], ax=axarr[<span class="number">0</span>]).set_title(<span class="string">&#x27;Passengers count by sex&#x27;</span>)</span><br><span class="line">axarr[<span class="number">1</span>].set_title(<span class="string">&#x27;Survival rate by sex&#x27;</span>)</span><br><span class="line">b = sns.barplot(x=<span class="string">&#x27;Sex&#x27;</span>, y=<span class="string">&#x27;Survived&#x27;</span>, data=train_data, ax=axarr[<span class="number">1</span>]).set_ylabel(<span class="string">&#x27;Survival rate&#x27;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>/usr/local/lib/python3.6/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.
  FutureWarning</code></pre>
<p><img src="/images/kaggle_titanic-eda-simple-model-0-80622_mviola/output_27_1.png" alt="png"></p>
<h3 id="Pclass"><a href="#Pclass" class="headerlink" title="Pclass"></a>Pclass</h3><p>배에는 세 개의 클래스가 있었고 플롯에서 우리는 세 번째 클래스의 승객 수가 1 등석과 2 등석의 승객 수를 합친 것보다 더 많음을 알 수 있습니다.<br>그러나 등급별 생존율은 동일하지 않습니다. 1 등석 승객의 60 % 이상과 2 등석 승객의 약 절반이 구조 된 반면 3 등석 승객의 75 %는 재난에서 살아남지 못했습니다.<br>이러한 이유로 이것은 확실히 고려해야 할 중요한 측면입니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_data.groupby(<span class="string">&#x27;Pclass&#x27;</span>).Survived.mean()</span><br></pre></td></tr></table></figure>




<pre><code>Pclass
1    0.629630
2    0.472826
3    0.242363
Name: Survived, dtype: float64</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">fig, axarr = plt.subplots(<span class="number">1</span>,<span class="number">2</span>,figsize=(<span class="number">12</span>,<span class="number">6</span>))</span><br><span class="line">a = sns.countplot(x=<span class="string">&#x27;Pclass&#x27;</span>, hue=<span class="string">&#x27;Survived&#x27;</span>, data=train_data, ax=axarr[<span class="number">0</span>]).set_title(<span class="string">&#x27;Survivors and deads count by class&#x27;</span>)</span><br><span class="line">axarr[<span class="number">1</span>].set_title(<span class="string">&#x27;Survival rate by class&#x27;</span>)</span><br><span class="line">b = sns.barplot(x=<span class="string">&#x27;Pclass&#x27;</span>, y=<span class="string">&#x27;Survived&#x27;</span>, data=train_data, ax=axarr[<span class="number">1</span>]).set_ylabel(<span class="string">&#x27;Survival rate&#x27;</span>)</span><br></pre></td></tr></table></figure>



<p><img src="/images/kaggle_titanic-eda-simple-model-0-80622_mviola/output_31_0.png" alt="png"></p>
<p>우리는 또한 생존에 의해 속도를 볼 수 Sex와 Pclass매우 인상적이다 : 비율이 세 번째 수준의 여성이 50 %로 떨어진다 동안 구출 첫 번째 클래스 및 두 번째 클래스 여성은 각각 97 %와 92 %였다.<br>그럼에도 불구하고 이것은 일류 남성의 37 % 생존율보다 여전히 높습니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_data.groupby([<span class="string">&#x27;Pclass&#x27;</span>, <span class="string">&#x27;Sex&#x27;</span>]).Survived.mean()</span><br></pre></td></tr></table></figure>




<pre><code>Pclass  Sex   
1       female    0.968085
        male      0.368852
2       female    0.921053
        male      0.157407
3       female    0.500000
        male      0.135447
Name: Survived, dtype: float64</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plt.title(<span class="string">&#x27;Survival rate by sex and class&#x27;</span>)</span><br><span class="line">g = sns.barplot(x=<span class="string">&#x27;Pclass&#x27;</span>, y=<span class="string">&#x27;Survived&#x27;</span>, hue=<span class="string">&#x27;Sex&#x27;</span>, data=train_data).set_ylabel(<span class="string">&#x27;Survival rate&#x27;</span>)</span><br></pre></td></tr></table></figure>



<p><img src="/images/kaggle_titanic-eda-simple-model-0-80622_mviola/output_34_0.png" alt="png"></p>
<h3 id="Age"><a href="#Age" class="headerlink" title="Age"></a>Age</h3><p>이 열에는 많은 결 측값이 포함되어 있지만 훈련 데이터에서 평균 연령이 30 세 미만임을 알 수 있습니다.<br>다음은 일반적으로 생존자와 사망자에 대한 연령 분포의 플롯입니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">fig, axarr = plt.subplots(<span class="number">1</span>,<span class="number">2</span>,figsize=(<span class="number">12</span>,<span class="number">6</span>))</span><br><span class="line">axarr[<span class="number">0</span>].set_title(<span class="string">&#x27;Age distribution&#x27;</span>)</span><br><span class="line">f = sns.distplot(train_data[<span class="string">&#x27;Age&#x27;</span>], color=<span class="string">&#x27;g&#x27;</span>, bins=<span class="number">40</span>, ax=axarr[<span class="number">0</span>])</span><br><span class="line">axarr[<span class="number">1</span>].set_title(<span class="string">&#x27;Age distribution for the two subpopulations&#x27;</span>)</span><br><span class="line">g = sns.kdeplot(train_data[<span class="string">&#x27;Age&#x27;</span>].loc[train_data[<span class="string">&#x27;Survived&#x27;</span>] == <span class="number">1</span>], </span><br><span class="line">                shade= <span class="literal">True</span>, ax=axarr[<span class="number">1</span>], label=<span class="string">&#x27;Survived&#x27;</span>).set_xlabel(<span class="string">&#x27;Age&#x27;</span>)</span><br><span class="line">g = sns.kdeplot(train_data[<span class="string">&#x27;Age&#x27;</span>].loc[train_data[<span class="string">&#x27;Survived&#x27;</span>] == <span class="number">0</span>], </span><br><span class="line">                shade=<span class="literal">True</span>, ax=axarr[<span class="number">1</span>], label=<span class="string">&#x27;Not Survived&#x27;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>/usr/local/lib/python3.6/dist-packages/seaborn/distributions.py:2551: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).
  warnings.warn(msg, FutureWarning)</code></pre>
<p><img src="/images/kaggle_titanic-eda-simple-model-0-80622_mviola/output_37_1.png" alt="png"></p>
<p>첫 번째 모습에서, 사이의 관계 Age와 Survived이 나타납니다 매우 명확하지 : 우리는 살아 사람들을 위해 젊은 승객에 해당하는 피크가 있다는 것을 확실히 알 수 있지만, 그 외에도 나머지는 매우 유익하지 않습니다.<br>우리도 고려해 보면이 기능을 더 잘 이해할 수 Sex있습니다. 이제 많은 수의 남성 생존자가 12 년 미만을 보냈고 여성 그룹에는 특별한 속성이 없다는 것이 더 분명해졌습니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">8</span>,<span class="number">5</span>))</span><br><span class="line">g = sns.swarmplot(y=<span class="string">&#x27;Sex&#x27;</span>, x=<span class="string">&#x27;Age&#x27;</span>, hue=<span class="string">&#x27;Survived&#x27;</span>, data=train_data).set_title(<span class="string">&#x27;Survived by age and sex&#x27;</span>)</span><br></pre></td></tr></table></figure>



<p><img src="/images/kaggle_titanic-eda-simple-model-0-80622_mviola/output_39_0.png" alt="png"></p>
<p>살펴볼 또 다른 흥미로운 점은 Age, Pclass및 간의 관계 Survived입니다.<br>우리는 Pclass매우 명확한 수평 패턴이 없기 때문에 그 영향이 중요하다고 생각합니다.<br>또한 1 등석에는 아이들이 많지 않다는 것을 알 수 있습니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">8</span>,<span class="number">5</span>))</span><br><span class="line">h = sns.swarmplot(x=<span class="string">&#x27;Pclass&#x27;</span>, y=<span class="string">&#x27;Age&#x27;</span>, hue=<span class="string">&#x27;Survived&#x27;</span>, data=train_data).set_title(<span class="string">&#x27;Survived by age and class&#x27;</span>)</span><br></pre></td></tr></table></figure>



<p><img src="/images/kaggle_titanic-eda-simple-model-0-80622_mviola/output_41_0.png" alt="png"></p>
<p>이 모든 플롯 후에 나는 Age모델에서 의 중요성에 대해 확신하지 못합니다 . 나는 그것을 사용하지 않을 생각이지만 나중에 보게 될 것이라고 생각합니다.</p>
<h3 id="Fare"><a href="#Fare" class="headerlink" title="Fare"></a>Fare</h3><p>설명에서 Fare분포가 양으로 치우친 것을 볼 수 있습니다. 데이터의 75 %는 31 미만이고 최대는 512입니다.<br>이 기능을 더 잘 이해하기 위해 여기서 가장 간단한 아이디어는 사 분위수를 사용하여 요금 범위를 만드는 것입니다.<br>처음 보면 운임이 높을수록 생존 가능성이 높아진다는 것을 알 수 있습니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_data.Fare.describe()</span><br></pre></td></tr></table></figure>




<pre><code>count    891.000000
mean      32.204208
std       49.693429
min        0.000000
25%        7.910400
50%       14.454200
75%       31.000000
max      512.329200
Name: Fare, dtype: float64</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">fig, axarr = plt.subplots(<span class="number">1</span>,<span class="number">2</span>,figsize=(<span class="number">12</span>,<span class="number">6</span>))</span><br><span class="line">f = sns.distplot(train_data.Fare, color=<span class="string">&#x27;g&#x27;</span>, ax=axarr[<span class="number">0</span>]).set_title(<span class="string">&#x27;Fare distribution&#x27;</span>)</span><br><span class="line">fare_ranges = pd.qcut(train_data.Fare, <span class="number">4</span>, labels = [<span class="string">&#x27;Low&#x27;</span>, <span class="string">&#x27;Mid&#x27;</span>, <span class="string">&#x27;High&#x27;</span>, <span class="string">&#x27;Very high&#x27;</span>])</span><br><span class="line">axarr[<span class="number">1</span>].set_title(<span class="string">&#x27;Survival rate by fare category&#x27;</span>)</span><br><span class="line">g = sns.barplot(x=fare_ranges, y=train_data.Survived, ax=axarr[<span class="number">1</span>]).set_ylabel(<span class="string">&#x27;Survival rate&#x27;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>/usr/local/lib/python3.6/dist-packages/seaborn/distributions.py:2551: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).
  warnings.warn(msg, FutureWarning)</code></pre>
<p><img src="/images/kaggle_titanic-eda-simple-model-0-80622_mviola/output_46_1.png" alt="png"></p>
<p>그러나 모델링에 관해서는 이러한 운임 범주가 상당히 부족하여 전혀 도움이되지 않았습니다.<br>아래의 더 자세한 플롯을 살펴보면 예를 들어 요금이 200에서 300 사이 인 모든 남성이 사망 한 것을 볼 수 있습니다.<br>이런 이유로 Fare너무 많은 정보를 잃지 않도록 기능을 그대로 두었습니다 . 트리의 더 깊은 수준에서 더 차별적 인 관계가 열리고 좋은 그룹 탐지기가 될 수 있습니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">8</span>,<span class="number">5</span>))</span><br><span class="line"><span class="comment"># 이 플롯에서 운임이 500 초과인 이상 값 3개를 제외했습니다.</span></span><br><span class="line">a = sns.swarmplot(x=<span class="string">&#x27;Sex&#x27;</span>, y=<span class="string">&#x27;Fare&#x27;</span>, hue=<span class="string">&#x27;Survived&#x27;</span>, data=train_data.loc[train_data.Fare&lt;<span class="number">500</span>]).set_title(<span class="string">&#x27;Survived by fare and sex&#x27;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>/usr/local/lib/python3.6/dist-packages/seaborn/categorical.py:1296: UserWarning: 53.4% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.
  warnings.warn(msg, UserWarning)
/usr/local/lib/python3.6/dist-packages/seaborn/categorical.py:1296: UserWarning: 21.1% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.
  warnings.warn(msg, UserWarning)</code></pre>
<p><img src="/images/kaggle_titanic-eda-simple-model-0-80622_mviola/output_48_1.png" alt="png"></p>
<p><a target="_blank" rel="noopener" href="https://www.kaggle.com/erikbruin/titanic-2nd-degree-families-and-majority-voting">여기</a> 에서 Erik의 커널을 본 후이 기능을 충분히 분석하지 않았 음을 상기 시켰습니다.<br>설명을 인쇄 할 때의 최소값 Fare이 0이고 조금 이상하다는 사실도 알아 차렸어야합니다 .<br>이 정보가 정확합니까? 이 승객이 누구인지 봅시다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_data.loc[train_data.Fare==<span class="number">0</span>]</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>PassengerId</th>
      <th>Survived</th>
      <th>Pclass</th>
      <th>Name</th>
      <th>Sex</th>
      <th>Age</th>
      <th>SibSp</th>
      <th>Parch</th>
      <th>Ticket</th>
      <th>Fare</th>
      <th>Cabin</th>
      <th>Embarked</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>179</th>
      <td>180</td>
      <td>0</td>
      <td>3</td>
      <td>Leonard, Mr. Lionel</td>
      <td>male</td>
      <td>36.0</td>
      <td>0</td>
      <td>0</td>
      <td>LINE</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
    <tr>
      <th>263</th>
      <td>264</td>
      <td>0</td>
      <td>1</td>
      <td>Harrison, Mr. William</td>
      <td>male</td>
      <td>40.0</td>
      <td>0</td>
      <td>0</td>
      <td>112059</td>
      <td>0.0</td>
      <td>B94</td>
      <td>S</td>
    </tr>
    <tr>
      <th>271</th>
      <td>272</td>
      <td>1</td>
      <td>3</td>
      <td>Tornquist, Mr. William Henry</td>
      <td>male</td>
      <td>25.0</td>
      <td>0</td>
      <td>0</td>
      <td>LINE</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
    <tr>
      <th>277</th>
      <td>278</td>
      <td>0</td>
      <td>2</td>
      <td>Parkes, Mr. Francis "Frank"</td>
      <td>male</td>
      <td>NaN</td>
      <td>0</td>
      <td>0</td>
      <td>239853</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
    <tr>
      <th>302</th>
      <td>303</td>
      <td>0</td>
      <td>3</td>
      <td>Johnson, Mr. William Cahoone Jr</td>
      <td>male</td>
      <td>19.0</td>
      <td>0</td>
      <td>0</td>
      <td>LINE</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
    <tr>
      <th>413</th>
      <td>414</td>
      <td>0</td>
      <td>2</td>
      <td>Cunningham, Mr. Alfred Fleming</td>
      <td>male</td>
      <td>NaN</td>
      <td>0</td>
      <td>0</td>
      <td>239853</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
    <tr>
      <th>466</th>
      <td>467</td>
      <td>0</td>
      <td>2</td>
      <td>Campbell, Mr. William</td>
      <td>male</td>
      <td>NaN</td>
      <td>0</td>
      <td>0</td>
      <td>239853</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
    <tr>
      <th>481</th>
      <td>482</td>
      <td>0</td>
      <td>2</td>
      <td>Frost, Mr. Anthony Wood "Archie"</td>
      <td>male</td>
      <td>NaN</td>
      <td>0</td>
      <td>0</td>
      <td>239854</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
    <tr>
      <th>597</th>
      <td>598</td>
      <td>0</td>
      <td>3</td>
      <td>Johnson, Mr. Alfred</td>
      <td>male</td>
      <td>49.0</td>
      <td>0</td>
      <td>0</td>
      <td>LINE</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
    <tr>
      <th>633</th>
      <td>634</td>
      <td>0</td>
      <td>1</td>
      <td>Parr, Mr. William Henry Marsh</td>
      <td>male</td>
      <td>NaN</td>
      <td>0</td>
      <td>0</td>
      <td>112052</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
    <tr>
      <th>674</th>
      <td>675</td>
      <td>0</td>
      <td>2</td>
      <td>Watson, Mr. Ennis Hastings</td>
      <td>male</td>
      <td>NaN</td>
      <td>0</td>
      <td>0</td>
      <td>239856</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
    <tr>
      <th>732</th>
      <td>733</td>
      <td>0</td>
      <td>2</td>
      <td>Knight, Mr. Robert J</td>
      <td>male</td>
      <td>NaN</td>
      <td>0</td>
      <td>0</td>
      <td>239855</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
    <tr>
      <th>806</th>
      <td>807</td>
      <td>0</td>
      <td>1</td>
      <td>Andrews, Mr. Thomas Jr</td>
      <td>male</td>
      <td>39.0</td>
      <td>0</td>
      <td>0</td>
      <td>112050</td>
      <td>0.0</td>
      <td>A36</td>
      <td>S</td>
    </tr>
    <tr>
      <th>815</th>
      <td>816</td>
      <td>0</td>
      <td>1</td>
      <td>Fry, Mr. Richard</td>
      <td>male</td>
      <td>NaN</td>
      <td>0</td>
      <td>0</td>
      <td>112058</td>
      <td>0.0</td>
      <td>B102</td>
      <td>S</td>
    </tr>
    <tr>
      <th>822</th>
      <td>823</td>
      <td>0</td>
      <td>1</td>
      <td>Reuchlin, Jonkheer. John George</td>
      <td>male</td>
      <td>38.0</td>
      <td>0</td>
      <td>0</td>
      <td>19972</td>
      <td>0.0</td>
      <td>NaN</td>
      <td>S</td>
    </tr>
  </tbody>
</table>
</div>



<p>그들 중 일부는 1 등석 또는 2 등석 승객이기 때문에 내 모델을 혼동시킬 수있는 제로 요금을 제거하기로 결정했습니다.<br>이 함수의 도움으로에 대해 0 값을 만날 때마다 null 값을 설정합니다 Fare.<br>그것들은 나중에 모델을 훈련 할 때 전가 될 것입니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">remove_zero_fares</span>(<span class="params">row</span>):</span></span><br><span class="line">    <span class="keyword">if</span> row.Fare == <span class="number">0</span>:</span><br><span class="line">        row.Fare = np.NaN</span><br><span class="line">    <span class="keyword">return</span> row</span><br><span class="line"><span class="comment"># Apply the function</span></span><br><span class="line">train_data = train_data.apply(remove_zero_fares, axis=<span class="number">1</span>)</span><br><span class="line">test_data = test_data.apply(remove_zero_fares, axis=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># Check if it did the job</span></span><br><span class="line">print(<span class="string">&#x27;Number of zero-Fares: &#123;:d&#125;&#x27;</span>.<span class="built_in">format</span>(train_data.loc[train_data.Fare==<span class="number">0</span>].shape[<span class="number">0</span>]))</span><br></pre></td></tr></table></figure>

<pre><code>Number of zero-Fares: 0</code></pre>
<h3 id="Embarked"><a href="#Embarked" class="headerlink" title="Embarked"></a>Embarked</h3><p>Embarked승객이 어디에서 탑승했는지 알려줍니다.<br>세 가지 가능한 값이 있습니다 : Southampton, Cherbourg 및 Queenstown.<br>교육 데이터에서 70 % 이상의 사람들이 Southampton에서 탑승했으며 Cherbourg에서 20 % 미만, 나머지는 Queenstown에서 탑승했습니다.<br>탑승 지점으로 생존자를 세어 보면 Cherbourg에서 출발 한 사람들이 사망 한 사람들보다 더 많은 사람들이 살아 남았다는 것을 알 수 있습니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">fig, axarr = plt.subplots(<span class="number">1</span>,<span class="number">2</span>,figsize=(<span class="number">12</span>,<span class="number">6</span>))</span><br><span class="line">sns.countplot(train_data[<span class="string">&#x27;Embarked&#x27;</span>], ax=axarr[<span class="number">0</span>]).set_title(<span class="string">&#x27;Passengers count by boarding point&#x27;</span>)</span><br><span class="line">p = sns.countplot(x = <span class="string">&#x27;Embarked&#x27;</span>, hue = <span class="string">&#x27;Survived&#x27;</span>, data = train_data, </span><br><span class="line">                  ax=axarr[<span class="number">1</span>]).set_title(<span class="string">&#x27;Survivors and deads count by boarding point&#x27;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>/usr/local/lib/python3.6/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.
  FutureWarning</code></pre>
<p><img src="/images/kaggle_titanic-eda-simple-model-0-80622_mviola/output_55_1.png" alt="png"></p>
<p>승객의 탑승 지점이 생존 기회를 바꿀 수있을 것이라고 예상하지 않기 때문에 아마도 퀸스 타운과 사우 샘프 턴이 아닌 Cherbourg에서 온 승객의 1 등석 및 2 등석 승객 비율이 더 높기 때문일 것입니다.<br>이를 확인하기 위해 다른 착수 지점에 대한 클래스 분포를 확인합니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">g = sns.countplot(data=train_data, x=<span class="string">&#x27;Embarked&#x27;</span>, hue=<span class="string">&#x27;Pclass&#x27;</span>).set_title(<span class="string">&#x27;Pclass count by embarking point&#x27;</span>)</span><br></pre></td></tr></table></figure>



<p><img src="/images/kaggle_titanic-eda-simple-model-0-80622_mviola/output_57_0.png" alt="png"></p>
<p>그 주장은 정확하고 왜 그 생존율이 그렇게 높은지 정당화합니다.<br>이 기능은 트리의 더 깊은 수준에서 그룹을 감지하는 데 유용 할 수 있으며 이것이 제가 그것을 유지하는 유일한 이유입니다.</p>
<h3 id="Name"><a href="#Name" class="headerlink" title="Name"></a>Name</h3><p>Name열은 우리가 성씨을 사용하여 가족 그룹을 식별 할 수 예를 들어 같은 유용한 정보가 포함되어 있습니다.<br>그러나이 노트북에서는 승객의 직함 만 추출하여 열차 및 테스트 데이터 모두에 대한 새로운 기능을 생성했습니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_data[<span class="string">&#x27;Title&#x27;</span>] = train_data[<span class="string">&#x27;Name&#x27;</span>].apply(<span class="keyword">lambda</span> x: x.split(<span class="string">&#x27;,&#x27;</span>)[<span class="number">1</span>].split(<span class="string">&#x27;.&#x27;</span>)[<span class="number">0</span>].strip())</span><br><span class="line">test_data[<span class="string">&#x27;Title&#x27;</span>] = test_data[<span class="string">&#x27;Name&#x27;</span>].apply(<span class="keyword">lambda</span> x: x.split(<span class="string">&#x27;,&#x27;</span>)[<span class="number">1</span>].split(<span class="string">&#x27;.&#x27;</span>)[<span class="number">0</span>].strip())</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_data[<span class="string">&#x27;Title&#x27;</span>].value_counts()</span><br></pre></td></tr></table></figure>




<pre><code>Mr              517
Miss            182
Mrs             125
Master           40
Dr                7
Rev               6
Mlle              2
Col               2
Major             2
Mme               1
Lady              1
Capt              1
the Countess      1
Don               1
Jonkheer          1
Ms                1
Sir               1
Name: Title, dtype: int64</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">test_data[<span class="string">&#x27;Title&#x27;</span>].value_counts()</span><br></pre></td></tr></table></figure>




<pre><code>Mr        240
Miss       78
Mrs        72
Master     21
Col         2
Rev         2
Dr          1
Dona        1
Ms          1
Name: Title, dtype: int64</code></pre>
<p>타이틀의 분포를 살펴보면 정말 저주파 타이틀을 더 큰 그룹으로 옮기는 것이 편리 할 수 ​​있습니다.<br>이를 분석 한 후 모든 희귀 여성 타이틀을 미스로, 모든 희귀 남성 타이틀을 Mr.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 희귀 여성 타이틀 대체</span></span><br><span class="line">train_data[<span class="string">&#x27;Title&#x27;</span>].replace([<span class="string">&#x27;Mme&#x27;</span>, <span class="string">&#x27;Ms&#x27;</span>, <span class="string">&#x27;Lady&#x27;</span>, <span class="string">&#x27;Mlle&#x27;</span>, <span class="string">&#x27;the Countess&#x27;</span>, <span class="string">&#x27;Dona&#x27;</span>], <span class="string">&#x27;Miss&#x27;</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">test_data[<span class="string">&#x27;Title&#x27;</span>].replace([<span class="string">&#x27;Mme&#x27;</span>, <span class="string">&#x27;Ms&#x27;</span>, <span class="string">&#x27;Lady&#x27;</span>, <span class="string">&#x27;Mlle&#x27;</span>, <span class="string">&#x27;the Countess&#x27;</span>, <span class="string">&#x27;Dona&#x27;</span>], <span class="string">&#x27;Miss&#x27;</span>, inplace=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># Substitute rare male titles</span></span><br><span class="line">train_data[<span class="string">&#x27;Title&#x27;</span>].replace([<span class="string">&#x27;Major&#x27;</span>, <span class="string">&#x27;Col&#x27;</span>, <span class="string">&#x27;Capt&#x27;</span>, <span class="string">&#x27;Don&#x27;</span>, <span class="string">&#x27;Sir&#x27;</span>, <span class="string">&#x27;Jonkheer&#x27;</span>], <span class="string">&#x27;Mr&#x27;</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">test_data[<span class="string">&#x27;Title&#x27;</span>].replace([<span class="string">&#x27;Major&#x27;</span>, <span class="string">&#x27;Col&#x27;</span>, <span class="string">&#x27;Capt&#x27;</span>, <span class="string">&#x27;Don&#x27;</span>, <span class="string">&#x27;Sir&#x27;</span>, <span class="string">&#x27;Jonkheer&#x27;</span>], <span class="string">&#x27;Mr&#x27;</span>, inplace=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p>다음은 최종 결과입니다. 대부분의 경우 생존율이 평균 생존율보다 상당히 높거나 낮은 것으로 나타나기 때문에이 새로운 기능에 대해 상대적으로 높은 기대를 가지고 있습니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_data.groupby(<span class="string">&#x27;Title&#x27;</span>).Survived.mean()</span><br></pre></td></tr></table></figure>




<pre><code>Title
Dr        0.428571
Master    0.575000
Miss      0.707447
Mr        0.160000
Mrs       0.792000
Rev       0.000000
Name: Survived, dtype: float64</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plt.title(<span class="string">&#x27;Survival rate by Title&#x27;</span>)</span><br><span class="line">g = sns.barplot(x=<span class="string">&#x27;Title&#x27;</span>, y=<span class="string">&#x27;Survived&#x27;</span>, data=train_data).set_ylabel(<span class="string">&#x27;Survival rate&#x27;</span>)</span><br></pre></td></tr></table></figure>



<p><img src="/images/kaggle_titanic-eda-simple-model-0-80622_mviola/output_68_0.png" alt="png"></p>
<h3 id="Cabin-and-Ticket"><a href="#Cabin-and-Ticket" class="headerlink" title="Cabin and Ticket"></a>Cabin and Ticket</h3><p>이 Cabin기능은 결 측값이 많기 때문에 다소 문제가 있습니다.<br>나는 그것이 우리 모델에 너무 많은 도움이 될 것이라고 기대하지 않으므로 그것을 분석조차하지 않습니다.<br>다른 한편으로 올바르게 설계된 Ticket컬럼은 가족 그룹을 찾는 가장 좋은 방법이지만이 노트북에 대해 선택한 접근 방식은 아닙니다 (다른 노트북에서 시도해 볼 것입니다).<br>잠재력을 완전히 알고 삭제하는 것이 아쉽기 때문에 두 개의 새로운 열을 만들기로 결정했습니다. 하나는 티켓 처음 두 글자이고 다른 하나는 티켓 길이입니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 처음 두 글자 추출</span></span><br><span class="line">train_data[<span class="string">&#x27;Ticket_lett&#x27;</span>] = train_data.Ticket.apply(<span class="keyword">lambda</span> x: x[:<span class="number">2</span>])</span><br><span class="line">test_data[<span class="string">&#x27;Ticket_lett&#x27;</span>] = test_data.Ticket.apply(<span class="keyword">lambda</span> x: x[:<span class="number">2</span>])</span><br><span class="line"><span class="comment"># 티켓 길이 계산</span></span><br><span class="line">train_data[<span class="string">&#x27;Ticket_len&#x27;</span>] = train_data.Ticket.apply(<span class="keyword">lambda</span> x: <span class="built_in">len</span>(x))</span><br><span class="line">test_data[<span class="string">&#x27;Ticket_len&#x27;</span>] = test_data.Ticket.apply(<span class="keyword">lambda</span> x: <span class="built_in">len</span>(x))</span><br></pre></td></tr></table></figure>

<p>이것은 우리 모델에 약간 도움이 될 것이므로 여기에서 괜찮다고 생각합니다.</p>
<h3 id="SibSp"><a href="#SibSp" class="headerlink" title="SibSp"></a>SibSp</h3><p>SibSp타이타닉 호에 탑승 한 사람의 형제 자매 또는 배우자의 수입니다.<br>90 % 이상의 사람들이 혼자 여행하거나 형제 자매 또는 배우자와 함께 여행 한 것으로 나타났습니다.<br>서로 다른 범주 간의 생존율은 다소 혼란 스럽지만 혼자 여행하거나 형제가 2 명 이상인 사람들의 생존 가능성이 낮다는 것을 알 수 있습니다.<br>또한, 5 ~ 8 명의 형제 자매가있는 대가족 중 누구도 살아남을 수 없었 음을 알 수 있습니다.  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">fig, axarr = plt.subplots(<span class="number">1</span>,<span class="number">2</span>,figsize=(<span class="number">12</span>,<span class="number">6</span>))</span><br><span class="line">a = sns.countplot(train_data[<span class="string">&#x27;SibSp&#x27;</span>], ax=axarr[<span class="number">0</span>]).set_title(<span class="string">&#x27;Passengers count by SibSp&#x27;</span>)</span><br><span class="line">axarr[<span class="number">1</span>].set_title(<span class="string">&#x27;Survival rate by SibSp&#x27;</span>)</span><br><span class="line">b = sns.barplot(x=<span class="string">&#x27;SibSp&#x27;</span>, y=<span class="string">&#x27;Survived&#x27;</span>, data=train_data, ax=axarr[<span class="number">1</span>]).set_ylabel(<span class="string">&#x27;Survival rate&#x27;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>/usr/local/lib/python3.6/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.
  FutureWarning</code></pre>
<p><img src="/images/kaggle_titanic-eda-simple-model-0-80622_mviola/output_75_1.png" alt="png"></p>
<h3 id="Parch"><a href="#Parch" class="headerlink" title="Parch"></a>Parch</h3><p>받는 유사 SibSp열이 기능은 부모 또는 각 승객이 함께 여행 한 아이의 수를 포함합니다.<br>여기에서 우리는 같은 결론을 내립니다. SibSp작은 가족이 더 큰 가족과 혼자 여행하는 승객보다 생존 할 기회가 더 많다는 것을 다시 한 번 볼 수 있습니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">fig, axarr = plt.subplots(<span class="number">1</span>,<span class="number">2</span>,figsize=(<span class="number">12</span>,<span class="number">6</span>))</span><br><span class="line">a = sns.countplot(train_data[<span class="string">&#x27;Parch&#x27;</span>], ax=axarr[<span class="number">0</span>]).set_title(<span class="string">&#x27;Passengers count by Parch&#x27;</span>)</span><br><span class="line">axarr[<span class="number">1</span>].set_title(<span class="string">&#x27;Survival rate by Parch&#x27;</span>)</span><br><span class="line">b = sns.barplot(x=<span class="string">&#x27;Parch&#x27;</span>, y=<span class="string">&#x27;Survived&#x27;</span>, data=train_data, ax=axarr[<span class="number">1</span>]).set_ylabel(<span class="string">&#x27;Survival rate&#x27;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>/usr/local/lib/python3.6/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.
  FutureWarning</code></pre>
<p><img src="/images/kaggle_titanic-eda-simple-model-0-80622_mviola/output_78_1.png" alt="png"></p>
<h3 id="Family-type"><a href="#Family-type" class="headerlink" title="Family type"></a>Family type</h3><p>약해 보이는 두 개의 예측 변수가 있기 때문에 우리가 할 수있는 한 가지는 이들을 결합하여 더 강한 예측 변수를 얻는 것입니다. 및<br>의 경우 두 변수를 결합하여 , 및 1 (누가 승객 자신) 의 합인 가족 크기 특성을 얻을 수 있습니다.SibSpParchSibSpParch</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 새로운 Fam_size 컬럼 생성</span></span><br><span class="line">train_data[<span class="string">&#x27;Fam_size&#x27;</span>] = train_data[<span class="string">&#x27;SibSp&#x27;</span>] + train_data[<span class="string">&#x27;Parch&#x27;</span>] + <span class="number">1</span></span><br><span class="line">test_data[<span class="string">&#x27;Fam_size&#x27;</span>] = test_data[<span class="string">&#x27;SibSp&#x27;</span>] + test_data[<span class="string">&#x27;Parch&#x27;</span>] + <span class="number">1</span></span><br></pre></td></tr></table></figure>

<p>가족 규모별로 생존율을 도표화하면 혼자있는 사람들이 최대 4 개 구성 요소의 가족보다 생존 확률이 낮았고, 대가족의 경우 생존율이 떨어지고 궁극적으로 매우 큰 가족의 경우 0이되는 것이 분명합니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plt.title(<span class="string">&#x27;Survival rate by family size&#x27;</span>)</span><br><span class="line">g = sns.barplot(x=<span class="string">&#x27;Fam_size&#x27;</span>, y=<span class="string">&#x27;Survived&#x27;</span>, data=train_data).set_ylabel(<span class="string">&#x27;Survival rate&#x27;</span>)</span><br></pre></td></tr></table></figure>



<p><img src="/images/kaggle_titanic-eda-simple-model-0-80622_mviola/output_83_0.png" alt="png"></p>
<p>이전 트렌드를 더 요약하기 위해 마지막 기능으로 가족 규모에 대해 4 개의 그룹을 만들었습니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 4 개 그룹 생성</span></span><br><span class="line">train_data[<span class="string">&#x27;Fam_type&#x27;</span>] = pd.cut(train_data.Fam_size, [<span class="number">0</span>,<span class="number">1</span>,<span class="number">4</span>,<span class="number">7</span>,<span class="number">11</span>], labels=[<span class="string">&#x27;Solo&#x27;</span>, <span class="string">&#x27;Small&#x27;</span>, <span class="string">&#x27;Big&#x27;</span>, <span class="string">&#x27;Very big&#x27;</span>])</span><br><span class="line">test_data[<span class="string">&#x27;Fam_type&#x27;</span>] = pd.cut(test_data.Fam_size, [<span class="number">0</span>,<span class="number">1</span>,<span class="number">4</span>,<span class="number">7</span>,<span class="number">11</span>], labels=[<span class="string">&#x27;Solo&#x27;</span>, <span class="string">&#x27;Small&#x27;</span>, <span class="string">&#x27;Big&#x27;</span>, <span class="string">&#x27;Very big&#x27;</span>])</span><br></pre></td></tr></table></figure>

<p>최종 결과는 다음과 같습니다. 좋은 패턴을 발견 한 것 같습니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plt.title(<span class="string">&#x27;Survival rate by family type&#x27;</span>)</span><br><span class="line">g = sns.barplot(x=train_data.Fam_type, y=train_data.Survived).set_ylabel(<span class="string">&#x27;Survival rate&#x27;</span>)</span><br></pre></td></tr></table></figure>



<p><img src="/images/kaggle_titanic-eda-simple-model-0-80622_mviola/output_87_0.png" alt="png"></p>
<p>이러한 모든 고려 사항이 끝나면 마침내 모든 것을 간단하고 매우 효율적인 모델에 통합 할 때입니다.</p>
<h2 id="Modeling"><a href="#Modeling" class="headerlink" title="Modeling"></a>Modeling</h2><p>사용할 기능을 선택하고 대상을 격리하는 것으로 시작합니다.<br>말씀 드렸듯이 저는 고려하지 않을 것이며 Cabin, Age청년 인 관련 정보가 마스터 제목에 암호화되어 있으므로 결국 제외 했습니다.<br>나는 또한 칼럼을 Sex고려할 때 유용 하지 않기 때문에 사용하지 않았습니다 Title. 성인 남성과 어린 아이들은 같은 성별을 가지고 있지만 이전에 본 것과 정말 다른 범주이므로 알고리즘을 혼동하고 싶지 않습니다.<br>열을 추출하지 않으면 매우 중요하므로 모델 Title을 입력 Sex하는 것을 잊지 마십시오 !</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">y = train_data[<span class="string">&#x27;Survived&#x27;</span>]</span><br><span class="line">features = [<span class="string">&#x27;Pclass&#x27;</span>, <span class="string">&#x27;Fare&#x27;</span>, <span class="string">&#x27;Title&#x27;</span>, <span class="string">&#x27;Embarked&#x27;</span>, <span class="string">&#x27;Fam_type&#x27;</span>, <span class="string">&#x27;Ticket_len&#x27;</span>, <span class="string">&#x27;Ticket_lett&#x27;</span>]</span><br><span class="line">X = train_data[features]</span><br><span class="line">X.head()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Pclass</th>
      <th>Fare</th>
      <th>Title</th>
      <th>Embarked</th>
      <th>Fam_type</th>
      <th>Ticket_len</th>
      <th>Ticket_lett</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>3</td>
      <td>7.2500</td>
      <td>Mr</td>
      <td>S</td>
      <td>Small</td>
      <td>9</td>
      <td>A/</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>71.2833</td>
      <td>Mrs</td>
      <td>C</td>
      <td>Small</td>
      <td>8</td>
      <td>PC</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>7.9250</td>
      <td>Miss</td>
      <td>S</td>
      <td>Solo</td>
      <td>16</td>
      <td>ST</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>53.1000</td>
      <td>Mrs</td>
      <td>S</td>
      <td>Small</td>
      <td>6</td>
      <td>11</td>
    </tr>
    <tr>
      <th>4</th>
      <td>3</td>
      <td>8.0500</td>
      <td>Mr</td>
      <td>S</td>
      <td>Solo</td>
      <td>6</td>
      <td>37</td>
    </tr>
  </tbody>
</table>
</div>



<p>EDA에서 학습 및 테스트 데이터 모두에 누락 된 값이 있고 처리 할 여러 범주 형 변수가 있다는 것을 기억하기 때문에 파이프 라인을 사용하여 모든 작업을 단순화하기로 결정했습니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">numerical_cols = [<span class="string">&#x27;Fare&#x27;</span>]</span><br><span class="line">categorical_cols = [<span class="string">&#x27;Pclass&#x27;</span>, <span class="string">&#x27;Title&#x27;</span>, <span class="string">&#x27;Embarked&#x27;</span>, <span class="string">&#x27;Fam_type&#x27;</span>, <span class="string">&#x27;Ticket_len&#x27;</span>, <span class="string">&#x27;Ticket_lett&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 수치 데이터 전처리</span></span><br><span class="line">numerical_transformer = SimpleImputer(strategy=<span class="string">&#x27;median&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 범주 형 데이터 전처리</span></span><br><span class="line">categorical_transformer = Pipeline(steps=[</span><br><span class="line">    (<span class="string">&#x27;imputer&#x27;</span>, SimpleImputer(strategy=<span class="string">&#x27;most_frequent&#x27;</span>)),</span><br><span class="line">    (<span class="string">&#x27;onehot&#x27;</span>, OneHotEncoder(handle_unknown=<span class="string">&#x27;ignore&#x27;</span>))</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 숫자 및 범주 데이터에 대한 번들 전처리</span></span><br><span class="line">preprocessor = ColumnTransformer(</span><br><span class="line">    transformers=[</span><br><span class="line">        (<span class="string">&#x27;num&#x27;</span>, numerical_transformer, numerical_cols),</span><br><span class="line">        (<span class="string">&#x27;cat&#x27;</span>, categorical_transformer, categorical_cols)</span><br><span class="line">    ])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 번들 전처리 및 모델링 코드</span></span><br><span class="line">titanic_pipeline = Pipeline(steps=[</span><br><span class="line">    (<span class="string">&#x27;preprocessor&#x27;</span>, preprocessor),</span><br><span class="line">    (<span class="string">&#x27;model&#x27;</span>, RandomForestClassifier(random_state=<span class="number">0</span>, n_estimators=<span class="number">500</span>, max_depth=<span class="number">5</span>))</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 훈련 데이터 전처리, 모델 적합</span></span><br><span class="line">titanic_pipeline.fit(X,y)</span><br><span class="line"></span><br><span class="line">print(<span class="string">&#x27;Cross validation score: &#123;:.3f&#125;&#x27;</span>.<span class="built_in">format</span>(cross_val_score(titanic_pipeline, X, y, cv=<span class="number">10</span>).mean()))</span><br></pre></td></tr></table></figure>

<pre><code>Cross validation score: 0.826</code></pre>
<p>이제 테스트 데이터에서 predict 메서드를 호출하기 만하면 예측을 수행 할 준비가되었습니다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X_test = test_data[features]</span><br><span class="line">X_test.head()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Pclass</th>
      <th>Fare</th>
      <th>Title</th>
      <th>Embarked</th>
      <th>Fam_type</th>
      <th>Ticket_len</th>
      <th>Ticket_lett</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>3</td>
      <td>7.8292</td>
      <td>Mr</td>
      <td>Q</td>
      <td>Solo</td>
      <td>6</td>
      <td>33</td>
    </tr>
    <tr>
      <th>1</th>
      <td>3</td>
      <td>7.0000</td>
      <td>Mrs</td>
      <td>S</td>
      <td>Small</td>
      <td>6</td>
      <td>36</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2</td>
      <td>9.6875</td>
      <td>Mr</td>
      <td>Q</td>
      <td>Solo</td>
      <td>6</td>
      <td>24</td>
    </tr>
    <tr>
      <th>3</th>
      <td>3</td>
      <td>8.6625</td>
      <td>Mr</td>
      <td>S</td>
      <td>Solo</td>
      <td>6</td>
      <td>31</td>
    </tr>
    <tr>
      <th>4</th>
      <td>3</td>
      <td>12.2875</td>
      <td>Mrs</td>
      <td>S</td>
      <td>Small</td>
      <td>7</td>
      <td>31</td>
    </tr>
  </tbody>
</table>
</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 테스트 데이터 전처리, 예측 얻기</span></span><br><span class="line">predictions = titanic_pipeline.predict(X_test)</span><br></pre></td></tr></table></figure>

<p>지금해야 할 일은 제출 파일로 변환하는 것입니다!</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">output = pd.DataFrame(&#123;<span class="string">&#x27;PassengerId&#x27;</span>: test_data.PassengerId, <span class="string">&#x27;Survived&#x27;</span>: predictions&#125;)</span><br><span class="line">output.to_csv(<span class="string">&#x27;my_submission.csv&#x27;</span>, index=<span class="literal">False</span>)</span><br><span class="line">files.download(<span class="string">&quot;my_submission.csv&quot;</span>) <span class="comment"># 구글 코랩 다운로드 추가 https://stackoverflow.com/questions/49394737/exporting-data-from-google-colab-to-local-machine</span></span><br><span class="line">print(<span class="string">&#x27;Your submission was successfully saved!&#x27;</span>)</span><br></pre></td></tr></table></figure>


<pre><code>&lt;IPython.core.display.Javascript object&gt;



&lt;IPython.core.display.Javascript object&gt;


Your submission was successfully saved!</code></pre>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>이것은 저의 첫 Kaggle 대회였으며 한 번에 한 단계 씩 순위표를 오르는 것은 정말 멋진 여정이었습니다.<br>아래에서 2020 년 9 월 Titanic LB 점수의 히스토그램을 확인할 수 있습니다. 현재이 노트북은 상위 4 %에 속합니다.  이것은 매우 좋은 점수이지만 영리한 그룹화 접근 방식과 모델 앙상블 링으로 분류기를 개선 할 수 있습니다.<br>당신은이 작업을 수행하는 방법에 관심이 있다면, 여기 당신이 도달 0.82775가 (검사 할 수 있음을 내 다른 노트북 찾을 수 이 지금은 최고 점수의 몇 가지 아이디어를 가지고뿐만 아니라 아웃).<br>지금은이 노트북이 유용했거나 마음에 들었다면 알려주세요. 정말 감사하겠습니다!<br>이 대회에 행운을 빕니다. 다음 대회에서 만나요.</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-11-30T00:15:42.563Z" title="2020-11-30T00:15:42.563Z">2020-11-30</time></span><span class="level-item">Updated&nbsp;<time dateTime="2020-12-01T01:05:28.598Z" title="2020-12-01T01:05:28.598Z">2020-12-01</time></span><span class="level-item"><a class="link-muted" href="/categories/study/">study</a></span><span class="level-item">15 minutes read (About 2186 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/11/30/study/4.2%20%EA%B2%B0%EC%A0%95%20%ED%8A%B8%EB%A6%AC_Ver01/">파이썬 머신러닝 완벽가이드 4장</a></h1><div class="content"><h3 id="결정-트리-모델의-시각화-Decision-Tree-Visualization"><a href="#결정-트리-모델의-시각화-Decision-Tree-Visualization" class="headerlink" title="결정 트리 모델의 시각화(Decision Tree Visualization)"></a>결정 트리 모델의 시각화(Decision Tree Visualization)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">&#x27;ignore&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># DecisionTree Classifier 생성</span></span><br><span class="line">dt_clf = DecisionTreeClassifier(random_state=<span class="number">156</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 붓꽃 데이터를 로딩하고, 학습과 테스트 데이터 셋으로 분리</span></span><br><span class="line">iris_data = load_iris()</span><br><span class="line">X_train , X_test , y_train , y_test = train_test_split(iris_data.data, iris_data.target,</span><br><span class="line">                                                       test_size=<span class="number">0.2</span>,  random_state=<span class="number">11</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># DecisionTreeClassifer 학습. </span></span><br><span class="line">dt_clf.fit(X_train , y_train)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> export_graphviz</span><br><span class="line"></span><br><span class="line"><span class="comment"># export_graphviz()의 호출 결과로 out_file로 지정된 tree.dot 파일을 생성함. </span></span><br><span class="line">export_graphviz(dt_clf, out_file=<span class="string">&quot;tree.dot&quot;</span>, class_names=iris_data.target_names , \</span><br><span class="line">feature_names = iris_data.feature_names, impurity=<span class="literal">True</span>, filled=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!<span class="built_in">dir</span>/w</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> graphviz</span><br><span class="line"></span><br><span class="line"><span class="comment"># 위에서 생성된 tree.dot 파일을 Graphviz 읽어서 Jupyter Notebook상에서 시각화 </span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;tree.dot&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    dot_graph = f.read()</span><br><span class="line">graphviz.Source(dot_graph)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> subprocess <span class="keyword">import</span> call</span><br><span class="line">call([<span class="string">&#x27;dot&#x27;</span>, <span class="string">&#x27;-Tpng&#x27;</span>, <span class="string">&#x27;tree.dot&#x27;</span>, <span class="string">&#x27;-o&#x27;</span>, <span class="string">&#x27;tree.png&#x27;</span>, <span class="string">&#x27;-Gdpi=600&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Display in jupyter notebook</span></span><br><span class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> Image</span><br><span class="line">Image(filename = <span class="string">&#x27;tree.png&#x27;</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="comment"># feature importance 추출 </span></span><br><span class="line">print(<span class="string">&quot;Feature importances:\n&#123;0&#125;&quot;</span>.<span class="built_in">format</span>(np.<span class="built_in">round</span>(dt_clf.feature_importances_, <span class="number">3</span>)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># feature별 importance 매핑</span></span><br><span class="line"><span class="keyword">for</span> name, value <span class="keyword">in</span> <span class="built_in">zip</span>(iris_data.feature_names , dt_clf.feature_importances_):</span><br><span class="line">    print(<span class="string">&#x27;&#123;0&#125; : &#123;1:.3f&#125;&#x27;</span>.<span class="built_in">format</span>(name, value))</span><br><span class="line"></span><br><span class="line"><span class="comment"># feature importance를 column 별로 시각화 하기 </span></span><br><span class="line">sns.barplot(x=dt_clf.feature_importances_ , y=iris_data.feature_names)</span><br></pre></td></tr></table></figure>

<h3 id="결정-트리-Decision-TREE-과적합-Overfitting"><a href="#결정-트리-Decision-TREE-과적합-Overfitting" class="headerlink" title="결정 트리(Decision TREE) 과적합(Overfitting)"></a>결정 트리(Decision TREE) 과적합(Overfitting)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_classification</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line">plt.title(<span class="string">&quot;3 Class values with 2 Features Sample data creation&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2차원 시각화를 위해서 feature는 2개, 결정값 클래스는 3가지 유형의 classification 샘플 데이터 생성. </span></span><br><span class="line">X_features, y_labels = make_classification(n_features=<span class="number">2</span>, n_redundant=<span class="number">0</span>, n_informative=<span class="number">2</span>,</span><br><span class="line">                             n_classes=<span class="number">3</span>, n_clusters_per_class=<span class="number">1</span>,random_state=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># plot 형태로 2개의 feature로 2차원 좌표 시각화, 각 클래스값은 다른 색깔로 표시됨. </span></span><br><span class="line">plt.scatter(X_features[:, <span class="number">0</span>], X_features[:, <span class="number">1</span>], marker=<span class="string">&#x27;o&#x27;</span>, c=y_labels, s=<span class="number">25</span>, cmap=<span class="string">&#x27;rainbow&#x27;</span>, edgecolor=<span class="string">&#x27;k&#x27;</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># Classifier의 Decision Boundary를 시각화 하는 함수</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">visualize_boundary</span>(<span class="params">model, X, y</span>):</span></span><br><span class="line">    fig,ax = plt.subplots()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 학습 데이타 scatter plot으로 나타내기</span></span><br><span class="line">    ax.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y, s=<span class="number">25</span>, cmap=<span class="string">&#x27;rainbow&#x27;</span>, edgecolor=<span class="string">&#x27;k&#x27;</span>,</span><br><span class="line">               clim=(y.<span class="built_in">min</span>(), y.<span class="built_in">max</span>()), zorder=<span class="number">3</span>)</span><br><span class="line">    ax.axis(<span class="string">&#x27;tight&#x27;</span>)</span><br><span class="line">    ax.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">    xlim_start , xlim_end = ax.get_xlim()</span><br><span class="line">    ylim_start , ylim_end = ax.get_ylim()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 호출 파라미터로 들어온 training 데이타로 model 학습 . </span></span><br><span class="line">    model.fit(X, y)</span><br><span class="line">    <span class="comment"># meshgrid 형태인 모든 좌표값으로 예측 수행. </span></span><br><span class="line">    xx, yy = np.meshgrid(np.linspace(xlim_start,xlim_end, num=<span class="number">200</span>),np.linspace(ylim_start,ylim_end, num=<span class="number">200</span>))</span><br><span class="line">    Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># contourf() 를 이용하여 class boundary 를 visualization 수행. </span></span><br><span class="line">    n_classes = <span class="built_in">len</span>(np.unique(y))</span><br><span class="line">    contours = ax.contourf(xx, yy, Z, alpha=<span class="number">0.3</span>,</span><br><span class="line">                           levels=np.arange(n_classes + <span class="number">1</span>) - <span class="number">0.5</span>,</span><br><span class="line">                           cmap=<span class="string">&#x27;rainbow&#x27;</span>, clim=(y.<span class="built_in">min</span>(), y.<span class="built_in">max</span>()),</span><br><span class="line">                           zorder=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"></span><br><span class="line"><span class="comment"># 특정한 트리 생성 제약없는 결정 트리의 Decsion Boundary 시각화.</span></span><br><span class="line">dt_clf = DecisionTreeClassifier().fit(X_features, y_labels)</span><br><span class="line">visualize_boundary(dt_clf, X_features, y_labels)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># min_samples_leaf=6 으로 트리 생성 조건을 제약한 Decision Boundary 시각화</span></span><br><span class="line">dt_clf = DecisionTreeClassifier( min_samples_leaf=<span class="number">6</span>).fit(X_features, y_labels)</span><br><span class="line">visualize_boundary(dt_clf, X_features, y_labels)</span><br></pre></td></tr></table></figure>

<h3 id="결정-트리-실습-Human-Activity-Recognition"><a href="#결정-트리-실습-Human-Activity-Recognition" class="headerlink" title="결정 트리 실습 - Human Activity Recognition"></a>결정 트리 실습 - Human Activity Recognition</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="comment"># features.txt 파일에는 피처 이름 index와 피처명이 공백으로 분리되어 있음. 이를 DataFrame으로 로드.</span></span><br><span class="line">feature_name_df = pd.read_csv(<span class="string">&#x27;./human_activity/features.txt&#x27;</span>,sep=<span class="string">&#x27;\s+&#x27;</span>,</span><br><span class="line">                        header=<span class="literal">None</span>,names=[<span class="string">&#x27;column_index&#x27;</span>,<span class="string">&#x27;column_name&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 피처명 index를 제거하고, 피처명만 리스트 객체로 생성한 뒤 샘플로 10개만 추출</span></span><br><span class="line">feature_name = feature_name_df.iloc[:, <span class="number">1</span>].values.tolist()</span><br><span class="line">print(<span class="string">&#x27;전체 피처명에서 10개만 추출:&#x27;</span>, feature_name[:<span class="number">10</span>])</span><br><span class="line">feature_name_df.head(<span class="number">20</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="수정-버전-01-날짜-2019-10-27일"><a href="#수정-버전-01-날짜-2019-10-27일" class="headerlink" title="수정 버전 01: 날짜 2019.10.27일"></a>수정 버전 01: 날짜 2019.10.27일</h3><p><strong>원본 데이터에 중복된 Feature 명으로 인하여 신규 버전의 Pandas에서 Duplicate name 에러를 발생.</strong><br><strong>중복 feature명에 대해서 원본 feature 명에 ‘_1(또는2)’를 추가로 부여하는 함수인 get_new_feature_name_df() 생성</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### features.txt 파일에 있는 컬럼명을 입력 받아서 중복된 컬럼명은 원본 컬럼명+_1, _2와 같이 중복된 차수를 원본 컬럼명에 더해서 컬럼명을 update 하는 함수임. . </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_new_feature_name_df</span>(<span class="params">old_feature_name_df</span>):</span></span><br><span class="line">    <span class="comment">#column_name으로 중복된 컬럼명에 대해서는 중복 차수 부여, col1, col1과 같이 2개의 중복 컬럼이 있을 경우 1, 2 </span></span><br><span class="line">    feature_dup_df = pd.DataFrame(data=old_feature_name_df.groupby(<span class="string">&#x27;column_name&#x27;</span>).cumcount(), columns=[<span class="string">&#x27;dup_cnt&#x27;</span>])</span><br><span class="line">    <span class="comment"># feature_dup_df의 index인 column_name을 reset_index()를 이용하여 컬럼으로 변환. </span></span><br><span class="line">    feature_dup_df = feature_dup_df.reset_index()</span><br><span class="line">    <span class="comment"># 인자로 받은 features_txt의 컬럼명 DataFrame과 feature_dup_df를 조인. </span></span><br><span class="line">    new_feature_name_df = pd.merge(old_feature_name_df.reset_index(), feature_dup_df, how=<span class="string">&#x27;outer&#x27;</span>)</span><br><span class="line">    <span class="comment"># 새로운 컬럼명은 앞에 중복 차수를 접미어로 결합. </span></span><br><span class="line">    new_feature_name_df[<span class="string">&#x27;column_name&#x27;</span>] = new_feature_name_df[[<span class="string">&#x27;column_name&#x27;</span>, <span class="string">&#x27;dup_cnt&#x27;</span>]].apply(<span class="keyword">lambda</span> x : x[<span class="number">0</span>]+<span class="string">&#x27;_&#x27;</span>+<span class="built_in">str</span>(x[<span class="number">1</span>]) </span><br><span class="line">                                                                                           <span class="keyword">if</span> x[<span class="number">1</span>] &gt;<span class="number">0</span> <span class="keyword">else</span> x[<span class="number">0</span>] ,  axis=<span class="number">1</span>)</span><br><span class="line">    new_feature_name_df = new_feature_name_df.drop([<span class="string">&#x27;index&#x27;</span>], axis=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> new_feature_name_df</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pd.options.display.max_rows = <span class="number">999</span></span><br><span class="line">new_feature_name_df = get_new_feature_name_df(feature_name_df)</span><br><span class="line">new_feature_name_df[new_feature_name_df[<span class="string">&#x27;dup_cnt&#x27;</span>] &gt; <span class="number">0</span>]</span><br></pre></td></tr></table></figure>

<p><strong>아래 get_human_dataset() 함수는 중복된 feature명을 새롭게 수정하는 get_new_feature_name_df() 함수를 반영하여 수정</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_human_dataset</span>( ):</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 각 데이터 파일들은 공백으로 분리되어 있으므로 read_csv에서 공백 문자를 sep으로 할당.</span></span><br><span class="line">    feature_name_df = pd.read_csv(<span class="string">&#x27;./human_activity/features.txt&#x27;</span>,sep=<span class="string">&#x27;\s+&#x27;</span>,</span><br><span class="line">                        header=<span class="literal">None</span>,names=[<span class="string">&#x27;column_index&#x27;</span>,<span class="string">&#x27;column_name&#x27;</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 중복된 feature명을 새롭게 수정하는 get_new_feature_name_df()를 이용하여 새로운 feature명 DataFrame생성. </span></span><br><span class="line">    new_feature_name_df = get_new_feature_name_df(feature_name_df)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># DataFrame에 피처명을 컬럼으로 부여하기 위해 리스트 객체로 다시 변환</span></span><br><span class="line">    feature_name = new_feature_name_df.iloc[:, <span class="number">1</span>].values.tolist()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 학습 피처 데이터 셋과 테스트 피처 데이터을 DataFrame으로 로딩. 컬럼명은 feature_name 적용</span></span><br><span class="line">    X_train = pd.read_csv(<span class="string">&#x27;./human_activity/train/X_train.txt&#x27;</span>,sep=<span class="string">&#x27;\s+&#x27;</span>, names=feature_name )</span><br><span class="line">    X_test = pd.read_csv(<span class="string">&#x27;./human_activity/test/X_test.txt&#x27;</span>,sep=<span class="string">&#x27;\s+&#x27;</span>, names=feature_name)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 학습 레이블과 테스트 레이블 데이터을 DataFrame으로 로딩하고 컬럼명은 action으로 부여</span></span><br><span class="line">    y_train = pd.read_csv(<span class="string">&#x27;./human_activity/train/y_train.txt&#x27;</span>,sep=<span class="string">&#x27;\s+&#x27;</span>,header=<span class="literal">None</span>,names=[<span class="string">&#x27;action&#x27;</span>])</span><br><span class="line">    y_test = pd.read_csv(<span class="string">&#x27;./human_activity/test/y_test.txt&#x27;</span>,sep=<span class="string">&#x27;\s+&#x27;</span>,header=<span class="literal">None</span>,names=[<span class="string">&#x27;action&#x27;</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 로드된 학습/테스트용 DataFrame을 모두 반환 </span></span><br><span class="line">    <span class="keyword">return</span> X_train, X_test, y_train, y_test</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">X_train, X_test, y_train, y_test = get_human_dataset()</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">&#x27;## 학습 피처 데이터셋 info()&#x27;</span>)</span><br><span class="line">print(X_train.info())</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(y_train[<span class="string">&#x27;action&#x27;</span>].value_counts())</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_train.isna().<span class="built_in">sum</span>().<span class="built_in">sum</span>()</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># 예제 반복 시 마다 동일한 예측 결과 도출을 위해 random_state 설정</span></span><br><span class="line">dt_clf = DecisionTreeClassifier(random_state=<span class="number">156</span>)</span><br><span class="line">dt_clf.fit(X_train , y_train)</span><br><span class="line">pred = dt_clf.predict(X_test)</span><br><span class="line">accuracy = accuracy_score(y_test , pred)</span><br><span class="line">print(<span class="string">&#x27;결정 트리 예측 정확도: &#123;0:.4f&#125;&#x27;</span>.<span class="built_in">format</span>(accuracy))</span><br><span class="line"></span><br><span class="line"><span class="comment"># DecisionTreeClassifier의 하이퍼 파라미터 추출</span></span><br><span class="line">print(<span class="string">&#x27;DecisionTreeClassifier 기본 하이퍼 파라미터:\n&#x27;</span>, dt_clf.get_params())</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"></span><br><span class="line">params = &#123;</span><br><span class="line">    <span class="string">&#x27;max_depth&#x27;</span> : [ <span class="number">6</span>, <span class="number">8</span> ,<span class="number">10</span>, <span class="number">12</span>, <span class="number">16</span> ,<span class="number">20</span>, <span class="number">24</span>]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">grid_cv = GridSearchCV(dt_clf, param_grid=params, scoring=<span class="string">&#x27;accuracy&#x27;</span>, cv=<span class="number">5</span>, verbose=<span class="number">1</span> )</span><br><span class="line">grid_cv.fit(X_train , y_train)</span><br><span class="line">print(<span class="string">&#x27;GridSearchCV 최고 평균 정확도 수치:&#123;0:.4f&#125;&#x27;</span>.<span class="built_in">format</span>(grid_cv.best_score_))</span><br><span class="line">print(<span class="string">&#x27;GridSearchCV 최적 하이퍼 파라미터:&#x27;</span>, grid_cv.best_params_)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="수정-버전-01-날짜-2019-10-27일-1"><a href="#수정-버전-01-날짜-2019-10-27일-1" class="headerlink" title="수정 버전 01: 날짜 2019.10.27일"></a>수정 버전 01: 날짜 2019.10.27일</h3><p><strong>사이킷런 버전이 업그레이드 되면서 아래의 GridSearchCV 객체의 cv_results_에서 mean_train_score는 더이상 제공되지 않습니다.</strong><br><strong>기존 코드에서 오류가 발생하시면 아래와 같이 ‘mean_train_score’를 제거해 주십시요</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GridSearchCV객체의 cv_results_ 속성을 DataFrame으로 생성. </span></span><br><span class="line">cv_results_df = pd.DataFrame(grid_cv.cv_results_)</span><br><span class="line"></span><br><span class="line"><span class="comment"># max_depth 파라미터 값과 그때의 테스트(Evaluation)셋, 학습 데이터 셋의 정확도 수치 추출</span></span><br><span class="line"><span class="comment"># 사이킷런 버전이 업그레이드 되면서 아래의 GridSearchCV 객체의 cv_results_에서 mean_train_score는 더이상 제공되지 않습니다</span></span><br><span class="line"><span class="comment"># cv_results_df[[&#x27;param_max_depth&#x27;, &#x27;mean_test_score&#x27;, &#x27;mean_train_score&#x27;]]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># max_depth 파라미터 값과 그때의 테스트(Evaluation)셋, 학습 데이터 셋의 정확도 수치 추출</span></span><br><span class="line">cv_results_df[[<span class="string">&#x27;param_max_depth&#x27;</span>, <span class="string">&#x27;mean_test_score&#x27;</span>]]</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">max_depths = [ <span class="number">6</span>, <span class="number">8</span> ,<span class="number">10</span>, <span class="number">12</span>, <span class="number">16</span> ,<span class="number">20</span>, <span class="number">24</span>]</span><br><span class="line"><span class="comment"># max_depth 값을 변화 시키면서 그때마다 학습과 테스트 셋에서의 예측 성능 측정</span></span><br><span class="line"><span class="keyword">for</span> depth <span class="keyword">in</span> max_depths:</span><br><span class="line">    dt_clf = DecisionTreeClassifier(max_depth=depth, random_state=<span class="number">156</span>)</span><br><span class="line">    dt_clf.fit(X_train , y_train)</span><br><span class="line">    pred = dt_clf.predict(X_test)</span><br><span class="line">    accuracy = accuracy_score(y_test , pred)</span><br><span class="line">    print(<span class="string">&#x27;max_depth = &#123;0&#125; 정확도: &#123;1:.4f&#125;&#x27;</span>.<span class="built_in">format</span>(depth , accuracy))</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">params = &#123;</span><br><span class="line">    <span class="string">&#x27;max_depth&#x27;</span> : [ <span class="number">8</span> , <span class="number">12</span>, <span class="number">16</span> ,<span class="number">20</span>], </span><br><span class="line">    <span class="string">&#x27;min_samples_split&#x27;</span> : [<span class="number">16</span>,<span class="number">24</span>],</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">grid_cv = GridSearchCV(dt_clf, param_grid=params, scoring=<span class="string">&#x27;accuracy&#x27;</span>, cv=<span class="number">5</span>, verbose=<span class="number">1</span> )</span><br><span class="line">grid_cv.fit(X_train , y_train)</span><br><span class="line">print(<span class="string">&#x27;GridSearchCV 최고 평균 정확도 수치: &#123;0:.4f&#125;&#x27;</span>.<span class="built_in">format</span>(grid_cv.best_score_))</span><br><span class="line">print(<span class="string">&#x27;GridSearchCV 최적 하이퍼 파라미터:&#x27;</span>, grid_cv.best_params_)</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">best_df_clf = grid_cv.best_estimator_</span><br><span class="line"></span><br><span class="line">pred1 = best_df_clf.predict(X_test)</span><br><span class="line">accuracy = accuracy_score(y_test , pred1)</span><br><span class="line">print(<span class="string">&#x27;결정 트리 예측 정확도:&#123;0:.4f&#125;&#x27;</span>.<span class="built_in">format</span>(accuracy))</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"></span><br><span class="line">ftr_importances_values = best_df_clf.feature_importances_</span><br><span class="line"></span><br><span class="line"><span class="comment"># Top 중요도로 정렬을 쉽게 하고, 시본(Seaborn)의 막대그래프로 쉽게 표현하기 위해 Series변환</span></span><br><span class="line">ftr_importances = pd.Series(ftr_importances_values, index=X_train.columns  )</span><br><span class="line"></span><br><span class="line"><span class="comment"># 중요도값 순으로 Series를 정렬</span></span><br><span class="line">ftr_top20 = ftr_importances.sort_values(ascending=<span class="literal">False</span>)[:<span class="number">20</span>]</span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>,<span class="number">6</span>))</span><br><span class="line">plt.title(<span class="string">&#x27;Feature importances Top 20&#x27;</span>)</span><br><span class="line">sns.barplot(x=ftr_top20 , y = ftr_top20.index)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-11-30T00:15:42.532Z" title="2020-11-30T00:15:42.532Z">2020-11-30</time></span><span class="level-item">Updated&nbsp;<time dateTime="2020-12-01T01:05:22.237Z" title="2020-12-01T01:05:22.237Z">2020-12-01</time></span><span class="level-item"><a class="link-muted" href="/categories/study/">study</a></span><span class="level-item">10 minutes read (About 1509 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/11/30/study/4.3_%EC%95%99%EC%83%81%EB%B8%94%ED%95%99%EC%8A%B5_4.4_%EB%9E%9C%EB%8D%A4%ED%8F%AC%EB%A0%88%EC%8A%A4%ED%8A%B8_4.5_GBM_Ver01/">파이썬 머신러닝 완벽가이드 4장</a></h1><div class="content"><h2 id="4-3-앙상블-학습-개요"><a href="#4-3-앙상블-학습-개요" class="headerlink" title="4.3 앙상블 학습 개요"></a>4.3 앙상블 학습 개요</h2><h3 id="Voting-Classifier"><a href="#Voting-Classifier" class="headerlink" title="Voting Classifier"></a>Voting Classifier</h3><p><strong>위스콘신 유방암 데이터 로드</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> VotingClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line">cancer = load_breast_cancer()</span><br><span class="line"></span><br><span class="line">data_df = pd.DataFrame(cancer.data, columns=cancer.feature_names)</span><br><span class="line">data_df.head(<span class="number">3</span>)</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>mean radius</th>
      <th>mean texture</th>
      <th>mean perimeter</th>
      <th>mean area</th>
      <th>mean smoothness</th>
      <th>mean compactness</th>
      <th>mean concavity</th>
      <th>mean concave points</th>
      <th>mean symmetry</th>
      <th>mean fractal dimension</th>
      <th>...</th>
      <th>worst radius</th>
      <th>worst texture</th>
      <th>worst perimeter</th>
      <th>worst area</th>
      <th>worst smoothness</th>
      <th>worst compactness</th>
      <th>worst concavity</th>
      <th>worst concave points</th>
      <th>worst symmetry</th>
      <th>worst fractal dimension</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>17.99</td>
      <td>10.38</td>
      <td>122.8</td>
      <td>1001.0</td>
      <td>0.11840</td>
      <td>0.27760</td>
      <td>0.3001</td>
      <td>0.14710</td>
      <td>0.2419</td>
      <td>0.07871</td>
      <td>...</td>
      <td>25.38</td>
      <td>17.33</td>
      <td>184.6</td>
      <td>2019.0</td>
      <td>0.1622</td>
      <td>0.6656</td>
      <td>0.7119</td>
      <td>0.2654</td>
      <td>0.4601</td>
      <td>0.11890</td>
    </tr>
    <tr>
      <th>1</th>
      <td>20.57</td>
      <td>17.77</td>
      <td>132.9</td>
      <td>1326.0</td>
      <td>0.08474</td>
      <td>0.07864</td>
      <td>0.0869</td>
      <td>0.07017</td>
      <td>0.1812</td>
      <td>0.05667</td>
      <td>...</td>
      <td>24.99</td>
      <td>23.41</td>
      <td>158.8</td>
      <td>1956.0</td>
      <td>0.1238</td>
      <td>0.1866</td>
      <td>0.2416</td>
      <td>0.1860</td>
      <td>0.2750</td>
      <td>0.08902</td>
    </tr>
    <tr>
      <th>2</th>
      <td>19.69</td>
      <td>21.25</td>
      <td>130.0</td>
      <td>1203.0</td>
      <td>0.10960</td>
      <td>0.15990</td>
      <td>0.1974</td>
      <td>0.12790</td>
      <td>0.2069</td>
      <td>0.05999</td>
      <td>...</td>
      <td>23.57</td>
      <td>25.53</td>
      <td>152.5</td>
      <td>1709.0</td>
      <td>0.1444</td>
      <td>0.4245</td>
      <td>0.4504</td>
      <td>0.2430</td>
      <td>0.3613</td>
      <td>0.08758</td>
    </tr>
  </tbody>
</table>
<p>3 rows × 30 columns</p>
</div>



<p><strong>VotingClassifier로 개별모델은 로지스틱 회귀와 KNN을 보팅방식으로 결합하고 성능 비교</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 개별 모델은 로지스틱 회귀와 KNN 임. </span></span><br><span class="line">lr_clf = LogisticRegression()</span><br><span class="line">knn_clf = KNeighborsClassifier(n_neighbors=<span class="number">8</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 개별 모델을 소프트 보팅 기반의 앙상블 모델로 구현한 분류기 </span></span><br><span class="line">vo_clf = VotingClassifier( estimators=[(<span class="string">&#x27;LR&#x27;</span>,lr_clf),(<span class="string">&#x27;KNN&#x27;</span>,knn_clf)] , voting=<span class="string">&#x27;soft&#x27;</span> )</span><br><span class="line"></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, </span><br><span class="line">                                                    test_size=<span class="number">0.2</span> , random_state= <span class="number">156</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># VotingClassifier 학습/예측/평가. </span></span><br><span class="line">vo_clf.fit(X_train , y_train)</span><br><span class="line">pred = vo_clf.predict(X_test)</span><br><span class="line">print(<span class="string">&#x27;Voting 분류기 정확도: &#123;0:.4f&#125;&#x27;</span>.<span class="built_in">format</span>(accuracy_score(y_test , pred)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 개별 모델의 학습/예측/평가.</span></span><br><span class="line">classifiers = [lr_clf, knn_clf]</span><br><span class="line"><span class="keyword">for</span> classifier <span class="keyword">in</span> classifiers:</span><br><span class="line">    classifier.fit(X_train , y_train)</span><br><span class="line">    pred = classifier.predict(X_test)</span><br><span class="line">    class_name= classifier.__class__.__name__</span><br><span class="line">    print(<span class="string">&#x27;&#123;0&#125; 정확도: &#123;1:.4f&#125;&#x27;</span>.<span class="built_in">format</span>(class_name, accuracy_score(y_test , pred)))</span><br></pre></td></tr></table></figure>

<h2 id="4-4-Random-Forest"><a href="#4-4-Random-Forest" class="headerlink" title="4.4 Random Forest"></a>4.4 Random Forest</h2><p><strong>결정 트리에서 사용한 사용자 행동 인지 데이터 세트 로딩</strong></p>
<h3 id="수정-버전-01-날짜-2019-10-27일"><a href="#수정-버전-01-날짜-2019-10-27일" class="headerlink" title="수정 버전 01: 날짜 2019.10.27일"></a>수정 버전 01: 날짜 2019.10.27일</h3><p><strong>원본 데이터에 중복된 Feature 명으로 인하여 신규 버전의 Pandas에서 Duplicate name 에러를 발생.</strong><br><strong>중복 feature명에 대해서 원본 feature 명에 ‘_1(또는2)’를 추가로 부여하는 함수인 get_new_feature_name_df() 생성</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_new_feature_name_df</span>(<span class="params">old_feature_name_df</span>):</span></span><br><span class="line">    feature_dup_df = pd.DataFrame(data=old_feature_name_df.groupby(<span class="string">&#x27;column_name&#x27;</span>).cumcount(), columns=[<span class="string">&#x27;dup_cnt&#x27;</span>])</span><br><span class="line">    feature_dup_df = feature_dup_df.reset_index()</span><br><span class="line">    new_feature_name_df = pd.merge(old_feature_name_df.reset_index(), feature_dup_df, how=<span class="string">&#x27;outer&#x27;</span>)</span><br><span class="line">    new_feature_name_df[<span class="string">&#x27;column_name&#x27;</span>] = new_feature_name_df[[<span class="string">&#x27;column_name&#x27;</span>, <span class="string">&#x27;dup_cnt&#x27;</span>]].apply(<span class="keyword">lambda</span> x : x[<span class="number">0</span>]+<span class="string">&#x27;_&#x27;</span>+<span class="built_in">str</span>(x[<span class="number">1</span>]) </span><br><span class="line">                                                                                           <span class="keyword">if</span> x[<span class="number">1</span>] &gt;<span class="number">0</span> <span class="keyword">else</span> x[<span class="number">0</span>] ,  axis=<span class="number">1</span>)</span><br><span class="line">    new_feature_name_df = new_feature_name_df.drop([<span class="string">&#x27;index&#x27;</span>], axis=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> new_feature_name_df</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_human_dataset</span>( ):</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 각 데이터 파일들은 공백으로 분리되어 있으므로 read_csv에서 공백 문자를 sep으로 할당.</span></span><br><span class="line">    feature_name_df = pd.read_csv(<span class="string">&#x27;./human_activity/features.txt&#x27;</span>,sep=<span class="string">&#x27;\s+&#x27;</span>,</span><br><span class="line">                        header=<span class="literal">None</span>,names=[<span class="string">&#x27;column_index&#x27;</span>,<span class="string">&#x27;column_name&#x27;</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 중복된 feature명을 새롭게 수정하는 get_new_feature_name_df()를 이용하여 새로운 feature명 DataFrame생성. </span></span><br><span class="line">    new_feature_name_df = get_new_feature_name_df(feature_name_df)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># DataFrame에 피처명을 컬럼으로 부여하기 위해 리스트 객체로 다시 변환</span></span><br><span class="line">    feature_name = new_feature_name_df.iloc[:, <span class="number">1</span>].values.tolist()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 학습 피처 데이터 셋과 테스트 피처 데이터을 DataFrame으로 로딩. 컬럼명은 feature_name 적용</span></span><br><span class="line">    X_train = pd.read_csv(<span class="string">&#x27;./human_activity/train/X_train.txt&#x27;</span>,sep=<span class="string">&#x27;\s+&#x27;</span>, names=feature_name )</span><br><span class="line">    X_test = pd.read_csv(<span class="string">&#x27;./human_activity/test/X_test.txt&#x27;</span>,sep=<span class="string">&#x27;\s+&#x27;</span>, names=feature_name)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 학습 레이블과 테스트 레이블 데이터을 DataFrame으로 로딩하고 컬럼명은 action으로 부여</span></span><br><span class="line">    y_train = pd.read_csv(<span class="string">&#x27;./human_activity/train/y_train.txt&#x27;</span>,sep=<span class="string">&#x27;\s+&#x27;</span>,header=<span class="literal">None</span>,names=[<span class="string">&#x27;action&#x27;</span>])</span><br><span class="line">    y_test = pd.read_csv(<span class="string">&#x27;./human_activity/test/y_test.txt&#x27;</span>,sep=<span class="string">&#x27;\s+&#x27;</span>,header=<span class="literal">None</span>,names=[<span class="string">&#x27;action&#x27;</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 로드된 학습/테스트용 DataFrame을 모두 반환 </span></span><br><span class="line">    <span class="keyword">return</span> X_train, X_test, y_train, y_test</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">X_train, X_test, y_train, y_test = get_human_dataset()</span><br></pre></td></tr></table></figure>

<p><strong>학습/테스트 데이터로 분리하고 랜덤 포레스트로 학습/예측/평가</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">&#x27;ignore&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 결정 트리에서 사용한 get_human_dataset( )을 이용해 학습/테스트용 DataFrame 반환</span></span><br><span class="line">X_train, X_test, y_train, y_test = get_human_dataset()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 랜덤 포레스트 학습 및 별도의 테스트 셋으로 예측 성능 평가</span></span><br><span class="line">rf_clf = RandomForestClassifier(random_state=<span class="number">0</span>)</span><br><span class="line">rf_clf.fit(X_train , y_train)</span><br><span class="line">pred = rf_clf.predict(X_test)</span><br><span class="line">accuracy = accuracy_score(y_test , pred)</span><br><span class="line">print(<span class="string">&#x27;랜덤 포레스트 정확도: &#123;0:.4f&#125;&#x27;</span>.<span class="built_in">format</span>(accuracy))</span><br></pre></td></tr></table></figure>

<pre><code>랜덤 포레스트 정확도: 0.9108</code></pre>
<p><strong>GridSearchCV 로 교차검증 및 하이퍼 파라미터 튜닝</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"></span><br><span class="line">params = &#123;</span><br><span class="line">    <span class="string">&#x27;n_estimators&#x27;</span>:[<span class="number">100</span>],</span><br><span class="line">    <span class="string">&#x27;max_depth&#x27;</span> : [<span class="number">6</span>, <span class="number">8</span>, <span class="number">10</span>, <span class="number">12</span>], </span><br><span class="line">    <span class="string">&#x27;min_samples_leaf&#x27;</span> : [<span class="number">8</span>, <span class="number">12</span>, <span class="number">18</span> ],</span><br><span class="line">    <span class="string">&#x27;min_samples_split&#x27;</span> : [<span class="number">8</span>, <span class="number">16</span>, <span class="number">20</span>]</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment"># RandomForestClassifier 객체 생성 후 GridSearchCV 수행</span></span><br><span class="line">rf_clf = RandomForestClassifier(random_state=<span class="number">0</span>, n_jobs=<span class="number">-1</span>)</span><br><span class="line">grid_cv = GridSearchCV(rf_clf , param_grid=params , cv=<span class="number">2</span>, n_jobs=<span class="number">-1</span> )</span><br><span class="line">grid_cv.fit(X_train , y_train)</span><br><span class="line"></span><br><span class="line">print(<span class="string">&#x27;최적 하이퍼 파라미터:\n&#x27;</span>, grid_cv.best_params_)</span><br><span class="line">print(<span class="string">&#x27;최고 예측 정확도: &#123;0:.4f&#125;&#x27;</span>.<span class="built_in">format</span>(grid_cv.best_score_))</span><br></pre></td></tr></table></figure>

<pre><code>최적 하이퍼 파라미터:
 &#123;&#39;max_depth&#39;: 10, &#39;min_samples_leaf&#39;: 8, &#39;min_samples_split&#39;: 8, &#39;n_estimators&#39;: 100&#125;
최고 예측 정확도: 0.9166</code></pre>
<p><strong>튜닝된 하이퍼 파라미터로 재 학습 및 예측/평가</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">rf_clf1 = RandomForestClassifier(n_estimators=<span class="number">300</span>, max_depth=<span class="number">10</span>, min_samples_leaf=<span class="number">8</span>, \</span><br><span class="line">                                 min_samples_split=<span class="number">8</span>, random_state=<span class="number">0</span>)</span><br><span class="line">rf_clf1.fit(X_train , y_train)</span><br><span class="line">pred = rf_clf1.predict(X_test)</span><br><span class="line">print(<span class="string">&#x27;예측 정확도: &#123;0:.4f&#125;&#x27;</span>.<span class="built_in">format</span>(accuracy_score(y_test , pred)))</span><br></pre></td></tr></table></figure>

<pre><code>예측 정확도: 0.9165</code></pre>
<p><strong>개별 feature들의 중요도 시각화</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line">ftr_importances_values = rf_clf1.feature_importances_</span><br><span class="line">ftr_importances = pd.Series(ftr_importances_values,index=X_train.columns  )</span><br><span class="line">ftr_top20 = ftr_importances.sort_values(ascending=<span class="literal">False</span>)[:<span class="number">20</span>]</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>,<span class="number">6</span>))</span><br><span class="line">plt.title(<span class="string">&#x27;Feature importances Top 20&#x27;</span>)</span><br><span class="line">sns.barplot(x=ftr_top20 , y = ftr_top20.index)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>



<p><img src="output_18_0.png" alt="png"></p>
<h2 id="4-5-GBM-Gradient-Boosting-Machine"><a href="#4-5-GBM-Gradient-Boosting-Machine" class="headerlink" title="4.5 GBM(Gradient Boosting Machine)"></a>4.5 GBM(Gradient Boosting Machine)</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingClassifier</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">&#x27;ignore&#x27;</span>)</span><br><span class="line"></span><br><span class="line">X_train, X_test, y_train, y_test = get_human_dataset()</span><br><span class="line"></span><br><span class="line"><span class="comment"># GBM 수행 시간 측정을 위함. 시작 시간 설정.</span></span><br><span class="line">start_time = time.time()</span><br><span class="line"></span><br><span class="line">gb_clf = GradientBoostingClassifier(random_state=<span class="number">0</span>)</span><br><span class="line">gb_clf.fit(X_train , y_train)</span><br><span class="line">gb_pred = gb_clf.predict(X_test)</span><br><span class="line">gb_accuracy = accuracy_score(y_test, gb_pred)</span><br><span class="line"></span><br><span class="line">print(<span class="string">&#x27;GBM 정확도: &#123;0:.4f&#125;&#x27;</span>.<span class="built_in">format</span>(gb_accuracy))</span><br><span class="line">print(<span class="string">&quot;GBM 수행 시간: &#123;0:.1f&#125; 초 &quot;</span>.<span class="built_in">format</span>(time.time() - start_time))</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>GBM 정확도: 0.9386
GBM 수행 시간: 191.9 초 </code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"></span><br><span class="line">params = &#123;</span><br><span class="line">    <span class="string">&#x27;n_estimators&#x27;</span>:[<span class="number">100</span>, <span class="number">500</span>],</span><br><span class="line">    <span class="string">&#x27;learning_rate&#x27;</span> : [ <span class="number">0.05</span>, <span class="number">0.1</span>]</span><br><span class="line">&#125;</span><br><span class="line">grid_cv = GridSearchCV(gb_clf , param_grid=params , cv=<span class="number">2</span> ,verbose=<span class="number">1</span>)</span><br><span class="line">grid_cv.fit(X_train , y_train)</span><br><span class="line">print(<span class="string">&#x27;최적 하이퍼 파라미터:\n&#x27;</span>, grid_cv.best_params_)</span><br><span class="line">print(<span class="string">&#x27;최고 예측 정확도: &#123;0:.4f&#125;&#x27;</span>.<span class="built_in">format</span>(grid_cv.best_score_))</span><br></pre></td></tr></table></figure>

<pre><code>Fitting 2 folds for each of 4 candidates, totalling 8 fits


[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scores_df = pd.DataFrame(grid_cv.cv_results_)</span><br><span class="line">scores_df[[<span class="string">&#x27;params&#x27;</span>, <span class="string">&#x27;mean_test_score&#x27;</span>, <span class="string">&#x27;rank_test_score&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;split0_test_score&#x27;</span>, <span class="string">&#x27;split1_test_score&#x27;</span>]]</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GridSearchCV를 이용하여 최적으로 학습된 estimator로 predict 수행. </span></span><br><span class="line">gb_pred = grid_cv.best_estimator_.predict(X_test)</span><br><span class="line">gb_accuracy = accuracy_score(y_test, gb_pred)</span><br><span class="line">print(<span class="string">&#x27;GBM 정확도: &#123;0:.4f&#125;&#x27;</span>.<span class="built_in">format</span>(gb_accuracy))</span><br></pre></td></tr></table></figure>

<pre><code>GBM 정확도: 0.9406</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-11-26T07:09:18.836Z" title="2020-11-26T07:09:18.836Z">2020-11-26</time></span><span class="level-item">Updated&nbsp;<time dateTime="2020-11-26T07:17:25.520Z" title="2020-11-26T07:17:25.520Z">2020-11-26</time></span><span class="level-item"><a class="link-muted" href="/categories/study/">study</a></span><span class="level-item">7 minutes read (About 987 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/11/26/study/pyqt5_ex03/">PyQt5 Tutorial - PyQt5 기초 (Basics)</a></h1><div class="content"><h1 id="PyQt5-기초-Basics"><a href="#PyQt5-기초-Basics" class="headerlink" title="PyQt5 기초 (Basics)"></a>PyQt5 기초 (Basics)</h1><h2 id="창-띄우기"><a href="#창-띄우기" class="headerlink" title="창 띄우기"></a>창 띄우기</h2><p>출처: <a target="_blank" rel="noopener" href="https://codetorial.net/pyqt5/basics/opening.html">Codetorial</a></p>
<h3 id="예제"><a href="#예제" class="headerlink" title="예제"></a>예제</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">from</span> PyQt5.QtWidgets <span class="keyword">import</span> QApplication, QWidget</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyApp</span>(<span class="params">QWidget</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.initUI()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initUI</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.setWindowTitle(<span class="string">&#x27;My First Application&#x27;</span>)</span><br><span class="line">        self.move(<span class="number">300</span>, <span class="number">300</span>)</span><br><span class="line">        self.resize(<span class="number">400</span>, <span class="number">200</span>)</span><br><span class="line">        self.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">   app = QApplication(sys.argv)</span><br><span class="line">   ex = MyApp()</span><br><span class="line">   sys.exit(app.exec_())</span><br></pre></td></tr></table></figure>

<h3 id="결과"><a href="#결과" class="headerlink" title="결과"></a>결과</h3><p><img src="https://user-images.githubusercontent.com/72365720/100169145-185d2980-2f06-11eb-8548-742228d268da.png" alt="K-20201125-100749"><br>(Windows7 환경에서 실행)</p>
<h3 id="설명"><a href="#설명" class="headerlink" title="설명"></a>설명</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">from</span> PyQt5.QtWidgets <span class="keyword">import</span> QApplication, QWidget</span><br></pre></td></tr></table></figure>

<p>기본적인 UI 구성요소를 제공하는 위젯(클래스)은 <code>PyQt5.QtWidgets</code> 모듈에 포함돼 있다. <a target="_blank" rel="noopener" href="https://doc.qt.io/qt-5/qtwidgets-index.html">QtWidgets 공식 문서</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyApp</span>(<span class="params">QWidget</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.initUI()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initUI</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.setWindowTitle(<span class="string">&#x27;My First Application&#x27;</span>)</span><br><span class="line">        self.move(<span class="number">300</span>, <span class="number">300</span>)</span><br><span class="line">        self.resize(<span class="number">400</span>, <span class="number">200</span>)</span><br><span class="line">        self.show()</span><br></pre></td></tr></table></figure>

<ul>
<li><code>self</code> MyApp 객체를 말한다.</li>
<li><code>setWindowTitle()</code> 타이틀바에 나타나는 창의 제목을 설정한다.</li>
<li><code>move()</code> 위젯을 스크린의 x = 300px, y = 300px의 위치로 이동시킨다.</li>
<li><code>resize()</code> 위젯의 크기를 너비 400px, 높이 200px로 조절한다.</li>
<li><code>show()</code> 위젯을 스크린에 보여준다.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">   app = QApplication(sys.argv)</span><br><span class="line">   ex = MyApp()</span><br><span class="line">   sys.exit(app.exec_())</span><br></pre></td></tr></table></figure>

<p><strong><code>if __name__ == &#39;__main__&#39;:</code></strong><br><code>__name__</code>은 현재 모듈의 이름이 저장되는 내장 변수이다. 예를 들어 ‘test.py’라는 코드를 import해서 예제 코드를 실행하면 <code>__name__</code>은 ‘test’가 된다. 그렇지 않고 코드를 직접 실행한다면 <code>__name__</code>은 <code>__main__</code>이 된다. 이 한 줄의 코드를 통해 프로그램이 직접 실행되는 지, 모듈을 통해 실행되는 지 확인할 수 있다.  </p>
<p><strong><code>app = QApplication(sys.argv)</code></strong><br>모든 PyQt5 어플리케이션은 어플리케이션 객체를 생성해야 한다. <a target="_blank" rel="noopener" href="https://doc.qt.io/qt-5/qapplication.html">QApplication 공식 문서</a></p>
<h2 id="어플리케이션-아이콘-넣기"><a href="#어플리케이션-아이콘-넣기" class="headerlink" title="어플리케이션 아이콘 넣기"></a>어플리케이션 아이콘 넣기</h2><p>출처: <a target="_blank" rel="noopener" href="https://codetorial.net/pyqt5/basics/icon.html">Codetorial</a></p>
<h3 id="예제-1"><a href="#예제-1" class="headerlink" title="예제"></a>예제</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">from</span> PyQt5.QtWidgets <span class="keyword">import</span> QApplication, QWidget</span><br><span class="line"><span class="keyword">from</span> PyQt5.QtGui <span class="keyword">import</span> QIcon</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyApp</span>(<span class="params">QWidget</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.initUI()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initUI</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.setWindowTitle(<span class="string">&#x27;Icon&#x27;</span>)</span><br><span class="line">        self.setWindowIcon(QIcon(<span class="string">&#x27;image/web.png&#x27;</span>))</span><br><span class="line">        self.setGeometry(<span class="number">300</span>, <span class="number">300</span>, <span class="number">300</span>, <span class="number">200</span>)</span><br><span class="line">        self.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    app = QApplication(sys.argv)</span><br><span class="line">    ex = MyApp()</span><br><span class="line">    sys.exit(app.exec_())</span><br></pre></td></tr></table></figure>

<h3 id="결과-1"><a href="#결과-1" class="headerlink" title="결과"></a>결과</h3><p><img src="https://user-images.githubusercontent.com/72365720/100170477-206a9880-2f09-11eb-90ac-08ae42a02a40.png" alt="K-20201125-102840"></p>
<h3 id="설명-1"><a href="#설명-1" class="headerlink" title="설명"></a>설명</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">self.setWindowIcon(QIcon(<span class="string">&#x27;image/web.png&#x27;</span>))</span><br></pre></td></tr></table></figure>

<p><strong><code>setWindowIcon()</code></strong> 메소드는 어플리케이션 아이콘을 설정하도록 한다.<br>이를 위해서 QIcon 객체를 생성했고, QIcon()에 보여질 이미지를 입력한다. (경로 확인)  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">self.setGeometry(<span class="number">300</span>, <span class="number">300</span>, <span class="number">300</span>, <span class="number">200</span>)</span><br></pre></td></tr></table></figure>

<p><strong><code>setGeometry()</code></strong> 메소드는 창의 위치와 크기를 설정한다.<br>앞의 두 매개변수는 창의 x, y 위치를 결정하고, 뒤의 두 매개변수는 각각 창의 너비와 높이를 결정한다. 이 메소드는 창 띄우기 예제에서 사용했던 move()와 resize() 메서드를 하나로 합쳐놓은 것과 같다.</p>
<h2 id="창-닫기"><a href="#창-닫기" class="headerlink" title="창 닫기"></a>창 닫기</h2><p>출처: <a target="_blank" rel="noopener" href="https://codetorial.net/pyqt5/basics/closing.html">Codetorial</a></p>
<h3 id="예제-2"><a href="#예제-2" class="headerlink" title="예제"></a>예제</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">from</span> PyQt5.QtWidgets <span class="keyword">import</span> QApplication, QWidget, QPushButton</span><br><span class="line"><span class="keyword">from</span> PyQt5.QtCore <span class="keyword">import</span> QCoreApplication</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyApp</span>(<span class="params">QWidget</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.initUI()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initUI</span>(<span class="params">self</span>):</span></span><br><span class="line">        btn = QPushButton(<span class="string">&#x27;Quit&#x27;</span>, self)</span><br><span class="line">        btn.move(<span class="number">50</span>, <span class="number">50</span>)</span><br><span class="line">        btn.resize(btn.sizeHint())</span><br><span class="line">        btn.clicked.connect(QCoreApplication.instance().quit)</span><br><span class="line"></span><br><span class="line">        self.setWindowTitle(<span class="string">&#x27;Quit Button&#x27;</span>)</span><br><span class="line">        self.setGeometry(<span class="number">300</span>, <span class="number">300</span>, <span class="number">300</span>, <span class="number">200</span>)</span><br><span class="line">        self.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    app = QApplication(sys.argv)</span><br><span class="line">    ex = MyApp()</span><br><span class="line">    sys.exit(app.exec_())</span><br></pre></td></tr></table></figure>

<h3 id="설명-2"><a href="#설명-2" class="headerlink" title="설명"></a>설명</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">btn = QPushButton(<span class="string">&#x27;Quit&#x27;</span>, self)</span><br></pre></td></tr></table></figure>

<p>푸시버튼을 하나 만든다. 이 btn은 QPushButton 클래스의 인스턴스이다.<br>첫번째 파라미터에는 버튼에 표시될 텍스트(Quit)를 입력하고, 두번째 파라미터에는 버튼이 위치할 부모 위젯(self)을 입력한다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">btn.clicked.connect(QCoreApplication.instance().quit)</span><br></pre></td></tr></table></figure>

<p>PyQt5에서의 이벤트 처리는 시그널과 슬롯 메커니즘으로 이루어진다. btn을 클릭하면 <code>clicked</code> 시그널이 만들어진다.</p>
<p><strong><code>instance()</code></strong> 메소드는 현재 인스턴스를 반환한다.</p>
<p><strong><code>clicked</code></strong> 시그널은 어플리케이션을 종료하는 quit() 메소드에 연결된다.</p>
<p>이렇게 두 객체 발신자와 수신자(Sender &amp; Receiver) 간에 커뮤니케이션이 이루어잔다. 이 예제에서 발신자는 푸시버튼(btn)이고, 수신자는 어플리케이션 객체(app)이다.</p>
<p><img src="https://user-images.githubusercontent.com/72365720/100317677-19728180-3000-11eb-916d-1a23c1e65568.png" alt="K-20201126-155725"></p>
<h2 id="툴팁-나타내기"><a href="#툴팁-나타내기" class="headerlink" title="툴팁 나타내기"></a>툴팁 나타내기</h2></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-11-26T07:09:05.726Z" title="2020-11-26T07:09:05.726Z">2020-11-26</time></span><span class="level-item">Updated&nbsp;<time dateTime="2020-11-26T07:16:28.962Z" title="2020-11-26T07:16:28.962Z">2020-11-26</time></span><span class="level-item"><a class="link-muted" href="/categories/study/">study</a></span><span class="level-item">27 minutes read (About 4106 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/11/26/study/kaggle_cassava_leaf_disease_classification/">캐글 카사바 잎 질병 분류 파파고 번역</a></h1><div class="content"><h1 id="구글-연동-생략"><a href="#구글-연동-생략" class="headerlink" title="구글 연동 (생략)"></a>구글 연동 (생략)</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!pip install kaggle</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> google.colab <span class="keyword">import</span> files</span><br><span class="line">uploaded = files.upload()</span><br><span class="line"><span class="keyword">for</span> fn <span class="keyword">in</span> uploaded.keys():</span><br><span class="line">  print(<span class="string">&#x27;uploaded file &quot;&#123;name&#125;&quot; with length &#123;length&#125; bytes&#x27;</span>.<span class="built_in">format</span>(</span><br><span class="line">      name=fn, length=<span class="built_in">len</span>(uploaded[fn])))</span><br><span class="line">  </span><br><span class="line"><span class="comment"># kaggle.json을 아래 폴더로 옮긴 뒤, file을 사용할 수 있도록 권한을 부여한다. </span></span><br><span class="line">!mkdir -p ~/.kaggle/ &amp;&amp; mv kaggle.json ~/.kaggle/ &amp;&amp; chmod <span class="number">600</span> ~/.kaggle/kaggle.json</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ls <span class="number">-1</span>ha ~/.kaggle/kaggle.json</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> google.colab <span class="keyword">import</span> drive <span class="comment"># 패키지 불러오기 </span></span><br><span class="line"><span class="keyword">from</span> os.path <span class="keyword">import</span> join  </span><br><span class="line"></span><br><span class="line"><span class="comment"># 구글 드라이브 마운트</span></span><br><span class="line">ROOT = <span class="string">&quot;/content/drive&quot;</span>     <span class="comment"># 드라이브 기본 경로</span></span><br><span class="line">print(ROOT)                 <span class="comment"># print content of ROOT (Optional)</span></span><br><span class="line">drive.mount(ROOT)           <span class="comment"># 드라이브 기본 경로 </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 프로젝트 파일 생성 및 다운받을 경로 이동</span></span><br><span class="line">MY_GOOGLE_DRIVE_PATH = <span class="string">&#x27;My Drive/Colab Notebooks/python_basic/kaggle_cassava-leaf-disease-classification/data&#x27;</span></span><br><span class="line">PROJECT_PATH = join(ROOT, MY_GOOGLE_DRIVE_PATH)</span><br><span class="line">print(PROJECT_PATH)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">%cd <span class="string">&quot;&#123;PROJECT_PATH&#125;&quot;</span></span><br></pre></td></tr></table></figure>

<h1 id="카사바-잎-질병-분류"><a href="#카사바-잎-질병-분류" class="headerlink" title="카사바 잎 질병 분류"></a><strong>카사바 잎 질병 분류</strong></h1><p>Cassava Leaf Disease Classification<br><a target="_blank" rel="noopener" href="https://www.kaggle.com/c/cassava-leaf-disease-classification">https://www.kaggle.com/c/cassava-leaf-disease-classification</a><br><br><br>이미지에 존재하는 질병 유형 식별<br>Identify the type of disease present on a Cassava Leaf image</p>
<h2 id="Description"><a href="#Description" class="headerlink" title="Description"></a><strong>Description</strong></h2><p><br><br><br>아프리카에서 두 번째로 많은 탄수화물을 공급하고 있는 카사바는 가혹한 조건을 견뎌낼 수 있기 때문에 소작농들이 재배하는 주요 식량안보 작물이다.<br>As the second-largest provider of carbohydrates in Africa, cassava is a key food security crop grown by smallholder farmers because it can withstand harsh conditions.<br><br><br>아프리카 사하라 사막 이남의 가정 농장의 80% 이상이 이 녹농 뿌리를 기르고 있지만 바이러스성 질병은 수확량이 저조한 주요 원인이다.<br>At least 80% of household farms in Sub-Saharan Africa grow this starchy root, but viral diseases are major sources of poor yields.<br><br><br>데이터 과학의 도움으로, 일반적인 질병들이 치료될 수 있도록 식별하는 것이 가능할 수도 있다.<br>With the help of data science, it may be possible to identify common diseases so they can be treated.<br><br><br></p>
<p>기존의 질병감지 방법은 농업인이 정부출연 농업전문가의 도움을 받아 식물을 육안으로 검사하고 진단할 수 있도록 해야 한다.<br>Existing methods of disease detection require farmers to solicit the help of government-funded agricultural experts to visually inspect and diagnose the plants.<br><br><br>이것은 노동집약적이고, 공급량이 적고, 비용이 많이 드는 것으로 고통받고 있다.<br>This suffers from being labor-intensive, low-supply and costly.<br><br><br>추가적인 도전으로서, 아프리카 농부들은 낮은 대역폭의 모바일 퀄리티 카메라에만 접근할 수 있기 때문에 농부들을 위한 효과적인 해결책은 상당한 제약 조건 하에서 좋은 성과를 거두어야 한다.<br>As an added challenge, effective solutions for farmers must perform well under significant constraints, since African farmers may only have access to mobile-quality cameras with low-bandwidth.<br><br><br></p>
<p>이번 대회에서는 우간다의 정기 조사 때 수집한 21,367개의 라벨 이미지 데이터 세트를 소개한다.<br>In this competition, we introduce a dataset of 21,367 labeled images collected during a regular survey in Uganda.<br><br><br>대부분의 이미지는 그들의 정원을 사진 찍는 농부들로부터 크라우드소싱되었고, 캄팔라 소재 마케레대학의 AI 연구소와 협력하여 국립작물자원연구소(NaCRRI)의 전문가들이 주석을 달았다.<br>Most images were crowdsourced from farmers taking photos of their gardens, and annotated by experts at the National Crops Resources Research Institute (NaCRRI) in collaboration with the AI lab at Makerere University, Kampala.<br><br><br>이것은 농부들이 실생활에서 진단해야 할 것을 가장 현실적으로 나타내는 형식이다.<br>This is in a format that most realistically represents what farmers would need to diagnose in real life.  </p>
<h2 id="Data-Description"><a href="#Data-Description" class="headerlink" title="Data Description"></a><strong>Data Description</strong></h2><p>비교적 저렴한 카메라의 사진을 사용하여 카사바 공장의 문제점을 식별할 수 있는가?<br>Can you identify a problem with a cassava plant using a photo from a relatively inexpensive camera?<br><br><br>이 대회는 많은 아프리카 국가들의 식량 공급에 물질적인 해를 끼치는 여러 질병들을 구별하는 것에 도전할 것이다.<br>This competition will challenge you to distinguish between several diseases that cause material harm to the food supply of many African countries.<br><br><br>어떤 경우에는 더 이상의 확산을 막기 위해 감염된 식물을 태우는 것이 주요 치료법인데, 이것은 농부들에게 꽤 유용한 빠른 자동 전환이 될 수 있다.<br>In some cases the main remedy is to burn the infected plants to prevent further spread, which can make a rapid automated turnaround quite useful to the farmers.</p>
<h2 id="Files"><a href="#Files" class="headerlink" title="Files"></a><strong>Files</strong></h2><p><strong>[train/test]_images</strong> the image files.<br><br><br>테스트 이미지의 전체 세트는 노트북이 채점을 위해 제출되었을 때만 사용할 수 있다.<br>The full set of test images will only be available to your notebook when it is submitted for scoring.<br><br><br>테스트 세트에서 약 15,000개의 이미지를 볼 수 있을 것으로 예상한다.<br>Expect to see roughly 15,000 images in the test set.<br><br><br><strong>train.csv</strong><br><br></p>
<ul>
<li><code>image_id</code> the image file name.  </li>
<li><code>label</code> 질병의 ID 코드 (the ID code for the disease)  <br>  

</li>
</ul>
<p><strong>sample_submission.csv</strong><br>공개된 테스트 세트 내용을 고려할 때 적절한 형식의 샘플 제출.<br><br><br>A properly formatted sample submission, given the disclosed test set content.<br><br></p>
<ul>
<li><code>image_id</code> the image file name.  </li>
<li><code>label</code> 질병의 예상 ID 코드 (the predicted ID code for the disease)  <br>  

</li>
</ul>
<p><strong>[train/test]_tfrecords</strong><br>tfrecord 형식의 이미지 파일<br>the image files in tfrecord format.<br><br><br><strong>label_num_to_disease_map.json</strong><br>각 질병 코드와 실제 질병 이름 간의 매핑.<br>The mapping between each disease code and the real disease name.</p>
<h1 id="카사바-잎-질병-EDA-탐색적-데이터-분석"><a href="#카사바-잎-질병-EDA-탐색적-데이터-분석" class="headerlink" title="카사바 잎 질병 - EDA(탐색적 데이터 분석)"></a><strong>카사바 잎 질병 - EDA(탐색적 데이터 분석)</strong></h1><p>Cassava Leaf Disease - Exploratory Data Analysis<br><a target="_blank" rel="noopener" href="https://www.kaggle.com/ihelon/cassava-leaf-disease-exploratory-data-analysis">https://www.kaggle.com/ihelon/cassava-leaf-disease-exploratory-data-analysis</a></p>
<p>Cassava Leaf 질병 분류 과제를 위한 빠른 탐색 데이터 분석<br>Quick Exploratory Data Analysis for Cassava Leaf Disease Classification challenge<br><br><br>이 대회는 많은 아프리카 국가들의 식량 공급에 물질적인 해를 끼치는 여러 질병들을 구별하는 것에 도전할 것이다.<br>This competition will challenge you to distinguish between several diseases that cause material harm to the food supply of many African countries.  </p>
<p>어떤 경우에는 더 이상의 확산을 막기 위해 감염된 식물을 태우는 것이 주요 치료법인데, 이것은 농부들에게 꽤 유용한 빠른 자동 전환이 될 수 있다.<br>In some cases the main remedy is to burn the infected plants to prevent further spread, which can make a rapid automated turnaround quite useful to the farmers.</p>
<h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sn</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> albumentations <span class="keyword">as</span> A</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics <span class="keyword">as</span> sk_metrics</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">BASE_DIR = <span class="string">&quot;../input/cassava-leaf-disease-classification/&quot;</span></span><br></pre></td></tr></table></figure>

<p>이번 대회에는 5개의 수업이 있다: <strong>4개의 질병</strong>과 <strong>1개의 건강</strong><br>클래스 번호와 클래스 이름 간의 매핑은 파일 label_num_to_disease_map.json에서 찾을 수 있다.  </p>
<p>In this competition we have 5 classes: <strong>4 diseases</strong> and <strong>1 healthy</strong><br>We can find the mapping between the class number and its name in the file label_num_to_disease_map.json</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(os.path.join(PROJECT_PATH, <span class="string">&quot;label_num_to_disease_map.json&quot;</span>)) <span class="keyword">as</span> file:</span><br><span class="line">    map_classes = json.loads(file.read())</span><br><span class="line">    </span><br><span class="line">print(json.dumps(map_classes, indent=<span class="number">4</span>))</span><br></pre></td></tr></table></figure>

<pre><code>&#123;
    &quot;0&quot;: &quot;Cassava Bacterial Blight (CBB)&quot;,
    &quot;1&quot;: &quot;Cassava Brown Streak Disease (CBSD)&quot;,
    &quot;2&quot;: &quot;Cassava Green Mottle (CGM)&quot;,
    &quot;3&quot;: &quot;Cassava Mosaic Disease (CMD)&quot;,
    &quot;4&quot;: &quot;Healthy&quot;
&#125;</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">input_files = os.listdir(os.path.join(BASE_DIR, <span class="string">&quot;train_images&quot;</span>))</span><br><span class="line">print(<span class="string">f&quot;Number of train images: <span class="subst">&#123;<span class="built_in">len</span>(input_files)&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<p><strong>이하 용량이 커서 결과 나중에</strong></p>
<p><img src="https://user-images.githubusercontent.com/72365720/100294328-ce3d7c00-2fc9-11eb-8b04-5c67610d2bda.png" alt="K-20201126-092849"></p>
<p>처음 300개의 이미지 치수를 살펴봅시다.<br>아래에서 볼 수 있듯이 모든 이미지는 크기가 동일하다(600, 800, 3)<br>Let’s take a look at the dimensions of the first 300 images<br>As you can see below, all images are the same size (600, 800, 3)  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">img_shapes = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> image_name <span class="keyword">in</span> os.listdir(os.path.join(BASE_DIR, <span class="string">&quot;train_images&quot;</span>))[:<span class="number">300</span>]:</span><br><span class="line">    image = cv2.imread(os.path.join(BASE_DIR, <span class="string">&quot;train_images&quot;</span>, image_name))</span><br><span class="line">    img_shapes[image.shape] = img_shapes.get(image.shape, <span class="number">0</span>) + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">print(img_shapes)</span><br></pre></td></tr></table></figure>

<p><img src="https://user-images.githubusercontent.com/72365720/100294402-0e9cfa00-2fca-11eb-82fc-af4ebe528d11.png" alt="K-20201126-093038"></p>
<p>교육 데이터 프레임을 로드하고 실제 클래스 이름이 포함된 열을 추가합시다.<br>Let’s load the training dataframe and add a column with the real class name to it.  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">df_train = pd.read_csv(os.path.join(BASE_DIR, <span class="string">&quot;train.csv&quot;</span>))</span><br><span class="line"></span><br><span class="line">df_train[<span class="string">&quot;class_name&quot;</span>] = df_train[<span class="string">&quot;label&quot;</span>].astype(<span class="built_in">str</span>).<span class="built_in">map</span>(map_classes)</span><br><span class="line"></span><br><span class="line">df_train</span><br></pre></td></tr></table></figure>

<p><img src="https://user-images.githubusercontent.com/72365720/100294470-3ee49880-2fca-11eb-9aba-2abc9b6e96e6.png" alt="K-20201126-093157"></p>
<p>각 반의 사진 수를 살펴보자.<br>Let’s look at the number of pictures in each class.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">4</span>))</span><br><span class="line">sn.countplot(y=<span class="string">&quot;class_name&quot;</span>, data=df_train);</span><br></pre></td></tr></table></figure>

<p><img src="https://user-images.githubusercontent.com/72365720/100294542-6c314680-2fca-11eb-969f-5dd5f3e83144.png" alt="__results___14_0"></p>
<p>우리가 알 수 있듯이 데이터 집합은 상당히 큰 불균형을 가지고 있다.<br>As we can see, the dataset has a fairly large imbalance.</p>
<h2 id="일반-시각화-General-Visualization"><a href="#일반-시각화-General-Visualization" class="headerlink" title="일반 시각화 (General Visualization)"></a>일반 시각화 (General Visualization)</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">visualize_batch</span>(<span class="params">image_ids, labels</span>):</span></span><br><span class="line">    plt.figure(figsize=(<span class="number">16</span>, <span class="number">12</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> ind, (image_id, label) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(image_ids, labels)):</span><br><span class="line">        plt.subplot(<span class="number">3</span>, <span class="number">3</span>, ind + <span class="number">1</span>)</span><br><span class="line">        image = cv2.imread(os.path.join(BASE_DIR, <span class="string">&quot;train_images&quot;</span>, image_id))</span><br><span class="line">        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)</span><br><span class="line"></span><br><span class="line">        plt.imshow(image)</span><br><span class="line">        plt.title(<span class="string">f&quot;Class: <span class="subst">&#123;label&#125;</span>&quot;</span>, fontsize=<span class="number">12</span>)</span><br><span class="line">        plt.axis(<span class="string">&quot;off&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tmp_df = df_train.sample(<span class="number">9</span>)</span><br><span class="line">image_ids = tmp_df[<span class="string">&quot;image_id&quot;</span>].values</span><br><span class="line">labels = tmp_df[<span class="string">&quot;class_name&quot;</span>].values</span><br><span class="line"></span><br><span class="line">visualize_batch(image_ids, labels)</span><br></pre></td></tr></table></figure>

<p><img src="https://user-images.githubusercontent.com/72365720/100294635-bf0afe00-2fca-11eb-917d-1783e7009cfa.png" alt="__results___18_0"></p>
<h2 id="Cassava-Bacterial-Blight-CBB"><a href="#Cassava-Bacterial-Blight-CBB" class="headerlink" title="Cassava Bacterial Blight (CBB)"></a>Cassava Bacterial Blight (CBB)</h2><p><img src="https://user-images.githubusercontent.com/72365720/100295143-39884d80-2fcc-11eb-936f-0059a1b61a6b.jpeg" alt="inbox_1865449_be9cdd94efb9b1660066ad10b55c8626_bact_bright"><br>The image from discussion: <a target="_blank" rel="noopener" href="https://www.kaggle.com/c/cassava-leaf-disease-classification/discussion/198143">Cassava Lead Diseases: Overview</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tmp_df = df_train[df_train[<span class="string">&quot;label&quot;</span>] == <span class="number">0</span>]</span><br><span class="line">print(<span class="string">f&quot;Total train images for class 0: <span class="subst">&#123;tmp_df.shape[<span class="number">0</span>]&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">tmp_df = tmp_df.sample(<span class="number">9</span>)</span><br><span class="line">image_ids = tmp_df[<span class="string">&quot;image_id&quot;</span>].values</span><br><span class="line">labels = tmp_df[<span class="string">&quot;label&quot;</span>].values</span><br><span class="line"></span><br><span class="line">visualize_batch(image_ids, labels)</span><br></pre></td></tr></table></figure>

<p><img src="https://user-images.githubusercontent.com/72365720/100295287-9b48b780-2fcc-11eb-9285-f29207d79ba6.png" alt="K-20201126-094743"></p>
<p><img src="https://user-images.githubusercontent.com/72365720/100295234-79e7cb80-2fcc-11eb-86e3-ff0e144ed678.png" alt="__results___21_1"></p>
<h2 id="Cassava-Brown-Streak-Disease-CBSD"><a href="#Cassava-Brown-Streak-Disease-CBSD" class="headerlink" title="Cassava Brown Streak Disease (CBSD)"></a>Cassava Brown Streak Disease (CBSD)</h2><p><img src="https://user-images.githubusercontent.com/72365720/100295367-c7fccf00-2fcc-11eb-9ee6-752ff3e9a02e.jpeg" alt="inbox_1865449_feba3dafc914d04517659650d137b77a_brown_st"><br>The image from discussion: <a target="_blank" rel="noopener" href="https://www.kaggle.com/c/cassava-leaf-disease-classification/discussion/198143">Cassava Lead Diseases: Overview</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tmp_df = df_train[df_train[<span class="string">&quot;label&quot;</span>] == <span class="number">1</span>]</span><br><span class="line">print(<span class="string">f&quot;Total train images for class 1: <span class="subst">&#123;tmp_df.shape[<span class="number">0</span>]&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">tmp_df = tmp_df.sample(<span class="number">9</span>)</span><br><span class="line">image_ids = tmp_df[<span class="string">&quot;image_id&quot;</span>].values</span><br><span class="line">labels = tmp_df[<span class="string">&quot;label&quot;</span>].values</span><br><span class="line"></span><br><span class="line">visualize_batch(image_ids, labels)</span><br></pre></td></tr></table></figure>

<p><img src="https://user-images.githubusercontent.com/72365720/100295429-ecf14200-2fcc-11eb-81d4-9db76e8e0637.png" alt="K-20201126-095102"></p>
<p><img src="https://user-images.githubusercontent.com/72365720/100295437-f2e72300-2fcc-11eb-8b1a-4cfb178de9ea.png" alt="__results___24_1"></p>
<h2 id="Cassava-Green-Mottle-CGM"><a href="#Cassava-Green-Mottle-CGM" class="headerlink" title="Cassava Green Mottle (CGM)"></a>Cassava Green Mottle (CGM)</h2><p><img src="https://user-images.githubusercontent.com/72365720/100295584-4eb1ac00-2fcd-11eb-8bad-a137d5d6d0fb.jpeg" alt="inbox_1865449_4f2975866feb2a1d4ef4111c2d57db29_green_mottle"><br>The image from discussion: <a target="_blank" rel="noopener" href="https://www.kaggle.com/c/cassava-leaf-disease-classification/discussion/198143">Cassava Lead Diseases: Overview</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tmp_df = df_train[df_train[<span class="string">&quot;label&quot;</span>] == <span class="number">2</span>]</span><br><span class="line">print(<span class="string">f&quot;Total train images for class 2: <span class="subst">&#123;tmp_df.shape[<span class="number">0</span>]&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">tmp_df = tmp_df.sample(<span class="number">9</span>)</span><br><span class="line">image_ids = tmp_df[<span class="string">&quot;image_id&quot;</span>].values</span><br><span class="line">labels = tmp_df[<span class="string">&quot;label&quot;</span>].values</span><br><span class="line"></span><br><span class="line">visualize_batch(image_ids, labels)</span><br></pre></td></tr></table></figure>

<p><img src="https://user-images.githubusercontent.com/72365720/100295638-71dc5b80-2fcd-11eb-96de-ecbc4b6b3a2f.png" alt="K-20201126-095448"></p>
<p><img src="https://user-images.githubusercontent.com/72365720/100295641-7274f200-2fcd-11eb-8768-13096f77e77e.png" alt="__results___27_1"></p>
<h2 id="Cassava-Mosaic-Disease-CMD"><a href="#Cassava-Mosaic-Disease-CMD" class="headerlink" title="Cassava Mosaic Disease (CMD)"></a>Cassava Mosaic Disease (CMD)</h2><p><img src="https://user-images.githubusercontent.com/72365720/100295912-3b531080-2fce-11eb-94a4-6ebb00a0c249.jpeg" alt="inbox_1865449_36990f77ded6667e5c30d19b5405d4d3_mosaic_disease"><br>The image from discussion: <a target="_blank" rel="noopener" href="https://www.kaggle.com/c/cassava-leaf-disease-classification/discussion/198143">Cassava Lead Diseases: Overview</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tmp_df = df_train[df_train[<span class="string">&quot;label&quot;</span>] == <span class="number">3</span>]</span><br><span class="line">print(<span class="string">f&quot;Total train images for class 3: <span class="subst">&#123;tmp_df.shape[<span class="number">0</span>]&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">tmp_df = tmp_df.sample(<span class="number">9</span>)</span><br><span class="line">image_ids = tmp_df[<span class="string">&quot;image_id&quot;</span>].values</span><br><span class="line">labels = tmp_df[<span class="string">&quot;label&quot;</span>].values</span><br><span class="line"></span><br><span class="line">visualize_batch(image_ids, labels)</span><br></pre></td></tr></table></figure>

<p><img src="https://user-images.githubusercontent.com/72365720/100295991-6a698200-2fce-11eb-992a-29d8bf0db854.png" alt="K-20201126-100145"></p>
<p><img src="https://user-images.githubusercontent.com/72365720/100295994-6b9aaf00-2fce-11eb-8310-2d703ee9233a.png" alt="__results___30_1"></p>
<h2 id="Healthy"><a href="#Healthy" class="headerlink" title="Healthy"></a>Healthy</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tmp_df = df_train[df_train[<span class="string">&quot;label&quot;</span>] == <span class="number">4</span>]</span><br><span class="line">print(<span class="string">f&quot;Total train images for class 4: <span class="subst">&#123;tmp_df.shape[<span class="number">0</span>]&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">tmp_df = tmp_df.sample(<span class="number">9</span>)</span><br><span class="line">image_ids = tmp_df[<span class="string">&quot;image_id&quot;</span>].values</span><br><span class="line">labels = tmp_df[<span class="string">&quot;label&quot;</span>].values</span><br><span class="line"></span><br><span class="line">visualize_batch(image_ids, labels)</span><br></pre></td></tr></table></figure>

<p><img src="https://user-images.githubusercontent.com/72365720/100296037-8c630480-2fce-11eb-818c-144ea91386cc.png" alt="K-20201126-100242"></p>
<p><img src="https://user-images.githubusercontent.com/72365720/100296042-8ec55e80-2fce-11eb-985b-ca44e8c3c63e.png" alt="__results___32_1"></p>
<h2 id="확대-예제-Augmentation-Examples"><a href="#확대-예제-Augmentation-Examples" class="headerlink" title="확대 예제 (Augmentation Examples)"></a>확대 예제 (Augmentation Examples)</h2><p>이미지 증가는 기존 교육 사례에서 새로운 교육 사례를 만드는 과정이다.<br>Image augmentation is a process of creating new training examples from the existing ones.  </p>
<p>새로운 샘플을 만들려면 원본 이미지를 약간 변경하십시오.<br>To make a new sample, you slightly change the original image.  </p>
<p>예를 들어, 새로운 이미지를 조금 더 밝게 만들 수 있다.<br>For instance, you could make a new image a little brighter;  </p>
<p>원본 이미지에서 조각을 잘라낼 수 있다.<br>you could cut a piece from the original image;  </p>
<p>원래 이미지를 미러링하는 등의 방법으로 새로운 이미지를 만들 수 있다.<br>you could make a new image by mirroring the original one, etc. <a target="_blank" rel="noopener" href="https://albumentations.ai/docs/introduction/image_augmentation/">source</a></p>
<p><img src="https://user-images.githubusercontent.com/72365720/100296265-14e1a500-2fcf-11eb-9423-bdea28961091.jpg" alt="augmentation"><br>The image from the <a target="_blank" rel="noopener" href="https://albumentations.ai/docs/introduction/image_augmentation/">Albumentations Documentation</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_augmentation</span>(<span class="params">image_id, transform</span>):</span></span><br><span class="line">    plt.figure(figsize=(<span class="number">16</span>, <span class="number">4</span>))</span><br><span class="line">    img = cv2.imread(os.path.join(BASE_DIR, <span class="string">&quot;train_images&quot;</span>, image_id))</span><br><span class="line">    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)</span><br><span class="line"></span><br><span class="line">    plt.subplot(<span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">    plt.imshow(img)</span><br><span class="line">    plt.axis(<span class="string">&quot;off&quot;</span>)</span><br><span class="line"></span><br><span class="line">    plt.subplot(<span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">    x = transform(image=img)[<span class="string">&quot;image&quot;</span>]</span><br><span class="line">    plt.imshow(x)</span><br><span class="line">    plt.axis(<span class="string">&quot;off&quot;</span>)</span><br><span class="line"></span><br><span class="line">    plt.subplot(<span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">    x = transform(image=img)[<span class="string">&quot;image&quot;</span>]</span><br><span class="line">    plt.imshow(x)</span><br><span class="line">    plt.axis(<span class="string">&quot;off&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>

<p>우리는 몇몇 수업의 수가 상당히 제한되어 있기 때문에, 우리는 증강을 사용할 수 있다.<br>Since we have a fairly limited number of some classes, we can use augmentation  </p>
<p>이 절은 연금술 라이브러리를 사용한 증축의 예를 보여준다.<br>This section shows examples of augmentation using the albumentations library<br><br></p>
<p>아래 예제는 지정학적 가장자리 보완과 함께 회전-시프트-척도 확대를 사용한다.<br>The example below uses rotate-shift-scale augmentation with specular edge complementation.  </p>
<p>이런 종류의 사진을 보면, 이런 증가는 꽤 자연스러워 보인다.<br>For this kind of pictures, this augmentation looks quite natural.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">transform_shift_scale_rotate = A.ShiftScaleRotate(</span><br><span class="line">    p=<span class="number">1.0</span>, </span><br><span class="line">    shift_limit=(<span class="number">-0.3</span>, <span class="number">0.3</span>), </span><br><span class="line">    scale_limit=(<span class="number">-0.1</span>, <span class="number">0.1</span>), </span><br><span class="line">    rotate_limit=(<span class="number">-180</span>, <span class="number">180</span>), </span><br><span class="line">    interpolation=<span class="number">0</span>, </span><br><span class="line">    border_mode=<span class="number">4</span>, </span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">plot_augmentation(<span class="string">&quot;1003442061.jpg&quot;</span>, transform_shift_scale_rotate)</span><br></pre></td></tr></table></figure>

<p><img src="https://user-images.githubusercontent.com/72365720/100296395-6e49d400-2fcf-11eb-95b5-28647fac52d4.png" alt="__results___39_0"></p>
<p>또 다른 유용한 증가는 ThoughDropout일 수 있다.<br>Another useful augmentation could be CoarseDropout.  </p>
<p>이 확대 덕분에, 당신은 모델의 수명을 복잡하게 만들 수 있어서 그녀가 이미지의 일부 세부사항을 너무 자세히 보지 않도록 할 수 있다.<br>Thanks to this augmentation, you can complicate the life of the model so that she does not look too closely at some of the details of the image.  </p>
<p>아래의 예를 보자.<br>Let’s look at the example below:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">transform_coarse_dropout = A.CoarseDropout(</span><br><span class="line">    p=<span class="number">1.0</span>, </span><br><span class="line">    max_holes=<span class="number">100</span>, </span><br><span class="line">    max_height=<span class="number">50</span>, </span><br><span class="line">    max_width=<span class="number">50</span>, </span><br><span class="line">    min_holes=<span class="number">30</span>, </span><br><span class="line">    min_height=<span class="number">20</span>, </span><br><span class="line">    min_width=<span class="number">20</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">plot_augmentation(<span class="string">&quot;1003442061.jpg&quot;</span>, transform_coarse_dropout)</span><br></pre></td></tr></table></figure>

<p><img src="https://user-images.githubusercontent.com/72365720/100296476-aea95200-2fcf-11eb-85ea-e4585ee247f3.png" alt="__results___41_0"></p>
<p>우리는 두 개 이상의 증강을 하나의 과정으로 구성할 수 있다.<br>We can compose two or more augmentations into one process.  </p>
<p>예를 들어 shift-scale-rotate 및 ThoughDropout을 일관되게 사용합시다.<br>For example, let’s use shift-scale-rotate and CoarseDropout consistently:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">transform = A.Compose(</span><br><span class="line">    transforms=[</span><br><span class="line">        transform_shift_scale_rotate,</span><br><span class="line">        transform_coarse_dropout,</span><br><span class="line">    ],</span><br><span class="line">    p=<span class="number">1.0</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">plot_augmentation(<span class="string">&quot;1003442061.jpg&quot;</span>, transform)</span><br></pre></td></tr></table></figure>

<p><img src="https://user-images.githubusercontent.com/72365720/100296522-d4cef200-2fcf-11eb-9d3c-d47762b11e9c.png" alt="__results___43_0"></p>
<h2 id="Submission-Example"><a href="#Submission-Example" class="headerlink" title="Submission Example"></a>Submission Example</h2><p>제출 템플릿 로드<br>Load the submission template</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df_sub = pd.read_csv(<span class="string">&quot;../input/cassava-leaf-disease-classification/sample_submission.csv&quot;</span>, index_col=<span class="number">0</span>)</span><br><span class="line">df_sub</span><br></pre></td></tr></table></figure>

<p><img src="https://user-images.githubusercontent.com/72365720/100296604-05169080-2fd0-11eb-9a3c-2a5ded46bda4.png" alt="K-20201126-101306"></p>
<p>제출 파일에서 하나의 파일만 볼 수 있기 때문에<br>As we can see only one file in the submission file</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">os.listdir(os.path.join(BASE_DIR, <span class="string">&quot;test_images&quot;</span>))</span><br></pre></td></tr></table></figure>

<p><img src="https://user-images.githubusercontent.com/72365720/100296650-26777c80-2fd0-11eb-901c-9fba48d1bd80.png" alt="K-20201126-101414"></p>
<p>코드 대회인데다 시험 데이터가 숨겨져 있기 때문이다.<br>This is because it is a Code Competition, and the test data is hidden  </p>
<p>노트북에서 볼 수 없는 테스트 데이터 집합으로 작업을 수정해야 함<br>Your notebook should correct working with unseen test dataset  </p>
<p>테스트 이미지의 전체 세트는 노트북이 채점을 위해 제출되었을 때만 사용할 수 있다.<br>The full set of test images will only be available to your notebook when it is submitted for scoring.  </p>
<p>테스트 세트에서 약 15,000개의 이미지를 볼 수 있을 것으로 예상한다.<br>Expect to see roughly 15,000 images in the test set.<br><br></p>
<p>이 경기의 척도는 <strong>정확성</strong>이다.<br>The metric of this competition is <strong>Accuracy</strong>.  </p>
<p>정확도 - 총 표본 수에 대해 올바르게 예측된 표본 수의 비율<br>Accuracy - the ratio of the number of samples predicted correctly to the total number of samples</p>
<p><img src="https://user-images.githubusercontent.com/72365720/100296861-9e45a700-2fd0-11eb-8c80-051105d4dcf5.png" alt="K-20201126-101734"></p>
<p>모든 예에 대해 하나의 클래스만 선택하면 교육 세트의 정확도를 계산해 봅시다.<br>Let’s calculate the accuracy on a training set if we select only one class for all examples.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> pred_class <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="number">5</span>):</span><br><span class="line">    y_true = df_train[<span class="string">&quot;label&quot;</span>].values</span><br><span class="line">    y_pred = np.full_like(y_true, pred_class)</span><br><span class="line">    print(<span class="string">f&quot;accuracy score (predict <span class="subst">&#123;pred_class&#125;</span>): <span class="subst">&#123;sk_metrics.accuracy_score(y_true, y_pred):<span class="number">.3</span>f&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<p><img src="https://user-images.githubusercontent.com/72365720/100296919-c3d2b080-2fd0-11eb-80a4-373fbb48049f.png" alt="K-20201126-101838"></p>
<p>우리는 계층의 불균형이 크기 때문에 가장 빈번한 수업을 예측하면 이 경우 정확도가 더 크다.<br>Since we have a large imbalance of classes, if we predict the most frequent class, then our accuracy is greater in this case  </p>
<p>테스트 세트에 있는 모든 이미지의 라벨로 가장 인기 있는 트레이닝 세트의 클래스를 선택하자.<br>Let’s choose the most popular class of training set as the label for all images in test set</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df_sub[<span class="string">&quot;label&quot;</span>] = <span class="number">3</span></span><br></pre></td></tr></table></figure>

<p>그리고 나서 제출 파일에 결과를 쓰세요.<br>And then write result to the submission file</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df_sub.to_csv(<span class="string">&quot;submission.csv&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>평가를 위해 결과를 제출하면 공용 라이더보드(열차 위는 0.615)에서 0.614의 정확도를 얻는다.<br>If you submit the result for evaluation, you will get an accuracy of 0.614 on a public liderboard (on the train it is 0.615).  </p>
<p>이는 공공 시험 분포에 대한 계층의 불균형도 있음을 나타낼 수 있다.<br>This may indicate that there is also an imbalance of classes on the public test distribution.</p>
<h1 id="WORK-IN-PROGRESS…"><a href="#WORK-IN-PROGRESS…" class="headerlink" title="WORK IN PROGRESS…"></a>WORK IN PROGRESS…</h1></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-11-12T06:37:53.281Z" title="2020-11-12T06:37:53.281Z">2020-11-12</time></span><span class="level-item">Updated&nbsp;<time dateTime="2020-11-12T06:38:36.579Z" title="2020-11-12T06:38:36.579Z">2020-11-12</time></span><span class="level-item"><a class="link-muted" href="/categories/study/">study</a></span><span class="level-item">3 hours read (About 29978 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/11/12/study/hello_coding_python/">Hello Coding 한입에 쏙 파이썬</a></h1><div class="content"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 코드 1-1 ‘Hello World!’를 출력하는 코드</span></span><br><span class="line">print(<span class="string">&#x27;Hello World!&#x27;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Hello World!</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 코드 1-2 print()를 잘못 사용한 코드</span></span><br><span class="line">Print(<span class="string">&#x27;Hello World!&#x27;</span>)</span><br></pre></td></tr></table></figure>


<pre><code>---------------------------------------------------------------------------

NameError                                 Traceback (most recent call last)

&lt;ipython-input-2-18101e960d2d&gt; in &lt;module&gt;()
      1 # 코드 1-2 print()를 잘못 사용한 코드
----&gt; 2 Print(&#39;Hello World!&#39;)


NameError: name &#39;Print&#39; is not defined</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 코드 1-3 ‘토끼야 안녕!’을 출력하는 코드</span></span><br><span class="line">print(<span class="string">&#x27;토끼야 안녕!&#x27;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>토끼야 안녕!</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 코드 2-1 다양한 숫자를 출력하는 코드</span></span><br><span class="line">print(<span class="number">1</span>)</span><br><span class="line">print(<span class="number">-2</span>)</span><br><span class="line">print(<span class="number">3.14</span>)</span><br></pre></td></tr></table></figure>

<pre><code>1
-2
3.14</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 코드 2-2 더하기, 빼기, 곱하기, 나누기를 하는 코드</span></span><br><span class="line">print(<span class="number">1</span> + <span class="number">2</span>)</span><br><span class="line">print(<span class="number">3</span> - <span class="number">2</span>)</span><br><span class="line">print(<span class="number">2</span> * <span class="number">4</span>)</span><br><span class="line">print(<span class="number">6</span> / <span class="number">3</span>)</span><br></pre></td></tr></table></figure>

<pre><code>3
1
8
2.0</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 코드 2-3 제곱, 몫, 나머지를 구하는 코드</span></span><br><span class="line">print(<span class="number">5</span> ** <span class="number">2</span>)</span><br><span class="line">print(<span class="number">5</span> // <span class="number">2</span>)</span><br><span class="line">print(<span class="number">5</span> % <span class="number">2</span>)</span><br></pre></td></tr></table></figure>

<pre><code>25
2
1</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 코드 2-4 다양한 연산을 하는 코드</span></span><br><span class="line">print(<span class="number">3</span> + <span class="number">7</span>)</span><br><span class="line">print(<span class="number">6</span> * <span class="number">3</span>)</span><br><span class="line">print(<span class="number">4</span> ** <span class="number">2</span>)</span><br><span class="line">print(<span class="number">9</span> % <span class="number">5</span>)</span><br></pre></td></tr></table></figure>

<pre><code>10
18
16
4</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 코드 2-5 문자열을 출력하는 코드</span></span><br><span class="line">print(<span class="string">&#x27;Hello World!&#x27;</span>)</span><br><span class="line">print(<span class="string">&#x27;3.14&#x27;</span>)</span><br><span class="line">print(<span class="string">&#x27;토끼야 안녕!&#x27;</span>)</span><br><span class="line">print(<span class="string">&quot;토끼야 안녕!&quot;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Hello World!
3.14
토끼야 안녕!
토끼야 안녕!</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 코드 2-6 문자열 + 연산을 하는 코드</span></span><br><span class="line">print(<span class="string">&#x27;토끼&#x27;</span> + <span class="string">&#x27;야 안녕!&#x27;</span>)</span><br><span class="line">print(<span class="string">&#x27;다람쥐&#x27;</span> + <span class="string">&#x27;야 안녕!&#x27;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>토끼야 안녕!
다람쥐야 안녕!</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 코드 2-7 문자열 * 연산을 하는 코드</span></span><br><span class="line">print(<span class="string">&#x27;데굴&#x27;</span> * <span class="number">2</span>)</span><br></pre></td></tr></table></figure>

<pre><code>데굴데굴</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 코드 2-8 색깔 문자를 연결하는 코드</span></span><br><span class="line">print(<span class="string">&#x27;빨&#x27;</span> + <span class="string">&#x27;주&#x27;</span> + <span class="string">&#x27;노&#x27;</span> + <span class="string">&#x27;초&#x27;</span> + <span class="string">&#x27;파&#x27;</span> + <span class="string">&#x27;남&#x27;</span> + <span class="string">&#x27;보&#x27;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>빨주노초파남보</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 코드 2-9 변수에 저장한 문자열을 출력하는 코드</span></span><br><span class="line">rainbow = <span class="string">&#x27;빨주노초파남보&#x27;</span></span><br><span class="line">print(rainbow)</span><br></pre></td></tr></table></figure>

<pre><code>빨주노초파남보</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 코드 2-10 변수에 저장한 값을 변경하는 코드</span></span><br><span class="line">count = <span class="number">0</span></span><br><span class="line">print(count)</span><br><span class="line">count = <span class="number">1</span></span><br><span class="line">print(count)</span><br><span class="line">count = count + <span class="number">1</span></span><br><span class="line">print(count)</span><br></pre></td></tr></table></figure>

<pre><code>0
1
2</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 코드 2-11 변수를 사용해 음료의 총 금액을 계산하는 코드</span></span><br><span class="line">coffee = <span class="number">4100</span></span><br><span class="line">juice = <span class="number">4600</span></span><br><span class="line">tea = <span class="number">3900</span></span><br><span class="line">print(coffee * <span class="number">3</span> + juice * <span class="number">2</span> + tea * <span class="number">1</span>)</span><br><span class="line">print(coffee * <span class="number">4</span> + juice * <span class="number">3</span> + tea * <span class="number">3</span>)</span><br><span class="line">print(coffee * <span class="number">1</span> + juice * <span class="number">1</span> + tea * <span class="number">2</span>)</span><br></pre></td></tr></table></figure>

<pre><code>25400
41900
16500</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 코드 2-12 주석으로 내용을 설명하는 코드</span></span><br><span class="line">coffee = <span class="number">4100</span>  <span class="comment"># 커피의 가격</span></span><br><span class="line">juice = <span class="number">4600</span>  <span class="comment"># 주스의 가격</span></span><br><span class="line">tea = <span class="number">3900</span>  <span class="comment"># 홍차의 가격</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 코드 2-13 주석으로 문장을 제외하는 코드</span></span><br><span class="line">print(<span class="string">&#x27;토끼야 안녕!&#x27;</span>)</span><br><span class="line"><span class="comment"># print(&#x27;토끼야 안녕!&#x27;)</span></span><br></pre></td></tr></table></figure>

<pre><code>토끼야 안녕!</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 코드 3-1 여러 개의 변수에 사탕을 저장하는 코드</span></span><br><span class="line">candy0 = <span class="string">&#x27;딸기맛&#x27;</span></span><br><span class="line">print(candy0)</span><br><span class="line">candy1 = <span class="string">&#x27;레몬맛&#x27;</span></span><br><span class="line">print(candy1)</span><br><span class="line">candy2 = <span class="string">&#x27;수박맛&#x27;</span></span><br><span class="line">print(candy2)</span><br><span class="line">candy3 = <span class="string">&#x27;박하맛&#x27;</span></span><br><span class="line">print(candy3)</span><br><span class="line">candy4 = <span class="string">&#x27;우유맛&#x27;</span></span><br><span class="line">print(candy4)</span><br></pre></td></tr></table></figure>

<pre><code>딸기맛
레몬맛
수박맛
박하맛
우유맛</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 코드 3-2 여러 개의 사탕을 하나의 변수에 저장하는 코드</span></span><br><span class="line">candies = [<span class="string">&#x27;딸기맛&#x27;</span>, <span class="string">&#x27;레몬맛&#x27;</span>, <span class="string">&#x27;수박맛&#x27;</span>, <span class="string">&#x27;박하맛&#x27;</span>, <span class="string">&#x27;우유맛&#x27;</span>]</span><br><span class="line">print(candies)</span><br></pre></td></tr></table></figure>

<pre><code>[&#39;딸기맛&#39;, &#39;레몬맛&#39;, &#39;수박맛&#39;, &#39;박하맛&#39;, &#39;우유맛&#39;]</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 코드 3-3 리스트를 만드는 코드</span></span><br><span class="line">my_list1 = []</span><br><span class="line">print(my_list1)</span><br><span class="line">my_list2 = [<span class="number">1</span>, <span class="number">-2</span>, <span class="number">3.14</span>]</span><br><span class="line">print(my_list2)</span><br><span class="line">my_list3 = [<span class="string">&#x27;앨리스&#x27;</span>, <span class="number">10</span>, [<span class="number">1.0</span>, <span class="number">1.2</span>]]</span><br><span class="line">print(my_list3)</span><br></pre></td></tr></table></figure>

<pre><code>[]
[1, -2, 3.14]
[&#39;앨리스&#39;, 10, [1.0, 1.2]]</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 코드 3-4 리스트에 값을 추가하는 코드</span></span><br><span class="line">clovers = []</span><br><span class="line">clovers.append(<span class="string">&#x27;클로버1&#x27;</span>)</span><br><span class="line">print(clovers)</span><br><span class="line">clovers.append(<span class="string">&#x27;하트2&#x27;</span>)</span><br><span class="line">print(clovers)</span><br><span class="line">clovers.append(<span class="string">&#x27;클로버3&#x27;</span>)</span><br><span class="line">print(clovers)</span><br></pre></td></tr></table></figure>

<pre><code>[&#39;클로버1&#39;]
[&#39;클로버1&#39;, &#39;하트2&#39;]
[&#39;클로버1&#39;, &#39;하트2&#39;, &#39;클로버3&#39;]</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 코드 3-5 리스트의 값에 접근하는 코드</span></span><br><span class="line">clovers = [<span class="string">&#x27;클로버1&#x27;</span>, <span class="string">&#x27;하트2&#x27;</span>, <span class="string">&#x27;클로버3&#x27;</span>]</span><br><span class="line">print(clovers[<span class="number">1</span>])</span><br><span class="line">clovers[<span class="number">1</span>] = <span class="string">&#x27;클로버2&#x27;</span></span><br><span class="line">print(clovers[<span class="number">1</span>])</span><br><span class="line">print(clovers[<span class="number">3</span>])</span><br></pre></td></tr></table></figure>

<pre><code>하트2
클로버2



---------------------------------------------------------------------------

IndexError                                Traceback (most recent call last)

&lt;ipython-input-21-670b859913d2&gt; in &lt;module&gt;()
      4 clovers[1] = &#39;클로버2&#39;
      5 print(clovers[1])
----&gt; 6 print(clovers[3])


IndexError: list index out of range</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 코드 3-6 리스트에서 값을 제거하는 코드</span></span><br><span class="line">clovers = [<span class="string">&#x27;클로버1&#x27;</span>, <span class="string">&#x27;클로버2&#x27;</span>, <span class="string">&#x27;클로버3&#x27;</span>]</span><br><span class="line">print(clovers[<span class="number">1</span>])</span><br><span class="line"><span class="keyword">del</span> clovers[<span class="number">1</span>]</span><br><span class="line">print(clovers)</span><br><span class="line">print(clovers[<span class="number">1</span>])</span><br></pre></td></tr></table></figure>

<pre><code>클로버2
[&#39;클로버1&#39;, &#39;클로버3&#39;]
클로버3</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 코드 3-7 사탕을 추가하고 제거하는 코드</span></span><br><span class="line">candies = [<span class="string">&#x27;딸기맛&#x27;</span>, <span class="string">&#x27;레몬맛&#x27;</span>, <span class="string">&#x27;수박맛&#x27;</span>, <span class="string">&#x27;박하맛&#x27;</span>, <span class="string">&#x27;우유맛&#x27;</span>]</span><br><span class="line">print(candies)</span><br><span class="line">candies.append(<span class="string">&#x27;콜라맛&#x27;</span>)</span><br><span class="line">candies.append(<span class="string">&#x27;포도맛&#x27;</span>)</span><br><span class="line">print(candies)</span><br><span class="line"><span class="keyword">del</span> candies[<span class="number">3</span>]</span><br><span class="line">print(candies)</span><br></pre></td></tr></table></figure>

<pre><code>[&#39;딸기맛&#39;, &#39;레몬맛&#39;, &#39;수박맛&#39;, &#39;박하맛&#39;, &#39;우유맛&#39;]
[&#39;딸기맛&#39;, &#39;레몬맛&#39;, &#39;수박맛&#39;, &#39;박하맛&#39;, &#39;우유맛&#39;, &#39;콜라맛&#39;, &#39;포도맛&#39;]
[&#39;딸기맛&#39;, &#39;레몬맛&#39;, &#39;수박맛&#39;, &#39;우유맛&#39;, &#39;콜라맛&#39;, &#39;포도맛&#39;]</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 코드 3-8 리스트에서 여러 개의 값을 가져오는 코드</span></span><br><span class="line">week = [<span class="string">&#x27;월&#x27;</span>, <span class="string">&#x27;화&#x27;</span>, <span class="string">&#x27;수&#x27;</span>, <span class="string">&#x27;목&#x27;</span>, <span class="string">&#x27;금&#x27;</span>, <span class="string">&#x27;토&#x27;</span>, <span class="string">&#x27;일&#x27;</span>]</span><br><span class="line">print(week)</span><br><span class="line">print(week[<span class="number">2</span>:<span class="number">5</span>])</span><br><span class="line">print(week)</span><br></pre></td></tr></table></figure>

<pre><code>[&#39;월&#39;, &#39;화&#39;, &#39;수&#39;, &#39;목&#39;, &#39;금&#39;, &#39;토&#39;, &#39;일&#39;]
[&#39;수&#39;, &#39;목&#39;, &#39;금&#39;]
[&#39;월&#39;, &#39;화&#39;, &#39;수&#39;, &#39;목&#39;, &#39;금&#39;, &#39;토&#39;, &#39;일&#39;]</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 코드 3-9 리스트에서 값을 가져오는 코드</span></span><br><span class="line">candies = [<span class="string">&#x27;딸기맛&#x27;</span>, <span class="string">&#x27;레몬맛&#x27;</span>, <span class="string">&#x27;수박맛&#x27;</span>, <span class="string">&#x27;우유맛&#x27;</span>, <span class="string">&#x27;콜라맛&#x27;</span>, <span class="string">&#x27;포도맛&#x27;</span>]</span><br><span class="line">cat_candy = candies[<span class="number">0</span>]</span><br><span class="line">print(<span class="string">&#x27;체셔고양이에게는&#x27;</span>, cat_candy, <span class="string">&#x27;사탕을 줘요.&#x27;</span>)</span><br><span class="line">duck_candy = candies[<span class="number">1</span>]</span><br><span class="line">print(<span class="string">&#x27;오리에게는&#x27;</span>, duck_candy, <span class="string">&#x27;사탕을 줘요.&#x27;</span>)</span><br><span class="line">dodo_candies = candies[<span class="number">3</span>:<span class="number">6</span>]</span><br><span class="line">print(<span class="string">&#x27;도도새에게는&#x27;</span>, dodo_candies, <span class="string">&#x27;사탕을 줘요.&#x27;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>체셔고양이에게는 딸기맛 사탕을 줘요.
오리에게는 레몬맛 사탕을 줘요.
도도새에게는 [&#39;우유맛&#39;, &#39;콜라맛&#39;, &#39;포도맛&#39;] 사탕을 줘요.</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 코드 3-10 리스트의 문자열을 정렬하는 코드</span></span><br><span class="line">animals = [<span class="string">&#x27;체셔고양이&#x27;</span>, <span class="string">&#x27;오리&#x27;</span>, <span class="string">&#x27;도도새&#x27;</span>]</span><br><span class="line">animals.sort()</span><br><span class="line">print(animals)</span><br></pre></td></tr></table></figure>

<pre><code>[&#39;도도새&#39;, &#39;오리&#39;, &#39;체셔고양이&#39;]</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 코드 3-11 리스트에서 특정 값의 개수를 세는 코드</span></span><br><span class="line">cards = [<span class="string">&#x27;하트&#x27;</span>, <span class="string">&#x27;클로버&#x27;</span>, <span class="string">&#x27;하트&#x27;</span>, <span class="string">&#x27;다이아&#x27;</span>]</span><br><span class="line">print(cards.count(<span class="string">&#x27;하트&#x27;</span>))</span><br><span class="line">print(cards.count(<span class="string">&#x27;클로버&#x27;</span>))</span><br></pre></td></tr></table></figure>

<pre><code>2
1</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 코드 4-1 거북이 100마리에게 인사하는 코드</span></span><br><span class="line">print(<span class="string">&#x27;안녕 거북이 0&#x27;</span>)</span><br><span class="line">print(<span class="string">&#x27;안녕 거북이 1&#x27;</span>)</span><br><span class="line"><span class="comment"># .</span></span><br><span class="line"><span class="comment"># .</span></span><br><span class="line"><span class="comment"># .</span></span><br><span class="line">print(<span class="string">&#x27;안녕 거북이 98&#x27;</span>)</span><br><span class="line">print(<span class="string">&#x27;안녕 거북이 99&#x27;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>안녕 거북이 0
안녕 거북이 1
안녕 거북이 98
안녕 거북이 99</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 코드 4-2 거북이 5,000마리에게 인사하는 코드</span></span><br><span class="line"><span class="keyword">for</span> num <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5000</span>):</span><br><span class="line">    print(<span class="string">&#x27;안녕 거북이&#x27;</span>, num)</span><br></pre></td></tr></table></figure>

<pre><code>[1;30;43m스트리밍 출력 내용이 길어서 마지막 5000줄이 삭제되었습니다.[0m
안녕 거북이 0
안녕 거북이 1
안녕 거북이 2
안녕 거북이 3
안녕 거북이 4
안녕 거북이 5
안녕 거북이 6
안녕 거북이 7
안녕 거북이 8
안녕 거북이 9
안녕 거북이 10
안녕 거북이 11
안녕 거북이 12
안녕 거북이 13
안녕 거북이 14
안녕 거북이 15
안녕 거북이 16
안녕 거북이 17
안녕 거북이 18
안녕 거북이 19
안녕 거북이 20
안녕 거북이 21
안녕 거북이 22
안녕 거북이 23
안녕 거북이 24
안녕 거북이 25
안녕 거북이 26
안녕 거북이 27
안녕 거북이 28
안녕 거북이 29
안녕 거북이 30
안녕 거북이 31
안녕 거북이 32
안녕 거북이 33
안녕 거북이 34
안녕 거북이 35
안녕 거북이 36
안녕 거북이 37
안녕 거북이 38
안녕 거북이 39
안녕 거북이 40
안녕 거북이 41
안녕 거북이 42
안녕 거북이 43
안녕 거북이 44
안녕 거북이 45
안녕 거북이 46
안녕 거북이 47
안녕 거북이 48
안녕 거북이 49
안녕 거북이 50
안녕 거북이 51
안녕 거북이 52
안녕 거북이 53
안녕 거북이 54
안녕 거북이 55
안녕 거북이 56
안녕 거북이 57
안녕 거북이 58
안녕 거북이 59
안녕 거북이 60
안녕 거북이 61
안녕 거북이 62
안녕 거북이 63
안녕 거북이 64
안녕 거북이 65
안녕 거북이 66
안녕 거북이 67
안녕 거북이 68
안녕 거북이 69
안녕 거북이 70
안녕 거북이 71
안녕 거북이 72
안녕 거북이 73
안녕 거북이 74
안녕 거북이 75
안녕 거북이 76
안녕 거북이 77
안녕 거북이 78
안녕 거북이 79
안녕 거북이 80
안녕 거북이 81
안녕 거북이 82
안녕 거북이 83
안녕 거북이 84
안녕 거북이 85
안녕 거북이 86
안녕 거북이 87
안녕 거북이 88
안녕 거북이 89
안녕 거북이 90
안녕 거북이 91
안녕 거북이 92
안녕 거북이 93
안녕 거북이 94
안녕 거북이 95
안녕 거북이 96
안녕 거북이 97
안녕 거북이 98
안녕 거북이 99
안녕 거북이 100
안녕 거북이 101
안녕 거북이 102
안녕 거북이 103
안녕 거북이 104
안녕 거북이 105
안녕 거북이 106
안녕 거북이 107
안녕 거북이 108
안녕 거북이 109
안녕 거북이 110
안녕 거북이 111
안녕 거북이 112
안녕 거북이 113
안녕 거북이 114
안녕 거북이 115
안녕 거북이 116
안녕 거북이 117
안녕 거북이 118
안녕 거북이 119
안녕 거북이 120
안녕 거북이 121
안녕 거북이 122
안녕 거북이 123
안녕 거북이 124
안녕 거북이 125
안녕 거북이 126
안녕 거북이 127
안녕 거북이 128
안녕 거북이 129
안녕 거북이 130
안녕 거북이 131
안녕 거북이 132
안녕 거북이 133
안녕 거북이 134
안녕 거북이 135
안녕 거북이 136
안녕 거북이 137
안녕 거북이 138
안녕 거북이 139
안녕 거북이 140
안녕 거북이 141
안녕 거북이 142
안녕 거북이 143
안녕 거북이 144
안녕 거북이 145
안녕 거북이 146
안녕 거북이 147
안녕 거북이 148
안녕 거북이 149
안녕 거북이 150
안녕 거북이 151
안녕 거북이 152
안녕 거북이 153
안녕 거북이 154
안녕 거북이 155
안녕 거북이 156
안녕 거북이 157
안녕 거북이 158
안녕 거북이 159
안녕 거북이 160
안녕 거북이 161
안녕 거북이 162
안녕 거북이 163
안녕 거북이 164
안녕 거북이 165
안녕 거북이 166
안녕 거북이 167
안녕 거북이 168
안녕 거북이 169
안녕 거북이 170
안녕 거북이 171
안녕 거북이 172
안녕 거북이 173
안녕 거북이 174
안녕 거북이 175
안녕 거북이 176
안녕 거북이 177
안녕 거북이 178
안녕 거북이 179
안녕 거북이 180
안녕 거북이 181
안녕 거북이 182
안녕 거북이 183
안녕 거북이 184
안녕 거북이 185
안녕 거북이 186
안녕 거북이 187
안녕 거북이 188
안녕 거북이 189
안녕 거북이 190
안녕 거북이 191
안녕 거북이 192
안녕 거북이 193
안녕 거북이 194
안녕 거북이 195
안녕 거북이 196
안녕 거북이 197
안녕 거북이 198
안녕 거북이 199
안녕 거북이 200
안녕 거북이 201
안녕 거북이 202
안녕 거북이 203
안녕 거북이 204
안녕 거북이 205
안녕 거북이 206
안녕 거북이 207
안녕 거북이 208
안녕 거북이 209
안녕 거북이 210
안녕 거북이 211
안녕 거북이 212
안녕 거북이 213
안녕 거북이 214
안녕 거북이 215
안녕 거북이 216
안녕 거북이 217
안녕 거북이 218
안녕 거북이 219
안녕 거북이 220
안녕 거북이 221
안녕 거북이 222
안녕 거북이 223
안녕 거북이 224
안녕 거북이 225
안녕 거북이 226
안녕 거북이 227
안녕 거북이 228
안녕 거북이 229
안녕 거북이 230
안녕 거북이 231
안녕 거북이 232
안녕 거북이 233
안녕 거북이 234
안녕 거북이 235
안녕 거북이 236
안녕 거북이 237
안녕 거북이 238
안녕 거북이 239
안녕 거북이 240
안녕 거북이 241
안녕 거북이 242
안녕 거북이 243
안녕 거북이 244
안녕 거북이 245
안녕 거북이 246
안녕 거북이 247
안녕 거북이 248
안녕 거북이 249
안녕 거북이 250
안녕 거북이 251
안녕 거북이 252
안녕 거북이 253
안녕 거북이 254
안녕 거북이 255
안녕 거북이 256
안녕 거북이 257
안녕 거북이 258
안녕 거북이 259
안녕 거북이 260
안녕 거북이 261
안녕 거북이 262
안녕 거북이 263
안녕 거북이 264
안녕 거북이 265
안녕 거북이 266
안녕 거북이 267
안녕 거북이 268
안녕 거북이 269
안녕 거북이 270
안녕 거북이 271
안녕 거북이 272
안녕 거북이 273
안녕 거북이 274
안녕 거북이 275
안녕 거북이 276
안녕 거북이 277
안녕 거북이 278
안녕 거북이 279
안녕 거북이 280
안녕 거북이 281
안녕 거북이 282
안녕 거북이 283
안녕 거북이 284
안녕 거북이 285
안녕 거북이 286
안녕 거북이 287
안녕 거북이 288
안녕 거북이 289
안녕 거북이 290
안녕 거북이 291
안녕 거북이 292
안녕 거북이 293
안녕 거북이 294
안녕 거북이 295
안녕 거북이 296
안녕 거북이 297
안녕 거북이 298
안녕 거북이 299
안녕 거북이 300
안녕 거북이 301
안녕 거북이 302
안녕 거북이 303
안녕 거북이 304
안녕 거북이 305
안녕 거북이 306
안녕 거북이 307
안녕 거북이 308
안녕 거북이 309
안녕 거북이 310
안녕 거북이 311
안녕 거북이 312
안녕 거북이 313
안녕 거북이 314
안녕 거북이 315
안녕 거북이 316
안녕 거북이 317
안녕 거북이 318
안녕 거북이 319
안녕 거북이 320
안녕 거북이 321
안녕 거북이 322
안녕 거북이 323
안녕 거북이 324
안녕 거북이 325
안녕 거북이 326
안녕 거북이 327
안녕 거북이 328
안녕 거북이 329
안녕 거북이 330
안녕 거북이 331
안녕 거북이 332
안녕 거북이 333
안녕 거북이 334
안녕 거북이 335
안녕 거북이 336
안녕 거북이 337
안녕 거북이 338
안녕 거북이 339
안녕 거북이 340
안녕 거북이 341
안녕 거북이 342
안녕 거북이 343
안녕 거북이 344
안녕 거북이 345
안녕 거북이 346
안녕 거북이 347
안녕 거북이 348
안녕 거북이 349
안녕 거북이 350
안녕 거북이 351
안녕 거북이 352
안녕 거북이 353
안녕 거북이 354
안녕 거북이 355
안녕 거북이 356
안녕 거북이 357
안녕 거북이 358
안녕 거북이 359
안녕 거북이 360
안녕 거북이 361
안녕 거북이 362
안녕 거북이 363
안녕 거북이 364
안녕 거북이 365
안녕 거북이 366
안녕 거북이 367
안녕 거북이 368
안녕 거북이 369
안녕 거북이 370
안녕 거북이 371
안녕 거북이 372
안녕 거북이 373
안녕 거북이 374
안녕 거북이 375
안녕 거북이 376
안녕 거북이 377
안녕 거북이 378
안녕 거북이 379
안녕 거북이 380
안녕 거북이 381
안녕 거북이 382
안녕 거북이 383
안녕 거북이 384
안녕 거북이 385
안녕 거북이 386
안녕 거북이 387
안녕 거북이 388
안녕 거북이 389
안녕 거북이 390
안녕 거북이 391
안녕 거북이 392
안녕 거북이 393
안녕 거북이 394
안녕 거북이 395
안녕 거북이 396
안녕 거북이 397
안녕 거북이 398
안녕 거북이 399
안녕 거북이 400
안녕 거북이 401
안녕 거북이 402
안녕 거북이 403
안녕 거북이 404
안녕 거북이 405
안녕 거북이 406
안녕 거북이 407
안녕 거북이 408
안녕 거북이 409
안녕 거북이 410
안녕 거북이 411
안녕 거북이 412
안녕 거북이 413
안녕 거북이 414
안녕 거북이 415
안녕 거북이 416
안녕 거북이 417
안녕 거북이 418
안녕 거북이 419
안녕 거북이 420
안녕 거북이 421
안녕 거북이 422
안녕 거북이 423
안녕 거북이 424
안녕 거북이 425
안녕 거북이 426
안녕 거북이 427
안녕 거북이 428
안녕 거북이 429
안녕 거북이 430
안녕 거북이 431
안녕 거북이 432
안녕 거북이 433
안녕 거북이 434
안녕 거북이 435
안녕 거북이 436
안녕 거북이 437
안녕 거북이 438
안녕 거북이 439
안녕 거북이 440
안녕 거북이 441
안녕 거북이 442
안녕 거북이 443
안녕 거북이 444
안녕 거북이 445
안녕 거북이 446
안녕 거북이 447
안녕 거북이 448
안녕 거북이 449
안녕 거북이 450
안녕 거북이 451
안녕 거북이 452
안녕 거북이 453
안녕 거북이 454
안녕 거북이 455
안녕 거북이 456
안녕 거북이 457
안녕 거북이 458
안녕 거북이 459
안녕 거북이 460
안녕 거북이 461
안녕 거북이 462
안녕 거북이 463
안녕 거북이 464
안녕 거북이 465
안녕 거북이 466
안녕 거북이 467
안녕 거북이 468
안녕 거북이 469
안녕 거북이 470
안녕 거북이 471
안녕 거북이 472
안녕 거북이 473
안녕 거북이 474
안녕 거북이 475
안녕 거북이 476
안녕 거북이 477
안녕 거북이 478
안녕 거북이 479
안녕 거북이 480
안녕 거북이 481
안녕 거북이 482
안녕 거북이 483
안녕 거북이 484
안녕 거북이 485
안녕 거북이 486
안녕 거북이 487
안녕 거북이 488
안녕 거북이 489
안녕 거북이 490
안녕 거북이 491
안녕 거북이 492
안녕 거북이 493
안녕 거북이 494
안녕 거북이 495
안녕 거북이 496
안녕 거북이 497
안녕 거북이 498
안녕 거북이 499
안녕 거북이 500
안녕 거북이 501
안녕 거북이 502
안녕 거북이 503
안녕 거북이 504
안녕 거북이 505
안녕 거북이 506
안녕 거북이 507
안녕 거북이 508
안녕 거북이 509
안녕 거북이 510
안녕 거북이 511
안녕 거북이 512
안녕 거북이 513
안녕 거북이 514
안녕 거북이 515
안녕 거북이 516
안녕 거북이 517
안녕 거북이 518
안녕 거북이 519
안녕 거북이 520
안녕 거북이 521
안녕 거북이 522
안녕 거북이 523
안녕 거북이 524
안녕 거북이 525
안녕 거북이 526
안녕 거북이 527
안녕 거북이 528
안녕 거북이 529
안녕 거북이 530
안녕 거북이 531
안녕 거북이 532
안녕 거북이 533
안녕 거북이 534
안녕 거북이 535
안녕 거북이 536
안녕 거북이 537
안녕 거북이 538
안녕 거북이 539
안녕 거북이 540
안녕 거북이 541
안녕 거북이 542
안녕 거북이 543
안녕 거북이 544
안녕 거북이 545
안녕 거북이 546
안녕 거북이 547
안녕 거북이 548
안녕 거북이 549
안녕 거북이 550
안녕 거북이 551
안녕 거북이 552
안녕 거북이 553
안녕 거북이 554
안녕 거북이 555
안녕 거북이 556
안녕 거북이 557
안녕 거북이 558
안녕 거북이 559
안녕 거북이 560
안녕 거북이 561
안녕 거북이 562
안녕 거북이 563
안녕 거북이 564
안녕 거북이 565
안녕 거북이 566
안녕 거북이 567
안녕 거북이 568
안녕 거북이 569
안녕 거북이 570
안녕 거북이 571
안녕 거북이 572
안녕 거북이 573
안녕 거북이 574
안녕 거북이 575
안녕 거북이 576
안녕 거북이 577
안녕 거북이 578
안녕 거북이 579
안녕 거북이 580
안녕 거북이 581
안녕 거북이 582
안녕 거북이 583
안녕 거북이 584
안녕 거북이 585
안녕 거북이 586
안녕 거북이 587
안녕 거북이 588
안녕 거북이 589
안녕 거북이 590
안녕 거북이 591
안녕 거북이 592
안녕 거북이 593
안녕 거북이 594
안녕 거북이 595
안녕 거북이 596
안녕 거북이 597
안녕 거북이 598
안녕 거북이 599
안녕 거북이 600
안녕 거북이 601
안녕 거북이 602
안녕 거북이 603
안녕 거북이 604
안녕 거북이 605
안녕 거북이 606
안녕 거북이 607
안녕 거북이 608
안녕 거북이 609
안녕 거북이 610
안녕 거북이 611
안녕 거북이 612
안녕 거북이 613
안녕 거북이 614
안녕 거북이 615
안녕 거북이 616
안녕 거북이 617
안녕 거북이 618
안녕 거북이 619
안녕 거북이 620
안녕 거북이 621
안녕 거북이 622
안녕 거북이 623
안녕 거북이 624
안녕 거북이 625
안녕 거북이 626
안녕 거북이 627
안녕 거북이 628
안녕 거북이 629
안녕 거북이 630
안녕 거북이 631
안녕 거북이 632
안녕 거북이 633
안녕 거북이 634
안녕 거북이 635
안녕 거북이 636
안녕 거북이 637
안녕 거북이 638
안녕 거북이 639
안녕 거북이 640
안녕 거북이 641
안녕 거북이 642
안녕 거북이 643
안녕 거북이 644
안녕 거북이 645
안녕 거북이 646
안녕 거북이 647
안녕 거북이 648
안녕 거북이 649
안녕 거북이 650
안녕 거북이 651
안녕 거북이 652
안녕 거북이 653
안녕 거북이 654
안녕 거북이 655
안녕 거북이 656
안녕 거북이 657
안녕 거북이 658
안녕 거북이 659
안녕 거북이 660
안녕 거북이 661
안녕 거북이 662
안녕 거북이 663
안녕 거북이 664
안녕 거북이 665
안녕 거북이 666
안녕 거북이 667
안녕 거북이 668
안녕 거북이 669
안녕 거북이 670
안녕 거북이 671
안녕 거북이 672
안녕 거북이 673
안녕 거북이 674
안녕 거북이 675
안녕 거북이 676
안녕 거북이 677
안녕 거북이 678
안녕 거북이 679
안녕 거북이 680
안녕 거북이 681
안녕 거북이 682
안녕 거북이 683
안녕 거북이 684
안녕 거북이 685
안녕 거북이 686
안녕 거북이 687
안녕 거북이 688
안녕 거북이 689
안녕 거북이 690
안녕 거북이 691
안녕 거북이 692
안녕 거북이 693
안녕 거북이 694
안녕 거북이 695
안녕 거북이 696
안녕 거북이 697
안녕 거북이 698
안녕 거북이 699
안녕 거북이 700
안녕 거북이 701
안녕 거북이 702
안녕 거북이 703
안녕 거북이 704
안녕 거북이 705
안녕 거북이 706
안녕 거북이 707
안녕 거북이 708
안녕 거북이 709
안녕 거북이 710
안녕 거북이 711
안녕 거북이 712
안녕 거북이 713
안녕 거북이 714
안녕 거북이 715
안녕 거북이 716
안녕 거북이 717
안녕 거북이 718
안녕 거북이 719
안녕 거북이 720
안녕 거북이 721
안녕 거북이 722
안녕 거북이 723
안녕 거북이 724
안녕 거북이 725
안녕 거북이 726
안녕 거북이 727
안녕 거북이 728
안녕 거북이 729
안녕 거북이 730
안녕 거북이 731
안녕 거북이 732
안녕 거북이 733
안녕 거북이 734
안녕 거북이 735
안녕 거북이 736
안녕 거북이 737
안녕 거북이 738
안녕 거북이 739
안녕 거북이 740
안녕 거북이 741
안녕 거북이 742
안녕 거북이 743
안녕 거북이 744
안녕 거북이 745
안녕 거북이 746
안녕 거북이 747
안녕 거북이 748
안녕 거북이 749
안녕 거북이 750
안녕 거북이 751
안녕 거북이 752
안녕 거북이 753
안녕 거북이 754
안녕 거북이 755
안녕 거북이 756
안녕 거북이 757
안녕 거북이 758
안녕 거북이 759
안녕 거북이 760
안녕 거북이 761
안녕 거북이 762
안녕 거북이 763
안녕 거북이 764
안녕 거북이 765
안녕 거북이 766
안녕 거북이 767
안녕 거북이 768
안녕 거북이 769
안녕 거북이 770
안녕 거북이 771
안녕 거북이 772
안녕 거북이 773
안녕 거북이 774
안녕 거북이 775
안녕 거북이 776
안녕 거북이 777
안녕 거북이 778
안녕 거북이 779
안녕 거북이 780
안녕 거북이 781
안녕 거북이 782
안녕 거북이 783
안녕 거북이 784
안녕 거북이 785
안녕 거북이 786
안녕 거북이 787
안녕 거북이 788
안녕 거북이 789
안녕 거북이 790
안녕 거북이 791
안녕 거북이 792
안녕 거북이 793
안녕 거북이 794
안녕 거북이 795
안녕 거북이 796
안녕 거북이 797
안녕 거북이 798
안녕 거북이 799
안녕 거북이 800
안녕 거북이 801
안녕 거북이 802
안녕 거북이 803
안녕 거북이 804
안녕 거북이 805
안녕 거북이 806
안녕 거북이 807
안녕 거북이 808
안녕 거북이 809
안녕 거북이 810
안녕 거북이 811
안녕 거북이 812
안녕 거북이 813
안녕 거북이 814
안녕 거북이 815
안녕 거북이 816
안녕 거북이 817
안녕 거북이 818
안녕 거북이 819
안녕 거북이 820
안녕 거북이 821
안녕 거북이 822
안녕 거북이 823
안녕 거북이 824
안녕 거북이 825
안녕 거북이 826
안녕 거북이 827
안녕 거북이 828
안녕 거북이 829
안녕 거북이 830
안녕 거북이 831
안녕 거북이 832
안녕 거북이 833
안녕 거북이 834
안녕 거북이 835
안녕 거북이 836
안녕 거북이 837
안녕 거북이 838
안녕 거북이 839
안녕 거북이 840
안녕 거북이 841
안녕 거북이 842
안녕 거북이 843
안녕 거북이 844
안녕 거북이 845
안녕 거북이 846
안녕 거북이 847
안녕 거북이 848
안녕 거북이 849
안녕 거북이 850
안녕 거북이 851
안녕 거북이 852
안녕 거북이 853
안녕 거북이 854
안녕 거북이 855
안녕 거북이 856
안녕 거북이 857
안녕 거북이 858
안녕 거북이 859
안녕 거북이 860
안녕 거북이 861
안녕 거북이 862
안녕 거북이 863
안녕 거북이 864
안녕 거북이 865
안녕 거북이 866
안녕 거북이 867
안녕 거북이 868
안녕 거북이 869
안녕 거북이 870
안녕 거북이 871
안녕 거북이 872
안녕 거북이 873
안녕 거북이 874
안녕 거북이 875
안녕 거북이 876
안녕 거북이 877
안녕 거북이 878
안녕 거북이 879
안녕 거북이 880
안녕 거북이 881
안녕 거북이 882
안녕 거북이 883
안녕 거북이 884
안녕 거북이 885
안녕 거북이 886
안녕 거북이 887
안녕 거북이 888
안녕 거북이 889
안녕 거북이 890
안녕 거북이 891
안녕 거북이 892
안녕 거북이 893
안녕 거북이 894
안녕 거북이 895
안녕 거북이 896
안녕 거북이 897
안녕 거북이 898
안녕 거북이 899
안녕 거북이 900
안녕 거북이 901
안녕 거북이 902
안녕 거북이 903
안녕 거북이 904
안녕 거북이 905
안녕 거북이 906
안녕 거북이 907
안녕 거북이 908
안녕 거북이 909
안녕 거북이 910
안녕 거북이 911
안녕 거북이 912
안녕 거북이 913
안녕 거북이 914
안녕 거북이 915
안녕 거북이 916
안녕 거북이 917
안녕 거북이 918
안녕 거북이 919
안녕 거북이 920
안녕 거북이 921
안녕 거북이 922
안녕 거북이 923
안녕 거북이 924
안녕 거북이 925
안녕 거북이 926
안녕 거북이 927
안녕 거북이 928
안녕 거북이 929
안녕 거북이 930
안녕 거북이 931
안녕 거북이 932
안녕 거북이 933
안녕 거북이 934
안녕 거북이 935
안녕 거북이 936
안녕 거북이 937
안녕 거북이 938
안녕 거북이 939
안녕 거북이 940
안녕 거북이 941
안녕 거북이 942
안녕 거북이 943
안녕 거북이 944
안녕 거북이 945
안녕 거북이 946
안녕 거북이 947
안녕 거북이 948
안녕 거북이 949
안녕 거북이 950
안녕 거북이 951
안녕 거북이 952
안녕 거북이 953
안녕 거북이 954
안녕 거북이 955
안녕 거북이 956
안녕 거북이 957
안녕 거북이 958
안녕 거북이 959
안녕 거북이 960
안녕 거북이 961
안녕 거북이 962
안녕 거북이 963
안녕 거북이 964
안녕 거북이 965
안녕 거북이 966
안녕 거북이 967
안녕 거북이 968
안녕 거북이 969
안녕 거북이 970
안녕 거북이 971
안녕 거북이 972
안녕 거북이 973
안녕 거북이 974
안녕 거북이 975
안녕 거북이 976
안녕 거북이 977
안녕 거북이 978
안녕 거북이 979
안녕 거북이 980
안녕 거북이 981
안녕 거북이 982
안녕 거북이 983
안녕 거북이 984
안녕 거북이 985
안녕 거북이 986
안녕 거북이 987
안녕 거북이 988
안녕 거북이 989
안녕 거북이 990
안녕 거북이 991
안녕 거북이 992
안녕 거북이 993
안녕 거북이 994
안녕 거북이 995
안녕 거북이 996
안녕 거북이 997
안녕 거북이 998
안녕 거북이 999
안녕 거북이 1000
안녕 거북이 1001
안녕 거북이 1002
안녕 거북이 1003
안녕 거북이 1004
안녕 거북이 1005
안녕 거북이 1006
안녕 거북이 1007
안녕 거북이 1008
안녕 거북이 1009
안녕 거북이 1010
안녕 거북이 1011
안녕 거북이 1012
안녕 거북이 1013
안녕 거북이 1014
안녕 거북이 1015
안녕 거북이 1016
안녕 거북이 1017
안녕 거북이 1018
안녕 거북이 1019
안녕 거북이 1020
안녕 거북이 1021
안녕 거북이 1022
안녕 거북이 1023
안녕 거북이 1024
안녕 거북이 1025
안녕 거북이 1026
안녕 거북이 1027
안녕 거북이 1028
안녕 거북이 1029
안녕 거북이 1030
안녕 거북이 1031
안녕 거북이 1032
안녕 거북이 1033
안녕 거북이 1034
안녕 거북이 1035
안녕 거북이 1036
안녕 거북이 1037
안녕 거북이 1038
안녕 거북이 1039
안녕 거북이 1040
안녕 거북이 1041
안녕 거북이 1042
안녕 거북이 1043
안녕 거북이 1044
안녕 거북이 1045
안녕 거북이 1046
안녕 거북이 1047
안녕 거북이 1048
안녕 거북이 1049
안녕 거북이 1050
안녕 거북이 1051
안녕 거북이 1052
안녕 거북이 1053
안녕 거북이 1054
안녕 거북이 1055
안녕 거북이 1056
안녕 거북이 1057
안녕 거북이 1058
안녕 거북이 1059
안녕 거북이 1060
안녕 거북이 1061
안녕 거북이 1062
안녕 거북이 1063
안녕 거북이 1064
안녕 거북이 1065
안녕 거북이 1066
안녕 거북이 1067
안녕 거북이 1068
안녕 거북이 1069
안녕 거북이 1070
안녕 거북이 1071
안녕 거북이 1072
안녕 거북이 1073
안녕 거북이 1074
안녕 거북이 1075
안녕 거북이 1076
안녕 거북이 1077
안녕 거북이 1078
안녕 거북이 1079
안녕 거북이 1080
안녕 거북이 1081
안녕 거북이 1082
안녕 거북이 1083
안녕 거북이 1084
안녕 거북이 1085
안녕 거북이 1086
안녕 거북이 1087
안녕 거북이 1088
안녕 거북이 1089
안녕 거북이 1090
안녕 거북이 1091
안녕 거북이 1092
안녕 거북이 1093
안녕 거북이 1094
안녕 거북이 1095
안녕 거북이 1096
안녕 거북이 1097
안녕 거북이 1098
안녕 거북이 1099
안녕 거북이 1100
안녕 거북이 1101
안녕 거북이 1102
안녕 거북이 1103
안녕 거북이 1104
안녕 거북이 1105
안녕 거북이 1106
안녕 거북이 1107
안녕 거북이 1108
안녕 거북이 1109
안녕 거북이 1110
안녕 거북이 1111
안녕 거북이 1112
안녕 거북이 1113
안녕 거북이 1114
안녕 거북이 1115
안녕 거북이 1116
안녕 거북이 1117
안녕 거북이 1118
안녕 거북이 1119
안녕 거북이 1120
안녕 거북이 1121
안녕 거북이 1122
안녕 거북이 1123
안녕 거북이 1124
안녕 거북이 1125
안녕 거북이 1126
안녕 거북이 1127
안녕 거북이 1128
안녕 거북이 1129
안녕 거북이 1130
안녕 거북이 1131
안녕 거북이 1132
안녕 거북이 1133
안녕 거북이 1134
안녕 거북이 1135
안녕 거북이 1136
안녕 거북이 1137
안녕 거북이 1138
안녕 거북이 1139
안녕 거북이 1140
안녕 거북이 1141
안녕 거북이 1142
안녕 거북이 1143
안녕 거북이 1144
안녕 거북이 1145
안녕 거북이 1146
안녕 거북이 1147
안녕 거북이 1148
안녕 거북이 1149
안녕 거북이 1150
안녕 거북이 1151
안녕 거북이 1152
안녕 거북이 1153
안녕 거북이 1154
안녕 거북이 1155
안녕 거북이 1156
안녕 거북이 1157
안녕 거북이 1158
안녕 거북이 1159
안녕 거북이 1160
안녕 거북이 1161
안녕 거북이 1162
안녕 거북이 1163
안녕 거북이 1164
안녕 거북이 1165
안녕 거북이 1166
안녕 거북이 1167
안녕 거북이 1168
안녕 거북이 1169
안녕 거북이 1170
안녕 거북이 1171
안녕 거북이 1172
안녕 거북이 1173
안녕 거북이 1174
안녕 거북이 1175
안녕 거북이 1176
안녕 거북이 1177
안녕 거북이 1178
안녕 거북이 1179
안녕 거북이 1180
안녕 거북이 1181
안녕 거북이 1182
안녕 거북이 1183
안녕 거북이 1184
안녕 거북이 1185
안녕 거북이 1186
안녕 거북이 1187
안녕 거북이 1188
안녕 거북이 1189
안녕 거북이 1190
안녕 거북이 1191
안녕 거북이 1192
안녕 거북이 1193
안녕 거북이 1194
안녕 거북이 1195
안녕 거북이 1196
안녕 거북이 1197
안녕 거북이 1198
안녕 거북이 1199
안녕 거북이 1200
안녕 거북이 1201
안녕 거북이 1202
안녕 거북이 1203
안녕 거북이 1204
안녕 거북이 1205
안녕 거북이 1206
안녕 거북이 1207
안녕 거북이 1208
안녕 거북이 1209
안녕 거북이 1210
안녕 거북이 1211
안녕 거북이 1212
안녕 거북이 1213
안녕 거북이 1214
안녕 거북이 1215
안녕 거북이 1216
안녕 거북이 1217
안녕 거북이 1218
안녕 거북이 1219
안녕 거북이 1220
안녕 거북이 1221
안녕 거북이 1222
안녕 거북이 1223
안녕 거북이 1224
안녕 거북이 1225
안녕 거북이 1226
안녕 거북이 1227
안녕 거북이 1228
안녕 거북이 1229
안녕 거북이 1230
안녕 거북이 1231
안녕 거북이 1232
안녕 거북이 1233
안녕 거북이 1234
안녕 거북이 1235
안녕 거북이 1236
안녕 거북이 1237
안녕 거북이 1238
안녕 거북이 1239
안녕 거북이 1240
안녕 거북이 1241
안녕 거북이 1242
안녕 거북이 1243
안녕 거북이 1244
안녕 거북이 1245
안녕 거북이 1246
안녕 거북이 1247
안녕 거북이 1248
안녕 거북이 1249
안녕 거북이 1250
안녕 거북이 1251
안녕 거북이 1252
안녕 거북이 1253
안녕 거북이 1254
안녕 거북이 1255
안녕 거북이 1256
안녕 거북이 1257
안녕 거북이 1258
안녕 거북이 1259
안녕 거북이 1260
안녕 거북이 1261
안녕 거북이 1262
안녕 거북이 1263
안녕 거북이 1264
안녕 거북이 1265
안녕 거북이 1266
안녕 거북이 1267
안녕 거북이 1268
안녕 거북이 1269
안녕 거북이 1270
안녕 거북이 1271
안녕 거북이 1272
안녕 거북이 1273
안녕 거북이 1274
안녕 거북이 1275
안녕 거북이 1276
안녕 거북이 1277
안녕 거북이 1278
안녕 거북이 1279
안녕 거북이 1280
안녕 거북이 1281
안녕 거북이 1282
안녕 거북이 1283
안녕 거북이 1284
안녕 거북이 1285
안녕 거북이 1286
안녕 거북이 1287
안녕 거북이 1288
안녕 거북이 1289
안녕 거북이 1290
안녕 거북이 1291
안녕 거북이 1292
안녕 거북이 1293
안녕 거북이 1294
안녕 거북이 1295
안녕 거북이 1296
안녕 거북이 1297
안녕 거북이 1298
안녕 거북이 1299
안녕 거북이 1300
안녕 거북이 1301
안녕 거북이 1302
안녕 거북이 1303
안녕 거북이 1304
안녕 거북이 1305
안녕 거북이 1306
안녕 거북이 1307
안녕 거북이 1308
안녕 거북이 1309
안녕 거북이 1310
안녕 거북이 1311
안녕 거북이 1312
안녕 거북이 1313
안녕 거북이 1314
안녕 거북이 1315
안녕 거북이 1316
안녕 거북이 1317
안녕 거북이 1318
안녕 거북이 1319
안녕 거북이 1320
안녕 거북이 1321
안녕 거북이 1322
안녕 거북이 1323
안녕 거북이 1324
안녕 거북이 1325
안녕 거북이 1326
안녕 거북이 1327
안녕 거북이 1328
안녕 거북이 1329
안녕 거북이 1330
안녕 거북이 1331
안녕 거북이 1332
안녕 거북이 1333
안녕 거북이 1334
안녕 거북이 1335
안녕 거북이 1336
안녕 거북이 1337
안녕 거북이 1338
안녕 거북이 1339
안녕 거북이 1340
안녕 거북이 1341
안녕 거북이 1342
안녕 거북이 1343
안녕 거북이 1344
안녕 거북이 1345
안녕 거북이 1346
안녕 거북이 1347
안녕 거북이 1348
안녕 거북이 1349
안녕 거북이 1350
안녕 거북이 1351
안녕 거북이 1352
안녕 거북이 1353
안녕 거북이 1354
안녕 거북이 1355
안녕 거북이 1356
안녕 거북이 1357
안녕 거북이 1358
안녕 거북이 1359
안녕 거북이 1360
안녕 거북이 1361
안녕 거북이 1362
안녕 거북이 1363
안녕 거북이 1364
안녕 거북이 1365
안녕 거북이 1366
안녕 거북이 1367
안녕 거북이 1368
안녕 거북이 1369
안녕 거북이 1370
안녕 거북이 1371
안녕 거북이 1372
안녕 거북이 1373
안녕 거북이 1374
안녕 거북이 1375
안녕 거북이 1376
안녕 거북이 1377
안녕 거북이 1378
안녕 거북이 1379
안녕 거북이 1380
안녕 거북이 1381
안녕 거북이 1382
안녕 거북이 1383
안녕 거북이 1384
안녕 거북이 1385
안녕 거북이 1386
안녕 거북이 1387
안녕 거북이 1388
안녕 거북이 1389
안녕 거북이 1390
안녕 거북이 1391
안녕 거북이 1392
안녕 거북이 1393
안녕 거북이 1394
안녕 거북이 1395
안녕 거북이 1396
안녕 거북이 1397
안녕 거북이 1398
안녕 거북이 1399
안녕 거북이 1400
안녕 거북이 1401
안녕 거북이 1402
안녕 거북이 1403
안녕 거북이 1404
안녕 거북이 1405
안녕 거북이 1406
안녕 거북이 1407
안녕 거북이 1408
안녕 거북이 1409
안녕 거북이 1410
안녕 거북이 1411
안녕 거북이 1412
안녕 거북이 1413
안녕 거북이 1414
안녕 거북이 1415
안녕 거북이 1416
안녕 거북이 1417
안녕 거북이 1418
안녕 거북이 1419
안녕 거북이 1420
안녕 거북이 1421
안녕 거북이 1422
안녕 거북이 1423
안녕 거북이 1424
안녕 거북이 1425
안녕 거북이 1426
안녕 거북이 1427
안녕 거북이 1428
안녕 거북이 1429
안녕 거북이 1430
안녕 거북이 1431
안녕 거북이 1432
안녕 거북이 1433
안녕 거북이 1434
안녕 거북이 1435
안녕 거북이 1436
안녕 거북이 1437
안녕 거북이 1438
안녕 거북이 1439
안녕 거북이 1440
안녕 거북이 1441
안녕 거북이 1442
안녕 거북이 1443
안녕 거북이 1444
안녕 거북이 1445
안녕 거북이 1446
안녕 거북이 1447
안녕 거북이 1448
안녕 거북이 1449
안녕 거북이 1450
안녕 거북이 1451
안녕 거북이 1452
안녕 거북이 1453
안녕 거북이 1454
안녕 거북이 1455
안녕 거북이 1456
안녕 거북이 1457
안녕 거북이 1458
안녕 거북이 1459
안녕 거북이 1460
안녕 거북이 1461
안녕 거북이 1462
안녕 거북이 1463
안녕 거북이 1464
안녕 거북이 1465
안녕 거북이 1466
안녕 거북이 1467
안녕 거북이 1468
안녕 거북이 1469
안녕 거북이 1470
안녕 거북이 1471
안녕 거북이 1472
안녕 거북이 1473
안녕 거북이 1474
안녕 거북이 1475
안녕 거북이 1476
안녕 거북이 1477
안녕 거북이 1478
안녕 거북이 1479
안녕 거북이 1480
안녕 거북이 1481
안녕 거북이 1482
안녕 거북이 1483
안녕 거북이 1484
안녕 거북이 1485
안녕 거북이 1486
안녕 거북이 1487
안녕 거북이 1488
안녕 거북이 1489
안녕 거북이 1490
안녕 거북이 1491
안녕 거북이 1492
안녕 거북이 1493
안녕 거북이 1494
안녕 거북이 1495
안녕 거북이 1496
안녕 거북이 1497
안녕 거북이 1498
안녕 거북이 1499
안녕 거북이 1500
안녕 거북이 1501
안녕 거북이 1502
안녕 거북이 1503
안녕 거북이 1504
안녕 거북이 1505
안녕 거북이 1506
안녕 거북이 1507
안녕 거북이 1508
안녕 거북이 1509
안녕 거북이 1510
안녕 거북이 1511
안녕 거북이 1512
안녕 거북이 1513
안녕 거북이 1514
안녕 거북이 1515
안녕 거북이 1516
안녕 거북이 1517
안녕 거북이 1518
안녕 거북이 1519
안녕 거북이 1520
안녕 거북이 1521
안녕 거북이 1522
안녕 거북이 1523
안녕 거북이 1524
안녕 거북이 1525
안녕 거북이 1526
안녕 거북이 1527
안녕 거북이 1528
안녕 거북이 1529
안녕 거북이 1530
안녕 거북이 1531
안녕 거북이 1532
안녕 거북이 1533
안녕 거북이 1534
안녕 거북이 1535
안녕 거북이 1536
안녕 거북이 1537
안녕 거북이 1538
안녕 거북이 1539
안녕 거북이 1540
안녕 거북이 1541
안녕 거북이 1542
안녕 거북이 1543
안녕 거북이 1544
안녕 거북이 1545
안녕 거북이 1546
안녕 거북이 1547
안녕 거북이 1548
안녕 거북이 1549
안녕 거북이 1550
안녕 거북이 1551
안녕 거북이 1552
안녕 거북이 1553
안녕 거북이 1554
안녕 거북이 1555
안녕 거북이 1556
안녕 거북이 1557
안녕 거북이 1558
안녕 거북이 1559
안녕 거북이 1560
안녕 거북이 1561
안녕 거북이 1562
안녕 거북이 1563
안녕 거북이 1564
안녕 거북이 1565
안녕 거북이 1566
안녕 거북이 1567
안녕 거북이 1568
안녕 거북이 1569
안녕 거북이 1570
안녕 거북이 1571
안녕 거북이 1572
안녕 거북이 1573
안녕 거북이 1574
안녕 거북이 1575
안녕 거북이 1576
안녕 거북이 1577
안녕 거북이 1578
안녕 거북이 1579
안녕 거북이 1580
안녕 거북이 1581
안녕 거북이 1582
안녕 거북이 1583
안녕 거북이 1584
안녕 거북이 1585
안녕 거북이 1586
안녕 거북이 1587
안녕 거북이 1588
안녕 거북이 1589
안녕 거북이 1590
안녕 거북이 1591
안녕 거북이 1592
안녕 거북이 1593
안녕 거북이 1594
안녕 거북이 1595
안녕 거북이 1596
안녕 거북이 1597
안녕 거북이 1598
안녕 거북이 1599
안녕 거북이 1600
안녕 거북이 1601
안녕 거북이 1602
안녕 거북이 1603
안녕 거북이 1604
안녕 거북이 1605
안녕 거북이 1606
안녕 거북이 1607
안녕 거북이 1608
안녕 거북이 1609
안녕 거북이 1610
안녕 거북이 1611
안녕 거북이 1612
안녕 거북이 1613
안녕 거북이 1614
안녕 거북이 1615
안녕 거북이 1616
안녕 거북이 1617
안녕 거북이 1618
안녕 거북이 1619
안녕 거북이 1620
안녕 거북이 1621
안녕 거북이 1622
안녕 거북이 1623
안녕 거북이 1624
안녕 거북이 1625
안녕 거북이 1626
안녕 거북이 1627
안녕 거북이 1628
안녕 거북이 1629
안녕 거북이 1630
안녕 거북이 1631
안녕 거북이 1632
안녕 거북이 1633
안녕 거북이 1634
안녕 거북이 1635
안녕 거북이 1636
안녕 거북이 1637
안녕 거북이 1638
안녕 거북이 1639
안녕 거북이 1640
안녕 거북이 1641
안녕 거북이 1642
안녕 거북이 1643
안녕 거북이 1644
안녕 거북이 1645
안녕 거북이 1646
안녕 거북이 1647
안녕 거북이 1648
안녕 거북이 1649
안녕 거북이 1650
안녕 거북이 1651
안녕 거북이 1652
안녕 거북이 1653
안녕 거북이 1654
안녕 거북이 1655
안녕 거북이 1656
안녕 거북이 1657
안녕 거북이 1658
안녕 거북이 1659
안녕 거북이 1660
안녕 거북이 1661
안녕 거북이 1662
안녕 거북이 1663
안녕 거북이 1664
안녕 거북이 1665
안녕 거북이 1666
안녕 거북이 1667
안녕 거북이 1668
안녕 거북이 1669
안녕 거북이 1670
안녕 거북이 1671
안녕 거북이 1672
안녕 거북이 1673
안녕 거북이 1674
안녕 거북이 1675
안녕 거북이 1676
안녕 거북이 1677
안녕 거북이 1678
안녕 거북이 1679
안녕 거북이 1680
안녕 거북이 1681
안녕 거북이 1682
안녕 거북이 1683
안녕 거북이 1684
안녕 거북이 1685
안녕 거북이 1686
안녕 거북이 1687
안녕 거북이 1688
안녕 거북이 1689
안녕 거북이 1690
안녕 거북이 1691
안녕 거북이 1692
안녕 거북이 1693
안녕 거북이 1694
안녕 거북이 1695
안녕 거북이 1696
안녕 거북이 1697
안녕 거북이 1698
안녕 거북이 1699
안녕 거북이 1700
안녕 거북이 1701
안녕 거북이 1702
안녕 거북이 1703
안녕 거북이 1704
안녕 거북이 1705
안녕 거북이 1706
안녕 거북이 1707
안녕 거북이 1708
안녕 거북이 1709
안녕 거북이 1710
안녕 거북이 1711
안녕 거북이 1712
안녕 거북이 1713
안녕 거북이 1714
안녕 거북이 1715
안녕 거북이 1716
안녕 거북이 1717
안녕 거북이 1718
안녕 거북이 1719
안녕 거북이 1720
안녕 거북이 1721
안녕 거북이 1722
안녕 거북이 1723
안녕 거북이 1724
안녕 거북이 1725
안녕 거북이 1726
안녕 거북이 1727
안녕 거북이 1728
안녕 거북이 1729
안녕 거북이 1730
안녕 거북이 1731
안녕 거북이 1732
안녕 거북이 1733
안녕 거북이 1734
안녕 거북이 1735
안녕 거북이 1736
안녕 거북이 1737
안녕 거북이 1738
안녕 거북이 1739
안녕 거북이 1740
안녕 거북이 1741
안녕 거북이 1742
안녕 거북이 1743
안녕 거북이 1744
안녕 거북이 1745
안녕 거북이 1746
안녕 거북이 1747
안녕 거북이 1748
안녕 거북이 1749
안녕 거북이 1750
안녕 거북이 1751
안녕 거북이 1752
안녕 거북이 1753
안녕 거북이 1754
안녕 거북이 1755
안녕 거북이 1756
안녕 거북이 1757
안녕 거북이 1758
안녕 거북이 1759
안녕 거북이 1760
안녕 거북이 1761
안녕 거북이 1762
안녕 거북이 1763
안녕 거북이 1764
안녕 거북이 1765
안녕 거북이 1766
안녕 거북이 1767
안녕 거북이 1768
안녕 거북이 1769
안녕 거북이 1770
안녕 거북이 1771
안녕 거북이 1772
안녕 거북이 1773
안녕 거북이 1774
안녕 거북이 1775
안녕 거북이 1776
안녕 거북이 1777
안녕 거북이 1778
안녕 거북이 1779
안녕 거북이 1780
안녕 거북이 1781
안녕 거북이 1782
안녕 거북이 1783
안녕 거북이 1784
안녕 거북이 1785
안녕 거북이 1786
안녕 거북이 1787
안녕 거북이 1788
안녕 거북이 1789
안녕 거북이 1790
안녕 거북이 1791
안녕 거북이 1792
안녕 거북이 1793
안녕 거북이 1794
안녕 거북이 1795
안녕 거북이 1796
안녕 거북이 1797
안녕 거북이 1798
안녕 거북이 1799
안녕 거북이 1800
안녕 거북이 1801
안녕 거북이 1802
안녕 거북이 1803
안녕 거북이 1804
안녕 거북이 1805
안녕 거북이 1806
안녕 거북이 1807
안녕 거북이 1808
안녕 거북이 1809
안녕 거북이 1810
안녕 거북이 1811
안녕 거북이 1812
안녕 거북이 1813
안녕 거북이 1814
안녕 거북이 1815
안녕 거북이 1816
안녕 거북이 1817
안녕 거북이 1818
안녕 거북이 1819
안녕 거북이 1820
안녕 거북이 1821
안녕 거북이 1822
안녕 거북이 1823
안녕 거북이 1824
안녕 거북이 1825
안녕 거북이 1826
안녕 거북이 1827
안녕 거북이 1828
안녕 거북이 1829
안녕 거북이 1830
안녕 거북이 1831
안녕 거북이 1832
안녕 거북이 1833
안녕 거북이 1834
안녕 거북이 1835
안녕 거북이 1836
안녕 거북이 1837
안녕 거북이 1838
안녕 거북이 1839
안녕 거북이 1840
안녕 거북이 1841
안녕 거북이 1842
안녕 거북이 1843
안녕 거북이 1844
안녕 거북이 1845
안녕 거북이 1846
안녕 거북이 1847
안녕 거북이 1848
안녕 거북이 1849
안녕 거북이 1850
안녕 거북이 1851
안녕 거북이 1852
안녕 거북이 1853
안녕 거북이 1854
안녕 거북이 1855
안녕 거북이 1856
안녕 거북이 1857
안녕 거북이 1858
안녕 거북이 1859
안녕 거북이 1860
안녕 거북이 1861
안녕 거북이 1862
안녕 거북이 1863
안녕 거북이 1864
안녕 거북이 1865
안녕 거북이 1866
안녕 거북이 1867
안녕 거북이 1868
안녕 거북이 1869
안녕 거북이 1870
안녕 거북이 1871
안녕 거북이 1872
안녕 거북이 1873
안녕 거북이 1874
안녕 거북이 1875
안녕 거북이 1876
안녕 거북이 1877
안녕 거북이 1878
안녕 거북이 1879
안녕 거북이 1880
안녕 거북이 1881
안녕 거북이 1882
안녕 거북이 1883
안녕 거북이 1884
안녕 거북이 1885
안녕 거북이 1886
안녕 거북이 1887
안녕 거북이 1888
안녕 거북이 1889
안녕 거북이 1890
안녕 거북이 1891
안녕 거북이 1892
안녕 거북이 1893
안녕 거북이 1894
안녕 거북이 1895
안녕 거북이 1896
안녕 거북이 1897
안녕 거북이 1898
안녕 거북이 1899
안녕 거북이 1900
안녕 거북이 1901
안녕 거북이 1902
안녕 거북이 1903
안녕 거북이 1904
안녕 거북이 1905
안녕 거북이 1906
안녕 거북이 1907
안녕 거북이 1908
안녕 거북이 1909
안녕 거북이 1910
안녕 거북이 1911
안녕 거북이 1912
안녕 거북이 1913
안녕 거북이 1914
안녕 거북이 1915
안녕 거북이 1916
안녕 거북이 1917
안녕 거북이 1918
안녕 거북이 1919
안녕 거북이 1920
안녕 거북이 1921
안녕 거북이 1922
안녕 거북이 1923
안녕 거북이 1924
안녕 거북이 1925
안녕 거북이 1926
안녕 거북이 1927
안녕 거북이 1928
안녕 거북이 1929
안녕 거북이 1930
안녕 거북이 1931
안녕 거북이 1932
안녕 거북이 1933
안녕 거북이 1934
안녕 거북이 1935
안녕 거북이 1936
안녕 거북이 1937
안녕 거북이 1938
안녕 거북이 1939
안녕 거북이 1940
안녕 거북이 1941
안녕 거북이 1942
안녕 거북이 1943
안녕 거북이 1944
안녕 거북이 1945
안녕 거북이 1946
안녕 거북이 1947
안녕 거북이 1948
안녕 거북이 1949
안녕 거북이 1950
안녕 거북이 1951
안녕 거북이 1952
안녕 거북이 1953
안녕 거북이 1954
안녕 거북이 1955
안녕 거북이 1956
안녕 거북이 1957
안녕 거북이 1958
안녕 거북이 1959
안녕 거북이 1960
안녕 거북이 1961
안녕 거북이 1962
안녕 거북이 1963
안녕 거북이 1964
안녕 거북이 1965
안녕 거북이 1966
안녕 거북이 1967
안녕 거북이 1968
안녕 거북이 1969
안녕 거북이 1970
안녕 거북이 1971
안녕 거북이 1972
안녕 거북이 1973
안녕 거북이 1974
안녕 거북이 1975
안녕 거북이 1976
안녕 거북이 1977
안녕 거북이 1978
안녕 거북이 1979
안녕 거북이 1980
안녕 거북이 1981
안녕 거북이 1982
안녕 거북이 1983
안녕 거북이 1984
안녕 거북이 1985
안녕 거북이 1986
안녕 거북이 1987
안녕 거북이 1988
안녕 거북이 1989
안녕 거북이 1990
안녕 거북이 1991
안녕 거북이 1992
안녕 거북이 1993
안녕 거북이 1994
안녕 거북이 1995
안녕 거북이 1996
안녕 거북이 1997
안녕 거북이 1998
안녕 거북이 1999
안녕 거북이 2000
안녕 거북이 2001
안녕 거북이 2002
안녕 거북이 2003
안녕 거북이 2004
안녕 거북이 2005
안녕 거북이 2006
안녕 거북이 2007
안녕 거북이 2008
안녕 거북이 2009
안녕 거북이 2010
안녕 거북이 2011
안녕 거북이 2012
안녕 거북이 2013
안녕 거북이 2014
안녕 거북이 2015
안녕 거북이 2016
안녕 거북이 2017
안녕 거북이 2018
안녕 거북이 2019
안녕 거북이 2020
안녕 거북이 2021
안녕 거북이 2022
안녕 거북이 2023
안녕 거북이 2024
안녕 거북이 2025
안녕 거북이 2026
안녕 거북이 2027
안녕 거북이 2028
안녕 거북이 2029
안녕 거북이 2030
안녕 거북이 2031
안녕 거북이 2032
안녕 거북이 2033
안녕 거북이 2034
안녕 거북이 2035
안녕 거북이 2036
안녕 거북이 2037
안녕 거북이 2038
안녕 거북이 2039
안녕 거북이 2040
안녕 거북이 2041
안녕 거북이 2042
안녕 거북이 2043
안녕 거북이 2044
안녕 거북이 2045
안녕 거북이 2046
안녕 거북이 2047
안녕 거북이 2048
안녕 거북이 2049
안녕 거북이 2050
안녕 거북이 2051
안녕 거북이 2052
안녕 거북이 2053
안녕 거북이 2054
안녕 거북이 2055
안녕 거북이 2056
안녕 거북이 2057
안녕 거북이 2058
안녕 거북이 2059
안녕 거북이 2060
안녕 거북이 2061
안녕 거북이 2062
안녕 거북이 2063
안녕 거북이 2064
안녕 거북이 2065
안녕 거북이 2066
안녕 거북이 2067
안녕 거북이 2068
안녕 거북이 2069
안녕 거북이 2070
안녕 거북이 2071
안녕 거북이 2072
안녕 거북이 2073
안녕 거북이 2074
안녕 거북이 2075
안녕 거북이 2076
안녕 거북이 2077
안녕 거북이 2078
안녕 거북이 2079
안녕 거북이 2080
안녕 거북이 2081
안녕 거북이 2082
안녕 거북이 2083
안녕 거북이 2084
안녕 거북이 2085
안녕 거북이 2086
안녕 거북이 2087
안녕 거북이 2088
안녕 거북이 2089
안녕 거북이 2090
안녕 거북이 2091
안녕 거북이 2092
안녕 거북이 2093
안녕 거북이 2094
안녕 거북이 2095
안녕 거북이 2096
안녕 거북이 2097
안녕 거북이 2098
안녕 거북이 2099
안녕 거북이 2100
안녕 거북이 2101
안녕 거북이 2102
안녕 거북이 2103
안녕 거북이 2104
안녕 거북이 2105
안녕 거북이 2106
안녕 거북이 2107
안녕 거북이 2108
안녕 거북이 2109
안녕 거북이 2110
안녕 거북이 2111
안녕 거북이 2112
안녕 거북이 2113
안녕 거북이 2114
안녕 거북이 2115
안녕 거북이 2116
안녕 거북이 2117
안녕 거북이 2118
안녕 거북이 2119
안녕 거북이 2120
안녕 거북이 2121
안녕 거북이 2122
안녕 거북이 2123
안녕 거북이 2124
안녕 거북이 2125
안녕 거북이 2126
안녕 거북이 2127
안녕 거북이 2128
안녕 거북이 2129
안녕 거북이 2130
안녕 거북이 2131
안녕 거북이 2132
안녕 거북이 2133
안녕 거북이 2134
안녕 거북이 2135
안녕 거북이 2136
안녕 거북이 2137
안녕 거북이 2138
안녕 거북이 2139
안녕 거북이 2140
안녕 거북이 2141
안녕 거북이 2142
안녕 거북이 2143
안녕 거북이 2144
안녕 거북이 2145
안녕 거북이 2146
안녕 거북이 2147
안녕 거북이 2148
안녕 거북이 2149
안녕 거북이 2150
안녕 거북이 2151
안녕 거북이 2152
안녕 거북이 2153
안녕 거북이 2154
안녕 거북이 2155
안녕 거북이 2156
안녕 거북이 2157
안녕 거북이 2158
안녕 거북이 2159
안녕 거북이 2160
안녕 거북이 2161
안녕 거북이 2162
안녕 거북이 2163
안녕 거북이 2164
안녕 거북이 2165
안녕 거북이 2166
안녕 거북이 2167
안녕 거북이 2168
안녕 거북이 2169
안녕 거북이 2170
안녕 거북이 2171
안녕 거북이 2172
안녕 거북이 2173
안녕 거북이 2174
안녕 거북이 2175
안녕 거북이 2176
안녕 거북이 2177
안녕 거북이 2178
안녕 거북이 2179
안녕 거북이 2180
안녕 거북이 2181
안녕 거북이 2182
안녕 거북이 2183
안녕 거북이 2184
안녕 거북이 2185
안녕 거북이 2186
안녕 거북이 2187
안녕 거북이 2188
안녕 거북이 2189
안녕 거북이 2190
안녕 거북이 2191
안녕 거북이 2192
안녕 거북이 2193
안녕 거북이 2194
안녕 거북이 2195
안녕 거북이 2196
안녕 거북이 2197
안녕 거북이 2198
안녕 거북이 2199
안녕 거북이 2200
안녕 거북이 2201
안녕 거북이 2202
안녕 거북이 2203
안녕 거북이 2204
안녕 거북이 2205
안녕 거북이 2206
안녕 거북이 2207
안녕 거북이 2208
안녕 거북이 2209
안녕 거북이 2210
안녕 거북이 2211
안녕 거북이 2212
안녕 거북이 2213
안녕 거북이 2214
안녕 거북이 2215
안녕 거북이 2216
안녕 거북이 2217
안녕 거북이 2218
안녕 거북이 2219
안녕 거북이 2220
안녕 거북이 2221
안녕 거북이 2222
안녕 거북이 2223
안녕 거북이 2224
안녕 거북이 2225
안녕 거북이 2226
안녕 거북이 2227
안녕 거북이 2228
안녕 거북이 2229
안녕 거북이 2230
안녕 거북이 2231
안녕 거북이 2232
안녕 거북이 2233
안녕 거북이 2234
안녕 거북이 2235
안녕 거북이 2236
안녕 거북이 2237
안녕 거북이 2238
안녕 거북이 2239
안녕 거북이 2240
안녕 거북이 2241
안녕 거북이 2242
안녕 거북이 2243
안녕 거북이 2244
안녕 거북이 2245
안녕 거북이 2246
안녕 거북이 2247
안녕 거북이 2248
안녕 거북이 2249
안녕 거북이 2250
안녕 거북이 2251
안녕 거북이 2252
안녕 거북이 2253
안녕 거북이 2254
안녕 거북이 2255
안녕 거북이 2256
안녕 거북이 2257
안녕 거북이 2258
안녕 거북이 2259
안녕 거북이 2260
안녕 거북이 2261
안녕 거북이 2262
안녕 거북이 2263
안녕 거북이 2264
안녕 거북이 2265
안녕 거북이 2266
안녕 거북이 2267
안녕 거북이 2268
안녕 거북이 2269
안녕 거북이 2270
안녕 거북이 2271
안녕 거북이 2272
안녕 거북이 2273
안녕 거북이 2274
안녕 거북이 2275
안녕 거북이 2276
안녕 거북이 2277
안녕 거북이 2278
안녕 거북이 2279
안녕 거북이 2280
안녕 거북이 2281
안녕 거북이 2282
안녕 거북이 2283
안녕 거북이 2284
안녕 거북이 2285
안녕 거북이 2286
안녕 거북이 2287
안녕 거북이 2288
안녕 거북이 2289
안녕 거북이 2290
안녕 거북이 2291
안녕 거북이 2292
안녕 거북이 2293
안녕 거북이 2294
안녕 거북이 2295
안녕 거북이 2296
안녕 거북이 2297
안녕 거북이 2298
안녕 거북이 2299
안녕 거북이 2300
안녕 거북이 2301
안녕 거북이 2302
안녕 거북이 2303
안녕 거북이 2304
안녕 거북이 2305
안녕 거북이 2306
안녕 거북이 2307
안녕 거북이 2308
안녕 거북이 2309
안녕 거북이 2310
안녕 거북이 2311
안녕 거북이 2312
안녕 거북이 2313
안녕 거북이 2314
안녕 거북이 2315
안녕 거북이 2316
안녕 거북이 2317
안녕 거북이 2318
안녕 거북이 2319
안녕 거북이 2320
안녕 거북이 2321
안녕 거북이 2322
안녕 거북이 2323
안녕 거북이 2324
안녕 거북이 2325
안녕 거북이 2326
안녕 거북이 2327
안녕 거북이 2328
안녕 거북이 2329
안녕 거북이 2330
안녕 거북이 2331
안녕 거북이 2332
안녕 거북이 2333
안녕 거북이 2334
안녕 거북이 2335
안녕 거북이 2336
안녕 거북이 2337
안녕 거북이 2338
안녕 거북이 2339
안녕 거북이 2340
안녕 거북이 2341
안녕 거북이 2342
안녕 거북이 2343
안녕 거북이 2344
안녕 거북이 2345
안녕 거북이 2346
안녕 거북이 2347
안녕 거북이 2348
안녕 거북이 2349
안녕 거북이 2350
안녕 거북이 2351
안녕 거북이 2352
안녕 거북이 2353
안녕 거북이 2354
안녕 거북이 2355
안녕 거북이 2356
안녕 거북이 2357
안녕 거북이 2358
안녕 거북이 2359
안녕 거북이 2360
안녕 거북이 2361
안녕 거북이 2362
안녕 거북이 2363
안녕 거북이 2364
안녕 거북이 2365
안녕 거북이 2366
안녕 거북이 2367
안녕 거북이 2368
안녕 거북이 2369
안녕 거북이 2370
안녕 거북이 2371
안녕 거북이 2372
안녕 거북이 2373
안녕 거북이 2374
안녕 거북이 2375
안녕 거북이 2376
안녕 거북이 2377
안녕 거북이 2378
안녕 거북이 2379
안녕 거북이 2380
안녕 거북이 2381
안녕 거북이 2382
안녕 거북이 2383
안녕 거북이 2384
안녕 거북이 2385
안녕 거북이 2386
안녕 거북이 2387
안녕 거북이 2388
안녕 거북이 2389
안녕 거북이 2390
안녕 거북이 2391
안녕 거북이 2392
안녕 거북이 2393
안녕 거북이 2394
안녕 거북이 2395
안녕 거북이 2396
안녕 거북이 2397
안녕 거북이 2398
안녕 거북이 2399
안녕 거북이 2400
안녕 거북이 2401
안녕 거북이 2402
안녕 거북이 2403
안녕 거북이 2404
안녕 거북이 2405
안녕 거북이 2406
안녕 거북이 2407
안녕 거북이 2408
안녕 거북이 2409
안녕 거북이 2410
안녕 거북이 2411
안녕 거북이 2412
안녕 거북이 2413
안녕 거북이 2414
안녕 거북이 2415
안녕 거북이 2416
안녕 거북이 2417
안녕 거북이 2418
안녕 거북이 2419
안녕 거북이 2420
안녕 거북이 2421
안녕 거북이 2422
안녕 거북이 2423
안녕 거북이 2424
안녕 거북이 2425
안녕 거북이 2426
안녕 거북이 2427
안녕 거북이 2428
안녕 거북이 2429
안녕 거북이 2430
안녕 거북이 2431
안녕 거북이 2432
안녕 거북이 2433
안녕 거북이 2434
안녕 거북이 2435
안녕 거북이 2436
안녕 거북이 2437
안녕 거북이 2438
안녕 거북이 2439
안녕 거북이 2440
안녕 거북이 2441
안녕 거북이 2442
안녕 거북이 2443
안녕 거북이 2444
안녕 거북이 2445
안녕 거북이 2446
안녕 거북이 2447
안녕 거북이 2448
안녕 거북이 2449
안녕 거북이 2450
안녕 거북이 2451
안녕 거북이 2452
안녕 거북이 2453
안녕 거북이 2454
안녕 거북이 2455
안녕 거북이 2456
안녕 거북이 2457
안녕 거북이 2458
안녕 거북이 2459
안녕 거북이 2460
안녕 거북이 2461
안녕 거북이 2462
안녕 거북이 2463
안녕 거북이 2464
안녕 거북이 2465
안녕 거북이 2466
안녕 거북이 2467
안녕 거북이 2468
안녕 거북이 2469
안녕 거북이 2470
안녕 거북이 2471
안녕 거북이 2472
안녕 거북이 2473
안녕 거북이 2474
안녕 거북이 2475
안녕 거북이 2476
안녕 거북이 2477
안녕 거북이 2478
안녕 거북이 2479
안녕 거북이 2480
안녕 거북이 2481
안녕 거북이 2482
안녕 거북이 2483
안녕 거북이 2484
안녕 거북이 2485
안녕 거북이 2486
안녕 거북이 2487
안녕 거북이 2488
안녕 거북이 2489
안녕 거북이 2490
안녕 거북이 2491
안녕 거북이 2492
안녕 거북이 2493
안녕 거북이 2494
안녕 거북이 2495
안녕 거북이 2496
안녕 거북이 2497
안녕 거북이 2498
안녕 거북이 2499
안녕 거북이 2500
안녕 거북이 2501
안녕 거북이 2502
안녕 거북이 2503
안녕 거북이 2504
안녕 거북이 2505
안녕 거북이 2506
안녕 거북이 2507
안녕 거북이 2508
안녕 거북이 2509
안녕 거북이 2510
안녕 거북이 2511
안녕 거북이 2512
안녕 거북이 2513
안녕 거북이 2514
안녕 거북이 2515
안녕 거북이 2516
안녕 거북이 2517
안녕 거북이 2518
안녕 거북이 2519
안녕 거북이 2520
안녕 거북이 2521
안녕 거북이 2522
안녕 거북이 2523
안녕 거북이 2524
안녕 거북이 2525
안녕 거북이 2526
안녕 거북이 2527
안녕 거북이 2528
안녕 거북이 2529
안녕 거북이 2530
안녕 거북이 2531
안녕 거북이 2532
안녕 거북이 2533
안녕 거북이 2534
안녕 거북이 2535
안녕 거북이 2536
안녕 거북이 2537
안녕 거북이 2538
안녕 거북이 2539
안녕 거북이 2540
안녕 거북이 2541
안녕 거북이 2542
안녕 거북이 2543
안녕 거북이 2544
안녕 거북이 2545
안녕 거북이 2546
안녕 거북이 2547
안녕 거북이 2548
안녕 거북이 2549
안녕 거북이 2550
안녕 거북이 2551
안녕 거북이 2552
안녕 거북이 2553
안녕 거북이 2554
안녕 거북이 2555
안녕 거북이 2556
안녕 거북이 2557
안녕 거북이 2558
안녕 거북이 2559
안녕 거북이 2560
안녕 거북이 2561
안녕 거북이 2562
안녕 거북이 2563
안녕 거북이 2564
안녕 거북이 2565
안녕 거북이 2566
안녕 거북이 2567
안녕 거북이 2568
안녕 거북이 2569
안녕 거북이 2570
안녕 거북이 2571
안녕 거북이 2572
안녕 거북이 2573
안녕 거북이 2574
안녕 거북이 2575
안녕 거북이 2576
안녕 거북이 2577
안녕 거북이 2578
안녕 거북이 2579
안녕 거북이 2580
안녕 거북이 2581
안녕 거북이 2582
안녕 거북이 2583
안녕 거북이 2584
안녕 거북이 2585
안녕 거북이 2586
안녕 거북이 2587
안녕 거북이 2588
안녕 거북이 2589
안녕 거북이 2590
안녕 거북이 2591
안녕 거북이 2592
안녕 거북이 2593
안녕 거북이 2594
안녕 거북이 2595
안녕 거북이 2596
안녕 거북이 2597
안녕 거북이 2598
안녕 거북이 2599
안녕 거북이 2600
안녕 거북이 2601
안녕 거북이 2602
안녕 거북이 2603
안녕 거북이 2604
안녕 거북이 2605
안녕 거북이 2606
안녕 거북이 2607
안녕 거북이 2608
안녕 거북이 2609
안녕 거북이 2610
안녕 거북이 2611
안녕 거북이 2612
안녕 거북이 2613
안녕 거북이 2614
안녕 거북이 2615
안녕 거북이 2616
안녕 거북이 2617
안녕 거북이 2618
안녕 거북이 2619
안녕 거북이 2620
안녕 거북이 2621
안녕 거북이 2622
안녕 거북이 2623
안녕 거북이 2624
안녕 거북이 2625
안녕 거북이 2626
안녕 거북이 2627
안녕 거북이 2628
안녕 거북이 2629
안녕 거북이 2630
안녕 거북이 2631
안녕 거북이 2632
안녕 거북이 2633
안녕 거북이 2634
안녕 거북이 2635
안녕 거북이 2636
안녕 거북이 2637
안녕 거북이 2638
안녕 거북이 2639
안녕 거북이 2640
안녕 거북이 2641
안녕 거북이 2642
안녕 거북이 2643
안녕 거북이 2644
안녕 거북이 2645
안녕 거북이 2646
안녕 거북이 2647
안녕 거북이 2648
안녕 거북이 2649
안녕 거북이 2650
안녕 거북이 2651
안녕 거북이 2652
안녕 거북이 2653
안녕 거북이 2654
안녕 거북이 2655
안녕 거북이 2656
안녕 거북이 2657
안녕 거북이 2658
안녕 거북이 2659
안녕 거북이 2660
안녕 거북이 2661
안녕 거북이 2662
안녕 거북이 2663
안녕 거북이 2664
안녕 거북이 2665
안녕 거북이 2666
안녕 거북이 2667
안녕 거북이 2668
안녕 거북이 2669
안녕 거북이 2670
안녕 거북이 2671
안녕 거북이 2672
안녕 거북이 2673
안녕 거북이 2674
안녕 거북이 2675
안녕 거북이 2676
안녕 거북이 2677
안녕 거북이 2678
안녕 거북이 2679
안녕 거북이 2680
안녕 거북이 2681
안녕 거북이 2682
안녕 거북이 2683
안녕 거북이 2684
안녕 거북이 2685
안녕 거북이 2686
안녕 거북이 2687
안녕 거북이 2688
안녕 거북이 2689
안녕 거북이 2690
안녕 거북이 2691
안녕 거북이 2692
안녕 거북이 2693
안녕 거북이 2694
안녕 거북이 2695
안녕 거북이 2696
안녕 거북이 2697
안녕 거북이 2698
안녕 거북이 2699
안녕 거북이 2700
안녕 거북이 2701
안녕 거북이 2702
안녕 거북이 2703
안녕 거북이 2704
안녕 거북이 2705
안녕 거북이 2706
안녕 거북이 2707
안녕 거북이 2708
안녕 거북이 2709
안녕 거북이 2710
안녕 거북이 2711
안녕 거북이 2712
안녕 거북이 2713
안녕 거북이 2714
안녕 거북이 2715
안녕 거북이 2716
안녕 거북이 2717
안녕 거북이 2718
안녕 거북이 2719
안녕 거북이 2720
안녕 거북이 2721
안녕 거북이 2722
안녕 거북이 2723
안녕 거북이 2724
안녕 거북이 2725
안녕 거북이 2726
안녕 거북이 2727
안녕 거북이 2728
안녕 거북이 2729
안녕 거북이 2730
안녕 거북이 2731
안녕 거북이 2732
안녕 거북이 2733
안녕 거북이 2734
안녕 거북이 2735
안녕 거북이 2736
안녕 거북이 2737
안녕 거북이 2738
안녕 거북이 2739
안녕 거북이 2740
안녕 거북이 2741
안녕 거북이 2742
안녕 거북이 2743
안녕 거북이 2744
안녕 거북이 2745
안녕 거북이 2746
안녕 거북이 2747
안녕 거북이 2748
안녕 거북이 2749
안녕 거북이 2750
안녕 거북이 2751
안녕 거북이 2752
안녕 거북이 2753
안녕 거북이 2754
안녕 거북이 2755
안녕 거북이 2756
안녕 거북이 2757
안녕 거북이 2758
안녕 거북이 2759
안녕 거북이 2760
안녕 거북이 2761
안녕 거북이 2762
안녕 거북이 2763
안녕 거북이 2764
안녕 거북이 2765
안녕 거북이 2766
안녕 거북이 2767
안녕 거북이 2768
안녕 거북이 2769
안녕 거북이 2770
안녕 거북이 2771
안녕 거북이 2772
안녕 거북이 2773
안녕 거북이 2774
안녕 거북이 2775
안녕 거북이 2776
안녕 거북이 2777
안녕 거북이 2778
안녕 거북이 2779
안녕 거북이 2780
안녕 거북이 2781
안녕 거북이 2782
안녕 거북이 2783
안녕 거북이 2784
안녕 거북이 2785
안녕 거북이 2786
안녕 거북이 2787
안녕 거북이 2788
안녕 거북이 2789
안녕 거북이 2790
안녕 거북이 2791
안녕 거북이 2792
안녕 거북이 2793
안녕 거북이 2794
안녕 거북이 2795
안녕 거북이 2796
안녕 거북이 2797
안녕 거북이 2798
안녕 거북이 2799
안녕 거북이 2800
안녕 거북이 2801
안녕 거북이 2802
안녕 거북이 2803
안녕 거북이 2804
안녕 거북이 2805
안녕 거북이 2806
안녕 거북이 2807
안녕 거북이 2808
안녕 거북이 2809
안녕 거북이 2810
안녕 거북이 2811
안녕 거북이 2812
안녕 거북이 2813
안녕 거북이 2814
안녕 거북이 2815
안녕 거북이 2816
안녕 거북이 2817
안녕 거북이 2818
안녕 거북이 2819
안녕 거북이 2820
안녕 거북이 2821
안녕 거북이 2822
안녕 거북이 2823
안녕 거북이 2824
안녕 거북이 2825
안녕 거북이 2826
안녕 거북이 2827
안녕 거북이 2828
안녕 거북이 2829
안녕 거북이 2830
안녕 거북이 2831
안녕 거북이 2832
안녕 거북이 2833
안녕 거북이 2834
안녕 거북이 2835
안녕 거북이 2836
안녕 거북이 2837
안녕 거북이 2838
안녕 거북이 2839
안녕 거북이 2840
안녕 거북이 2841
안녕 거북이 2842
안녕 거북이 2843
안녕 거북이 2844
안녕 거북이 2845
안녕 거북이 2846
안녕 거북이 2847
안녕 거북이 2848
안녕 거북이 2849
안녕 거북이 2850
안녕 거북이 2851
안녕 거북이 2852
안녕 거북이 2853
안녕 거북이 2854
안녕 거북이 2855
안녕 거북이 2856
안녕 거북이 2857
안녕 거북이 2858
안녕 거북이 2859
안녕 거북이 2860
안녕 거북이 2861
안녕 거북이 2862
안녕 거북이 2863
안녕 거북이 2864
안녕 거북이 2865
안녕 거북이 2866
안녕 거북이 2867
안녕 거북이 2868
안녕 거북이 2869
안녕 거북이 2870
안녕 거북이 2871
안녕 거북이 2872
안녕 거북이 2873
안녕 거북이 2874
안녕 거북이 2875
안녕 거북이 2876
안녕 거북이 2877
안녕 거북이 2878
안녕 거북이 2879
안녕 거북이 2880
안녕 거북이 2881
안녕 거북이 2882
안녕 거북이 2883
안녕 거북이 2884
안녕 거북이 2885
안녕 거북이 2886
안녕 거북이 2887
안녕 거북이 2888
안녕 거북이 2889
안녕 거북이 2890
안녕 거북이 2891
안녕 거북이 2892
안녕 거북이 2893
안녕 거북이 2894
안녕 거북이 2895
안녕 거북이 2896
안녕 거북이 2897
안녕 거북이 2898
안녕 거북이 2899
안녕 거북이 2900
안녕 거북이 2901
안녕 거북이 2902
안녕 거북이 2903
안녕 거북이 2904
안녕 거북이 2905
안녕 거북이 2906
안녕 거북이 2907
안녕 거북이 2908
안녕 거북이 2909
안녕 거북이 2910
안녕 거북이 2911
안녕 거북이 2912
안녕 거북이 2913
안녕 거북이 2914
안녕 거북이 2915
안녕 거북이 2916
안녕 거북이 2917
안녕 거북이 2918
안녕 거북이 2919
안녕 거북이 2920
안녕 거북이 2921
안녕 거북이 2922
안녕 거북이 2923
안녕 거북이 2924
안녕 거북이 2925
안녕 거북이 2926
안녕 거북이 2927
안녕 거북이 2928
안녕 거북이 2929
안녕 거북이 2930
안녕 거북이 2931
안녕 거북이 2932
안녕 거북이 2933
안녕 거북이 2934
안녕 거북이 2935
안녕 거북이 2936
안녕 거북이 2937
안녕 거북이 2938
안녕 거북이 2939
안녕 거북이 2940
안녕 거북이 2941
안녕 거북이 2942
안녕 거북이 2943
안녕 거북이 2944
안녕 거북이 2945
안녕 거북이 2946
안녕 거북이 2947
안녕 거북이 2948
안녕 거북이 2949
안녕 거북이 2950
안녕 거북이 2951
안녕 거북이 2952
안녕 거북이 2953
안녕 거북이 2954
안녕 거북이 2955
안녕 거북이 2956
안녕 거북이 2957
안녕 거북이 2958
안녕 거북이 2959
안녕 거북이 2960
안녕 거북이 2961
안녕 거북이 2962
안녕 거북이 2963
안녕 거북이 2964
안녕 거북이 2965
안녕 거북이 2966
안녕 거북이 2967
안녕 거북이 2968
안녕 거북이 2969
안녕 거북이 2970
안녕 거북이 2971
안녕 거북이 2972
안녕 거북이 2973
안녕 거북이 2974
안녕 거북이 2975
안녕 거북이 2976
안녕 거북이 2977
안녕 거북이 2978
안녕 거북이 2979
안녕 거북이 2980
안녕 거북이 2981
안녕 거북이 2982
안녕 거북이 2983
안녕 거북이 2984
안녕 거북이 2985
안녕 거북이 2986
안녕 거북이 2987
안녕 거북이 2988
안녕 거북이 2989
안녕 거북이 2990
안녕 거북이 2991
안녕 거북이 2992
안녕 거북이 2993
안녕 거북이 2994
안녕 거북이 2995
안녕 거북이 2996
안녕 거북이 2997
안녕 거북이 2998
안녕 거북이 2999
안녕 거북이 3000
안녕 거북이 3001
안녕 거북이 3002
안녕 거북이 3003
안녕 거북이 3004
안녕 거북이 3005
안녕 거북이 3006
안녕 거북이 3007
안녕 거북이 3008
안녕 거북이 3009
안녕 거북이 3010
안녕 거북이 3011
안녕 거북이 3012
안녕 거북이 3013
안녕 거북이 3014
안녕 거북이 3015
안녕 거북이 3016
안녕 거북이 3017
안녕 거북이 3018
안녕 거북이 3019
안녕 거북이 3020
안녕 거북이 3021
안녕 거북이 3022
안녕 거북이 3023
안녕 거북이 3024
안녕 거북이 3025
안녕 거북이 3026
안녕 거북이 3027
안녕 거북이 3028
안녕 거북이 3029
안녕 거북이 3030
안녕 거북이 3031
안녕 거북이 3032
안녕 거북이 3033
안녕 거북이 3034
안녕 거북이 3035
안녕 거북이 3036
안녕 거북이 3037
안녕 거북이 3038
안녕 거북이 3039
안녕 거북이 3040
안녕 거북이 3041
안녕 거북이 3042
안녕 거북이 3043
안녕 거북이 3044
안녕 거북이 3045
안녕 거북이 3046
안녕 거북이 3047
안녕 거북이 3048
안녕 거북이 3049
안녕 거북이 3050
안녕 거북이 3051
안녕 거북이 3052
안녕 거북이 3053
안녕 거북이 3054
안녕 거북이 3055
안녕 거북이 3056
안녕 거북이 3057
안녕 거북이 3058
안녕 거북이 3059
안녕 거북이 3060
안녕 거북이 3061
안녕 거북이 3062
안녕 거북이 3063
안녕 거북이 3064
안녕 거북이 3065
안녕 거북이 3066
안녕 거북이 3067
안녕 거북이 3068
안녕 거북이 3069
안녕 거북이 3070
안녕 거북이 3071
안녕 거북이 3072
안녕 거북이 3073
안녕 거북이 3074
안녕 거북이 3075
안녕 거북이 3076
안녕 거북이 3077
안녕 거북이 3078
안녕 거북이 3079
안녕 거북이 3080
안녕 거북이 3081
안녕 거북이 3082
안녕 거북이 3083
안녕 거북이 3084
안녕 거북이 3085
안녕 거북이 3086
안녕 거북이 3087
안녕 거북이 3088
안녕 거북이 3089
안녕 거북이 3090
안녕 거북이 3091
안녕 거북이 3092
안녕 거북이 3093
안녕 거북이 3094
안녕 거북이 3095
안녕 거북이 3096
안녕 거북이 3097
안녕 거북이 3098
안녕 거북이 3099
안녕 거북이 3100
안녕 거북이 3101
안녕 거북이 3102
안녕 거북이 3103
안녕 거북이 3104
안녕 거북이 3105
안녕 거북이 3106
안녕 거북이 3107
안녕 거북이 3108
안녕 거북이 3109
안녕 거북이 3110
안녕 거북이 3111
안녕 거북이 3112
안녕 거북이 3113
안녕 거북이 3114
안녕 거북이 3115
안녕 거북이 3116
안녕 거북이 3117
안녕 거북이 3118
안녕 거북이 3119
안녕 거북이 3120
안녕 거북이 3121
안녕 거북이 3122
안녕 거북이 3123
안녕 거북이 3124
안녕 거북이 3125
안녕 거북이 3126
안녕 거북이 3127
안녕 거북이 3128
안녕 거북이 3129
안녕 거북이 3130
안녕 거북이 3131
안녕 거북이 3132
안녕 거북이 3133
안녕 거북이 3134
안녕 거북이 3135
안녕 거북이 3136
안녕 거북이 3137
안녕 거북이 3138
안녕 거북이 3139
안녕 거북이 3140
안녕 거북이 3141
안녕 거북이 3142
안녕 거북이 3143
안녕 거북이 3144
안녕 거북이 3145
안녕 거북이 3146
안녕 거북이 3147
안녕 거북이 3148
안녕 거북이 3149
안녕 거북이 3150
안녕 거북이 3151
안녕 거북이 3152
안녕 거북이 3153
안녕 거북이 3154
안녕 거북이 3155
안녕 거북이 3156
안녕 거북이 3157
안녕 거북이 3158
안녕 거북이 3159
안녕 거북이 3160
안녕 거북이 3161
안녕 거북이 3162
안녕 거북이 3163
안녕 거북이 3164
안녕 거북이 3165
안녕 거북이 3166
안녕 거북이 3167
안녕 거북이 3168
안녕 거북이 3169
안녕 거북이 3170
안녕 거북이 3171
안녕 거북이 3172
안녕 거북이 3173
안녕 거북이 3174
안녕 거북이 3175
안녕 거북이 3176
안녕 거북이 3177
안녕 거북이 3178
안녕 거북이 3179
안녕 거북이 3180
안녕 거북이 3181
안녕 거북이 3182
안녕 거북이 3183
안녕 거북이 3184
안녕 거북이 3185
안녕 거북이 3186
안녕 거북이 3187
안녕 거북이 3188
안녕 거북이 3189
안녕 거북이 3190
안녕 거북이 3191
안녕 거북이 3192
안녕 거북이 3193
안녕 거북이 3194
안녕 거북이 3195
안녕 거북이 3196
안녕 거북이 3197
안녕 거북이 3198
안녕 거북이 3199
안녕 거북이 3200
안녕 거북이 3201
안녕 거북이 3202
안녕 거북이 3203
안녕 거북이 3204
안녕 거북이 3205
안녕 거북이 3206
안녕 거북이 3207
안녕 거북이 3208
안녕 거북이 3209
안녕 거북이 3210
안녕 거북이 3211
안녕 거북이 3212
안녕 거북이 3213
안녕 거북이 3214
안녕 거북이 3215
안녕 거북이 3216
안녕 거북이 3217
안녕 거북이 3218
안녕 거북이 3219
안녕 거북이 3220
안녕 거북이 3221
안녕 거북이 3222
안녕 거북이 3223
안녕 거북이 3224
안녕 거북이 3225
안녕 거북이 3226
안녕 거북이 3227
안녕 거북이 3228
안녕 거북이 3229
안녕 거북이 3230
안녕 거북이 3231
안녕 거북이 3232
안녕 거북이 3233
안녕 거북이 3234
안녕 거북이 3235
안녕 거북이 3236
안녕 거북이 3237
안녕 거북이 3238
안녕 거북이 3239
안녕 거북이 3240
안녕 거북이 3241
안녕 거북이 3242
안녕 거북이 3243
안녕 거북이 3244
안녕 거북이 3245
안녕 거북이 3246
안녕 거북이 3247
안녕 거북이 3248
안녕 거북이 3249
안녕 거북이 3250
안녕 거북이 3251
안녕 거북이 3252
안녕 거북이 3253
안녕 거북이 3254
안녕 거북이 3255
안녕 거북이 3256
안녕 거북이 3257
안녕 거북이 3258
안녕 거북이 3259
안녕 거북이 3260
안녕 거북이 3261
안녕 거북이 3262
안녕 거북이 3263
안녕 거북이 3264
안녕 거북이 3265
안녕 거북이 3266
안녕 거북이 3267
안녕 거북이 3268
안녕 거북이 3269
안녕 거북이 3270
안녕 거북이 3271
안녕 거북이 3272
안녕 거북이 3273
안녕 거북이 3274
안녕 거북이 3275
안녕 거북이 3276
안녕 거북이 3277
안녕 거북이 3278
안녕 거북이 3279
안녕 거북이 3280
안녕 거북이 3281
안녕 거북이 3282
안녕 거북이 3283
안녕 거북이 3284
안녕 거북이 3285
안녕 거북이 3286
안녕 거북이 3287
안녕 거북이 3288
안녕 거북이 3289
안녕 거북이 3290
안녕 거북이 3291
안녕 거북이 3292
안녕 거북이 3293
안녕 거북이 3294
안녕 거북이 3295
안녕 거북이 3296
안녕 거북이 3297
안녕 거북이 3298
안녕 거북이 3299
안녕 거북이 3300
안녕 거북이 3301
안녕 거북이 3302
안녕 거북이 3303
안녕 거북이 3304
안녕 거북이 3305
안녕 거북이 3306
안녕 거북이 3307
안녕 거북이 3308
안녕 거북이 3309
안녕 거북이 3310
안녕 거북이 3311
안녕 거북이 3312
안녕 거북이 3313
안녕 거북이 3314
안녕 거북이 3315
안녕 거북이 3316
안녕 거북이 3317
안녕 거북이 3318
안녕 거북이 3319
안녕 거북이 3320
안녕 거북이 3321
안녕 거북이 3322
안녕 거북이 3323
안녕 거북이 3324
안녕 거북이 3325
안녕 거북이 3326
안녕 거북이 3327
안녕 거북이 3328
안녕 거북이 3329
안녕 거북이 3330
안녕 거북이 3331
안녕 거북이 3332
안녕 거북이 3333
안녕 거북이 3334
안녕 거북이 3335
안녕 거북이 3336
안녕 거북이 3337
안녕 거북이 3338
안녕 거북이 3339
안녕 거북이 3340
안녕 거북이 3341
안녕 거북이 3342
안녕 거북이 3343
안녕 거북이 3344
안녕 거북이 3345
안녕 거북이 3346
안녕 거북이 3347
안녕 거북이 3348
안녕 거북이 3349
안녕 거북이 3350
안녕 거북이 3351
안녕 거북이 3352
안녕 거북이 3353
안녕 거북이 3354
안녕 거북이 3355
안녕 거북이 3356
안녕 거북이 3357
안녕 거북이 3358
안녕 거북이 3359
안녕 거북이 3360
안녕 거북이 3361
안녕 거북이 3362
안녕 거북이 3363
안녕 거북이 3364
안녕 거북이 3365
안녕 거북이 3366
안녕 거북이 3367
안녕 거북이 3368
안녕 거북이 3369
안녕 거북이 3370
안녕 거북이 3371
안녕 거북이 3372
안녕 거북이 3373
안녕 거북이 3374
안녕 거북이 3375
안녕 거북이 3376
안녕 거북이 3377
안녕 거북이 3378
안녕 거북이 3379
안녕 거북이 3380
안녕 거북이 3381
안녕 거북이 3382
안녕 거북이 3383
안녕 거북이 3384
안녕 거북이 3385
안녕 거북이 3386
안녕 거북이 3387
안녕 거북이 3388
안녕 거북이 3389
안녕 거북이 3390
안녕 거북이 3391
안녕 거북이 3392
안녕 거북이 3393
안녕 거북이 3394
안녕 거북이 3395
안녕 거북이 3396
안녕 거북이 3397
안녕 거북이 3398
안녕 거북이 3399
안녕 거북이 3400
안녕 거북이 3401
안녕 거북이 3402
안녕 거북이 3403
안녕 거북이 3404
안녕 거북이 3405
안녕 거북이 3406
안녕 거북이 3407
안녕 거북이 3408
안녕 거북이 3409
안녕 거북이 3410
안녕 거북이 3411
안녕 거북이 3412
안녕 거북이 3413
안녕 거북이 3414
안녕 거북이 3415
안녕 거북이 3416
안녕 거북이 3417
안녕 거북이 3418
안녕 거북이 3419
안녕 거북이 3420
안녕 거북이 3421
안녕 거북이 3422
안녕 거북이 3423
안녕 거북이 3424
안녕 거북이 3425
안녕 거북이 3426
안녕 거북이 3427
안녕 거북이 3428
안녕 거북이 3429
안녕 거북이 3430
안녕 거북이 3431
안녕 거북이 3432
안녕 거북이 3433
안녕 거북이 3434
안녕 거북이 3435
안녕 거북이 3436
안녕 거북이 3437
안녕 거북이 3438
안녕 거북이 3439
안녕 거북이 3440
안녕 거북이 3441
안녕 거북이 3442
안녕 거북이 3443
안녕 거북이 3444
안녕 거북이 3445
안녕 거북이 3446
안녕 거북이 3447
안녕 거북이 3448
안녕 거북이 3449
안녕 거북이 3450
안녕 거북이 3451
안녕 거북이 3452
안녕 거북이 3453
안녕 거북이 3454
안녕 거북이 3455
안녕 거북이 3456
안녕 거북이 3457
안녕 거북이 3458
안녕 거북이 3459
안녕 거북이 3460
안녕 거북이 3461
안녕 거북이 3462
안녕 거북이 3463
안녕 거북이 3464
안녕 거북이 3465
안녕 거북이 3466
안녕 거북이 3467
안녕 거북이 3468
안녕 거북이 3469
안녕 거북이 3470
안녕 거북이 3471
안녕 거북이 3472
안녕 거북이 3473
안녕 거북이 3474
안녕 거북이 3475
안녕 거북이 3476
안녕 거북이 3477
안녕 거북이 3478
안녕 거북이 3479
안녕 거북이 3480
안녕 거북이 3481
안녕 거북이 3482
안녕 거북이 3483
안녕 거북이 3484
안녕 거북이 3485
안녕 거북이 3486
안녕 거북이 3487
안녕 거북이 3488
안녕 거북이 3489
안녕 거북이 3490
안녕 거북이 3491
안녕 거북이 3492
안녕 거북이 3493
안녕 거북이 3494
안녕 거북이 3495
안녕 거북이 3496
안녕 거북이 3497
안녕 거북이 3498
안녕 거북이 3499
안녕 거북이 3500
안녕 거북이 3501
안녕 거북이 3502
안녕 거북이 3503
안녕 거북이 3504
안녕 거북이 3505
안녕 거북이 3506
안녕 거북이 3507
안녕 거북이 3508
안녕 거북이 3509
안녕 거북이 3510
안녕 거북이 3511
안녕 거북이 3512
안녕 거북이 3513
안녕 거북이 3514
안녕 거북이 3515
안녕 거북이 3516
안녕 거북이 3517
안녕 거북이 3518
안녕 거북이 3519
안녕 거북이 3520
안녕 거북이 3521
안녕 거북이 3522
안녕 거북이 3523
안녕 거북이 3524
안녕 거북이 3525
안녕 거북이 3526
안녕 거북이 3527
안녕 거북이 3528
안녕 거북이 3529
안녕 거북이 3530
안녕 거북이 3531
안녕 거북이 3532
안녕 거북이 3533
안녕 거북이 3534
안녕 거북이 3535
안녕 거북이 3536
안녕 거북이 3537
안녕 거북이 3538
안녕 거북이 3539
안녕 거북이 3540
안녕 거북이 3541
안녕 거북이 3542
안녕 거북이 3543
안녕 거북이 3544
안녕 거북이 3545
안녕 거북이 3546
안녕 거북이 3547
안녕 거북이 3548
안녕 거북이 3549
안녕 거북이 3550
안녕 거북이 3551
안녕 거북이 3552
안녕 거북이 3553
안녕 거북이 3554
안녕 거북이 3555
안녕 거북이 3556
안녕 거북이 3557
안녕 거북이 3558
안녕 거북이 3559
안녕 거북이 3560
안녕 거북이 3561
안녕 거북이 3562
안녕 거북이 3563
안녕 거북이 3564
안녕 거북이 3565
안녕 거북이 3566
안녕 거북이 3567
안녕 거북이 3568
안녕 거북이 3569
안녕 거북이 3570
안녕 거북이 3571
안녕 거북이 3572
안녕 거북이 3573
안녕 거북이 3574
안녕 거북이 3575
안녕 거북이 3576
안녕 거북이 3577
안녕 거북이 3578
안녕 거북이 3579
안녕 거북이 3580
안녕 거북이 3581
안녕 거북이 3582
안녕 거북이 3583
안녕 거북이 3584
안녕 거북이 3585
안녕 거북이 3586
안녕 거북이 3587
안녕 거북이 3588
안녕 거북이 3589
안녕 거북이 3590
안녕 거북이 3591
안녕 거북이 3592
안녕 거북이 3593
안녕 거북이 3594
안녕 거북이 3595
안녕 거북이 3596
안녕 거북이 3597
안녕 거북이 3598
안녕 거북이 3599
안녕 거북이 3600
안녕 거북이 3601
안녕 거북이 3602
안녕 거북이 3603
안녕 거북이 3604
안녕 거북이 3605
안녕 거북이 3606
안녕 거북이 3607
안녕 거북이 3608
안녕 거북이 3609
안녕 거북이 3610
안녕 거북이 3611
안녕 거북이 3612
안녕 거북이 3613
안녕 거북이 3614
안녕 거북이 3615
안녕 거북이 3616
안녕 거북이 3617
안녕 거북이 3618
안녕 거북이 3619
안녕 거북이 3620
안녕 거북이 3621
안녕 거북이 3622
안녕 거북이 3623
안녕 거북이 3624
안녕 거북이 3625
안녕 거북이 3626
안녕 거북이 3627
안녕 거북이 3628
안녕 거북이 3629
안녕 거북이 3630
안녕 거북이 3631
안녕 거북이 3632
안녕 거북이 3633
안녕 거북이 3634
안녕 거북이 3635
안녕 거북이 3636
안녕 거북이 3637
안녕 거북이 3638
안녕 거북이 3639
안녕 거북이 3640
안녕 거북이 3641
안녕 거북이 3642
안녕 거북이 3643
안녕 거북이 3644
안녕 거북이 3645
안녕 거북이 3646
안녕 거북이 3647
안녕 거북이 3648
안녕 거북이 3649
안녕 거북이 3650
안녕 거북이 3651
안녕 거북이 3652
안녕 거북이 3653
안녕 거북이 3654
안녕 거북이 3655
안녕 거북이 3656
안녕 거북이 3657
안녕 거북이 3658
안녕 거북이 3659
안녕 거북이 3660
안녕 거북이 3661
안녕 거북이 3662
안녕 거북이 3663
안녕 거북이 3664
안녕 거북이 3665
안녕 거북이 3666
안녕 거북이 3667
안녕 거북이 3668
안녕 거북이 3669
안녕 거북이 3670
안녕 거북이 3671
안녕 거북이 3672
안녕 거북이 3673
안녕 거북이 3674
안녕 거북이 3675
안녕 거북이 3676
안녕 거북이 3677
안녕 거북이 3678
안녕 거북이 3679
안녕 거북이 3680
안녕 거북이 3681
안녕 거북이 3682
안녕 거북이 3683
안녕 거북이 3684
안녕 거북이 3685
안녕 거북이 3686
안녕 거북이 3687
안녕 거북이 3688
안녕 거북이 3689
안녕 거북이 3690
안녕 거북이 3691
안녕 거북이 3692
안녕 거북이 3693
안녕 거북이 3694
안녕 거북이 3695
안녕 거북이 3696
안녕 거북이 3697
안녕 거북이 3698
안녕 거북이 3699
안녕 거북이 3700
안녕 거북이 3701
안녕 거북이 3702
안녕 거북이 3703
안녕 거북이 3704
안녕 거북이 3705
안녕 거북이 3706
안녕 거북이 3707
안녕 거북이 3708
안녕 거북이 3709
안녕 거북이 3710
안녕 거북이 3711
안녕 거북이 3712
안녕 거북이 3713
안녕 거북이 3714
안녕 거북이 3715
안녕 거북이 3716
안녕 거북이 3717
안녕 거북이 3718
안녕 거북이 3719
안녕 거북이 3720
안녕 거북이 3721
안녕 거북이 3722
안녕 거북이 3723
안녕 거북이 3724
안녕 거북이 3725
안녕 거북이 3726
안녕 거북이 3727
안녕 거북이 3728
안녕 거북이 3729
안녕 거북이 3730
안녕 거북이 3731
안녕 거북이 3732
안녕 거북이 3733
안녕 거북이 3734
안녕 거북이 3735
안녕 거북이 3736
안녕 거북이 3737
안녕 거북이 3738
안녕 거북이 3739
안녕 거북이 3740
안녕 거북이 3741
안녕 거북이 3742
안녕 거북이 3743
안녕 거북이 3744
안녕 거북이 3745
안녕 거북이 3746
안녕 거북이 3747
안녕 거북이 3748
안녕 거북이 3749
안녕 거북이 3750
안녕 거북이 3751
안녕 거북이 3752
안녕 거북이 3753
안녕 거북이 3754
안녕 거북이 3755
안녕 거북이 3756
안녕 거북이 3757
안녕 거북이 3758
안녕 거북이 3759
안녕 거북이 3760
안녕 거북이 3761
안녕 거북이 3762
안녕 거북이 3763
안녕 거북이 3764
안녕 거북이 3765
안녕 거북이 3766
안녕 거북이 3767
안녕 거북이 3768
안녕 거북이 3769
안녕 거북이 3770
안녕 거북이 3771
안녕 거북이 3772
안녕 거북이 3773
안녕 거북이 3774
안녕 거북이 3775
안녕 거북이 3776
안녕 거북이 3777
안녕 거북이 3778
안녕 거북이 3779
안녕 거북이 3780
안녕 거북이 3781
안녕 거북이 3782
안녕 거북이 3783
안녕 거북이 3784
안녕 거북이 3785
안녕 거북이 3786
안녕 거북이 3787
안녕 거북이 3788
안녕 거북이 3789
안녕 거북이 3790
안녕 거북이 3791
안녕 거북이 3792
안녕 거북이 3793
안녕 거북이 3794
안녕 거북이 3795
안녕 거북이 3796
안녕 거북이 3797
안녕 거북이 3798
안녕 거북이 3799
안녕 거북이 3800
안녕 거북이 3801
안녕 거북이 3802
안녕 거북이 3803
안녕 거북이 3804
안녕 거북이 3805
안녕 거북이 3806
안녕 거북이 3807
안녕 거북이 3808
안녕 거북이 3809
안녕 거북이 3810
안녕 거북이 3811
안녕 거북이 3812
안녕 거북이 3813
안녕 거북이 3814
안녕 거북이 3815
안녕 거북이 3816
안녕 거북이 3817
안녕 거북이 3818
안녕 거북이 3819
안녕 거북이 3820
안녕 거북이 3821
안녕 거북이 3822
안녕 거북이 3823
안녕 거북이 3824
안녕 거북이 3825
안녕 거북이 3826
안녕 거북이 3827
안녕 거북이 3828
안녕 거북이 3829
안녕 거북이 3830
안녕 거북이 3831
안녕 거북이 3832
안녕 거북이 3833
안녕 거북이 3834
안녕 거북이 3835
안녕 거북이 3836
안녕 거북이 3837
안녕 거북이 3838
안녕 거북이 3839
안녕 거북이 3840
안녕 거북이 3841
안녕 거북이 3842
안녕 거북이 3843
안녕 거북이 3844
안녕 거북이 3845
안녕 거북이 3846
안녕 거북이 3847
안녕 거북이 3848
안녕 거북이 3849
안녕 거북이 3850
안녕 거북이 3851
안녕 거북이 3852
안녕 거북이 3853
안녕 거북이 3854
안녕 거북이 3855
안녕 거북이 3856
안녕 거북이 3857
안녕 거북이 3858
안녕 거북이 3859
안녕 거북이 3860
안녕 거북이 3861
안녕 거북이 3862
안녕 거북이 3863
안녕 거북이 3864
안녕 거북이 3865
안녕 거북이 3866
안녕 거북이 3867
안녕 거북이 3868
안녕 거북이 3869
안녕 거북이 3870
안녕 거북이 3871
안녕 거북이 3872
안녕 거북이 3873
안녕 거북이 3874
안녕 거북이 3875
안녕 거북이 3876
안녕 거북이 3877
안녕 거북이 3878
안녕 거북이 3879
안녕 거북이 3880
안녕 거북이 3881
안녕 거북이 3882
안녕 거북이 3883
안녕 거북이 3884
안녕 거북이 3885
안녕 거북이 3886
안녕 거북이 3887
안녕 거북이 3888
안녕 거북이 3889
안녕 거북이 3890
안녕 거북이 3891
안녕 거북이 3892
안녕 거북이 3893
안녕 거북이 3894
안녕 거북이 3895
안녕 거북이 3896
안녕 거북이 3897
안녕 거북이 3898
안녕 거북이 3899
안녕 거북이 3900
안녕 거북이 3901
안녕 거북이 3902
안녕 거북이 3903
안녕 거북이 3904
안녕 거북이 3905
안녕 거북이 3906
안녕 거북이 3907
안녕 거북이 3908
안녕 거북이 3909
안녕 거북이 3910
안녕 거북이 3911
안녕 거북이 3912
안녕 거북이 3913
안녕 거북이 3914
안녕 거북이 3915
안녕 거북이 3916
안녕 거북이 3917
안녕 거북이 3918
안녕 거북이 3919
안녕 거북이 3920
안녕 거북이 3921
안녕 거북이 3922
안녕 거북이 3923
안녕 거북이 3924
안녕 거북이 3925
안녕 거북이 3926
안녕 거북이 3927
안녕 거북이 3928
안녕 거북이 3929
안녕 거북이 3930
안녕 거북이 3931
안녕 거북이 3932
안녕 거북이 3933
안녕 거북이 3934
안녕 거북이 3935
안녕 거북이 3936
안녕 거북이 3937
안녕 거북이 3938
안녕 거북이 3939
안녕 거북이 3940
안녕 거북이 3941
안녕 거북이 3942
안녕 거북이 3943
안녕 거북이 3944
안녕 거북이 3945
안녕 거북이 3946
안녕 거북이 3947
안녕 거북이 3948
안녕 거북이 3949
안녕 거북이 3950
안녕 거북이 3951
안녕 거북이 3952
안녕 거북이 3953
안녕 거북이 3954
안녕 거북이 3955
안녕 거북이 3956
안녕 거북이 3957
안녕 거북이 3958
안녕 거북이 3959
안녕 거북이 3960
안녕 거북이 3961
안녕 거북이 3962
안녕 거북이 3963
안녕 거북이 3964
안녕 거북이 3965
안녕 거북이 3966
안녕 거북이 3967
안녕 거북이 3968
안녕 거북이 3969
안녕 거북이 3970
안녕 거북이 3971
안녕 거북이 3972
안녕 거북이 3973
안녕 거북이 3974
안녕 거북이 3975
안녕 거북이 3976
안녕 거북이 3977
안녕 거북이 3978
안녕 거북이 3979
안녕 거북이 3980
안녕 거북이 3981
안녕 거북이 3982
안녕 거북이 3983
안녕 거북이 3984
안녕 거북이 3985
안녕 거북이 3986
안녕 거북이 3987
안녕 거북이 3988
안녕 거북이 3989
안녕 거북이 3990
안녕 거북이 3991
안녕 거북이 3992
안녕 거북이 3993
안녕 거북이 3994
안녕 거북이 3995
안녕 거북이 3996
안녕 거북이 3997
안녕 거북이 3998
안녕 거북이 3999
안녕 거북이 4000
안녕 거북이 4001
안녕 거북이 4002
안녕 거북이 4003
안녕 거북이 4004
안녕 거북이 4005
안녕 거북이 4006
안녕 거북이 4007
안녕 거북이 4008
안녕 거북이 4009
안녕 거북이 4010
안녕 거북이 4011
안녕 거북이 4012
안녕 거북이 4013
안녕 거북이 4014
안녕 거북이 4015
안녕 거북이 4016
안녕 거북이 4017
안녕 거북이 4018
안녕 거북이 4019
안녕 거북이 4020
안녕 거북이 4021
안녕 거북이 4022
안녕 거북이 4023
안녕 거북이 4024
안녕 거북이 4025
안녕 거북이 4026
안녕 거북이 4027
안녕 거북이 4028
안녕 거북이 4029
안녕 거북이 4030
안녕 거북이 4031
안녕 거북이 4032
안녕 거북이 4033
안녕 거북이 4034
안녕 거북이 4035
안녕 거북이 4036
안녕 거북이 4037
안녕 거북이 4038
안녕 거북이 4039
안녕 거북이 4040
안녕 거북이 4041
안녕 거북이 4042
안녕 거북이 4043
안녕 거북이 4044
안녕 거북이 4045
안녕 거북이 4046
안녕 거북이 4047
안녕 거북이 4048
안녕 거북이 4049
안녕 거북이 4050
안녕 거북이 4051
안녕 거북이 4052
안녕 거북이 4053
안녕 거북이 4054
안녕 거북이 4055
안녕 거북이 4056
안녕 거북이 4057
안녕 거북이 4058
안녕 거북이 4059
안녕 거북이 4060
안녕 거북이 4061
안녕 거북이 4062
안녕 거북이 4063
안녕 거북이 4064
안녕 거북이 4065
안녕 거북이 4066
안녕 거북이 4067
안녕 거북이 4068
안녕 거북이 4069
안녕 거북이 4070
안녕 거북이 4071
안녕 거북이 4072
안녕 거북이 4073
안녕 거북이 4074
안녕 거북이 4075
안녕 거북이 4076
안녕 거북이 4077
안녕 거북이 4078
안녕 거북이 4079
안녕 거북이 4080
안녕 거북이 4081
안녕 거북이 4082
안녕 거북이 4083
안녕 거북이 4084
안녕 거북이 4085
안녕 거북이 4086
안녕 거북이 4087
안녕 거북이 4088
안녕 거북이 4089
안녕 거북이 4090
안녕 거북이 4091
안녕 거북이 4092
안녕 거북이 4093
안녕 거북이 4094
안녕 거북이 4095
안녕 거북이 4096
안녕 거북이 4097
안녕 거북이 4098
안녕 거북이 4099
안녕 거북이 4100
안녕 거북이 4101
안녕 거북이 4102
안녕 거북이 4103
안녕 거북이 4104
안녕 거북이 4105
안녕 거북이 4106
안녕 거북이 4107
안녕 거북이 4108
안녕 거북이 4109
안녕 거북이 4110
안녕 거북이 4111
안녕 거북이 4112
안녕 거북이 4113
안녕 거북이 4114
안녕 거북이 4115
안녕 거북이 4116
안녕 거북이 4117
안녕 거북이 4118
안녕 거북이 4119
안녕 거북이 4120
안녕 거북이 4121
안녕 거북이 4122
안녕 거북이 4123
안녕 거북이 4124
안녕 거북이 4125
안녕 거북이 4126
안녕 거북이 4127
안녕 거북이 4128
안녕 거북이 4129
안녕 거북이 4130
안녕 거북이 4131
안녕 거북이 4132
안녕 거북이 4133
안녕 거북이 4134
안녕 거북이 4135
안녕 거북이 4136
안녕 거북이 4137
안녕 거북이 4138
안녕 거북이 4139
안녕 거북이 4140
안녕 거북이 4141
안녕 거북이 4142
안녕 거북이 4143
안녕 거북이 4144
안녕 거북이 4145
안녕 거북이 4146
안녕 거북이 4147
안녕 거북이 4148
안녕 거북이 4149
안녕 거북이 4150
안녕 거북이 4151
안녕 거북이 4152
안녕 거북이 4153
안녕 거북이 4154
안녕 거북이 4155
안녕 거북이 4156
안녕 거북이 4157
안녕 거북이 4158
안녕 거북이 4159
안녕 거북이 4160
안녕 거북이 4161
안녕 거북이 4162
안녕 거북이 4163
안녕 거북이 4164
안녕 거북이 4165
안녕 거북이 4166
안녕 거북이 4167
안녕 거북이 4168
안녕 거북이 4169
안녕 거북이 4170
안녕 거북이 4171
안녕 거북이 4172
안녕 거북이 4173
안녕 거북이 4174
안녕 거북이 4175
안녕 거북이 4176
안녕 거북이 4177
안녕 거북이 4178
안녕 거북이 4179
안녕 거북이 4180
안녕 거북이 4181
안녕 거북이 4182
안녕 거북이 4183
안녕 거북이 4184
안녕 거북이 4185
안녕 거북이 4186
안녕 거북이 4187
안녕 거북이 4188
안녕 거북이 4189
안녕 거북이 4190
안녕 거북이 4191
안녕 거북이 4192
안녕 거북이 4193
안녕 거북이 4194
안녕 거북이 4195
안녕 거북이 4196
안녕 거북이 4197
안녕 거북이 4198
안녕 거북이 4199
안녕 거북이 4200
안녕 거북이 4201
안녕 거북이 4202
안녕 거북이 4203
안녕 거북이 4204
안녕 거북이 4205
안녕 거북이 4206
안녕 거북이 4207
안녕 거북이 4208
안녕 거북이 4209
안녕 거북이 4210
안녕 거북이 4211
안녕 거북이 4212
안녕 거북이 4213
안녕 거북이 4214
안녕 거북이 4215
안녕 거북이 4216
안녕 거북이 4217
안녕 거북이 4218
안녕 거북이 4219
안녕 거북이 4220
안녕 거북이 4221
안녕 거북이 4222
안녕 거북이 4223
안녕 거북이 4224
안녕 거북이 4225
안녕 거북이 4226
안녕 거북이 4227
안녕 거북이 4228
안녕 거북이 4229
안녕 거북이 4230
안녕 거북이 4231
안녕 거북이 4232
안녕 거북이 4233
안녕 거북이 4234
안녕 거북이 4235
안녕 거북이 4236
안녕 거북이 4237
안녕 거북이 4238
안녕 거북이 4239
안녕 거북이 4240
안녕 거북이 4241
안녕 거북이 4242
안녕 거북이 4243
안녕 거북이 4244
안녕 거북이 4245
안녕 거북이 4246
안녕 거북이 4247
안녕 거북이 4248
안녕 거북이 4249
안녕 거북이 4250
안녕 거북이 4251
안녕 거북이 4252
안녕 거북이 4253
안녕 거북이 4254
안녕 거북이 4255
안녕 거북이 4256
안녕 거북이 4257
안녕 거북이 4258
안녕 거북이 4259
안녕 거북이 4260
안녕 거북이 4261
안녕 거북이 4262
안녕 거북이 4263
안녕 거북이 4264
안녕 거북이 4265
안녕 거북이 4266
안녕 거북이 4267
안녕 거북이 4268
안녕 거북이 4269
안녕 거북이 4270
안녕 거북이 4271
안녕 거북이 4272
안녕 거북이 4273
안녕 거북이 4274
안녕 거북이 4275
안녕 거북이 4276
안녕 거북이 4277
안녕 거북이 4278
안녕 거북이 4279
안녕 거북이 4280
안녕 거북이 4281
안녕 거북이 4282
안녕 거북이 4283
안녕 거북이 4284
안녕 거북이 4285
안녕 거북이 4286
안녕 거북이 4287
안녕 거북이 4288
안녕 거북이 4289
안녕 거북이 4290
안녕 거북이 4291
안녕 거북이 4292
안녕 거북이 4293
안녕 거북이 4294
안녕 거북이 4295
안녕 거북이 4296
안녕 거북이 4297
안녕 거북이 4298
안녕 거북이 4299
안녕 거북이 4300
안녕 거북이 4301
안녕 거북이 4302
안녕 거북이 4303
안녕 거북이 4304
안녕 거북이 4305
안녕 거북이 4306
안녕 거북이 4307
안녕 거북이 4308
안녕 거북이 4309
안녕 거북이 4310
안녕 거북이 4311
안녕 거북이 4312
안녕 거북이 4313
안녕 거북이 4314
안녕 거북이 4315
안녕 거북이 4316
안녕 거북이 4317
안녕 거북이 4318
안녕 거북이 4319
안녕 거북이 4320
안녕 거북이 4321
안녕 거북이 4322
안녕 거북이 4323
안녕 거북이 4324
안녕 거북이 4325
안녕 거북이 4326
안녕 거북이 4327
안녕 거북이 4328
안녕 거북이 4329
안녕 거북이 4330
안녕 거북이 4331
안녕 거북이 4332
안녕 거북이 4333
안녕 거북이 4334
안녕 거북이 4335
안녕 거북이 4336
안녕 거북이 4337
안녕 거북이 4338
안녕 거북이 4339
안녕 거북이 4340
안녕 거북이 4341
안녕 거북이 4342
안녕 거북이 4343
안녕 거북이 4344
안녕 거북이 4345
안녕 거북이 4346
안녕 거북이 4347
안녕 거북이 4348
안녕 거북이 4349
안녕 거북이 4350
안녕 거북이 4351
안녕 거북이 4352
안녕 거북이 4353
안녕 거북이 4354
안녕 거북이 4355
안녕 거북이 4356
안녕 거북이 4357
안녕 거북이 4358
안녕 거북이 4359
안녕 거북이 4360
안녕 거북이 4361
안녕 거북이 4362
안녕 거북이 4363
안녕 거북이 4364
안녕 거북이 4365
안녕 거북이 4366
안녕 거북이 4367
안녕 거북이 4368
안녕 거북이 4369
안녕 거북이 4370
안녕 거북이 4371
안녕 거북이 4372
안녕 거북이 4373
안녕 거북이 4374
안녕 거북이 4375
안녕 거북이 4376
안녕 거북이 4377
안녕 거북이 4378
안녕 거북이 4379
안녕 거북이 4380
안녕 거북이 4381
안녕 거북이 4382
안녕 거북이 4383
안녕 거북이 4384
안녕 거북이 4385
안녕 거북이 4386
안녕 거북이 4387
안녕 거북이 4388
안녕 거북이 4389
안녕 거북이 4390
안녕 거북이 4391
안녕 거북이 4392
안녕 거북이 4393
안녕 거북이 4394
안녕 거북이 4395
안녕 거북이 4396
안녕 거북이 4397
안녕 거북이 4398
안녕 거북이 4399
안녕 거북이 4400
안녕 거북이 4401
안녕 거북이 4402
안녕 거북이 4403
안녕 거북이 4404
안녕 거북이 4405
안녕 거북이 4406
안녕 거북이 4407
안녕 거북이 4408
안녕 거북이 4409
안녕 거북이 4410
안녕 거북이 4411
안녕 거북이 4412
안녕 거북이 4413
안녕 거북이 4414
안녕 거북이 4415
안녕 거북이 4416
안녕 거북이 4417
안녕 거북이 4418
안녕 거북이 4419
안녕 거북이 4420
안녕 거북이 4421
안녕 거북이 4422
안녕 거북이 4423
안녕 거북이 4424
안녕 거북이 4425
안녕 거북이 4426
안녕 거북이 4427
안녕 거북이 4428
안녕 거북이 4429
안녕 거북이 4430
안녕 거북이 4431
안녕 거북이 4432
안녕 거북이 4433
안녕 거북이 4434
안녕 거북이 4435
안녕 거북이 4436
안녕 거북이 4437
안녕 거북이 4438
안녕 거북이 4439
안녕 거북이 4440
안녕 거북이 4441
안녕 거북이 4442
안녕 거북이 4443
안녕 거북이 4444
안녕 거북이 4445
안녕 거북이 4446
안녕 거북이 4447
안녕 거북이 4448
안녕 거북이 4449
안녕 거북이 4450
안녕 거북이 4451
안녕 거북이 4452
안녕 거북이 4453
안녕 거북이 4454
안녕 거북이 4455
안녕 거북이 4456
안녕 거북이 4457
안녕 거북이 4458
안녕 거북이 4459
안녕 거북이 4460
안녕 거북이 4461
안녕 거북이 4462
안녕 거북이 4463
안녕 거북이 4464
안녕 거북이 4465
안녕 거북이 4466
안녕 거북이 4467
안녕 거북이 4468
안녕 거북이 4469
안녕 거북이 4470
안녕 거북이 4471
안녕 거북이 4472
안녕 거북이 4473
안녕 거북이 4474
안녕 거북이 4475
안녕 거북이 4476
안녕 거북이 4477
안녕 거북이 4478
안녕 거북이 4479
안녕 거북이 4480
안녕 거북이 4481
안녕 거북이 4482
안녕 거북이 4483
안녕 거북이 4484
안녕 거북이 4485
안녕 거북이 4486
안녕 거북이 4487
안녕 거북이 4488
안녕 거북이 4489
안녕 거북이 4490
안녕 거북이 4491
안녕 거북이 4492
안녕 거북이 4493
안녕 거북이 4494
안녕 거북이 4495
안녕 거북이 4496
안녕 거북이 4497
안녕 거북이 4498
안녕 거북이 4499
안녕 거북이 4500
안녕 거북이 4501
안녕 거북이 4502
안녕 거북이 4503
안녕 거북이 4504
안녕 거북이 4505
안녕 거북이 4506
안녕 거북이 4507
안녕 거북이 4508
안녕 거북이 4509
안녕 거북이 4510
안녕 거북이 4511
안녕 거북이 4512
안녕 거북이 4513
안녕 거북이 4514
안녕 거북이 4515
안녕 거북이 4516
안녕 거북이 4517
안녕 거북이 4518
안녕 거북이 4519
안녕 거북이 4520
안녕 거북이 4521
안녕 거북이 4522
안녕 거북이 4523
안녕 거북이 4524
안녕 거북이 4525
안녕 거북이 4526
안녕 거북이 4527
안녕 거북이 4528
안녕 거북이 4529
안녕 거북이 4530
안녕 거북이 4531
안녕 거북이 4532
안녕 거북이 4533
안녕 거북이 4534
안녕 거북이 4535
안녕 거북이 4536
안녕 거북이 4537
안녕 거북이 4538
안녕 거북이 4539
안녕 거북이 4540
안녕 거북이 4541
안녕 거북이 4542
안녕 거북이 4543
안녕 거북이 4544
안녕 거북이 4545
안녕 거북이 4546
안녕 거북이 4547
안녕 거북이 4548
안녕 거북이 4549
안녕 거북이 4550
안녕 거북이 4551
안녕 거북이 4552
안녕 거북이 4553
안녕 거북이 4554
안녕 거북이 4555
안녕 거북이 4556
안녕 거북이 4557
안녕 거북이 4558
안녕 거북이 4559
안녕 거북이 4560
안녕 거북이 4561
안녕 거북이 4562
안녕 거북이 4563
안녕 거북이 4564
안녕 거북이 4565
안녕 거북이 4566
안녕 거북이 4567
안녕 거북이 4568
안녕 거북이 4569
안녕 거북이 4570
안녕 거북이 4571
안녕 거북이 4572
안녕 거북이 4573
안녕 거북이 4574
안녕 거북이 4575
안녕 거북이 4576
안녕 거북이 4577
안녕 거북이 4578
안녕 거북이 4579
안녕 거북이 4580
안녕 거북이 4581
안녕 거북이 4582
안녕 거북이 4583
안녕 거북이 4584
안녕 거북이 4585
안녕 거북이 4586
안녕 거북이 4587
안녕 거북이 4588
안녕 거북이 4589
안녕 거북이 4590
안녕 거북이 4591
안녕 거북이 4592
안녕 거북이 4593
안녕 거북이 4594
안녕 거북이 4595
안녕 거북이 4596
안녕 거북이 4597
안녕 거북이 4598
안녕 거북이 4599
안녕 거북이 4600
안녕 거북이 4601
안녕 거북이 4602
안녕 거북이 4603
안녕 거북이 4604
안녕 거북이 4605
안녕 거북이 4606
안녕 거북이 4607
안녕 거북이 4608
안녕 거북이 4609
안녕 거북이 4610
안녕 거북이 4611
안녕 거북이 4612
안녕 거북이 4613
안녕 거북이 4614
안녕 거북이 4615
안녕 거북이 4616
안녕 거북이 4617
안녕 거북이 4618
안녕 거북이 4619
안녕 거북이 4620
안녕 거북이 4621
안녕 거북이 4622
안녕 거북이 4623
안녕 거북이 4624
안녕 거북이 4625
안녕 거북이 4626
안녕 거북이 4627
안녕 거북이 4628
안녕 거북이 4629
안녕 거북이 4630
안녕 거북이 4631
안녕 거북이 4632
안녕 거북이 4633
안녕 거북이 4634
안녕 거북이 4635
안녕 거북이 4636
안녕 거북이 4637
안녕 거북이 4638
안녕 거북이 4639
안녕 거북이 4640
안녕 거북이 4641
안녕 거북이 4642
안녕 거북이 4643
안녕 거북이 4644
안녕 거북이 4645
안녕 거북이 4646
안녕 거북이 4647
안녕 거북이 4648
안녕 거북이 4649
안녕 거북이 4650
안녕 거북이 4651
안녕 거북이 4652
안녕 거북이 4653
안녕 거북이 4654
안녕 거북이 4655
안녕 거북이 4656
안녕 거북이 4657
안녕 거북이 4658
안녕 거북이 4659
안녕 거북이 4660
안녕 거북이 4661
안녕 거북이 4662
안녕 거북이 4663
안녕 거북이 4664
안녕 거북이 4665
안녕 거북이 4666
안녕 거북이 4667
안녕 거북이 4668
안녕 거북이 4669
안녕 거북이 4670
안녕 거북이 4671
안녕 거북이 4672
안녕 거북이 4673
안녕 거북이 4674
안녕 거북이 4675
안녕 거북이 4676
안녕 거북이 4677
안녕 거북이 4678
안녕 거북이 4679
안녕 거북이 4680
안녕 거북이 4681
안녕 거북이 4682
안녕 거북이 4683
안녕 거북이 4684
안녕 거북이 4685
안녕 거북이 4686
안녕 거북이 4687
안녕 거북이 4688
안녕 거북이 4689
안녕 거북이 4690
안녕 거북이 4691
안녕 거북이 4692
안녕 거북이 4693
안녕 거북이 4694
안녕 거북이 4695
안녕 거북이 4696
안녕 거북이 4697
안녕 거북이 4698
안녕 거북이 4699
안녕 거북이 4700
안녕 거북이 4701
안녕 거북이 4702
안녕 거북이 4703
안녕 거북이 4704
안녕 거북이 4705
안녕 거북이 4706
안녕 거북이 4707
안녕 거북이 4708
안녕 거북이 4709
안녕 거북이 4710
안녕 거북이 4711
안녕 거북이 4712
안녕 거북이 4713
안녕 거북이 4714
안녕 거북이 4715
안녕 거북이 4716
안녕 거북이 4717
안녕 거북이 4718
안녕 거북이 4719
안녕 거북이 4720
안녕 거북이 4721
안녕 거북이 4722
안녕 거북이 4723
안녕 거북이 4724
안녕 거북이 4725
안녕 거북이 4726
안녕 거북이 4727
안녕 거북이 4728
안녕 거북이 4729
안녕 거북이 4730
안녕 거북이 4731
안녕 거북이 4732
안녕 거북이 4733
안녕 거북이 4734
안녕 거북이 4735
안녕 거북이 4736
안녕 거북이 4737
안녕 거북이 4738
안녕 거북이 4739
안녕 거북이 4740
안녕 거북이 4741
안녕 거북이 4742
안녕 거북이 4743
안녕 거북이 4744
안녕 거북이 4745
안녕 거북이 4746
안녕 거북이 4747
안녕 거북이 4748
안녕 거북이 4749
안녕 거북이 4750
안녕 거북이 4751
안녕 거북이 4752
안녕 거북이 4753
안녕 거북이 4754
안녕 거북이 4755
안녕 거북이 4756
안녕 거북이 4757
안녕 거북이 4758
안녕 거북이 4759
안녕 거북이 4760
안녕 거북이 4761
안녕 거북이 4762
안녕 거북이 4763
안녕 거북이 4764
안녕 거북이 4765
안녕 거북이 4766
안녕 거북이 4767
안녕 거북이 4768
안녕 거북이 4769
안녕 거북이 4770
안녕 거북이 4771
안녕 거북이 4772
안녕 거북이 4773
안녕 거북이 4774
안녕 거북이 4775
안녕 거북이 4776
안녕 거북이 4777
안녕 거북이 4778
안녕 거북이 4779
안녕 거북이 4780
안녕 거북이 4781
안녕 거북이 4782
안녕 거북이 4783
안녕 거북이 4784
안녕 거북이 4785
안녕 거북이 4786
안녕 거북이 4787
안녕 거북이 4788
안녕 거북이 4789
안녕 거북이 4790
안녕 거북이 4791
안녕 거북이 4792
안녕 거북이 4793
안녕 거북이 4794
안녕 거북이 4795
안녕 거북이 4796
안녕 거북이 4797
안녕 거북이 4798
안녕 거북이 4799
안녕 거북이 4800
안녕 거북이 4801
안녕 거북이 4802
안녕 거북이 4803
안녕 거북이 4804
안녕 거북이 4805
안녕 거북이 4806
안녕 거북이 4807
안녕 거북이 4808
안녕 거북이 4809
안녕 거북이 4810
안녕 거북이 4811
안녕 거북이 4812
안녕 거북이 4813
안녕 거북이 4814
안녕 거북이 4815
안녕 거북이 4816
안녕 거북이 4817
안녕 거북이 4818
안녕 거북이 4819
안녕 거북이 4820
안녕 거북이 4821
안녕 거북이 4822
안녕 거북이 4823
안녕 거북이 4824
안녕 거북이 4825
안녕 거북이 4826
안녕 거북이 4827
안녕 거북이 4828
안녕 거북이 4829
안녕 거북이 4830
안녕 거북이 4831
안녕 거북이 4832
안녕 거북이 4833
안녕 거북이 4834
안녕 거북이 4835
안녕 거북이 4836
안녕 거북이 4837
안녕 거북이 4838
안녕 거북이 4839
안녕 거북이 4840
안녕 거북이 4841
안녕 거북이 4842
안녕 거북이 4843
안녕 거북이 4844
안녕 거북이 4845
안녕 거북이 4846
안녕 거북이 4847
안녕 거북이 4848
안녕 거북이 4849
안녕 거북이 4850
안녕 거북이 4851
안녕 거북이 4852
안녕 거북이 4853
안녕 거북이 4854
안녕 거북이 4855
안녕 거북이 4856
안녕 거북이 4857
안녕 거북이 4858
안녕 거북이 4859
안녕 거북이 4860
안녕 거북이 4861
안녕 거북이 4862
안녕 거북이 4863
안녕 거북이 4864
안녕 거북이 4865
안녕 거북이 4866
안녕 거북이 4867
안녕 거북이 4868
안녕 거북이 4869
안녕 거북이 4870
안녕 거북이 4871
안녕 거북이 4872
안녕 거북이 4873
안녕 거북이 4874
안녕 거북이 4875
안녕 거북이 4876
안녕 거북이 4877
안녕 거북이 4878
안녕 거북이 4879
안녕 거북이 4880
안녕 거북이 4881
안녕 거북이 4882
안녕 거북이 4883
안녕 거북이 4884
안녕 거북이 4885
안녕 거북이 4886
안녕 거북이 4887
안녕 거북이 4888
안녕 거북이 4889
안녕 거북이 4890
안녕 거북이 4891
안녕 거북이 4892
안녕 거북이 4893
안녕 거북이 4894
안녕 거북이 4895
안녕 거북이 4896
안녕 거북이 4897
안녕 거북이 4898
안녕 거북이 4899
안녕 거북이 4900
안녕 거북이 4901
안녕 거북이 4902
안녕 거북이 4903
안녕 거북이 4904
안녕 거북이 4905
안녕 거북이 4906
안녕 거북이 4907
안녕 거북이 4908
안녕 거북이 4909
안녕 거북이 4910
안녕 거북이 4911
안녕 거북이 4912
안녕 거북이 4913
안녕 거북이 4914
안녕 거북이 4915
안녕 거북이 4916
안녕 거북이 4917
안녕 거북이 4918
안녕 거북이 4919
안녕 거북이 4920
안녕 거북이 4921
안녕 거북이 4922
안녕 거북이 4923
안녕 거북이 4924
안녕 거북이 4925
안녕 거북이 4926
안녕 거북이 4927
안녕 거북이 4928
안녕 거북이 4929
안녕 거북이 4930
안녕 거북이 4931
안녕 거북이 4932
안녕 거북이 4933
안녕 거북이 4934
안녕 거북이 4935
안녕 거북이 4936
안녕 거북이 4937
안녕 거북이 4938
안녕 거북이 4939
안녕 거북이 4940
안녕 거북이 4941
안녕 거북이 4942
안녕 거북이 4943
안녕 거북이 4944
안녕 거북이 4945
안녕 거북이 4946
안녕 거북이 4947
안녕 거북이 4948
안녕 거북이 4949
안녕 거북이 4950
안녕 거북이 4951
안녕 거북이 4952
안녕 거북이 4953
안녕 거북이 4954
안녕 거북이 4955
안녕 거북이 4956
안녕 거북이 4957
안녕 거북이 4958
안녕 거북이 4959
안녕 거북이 4960
안녕 거북이 4961
안녕 거북이 4962
안녕 거북이 4963
안녕 거북이 4964
안녕 거북이 4965
안녕 거북이 4966
안녕 거북이 4967
안녕 거북이 4968
안녕 거북이 4969
안녕 거북이 4970
안녕 거북이 4971
안녕 거북이 4972
안녕 거북이 4973
안녕 거북이 4974
안녕 거북이 4975
안녕 거북이 4976
안녕 거북이 4977
안녕 거북이 4978
안녕 거북이 4979
안녕 거북이 4980
안녕 거북이 4981
안녕 거북이 4982
안녕 거북이 4983
안녕 거북이 4984
안녕 거북이 4985
안녕 거북이 4986
안녕 거북이 4987
안녕 거북이 4988
안녕 거북이 4989
안녕 거북이 4990
안녕 거북이 4991
안녕 거북이 4992
안녕 거북이 4993
안녕 거북이 4994
안녕 거북이 4995
안녕 거북이 4996
안녕 거북이 4997
안녕 거북이 4998
안녕 거북이 4999</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 코드 4-3 리스트의 숫자를 차례대로 출력하는 코드</span></span><br><span class="line"><span class="keyword">for</span> num <span class="keyword">in</span> [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>]:</span><br><span class="line">    print(num)</span><br></pre></td></tr></table></figure>

<pre><code>0
1
2</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 코드 4-4 리스트의 문자열을 차례대로 출력하는 코드</span></span><br><span class="line">characters = [<span class="string">&#x27;앨리스&#x27;</span>, <span class="string">&#x27;도도새&#x27;</span>, <span class="string">&#x27;3월토끼&#x27;</span>]</span><br><span class="line"><span class="keyword">for</span> character <span class="keyword">in</span> characters:</span><br><span class="line">    print(character)</span><br></pre></td></tr></table></figure>

<pre><code>앨리스
도도새
3월토끼</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 코드 4-5 모든 신하에게 퇴장 명령을 내리는 코드</span></span><br><span class="line">players = [<span class="string">&#x27;공작부인&#x27;</span>, <span class="string">&#x27;흰 토끼&#x27;</span>, <span class="string">&#x27;하트잭&#x27;</span>, <span class="string">&#x27;모자장수&#x27;</span>]</span><br><span class="line"><span class="keyword">for</span> player <span class="keyword">in</span> players:</span><br><span class="line">    print(player, <span class="string">&#x27;퇴장!&#x27;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>공작부인 퇴장!
흰 토끼 퇴장!
하트잭 퇴장!
모자장수 퇴장!</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 코드 4-6 문자열의 문자를 하나씩 출력하는 코드</span></span><br><span class="line"><span class="keyword">for</span> letter <span class="keyword">in</span> <span class="string">&#x27;체셔고양이&#x27;</span>:</span><br><span class="line">    print(letter)</span><br></pre></td></tr></table></figure>

<pre><code>체
셔
고
양
이</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 코드 4-7 리스트의 숫자를 차례대로 출력하는 코드</span></span><br><span class="line">nums = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>]</span><br><span class="line"><span class="keyword">for</span> num <span class="keyword">in</span> nums:</span><br><span class="line">    print(num)</span><br><span class="line">print(nums)</span><br></pre></td></tr></table></figure>

<pre><code>0
1
2
[0, 1, 2]</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 코드 4-8 리스트의 숫자를 차례대로 출력하는 코드</span></span><br><span class="line">nums = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>]</span><br><span class="line"><span class="keyword">for</span> num <span class="keyword">in</span> nums:</span><br><span class="line">    print(num)</span><br><span class="line">    print(nums)</span><br></pre></td></tr></table></figure>

<pre><code>0
[0, 1, 2]
1
[0, 1, 2]
2
[0, 1, 2]</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 코드 4-9 0부터 2까지 차례로 출력하는 코드</span></span><br><span class="line"><span class="keyword">for</span> num <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">    print(num)</span><br></pre></td></tr></table></figure>

<pre><code>0
1
2</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 코드 4-10 구구단 2단을 출력하는 코드</span></span><br><span class="line"><span class="keyword">for</span> y <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">10</span>):</span><br><span class="line">    print(<span class="number">2</span>, <span class="string">&#x27;x&#x27;</span>, y, <span class="string">&#x27;=&#x27;</span>, <span class="number">2</span> * y)</span><br></pre></td></tr></table></figure>

<pre><code>2 x 1 = 2
2 x 2 = 4
2 x 3 = 6
2 x 4 = 8
2 x 5 = 10
2 x 6 = 12
2 x 7 = 14
2 x 8 = 16
2 x 9 = 18</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 코드 4-11 모든 ‘하얀장미’를 ‘빨간장미’로 바꾸는 코드</span></span><br><span class="line">roses = [<span class="string">&#x27;하얀장미&#x27;</span>, <span class="string">&#x27;하얀장미&#x27;</span>, <span class="string">&#x27;하얀장미&#x27;</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">    roses[i] = <span class="string">&#x27;빨간장미&#x27;</span></span><br><span class="line">print(roses)</span><br></pre></td></tr></table></figure>

<pre><code>[&#39;빨간장미&#39;, &#39;빨간장미&#39;, &#39;빨간장미&#39;]</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 코드 5-1 True와 False를 출력하는 코드</span></span><br><span class="line">print(<span class="literal">True</span>)</span><br><span class="line">print(<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<pre><code>True
False</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 코드 5-2 값을 비교하는 코드</span></span><br><span class="line">print(<span class="number">1</span> &lt; <span class="number">2</span>)</span><br><span class="line">print(<span class="number">2</span> &lt; <span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<pre><code>True
False</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 코드 5-3 값을 비교하는 코드</span></span><br><span class="line">print(<span class="number">2</span> &gt; <span class="number">1</span>)</span><br><span class="line">print(<span class="number">2</span> &gt; <span class="number">2</span>)</span><br></pre></td></tr></table></figure>

<pre><code>True
False</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 코드 5-4 값을 비교하는 코드</span></span><br><span class="line">print(<span class="number">1</span> &lt;= <span class="number">1</span>)</span><br><span class="line">print(<span class="number">2</span> &lt;= <span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<pre><code>True
False</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 코드 5-5 값을 비교하는 코드</span></span><br><span class="line">print(<span class="number">1</span> &gt;= <span class="number">1</span>)</span><br><span class="line">print(<span class="number">1</span> &gt;= <span class="number">2</span>)</span><br></pre></td></tr></table></figure>

<pre><code>True
False</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 코드 5-6 값을 비교하는 코드</span></span><br><span class="line">print(<span class="number">1</span> == <span class="number">1</span>)</span><br><span class="line">print(<span class="number">2</span> == <span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<pre><code>True
False</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 코드 5-7 값을 비교하는 코드</span></span><br><span class="line">print(<span class="number">1</span> != <span class="number">2</span>)</span><br><span class="line">print(<span class="number">2</span> != <span class="number">2</span>)</span><br></pre></td></tr></table></figure>

<pre><code>True
False</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 코드 5-8 조건에 따라 출력하는 코드</span></span><br><span class="line"><span class="keyword">if</span> <span class="literal">True</span>:</span><br><span class="line">    print(<span class="string">&#x27;참입니다.&#x27;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>참입니다.</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 코드 5-9 조건에 따라 출력하는 코드</span></span><br><span class="line"><span class="keyword">if</span> <span class="literal">False</span>:</span><br><span class="line">    print(<span class="string">&#x27;참입니다.&#x27;</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 코드 5-10 점수에 따라 합격여부를 출력하는 코드</span></span><br><span class="line">score = <span class="number">90</span></span><br><span class="line"><span class="keyword">if</span> score &gt; <span class="number">80</span>:</span><br><span class="line">    print(<span class="string">&#x27;합격입니다.&#x27;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>합격입니다.</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 코드 5-11 점수에 따라 합격여부를 출력하는 코드</span></span><br><span class="line">score = <span class="number">60</span></span><br><span class="line"><span class="keyword">if</span> score &gt; <span class="number">80</span>:</span><br><span class="line">    print(<span class="string">&#x27;합격입니다.&#x27;</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    print(<span class="string">&#x27;불합격입니다.&#x27;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>불합격입니다.</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 코드 5-12 점수에 따라 학점을 출력하는 코드</span></span><br><span class="line">score = <span class="number">75</span></span><br><span class="line"><span class="keyword">if</span> <span class="number">80</span> &lt; score &lt;= <span class="number">100</span>:</span><br><span class="line">    print(<span class="string">&#x27;학점은 A입니다.&#x27;</span>)</span><br><span class="line"><span class="keyword">elif</span> <span class="number">60</span> &lt; score &lt;= <span class="number">80</span>:</span><br><span class="line">    print(<span class="string">&#x27;학점은 B입니다.&#x27;</span>)</span><br><span class="line"><span class="keyword">elif</span> <span class="number">40</span> &lt; score &lt;= <span class="number">60</span>:</span><br><span class="line">    print(<span class="string">&#x27;학점은 C입니다.&#x27;</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    print(<span class="string">&#x27;학점은 F입니다.&#x27;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>학점은 B입니다.</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 코드 5-13 총 입장료를 계산하는 코드</span></span><br><span class="line">total_price = <span class="number">0</span></span><br><span class="line">ages = [<span class="number">22</span>, <span class="number">21</span>, <span class="number">17</span>, <span class="number">32</span>, <span class="number">4</span>, <span class="number">28</span>, <span class="number">19</span>, <span class="number">8</span>]</span><br><span class="line"><span class="keyword">for</span> age <span class="keyword">in</span> ages:</span><br><span class="line">    <span class="keyword">if</span> age &gt;= <span class="number">20</span>:</span><br><span class="line">        total_price = total_price + <span class="number">8000</span></span><br><span class="line">    <span class="keyword">elif</span> age &gt;= <span class="number">10</span>:</span><br><span class="line">        total_price = total_price + <span class="number">5000</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        total_price = total_price + <span class="number">2500</span></span><br><span class="line">print(<span class="string">&#x27;총 입장료는&#x27;</span>, total_price, <span class="string">&#x27;원입니다.&#x27;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>총 입장료는 47000 원입니다.</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 코드 5-14 여러 조건을 판단하는 코드</span></span><br><span class="line">games = <span class="number">12</span></span><br><span class="line">points = <span class="number">25</span></span><br><span class="line"><span class="keyword">if</span> games &gt;= <span class="number">10</span> <span class="keyword">and</span> points &gt;= <span class="number">20</span>:</span><br><span class="line">    print(<span class="string">&#x27;MVP로 선정되었습니다.&#x27;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>MVP로 선정되었습니다.</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 코드 5-15 and의 결과를 확인하는 코드</span></span><br><span class="line">print(<span class="literal">True</span> <span class="keyword">and</span> <span class="literal">True</span>)</span><br><span class="line">print(<span class="literal">True</span> <span class="keyword">and</span> <span class="literal">False</span>)</span><br><span class="line">print(<span class="literal">False</span> <span class="keyword">and</span> <span class="literal">True</span>)</span><br><span class="line">print(<span class="literal">False</span> <span class="keyword">and</span> <span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<pre><code>True
False
False
False</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 코드 5-16 or의 결과를 확인하는 코드</span></span><br><span class="line">print(<span class="literal">True</span> <span class="keyword">or</span> <span class="literal">True</span>)</span><br><span class="line">print(<span class="literal">True</span> <span class="keyword">or</span> <span class="literal">False</span>)</span><br><span class="line">print(<span class="literal">False</span> <span class="keyword">or</span> <span class="literal">True</span>)</span><br><span class="line">print(<span class="literal">False</span> <span class="keyword">or</span> <span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<pre><code>True
True
True
False</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 코드 5-17 not의 결과를 확인하는 코드</span></span><br><span class="line">print(<span class="keyword">not</span> <span class="literal">True</span>)</span><br><span class="line">print(<span class="keyword">not</span> <span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<pre><code>False
True</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 코드 5-18 인상착의로 범인을 잡아내는 코드</span></span><br><span class="line">suspects = [[<span class="string">&#x27;거위&#x27;</span>, <span class="string">&#x27;새&#x27;</span>, <span class="string">&#x27;암컷&#x27;</span>], [<span class="string">&#x27;푸들&#x27;</span>, <span class="string">&#x27;개&#x27;</span>, <span class="string">&#x27;수컷&#x27;</span>], [<span class="string">&#x27;비글&#x27;</span>, <span class="string">&#x27;개&#x27;</span>, <span class="string">&#x27;암컷&#x27;</span>]]</span><br><span class="line"><span class="keyword">for</span> suspect <span class="keyword">in</span> suspects:</span><br><span class="line">    <span class="keyword">if</span> suspect[<span class="number">1</span>] == <span class="string">&#x27;개&#x27;</span> <span class="keyword">and</span> suspect[<span class="number">2</span>] == <span class="string">&#x27;암컷&#x27;</span>:</span><br><span class="line">        print(<span class="string">&#x27;범인은&#x27;</span>, suspect[<span class="number">0</span>], <span class="string">&#x27;입니다.&#x27;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>범인은 비글 입니다.</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line">rps = [<span class="string">&#x27;가위&#x27;</span>, <span class="string">&#x27;바위&#x27;</span>, <span class="string">&#x27;보&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">  player = <span class="built_in">input</span>(<span class="string">&#x27;가위/바위/보/끝: &#x27;</span>)</span><br><span class="line">  computer = random.choice(rps)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> player == <span class="string">&#x27;끝&#x27;</span>:</span><br><span class="line">    <span class="keyword">break</span></span><br><span class="line">  </span><br><span class="line">  print(player, computer)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> player == computer:</span><br><span class="line">    print(<span class="string">&#x27;비겼어요!&#x27;</span>)</span><br><span class="line">  <span class="keyword">elif</span> player == <span class="string">&#x27;가위&#x27;</span>:</span><br><span class="line">    <span class="keyword">if</span> computer == <span class="string">&#x27;바위&#x27;</span>:</span><br><span class="line">      print(<span class="string">&#x27;졌어요!&#x27;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      print(<span class="string">&#x27;이겼어요!&#x27;</span>)</span><br><span class="line">  <span class="keyword">elif</span> player == <span class="string">&#x27;바위&#x27;</span>:</span><br><span class="line">    <span class="keyword">if</span> computer == <span class="string">&#x27;보&#x27;</span>:</span><br><span class="line">      print(<span class="string">&#x27;졌어요!&#x27;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      print(<span class="string">&#x27;이겼어요!&#x27;</span>)</span><br><span class="line">  <span class="keyword">elif</span> player == <span class="string">&#x27;보&#x27;</span>:</span><br><span class="line">    <span class="keyword">if</span> computer == <span class="string">&#x27;가위&#x27;</span>:</span><br><span class="line">      print(<span class="string">&#x27;졌어요!&#x27;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      print(<span class="string">&#x27;이겼어요!&#x27;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>가위/바위/보/끝: 가위
가위 가위
비겼어요!
가위/바위/보/끝: 바위
바위 보
졌어요!
가위/바위/보/끝: 보
보 가위
졌어요!
가위/바위/보/끝: 끝</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 코드 7-1 튜플을 만드는 코드</span></span><br><span class="line">my_tuple1 = ()</span><br><span class="line">print(my_tuple1)</span><br><span class="line">my_tuple2 = (<span class="number">1</span>, <span class="number">-2</span>, <span class="number">3.14</span>)</span><br><span class="line">print(my_tuple2)</span><br><span class="line">my_tuple3 = <span class="string">&#x27;앨리스&#x27;</span>, <span class="number">10</span>, <span class="number">1.0</span>, <span class="number">1.2</span></span><br><span class="line">print(my_tuple3)</span><br></pre></td></tr></table></figure>

<pre><code>()
(1, -2, 3.14)
(&#39;앨리스&#39;, 10, 1.0, 1.2)</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 코드 7-2 값이 한 개인 튜플을 만드는 코드</span></span><br><span class="line">my_int = (<span class="number">1</span>)</span><br><span class="line">print(<span class="built_in">type</span>(my_int))</span><br><span class="line">my_tuple = (<span class="number">1</span>,)</span><br><span class="line">print(<span class="built_in">type</span>(my_tuple))</span><br></pre></td></tr></table></figure>

<pre><code>&lt;class &#39;int&#39;&gt;
&lt;class &#39;tuple&#39;&gt;</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 코드 7-3 튜플에서 값을 가져오는 코드</span></span><br><span class="line">clovers = (<span class="string">&#x27;클로버1&#x27;</span>, <span class="string">&#x27;하트2&#x27;</span>, <span class="string">&#x27;클로버3&#x27;</span>)</span><br><span class="line">print(clovers[<span class="number">1</span>])</span><br></pre></td></tr></table></figure>

<pre><code>하트2</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 코드 7-4 튜플의 값을 변경하려는 코드</span></span><br><span class="line">clovers = (<span class="string">&#x27;클로버1&#x27;</span>, <span class="string">&#x27;하트2&#x27;</span>, <span class="string">&#x27;클로버3&#x27;</span>)</span><br><span class="line">clovers[<span class="number">1</span>] = <span class="string">&#x27;클로버2&#x27;</span></span><br></pre></td></tr></table></figure>


<pre><code>---------------------------------------------------------------------------

TypeError                                 Traceback (most recent call last)

&lt;ipython-input-68-84a1a3ec1095&gt; in &lt;module&gt;()
      1 # 코드 7-4 튜플의 값을 변경하려는 코드
      2 clovers = (&#39;클로버1&#39;, &#39;하트2&#39;, &#39;클로버3&#39;)
----&gt; 3 clovers[1] = &#39;클로버2&#39;


TypeError: &#39;tuple&#39; object does not support item assignment</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 코드 7-5 패킹과 언패킹을 하는 코드</span></span><br><span class="line">clovers = <span class="string">&#x27;클로버1&#x27;</span>, <span class="string">&#x27;클로버2&#x27;</span>, <span class="string">&#x27;클로버3&#x27;</span></span><br><span class="line">print(clovers)</span><br><span class="line">alice_blue = (<span class="number">240</span>, <span class="number">248</span>, <span class="number">255</span>)</span><br><span class="line">r, g, b = alice_blue</span><br><span class="line">print(<span class="string">&#x27;R:&#x27;</span>, r, <span class="string">&#x27;G:&#x27;</span>, g, <span class="string">&#x27;B:&#x27;</span>, b)</span><br></pre></td></tr></table></figure>

<pre><code>(&#39;클로버1&#39;, &#39;클로버2&#39;, &#39;클로버3&#39;)
R: 240 G: 248 B: 255</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 코드 7-6 두 변수의 값을 서로 교환하는 코드</span></span><br><span class="line">dodo = <span class="string">&#x27;박하맛&#x27;</span></span><br><span class="line">alice = <span class="string">&#x27;딸기맛&#x27;</span></span><br><span class="line">print(<span class="string">&#x27;도도새:&#x27;</span>, dodo, <span class="string">&#x27;앨리스:&#x27;</span>, alice)</span><br><span class="line">dodo, alice = alice, dodo</span><br><span class="line">print(<span class="string">&#x27;도도새:&#x27;</span>, dodo, <span class="string">&#x27;앨리스:&#x27;</span>, alice)</span><br></pre></td></tr></table></figure>

<pre><code>도도새: 박하맛 앨리스: 딸기맛
도도새: 딸기맛 앨리스: 박하맛</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 코드 7-7 딕셔너리를 만드는 코드</span></span><br><span class="line">my_dict1 = &#123;&#125;</span><br><span class="line">print(my_dict1)</span><br><span class="line">my_dict2 = &#123;<span class="number">0</span>: <span class="number">1</span>, <span class="number">1</span>: <span class="number">-2</span>, <span class="number">2</span>: <span class="number">3.14</span>&#125;</span><br><span class="line">print(my_dict2)</span><br><span class="line">my_dict3 = &#123;<span class="string">&#x27;이름&#x27;</span>: <span class="string">&#x27;앨리스&#x27;</span>, <span class="string">&#x27;나이&#x27;</span>: <span class="number">10</span>, <span class="string">&#x27;시력&#x27;</span>: [<span class="number">1.0</span>, <span class="number">1.2</span>]&#125;</span><br><span class="line">print(my_dict3)</span><br></pre></td></tr></table></figure>

<pre><code>&#123;&#125;
&#123;0: 1, 1: -2, 2: 3.14&#125;
&#123;&#39;이름&#39;: &#39;앨리스&#39;, &#39;나이&#39;: 10, &#39;시력&#39;: [1.0, 1.2]&#125;</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 코드 7-8 딕셔너리에 키-값을 추가하는 코드</span></span><br><span class="line">clover = &#123;<span class="string">&#x27;나이&#x27;</span>: <span class="number">27</span>, <span class="string">&#x27;직업&#x27;</span>: <span class="string">&#x27;병사&#x27;</span>&#125;</span><br><span class="line">print(clover)</span><br><span class="line">clover[<span class="string">&#x27;번호&#x27;</span>] = <span class="number">9</span></span><br><span class="line">print(clover)</span><br></pre></td></tr></table></figure>

<pre><code>&#123;&#39;나이&#39;: 27, &#39;직업&#39;: &#39;병사&#39;&#125;
&#123;&#39;나이&#39;: 27, &#39;직업&#39;: &#39;병사&#39;, &#39;번호&#39;: 9&#125;</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 코드 7-9 딕셔너리의 값에 접근하는 코드</span></span><br><span class="line">clover = &#123;<span class="string">&#x27;나이&#x27;</span>: <span class="number">27</span>, <span class="string">&#x27;직업&#x27;</span>: <span class="string">&#x27;병사&#x27;</span>, <span class="string">&#x27;번호&#x27;</span>: <span class="number">9</span>&#125;</span><br><span class="line">print(clover[<span class="string">&#x27;번호&#x27;</span>])</span><br><span class="line">clover[<span class="string">&#x27;번호&#x27;</span>] = <span class="number">6</span></span><br><span class="line">print(clover[<span class="string">&#x27;번호&#x27;</span>])</span><br><span class="line">print(clover.get(<span class="string">&#x27;번호&#x27;</span>))</span><br></pre></td></tr></table></figure>

<pre><code>9
6
6</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 코드 7-10 딕셔너리에서 키-값을 제거하는 코드</span></span><br><span class="line">clover = &#123;<span class="string">&#x27;나이&#x27;</span>: <span class="number">27</span>, <span class="string">&#x27;직업&#x27;</span>: <span class="string">&#x27;병사&#x27;</span>, <span class="string">&#x27;번호&#x27;</span>: <span class="number">6</span>&#125;</span><br><span class="line">print(clover)</span><br><span class="line"><span class="keyword">del</span> clover[<span class="string">&#x27;나이&#x27;</span>]</span><br><span class="line">print(clover)</span><br></pre></td></tr></table></figure>

<pre><code>&#123;&#39;나이&#39;: 27, &#39;직업&#39;: &#39;병사&#39;, &#39;번호&#39;: 6&#125;
&#123;&#39;직업&#39;: &#39;병사&#39;, &#39;번호&#39;: 6&#125;</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 코드 7-11 라면 주문을 추가/수정/취소하는 코드</span></span><br><span class="line">order = &#123;<span class="string">&#x27;스페이드1&#x27;</span>: <span class="string">&#x27;비빔라면&#x27;</span>, <span class="string">&#x27;다이아2&#x27;</span>: <span class="string">&#x27;매운라면&#x27;</span>&#125;</span><br><span class="line">print(order)</span><br><span class="line">order[<span class="string">&#x27;클로버3&#x27;</span>] = <span class="string">&#x27;카레라면&#x27;</span></span><br><span class="line">print(order)</span><br><span class="line">order[<span class="string">&#x27;다이아2&#x27;</span>] = <span class="string">&#x27;짜장라면&#x27;</span></span><br><span class="line">print(order)</span><br><span class="line"><span class="keyword">del</span> order[<span class="string">&#x27;스페이드1&#x27;</span>]</span><br><span class="line">print(order)</span><br></pre></td></tr></table></figure>

<pre><code>&#123;&#39;스페이드1&#39;: &#39;비빔라면&#39;, &#39;다이아2&#39;: &#39;매운라면&#39;&#125;
&#123;&#39;스페이드1&#39;: &#39;비빔라면&#39;, &#39;다이아2&#39;: &#39;매운라면&#39;, &#39;클로버3&#39;: &#39;카레라면&#39;&#125;
&#123;&#39;스페이드1&#39;: &#39;비빔라면&#39;, &#39;다이아2&#39;: &#39;짜장라면&#39;, &#39;클로버3&#39;: &#39;카레라면&#39;&#125;
&#123;&#39;다이아2&#39;: &#39;짜장라면&#39;, &#39;클로버3&#39;: &#39;카레라면&#39;&#125;</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 코드 8-1 모든 카드 병사에게 유죄 판결을 내리는 코드</span></span><br><span class="line">print(<span class="string">&#x27;하트 1 유죄!&#x27;</span>)</span><br><span class="line">print(<span class="string">&#x27;하트 2 유죄!&#x27;</span>)</span><br><span class="line">print(<span class="string">&#x27;하트 3 유죄!&#x27;</span>)</span><br><span class="line">print(<span class="string">&#x27;클로버 1 유죄!&#x27;</span>)</span><br><span class="line">print(<span class="string">&#x27;클로버 2 유죄!&#x27;</span>)</span><br><span class="line">print(<span class="string">&#x27;클로버 3 유죄!&#x27;</span>)</span><br><span class="line">print(<span class="string">&#x27;스페이드 1 유죄!&#x27;</span>)</span><br><span class="line">print(<span class="string">&#x27;스페이드 2 유죄!&#x27;</span>)</span><br><span class="line">print(<span class="string">&#x27;스페이드 3 유죄!&#x27;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>하트 1 유죄!
하트 2 유죄!
하트 3 유죄!
클로버 1 유죄!
클로버 2 유죄!
클로버 3 유죄!
스페이드 1 유죄!
스페이드 2 유죄!
스페이드 3 유죄!</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 코드 8-2 함수를 사용해 문자열을 출력하는 코드</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_func</span>():</span></span><br><span class="line">    print(<span class="string">&#x27;토끼야 안녕!&#x27;</span>)</span><br><span class="line"> </span><br><span class="line">my_func()</span><br></pre></td></tr></table></figure>

<pre><code>토끼야 안녕!</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 코드 8-3 함수를 사용해 두 개의 숫자를 더하는 코드</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add</span>(<span class="params">num1, num2</span>):</span></span><br><span class="line">    <span class="keyword">return</span> num1 + num2</span><br><span class="line"> </span><br><span class="line">print(add(<span class="number">2</span>, <span class="number">3</span>))</span><br></pre></td></tr></table></figure>

<pre><code>5</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 코드 8-4 함수를 사용해 두 개의 숫자를 더하고 곱하는 코드</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_mul</span>(<span class="params">num1, num2</span>):</span></span><br><span class="line">    <span class="keyword">return</span> num1 + num2, num1 * num2</span><br><span class="line"></span><br><span class="line">print(add_mul(<span class="number">2</span>, <span class="number">3</span>))</span><br></pre></td></tr></table></figure>

<pre><code>(5, 6)</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 코드 8-5 함수를 사용해 판결을 내리는 코드</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">judge_cards</span>(<span class="params">name</span>):</span></span><br><span class="line">    print(name, <span class="string">&#x27;1 유죄!&#x27;</span>)</span><br><span class="line">    print(name, <span class="string">&#x27;2 유죄!&#x27;</span>)</span><br><span class="line">    print(name, <span class="string">&#x27;3 유죄!&#x27;</span>)</span><br><span class="line"></span><br><span class="line">judge_cards(<span class="string">&#x27;하트&#x27;</span>)</span><br><span class="line">judge_cards(<span class="string">&#x27;클로버&#x27;</span>)</span><br><span class="line">judge_cards(<span class="string">&#x27;스페이드&#x27;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>하트 1 유죄!
하트 2 유죄!
하트 3 유죄!
클로버 1 유죄!
클로버 2 유죄!
클로버 3 유죄!
스페이드 1 유죄!
스페이드 2 유죄!
스페이드 3 유죄!</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 코드 8-6 하나의 값을 임의로 선택하는 코드</span></span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line">animals = [<span class="string">&#x27;체셔고양이&#x27;</span>, <span class="string">&#x27;오리&#x27;</span>, <span class="string">&#x27;도도새&#x27;</span>]</span><br><span class="line">print(random.choice(animals))</span><br></pre></td></tr></table></figure>

<pre><code>도도새</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 코드 8-7 여러 개의 값을 임의로 선택하는 코드</span></span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line">animals = [<span class="string">&#x27;체셔고양이&#x27;</span>, <span class="string">&#x27;오리&#x27;</span>, <span class="string">&#x27;도도새&#x27;</span>]</span><br><span class="line">print(random.sample(animals, <span class="number">2</span>))</span><br></pre></td></tr></table></figure>

<pre><code>[&#39;체셔고양이&#39;, &#39;오리&#39;]</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 코드 8-8 지정한 범위에서 임의의 정수를 선택하는 코드</span></span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line">print(random.randint(<span class="number">5</span>, <span class="number">10</span>))</span><br></pre></td></tr></table></figure>

<pre><code>5</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 코드 8-9 임의로 카드 하나를 뽑는 코드</span></span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line">cards = [<span class="string">&#x27;하트&#x27;</span>, <span class="string">&#x27;클로버&#x27;</span>, <span class="string">&#x27;스페이드&#x27;</span>]</span><br><span class="line">chosen_card = random.choice(cards)</span><br><span class="line">print(chosen_card, <span class="string">&#x27;유죄!&#x27;</span>)</span><br></pre></td></tr></table></figure>

<pre><code>클로버 유죄!</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">help</span>(<span class="string">&#x27;modules&#x27;</span>)</span><br></pre></td></tr></table></figure>


<pre><code>Please wait a moment while I gather a list of all available modules...



/usr/local/lib/python3.6/dist-packages/IPython/kernel/__init__.py:13: ShimWarning: The `IPython.kernel` package has been deprecated since IPython 4.0.You should import from ipykernel or jupyter_client instead.
  &quot;You should import from ipykernel or jupyter_client instead.&quot;, ShimWarning)
/usr/local/lib/python3.6/dist-packages/datascience/tables.py:17: MatplotlibDeprecationWarning:

The &#39;warn&#39; parameter of use() is deprecated since Matplotlib 3.1 and will be removed in 3.3.  If any parameter follows &#39;warn&#39;, they should be pass as keyword, not positionally.

/usr/local/lib/python3.6/dist-packages/datascience/util.py:10: MatplotlibDeprecationWarning:

The &#39;warn&#39; parameter of use() is deprecated since Matplotlib 3.1 and will be removed in 3.3.  If any parameter follows &#39;warn&#39;, they should be pass as keyword, not positionally.

/usr/local/lib/python3.6/dist-packages/psycopg2/__init__.py:144: UserWarning:

The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use &quot;pip install psycopg2-binary&quot; instead. For details see: &lt;http://initd.org/psycopg/docs/install.html#binary-install-from-pypi&gt;.

/usr/local/lib/python3.6/dist-packages/sklearn/externals/six.py:31: FutureWarning:

The module is deprecated in version 0.21 and will be removed in version 0.23 since we&#39;ve dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).

/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning:

The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.

/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning:

The sklearn.datasets.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.datasets. Anything that cannot be imported from sklearn.datasets is now part of the private API.

/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning:

The sklearn.ensemble.bagging module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.

/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning:

The sklearn.ensemble.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.

/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning:

The sklearn.ensemble.forest module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.

/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning:

sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.

/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning:

The sklearn.utils.testing module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.utils. Anything that cannot be imported from sklearn.utils is now part of the private API.

/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning:

The sklearn.metrics.classification module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.



Downloading http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2 to /root/mlxtend_data/shape_predictor_68_face_landmarks.dat.bz2


/usr/lib/python3.6/pkgutil.py:92: UserWarning:

The DICOM readers are highly experimental, unstable, and only work for Siemens time-series at the moment
Please use with caution.  We would be grateful for your help in improving them

/usr/local/lib/python3.6/dist-packages/nltk/twitter/__init__.py:20: UserWarning:

The twython library has not been installed. Some functionality from the twitter package will not be available.

Exception ignored in: &lt;_io.FileIO name=&#39;/usr/local/lib/python3.6/dist-packages/theano/gof/c_code/lazylinker_c.c&#39; mode=&#39;rb&#39; closefd=True&gt;
ResourceWarning: unclosed file &lt;_io.TextIOWrapper name=&#39;/usr/local/lib/python3.6/dist-packages/theano/gof/c_code/lazylinker_c.c&#39; mode=&#39;r&#39; encoding=&#39;UTF-8&#39;&gt;
WARNING:pystan:This submodule contains experimental code, please use with caution


Cython              concurrent          knnimpute           regex
IPython             configparser        korean_lunar_calendar reprlib
OpenGL              contextlib          lib2to3             requests
PIL                 contextlib2         libfuturize         requests_oauthlib
ScreenResolution    convertdate         libpasteurize       resampy
__future__          copy                librosa             resource
_ast                copyreg             lightgbm            retrying
_asyncio            coverage            linecache           rlcompleter
_bisect             coveralls           llvmlite            rmagic
_blake2             crcmod              lmdb                rpy2
_bootlocale         crypt               locale              rsa
_bz2                csv                 logging             runpy
_cffi_backend       ctypes              lsb_release         samples
_codecs             cufflinks           lucid               sched
_codecs_cn          curses              lunarcalendar       scipy
_codecs_hk          cv2                 lxml                scs
_codecs_iso2022     cvxopt              lzma                seaborn
_codecs_jp          cvxpy               macpath             secrets
_codecs_kr          cycler              macurl2path         select
_codecs_tw          cymem               mailbox             selectors
_collections        cython              mailcap             send2trash
_collections_abc    cythonmagic         markdown            setuptools
_compat_pickle      daft                markupsafe          setuptools_git
_compression        dask                marshal             shapely
_crypt              dataclasses         math                shelve
_csv                datascience         matplotlib          shlex
_ctypes             datetime            matplotlib_venn     shutil
_ctypes_test        dateutil            mimetypes           signal
_curses             dbm                 missingno           simplegeneric
_curses_panel       dbus                mistune             site
_cvxcore            debugpy             mizani              sitecustomize
_datetime           decimal             mlxtend             six
_dbm                decorator           mmap                skimage
_dbus_bindings      defusedxml          modulefinder        sklearn
_dbus_glib_bindings descartes           more_itertools      sklearn_pandas
_decimal            difflib             moviepy             slugify
_distutils_hack     dill                mpmath              smart_open
_dummy_thread       dis                 msgpack             smtpd
_ecos               distributed         multiprocess        smtplib
_elementtree        distutils           multiprocessing     sndhdr
_foo                django              multitasking        snowballstemmer
_functools          dlib                murmurhash          socket
_hashlib            docopt              music21             socketserver
_heapq              docs                natsort             socks
_imp                doctest             nbclient            sockshandler
_io                 docutils            nbconvert           softwareproperties
_json               dopamine            nbformat            sortedcontainers
_locale             dot_parser          nest_asyncio        spacy
_lsprof             dummy_threading     netrc               sphinx
_lzma               easy_install        networkx            spwd
_markupbase         easydict            nibabel             sql
_md5                ecos                nis                 sqlalchemy
_multibytecodec     editdistance        nisext              sqlite3
_multiprocessing    ee                  nltk                sqlparse
_opcode             email               nntplib             sre_compile
_operator           en_core_web_sm      notebook            sre_constants
_osx_support        encodings           np_utils            sre_parse
_pickle             entrypoints         ntpath              srsly
_plotly_future_     enum                nturl2path          ssl
_plotly_utils       ephem               numba               stat
_posixsubprocess    errno               numbergen           statistics
_pydecimal          et_xmlfile          numbers             statsmodels
_pyio               examples            numexpr             storemagic
_pyrsistent_version fa2                 numpy               string
_pytest             fancyimpute         nvidia_smi          stringprep
_random             fastai              oauth2client        struct
_rinterface_cffi_abi fastdtw             oauthlib            subprocess
_rinterface_cffi_api fastprogress        ogr                 sunau
_scs_direct         fastrlock           okgrade             symbol
_scs_indirect       faulthandler        onnx_chainer        sympy
_scs_python         fbprophet           opcode              sympyprinting
_sha1               fcntl               openpyxl            symtable
_sha256             feather             operator            sys
_sha3               filecmp             opt_einsum          sysconfig
_sha512             fileinput           optparse            syslog
_signal             filelock            os                  tables
_sitebuiltins       firebase_admin      osgeo               tabnanny
_socket             fix_yahoo_finance   osqp                tabulate
_sqlite3            flask               osqppurepy          tarfile
_sre                fnmatch             osr                 tblib
_ssl                folium              ossaudiodev         telnetlib
_stat               formatter           packaging           tempfile
_string             fractions           palettable          tensorboard
_strptime           ftplib              pandas              tensorboard_plugin_wit
_struct             functools           pandas_datareader   tensorboardcolab
_symtable           future              pandas_gbq          tensorflow
_sysconfigdata_m_linux_x86_64-linux-gnu gast                pandas_profiling    tensorflow_addons
_testbuffer         gc                  pandocfilters       tensorflow_datasets
_testcapi           gdal                panel               tensorflow_estimator
_testimportmultiple gdalconst           param               tensorflow_gcs_config
_testmultiphase     gdalnumeric         parser              tensorflow_hub
_thread             gdown               parso               tensorflow_metadata
_threading_local    genericpath         past                tensorflow_privacy
_tkinter            gensim              pasta               tensorflow_probability
_tracemalloc        geographiclib       pathlib             termcolor
_warnings           geopy               patsy               terminado
_weakref            getopt              pdb                 termios
_weakrefset         getpass             pexpect             test
abc                 gettext             pickle              testpath
absl                gi                  pickleshare         tests
aifc                gin                 pickletools         text_unidecode
alabaster           github2pypi         pip                 textblob
albumentations      glob                pipes               textgenrnn
altair              glob2               piptools            textwrap
antigravity         gnm                 pkg_resources       theano
apiclient           google_auth_httplib2 pkgutil             thinc
apt                 google_auth_oauthlib plac                this
apt_inst            google_drive_downloader plac_core           threading
apt_pkg             googleapiclient     plac_ext            tifffile
aptsources          googlesearch        plac_tk             time
argon2              graphviz            platform            timeit
argparse            gridfs              plistlib            tkinter
array               grp                 plotly              tlz
asgiref             grpc                plotlywidget        token
ast                 gspread             plotnine            tokenize
astor               gspread_dataframe   pluggy              toml
astropy             gym                 poplib              toolz
astunparse          gzip                portpicker          torch
async_generator     h5py                posix               torchsummary
asynchat            hashlib             posixpath           torchtext
asyncio             heapdict            pprint              torchvision
asyncore            heapq               prefetch_generator  tornado
atari_py            hmac                preshed             tqdm
atexit              holidays            prettytable         trace
atomicwrites        holoviews           profile             traceback
attr                html                progressbar         tracemalloc
audioop             html5lib            prometheus_client   traitlets
audioread           http                promise             tree
autograd            httpimport          prompt_toolkit      tty
autoreload          httplib2            pstats              turtle
babel               httplib2shim        psutil              tweepy
backcall            humanize            psycopg2            typeguard
base64              hyperopt            pty                 types
bdb                 ideep4py            ptyprocess          typing
bin                 idna                pvectorc            typing_extensions
binascii            image               pwd                 tzlocal
binhex              imageio             py                  umap
bisect              imagesize           py_compile          unicodedata
bleach              imaplib             pyarrow             unittest
blis                imblearn            pyasn1              uritemplate
bokeh               imgaug              pyasn1_modules      urllib
boost               imghdr              pyclbr              urllib3
bottleneck          imp                 pycocotools         uu
branca              importlib           pycparser           uuid
bs4                 importlib_metadata  pyct                vega_datasets
bson                importlib_resources pydata_google_auth  venv
builtins            imutils             pydoc               vis
bz2                 inflect             pydoc_data          warnings
cProfile            iniconfig           pydot               wasabi
cachecontrol        inspect             pydot_ng            wave
cachetools          intervaltree        pydotplus           wcwidth
caffe2              io                  pydrive             weakref
calendar            ipaddress           pyemd               webbrowser
catalogue           ipykernel           pyexpat             webencodings
certifi             ipykernel_launcher  pyglet              werkzeug
cffi                ipython_genutils    pygments            wheel
cgi                 ipywidgets          pygtkcompat         widgetsnbextension
cgitb               itertools           pylab               wordcloud
chainer             itsdangerous        pymc3               wrapt
chainermn           jax                 pymeeus             wsgiref
chainerx            jaxlib              pymongo             xarray
chardet             jdcal               pymystem3           xdrlib
chess               jedi                pynvml              xgboost
chunk               jieba               pyparsing           xkit
click               jinja2              pyrsistent          xlrd
client              joblib              pysndfile           xlwt
cloudpickle         jpeg4py             pystan              xml
cmake               json                pytest              xmlrpc
cmath               jsonschema          python_utils        xxlimited
cmd                 jupyter             pytz                xxsubtype
cmdstanpy           jupyter_client      pyviz_comms         yaml
code                jupyter_console     pywt                yellowbrick
codecs              jupyter_core        pyximport           zict
codeop              jupyterlab_pygments qtconsole           zipapp
colab               kaggle              qtpy                zipfile
collections         kapre               queue               zipimport
colorlover          keras               quopri              zipp
colorsys            keras_preprocessing random              zlib
community           keyword             re                  zmq
compileall          kiwisolver          readline            

Enter any module name to get more help.  Or, type &quot;modules spam&quot; to search
for modules whose name or summary contain the string &quot;spam&quot;.</code></pre>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-11-05T06:48:07.094Z" title="2020-11-05T06:48:07.094Z">2020-11-05</time></span><span class="level-item">Updated&nbsp;<time dateTime="2020-11-05T07:00:32.311Z" title="2020-11-05T07:00:32.311Z">2020-11-05</time></span><span class="level-item"><a class="link-muted" href="/categories/study/">study</a></span><span class="level-item">9 minutes read (About 1295 words)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2020/11/05/study/seaborn_with_matplotlib/">Seaborn with Matplotlib</a></h1><div class="content"><h1 id="seaborn-matplotlib"><a href="#seaborn-matplotlib" class="headerlink" title="seaborn + matplotlib"></a>seaborn + matplotlib</h1><p>출처: <a href="images/seaborn_with_matplotlib/https://jehyunlee.github.io/2020/09/30/Python-DS-34-seaborn_matplotlib/">Pega Devlog</a></p>
<ul>
<li><p>seaborn은 matplotlib을 쉽고 아름답게 쓰고자 만들어졌다.</p>
<ul>
<li>따라서 seaborn의 결과물은 당연히 matplotlib의 결과물이다.</li>
<li>그러나 간혹 seaborn이 그린 그림의 폰트, 색상에 접근이 되지 않아서 난처하다.</li>
<li>seaborn의 구조를 잘 이해하지 못하면 해결도 어렵다.</li>
</ul>
</li>
<li><p>v0.11 기준으로 seaborn에는 다음과 같은 함수들이 있다.</p>
<p><img src="https://user-images.githubusercontent.com/72365720/98201051-34f2eb00-1f72-11eb-8e45-aafa0f39653b.png" alt="seaborn functions"></p>
</li>
</ul>
<ul>
<li>matplotlib의 출력물은 <code>figure</code>와 <code>axes(축)</code>만을 반환한다.<ul>
<li>seaborn의 명령어 중 <code>axes</code>를 반환하는 것들은 matplotlib와 섞어 쓰기 좋다.</li>
<li>먼저 matplotlib의 객체 지향 <code>object oriented</code> interface를 사용해서 그림의 틀을 만든 뒤, 특정 <code>axes</code>에 seaborn을 삽입하면 된다.</li>
<li>결론적으로, 하고 싶은 것이 다 된다.</li>
</ul>
</li>
</ul>
<h2 id="Load-data"><a href="#Load-data" class="headerlink" title="Load data"></a>Load data</h2><ul>
<li>예제로 사용할 펭귄 데이터를 불러온다. (이 데이터는 seaborn에 내장되어 있다.)</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"></span><br><span class="line"><span class="comment"># Pandas에서 나오는 경고문 무시</span></span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">&#x27;ignore&#x27;</span>)</span><br><span class="line"></span><br><span class="line">penguins = sns.load_dataset(<span class="string">&quot;penguins&quot;</span>)</span><br><span class="line">penguins.head()</span><br></pre></td></tr></table></figure>




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

<pre><code>.dataframe tbody tr th &#123;
    vertical-align: top;
&#125;

.dataframe thead th &#123;
    text-align: right;
&#125;</code></pre>
<p></style></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>species</th>
      <th>island</th>
      <th>bill_length_mm</th>
      <th>bill_depth_mm</th>
      <th>flipper_length_mm</th>
      <th>body_mass_g</th>
      <th>sex</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Adelie</td>
      <td>Torgersen</td>
      <td>39.1</td>
      <td>18.7</td>
      <td>181.0</td>
      <td>3750.0</td>
      <td>Male</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Adelie</td>
      <td>Torgersen</td>
      <td>39.5</td>
      <td>17.4</td>
      <td>186.0</td>
      <td>3800.0</td>
      <td>Female</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Adelie</td>
      <td>Torgersen</td>
      <td>40.3</td>
      <td>18.0</td>
      <td>195.0</td>
      <td>3250.0</td>
      <td>Female</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Adelie</td>
      <td>Torgersen</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Adelie</td>
      <td>Torgersen</td>
      <td>36.7</td>
      <td>19.3</td>
      <td>193.0</td>
      <td>3450.0</td>
      <td>Female</td>
    </tr>
  </tbody>
</table>
</div>



<h2 id="figure-and-axes"><a href="#figure-and-axes" class="headerlink" title="figure and axes"></a>figure and axes</h2><ul>
<li>matplotlib으로 도화지(figure)를 깔고, 축공간(axes)을 만든다.</li>
<li>1 * 2 축공간을 구성한다.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">fig, axes = plt.subplots(ncols = <span class="number">2</span>, figsize = (<span class="number">8</span>, <span class="number">4</span>))</span><br><span class="line">fig.tight_layout()</span><br></pre></td></tr></table></figure>


<p><img src="/images/seaborn_with_matplotlib/output_10_0.png" alt="png"></p>
<h2 id="plot-with-matplotlib"><a href="#plot-with-matplotlib" class="headerlink" title="plot with matplotlib"></a>plot with matplotlib</h2><ul>
<li>matplotlib 기능을 이용해서 산점도를 그린다.<ul>
<li>x축은 부리 길이(bill length)</li>
<li>y축은 부리 위 아래 두께(bill depth)</li>
<li>색상은 종(species)으로 한다. (Adelie, Chinstrap, Gentoo가 있다.)</li>
</ul>
</li>
<li>두 축공간 중 왼쪽에만 그린다.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">fig, axes = plt.subplots(ncols = <span class="number">2</span>, figsize = (<span class="number">8</span>, <span class="number">4</span>))</span><br><span class="line"></span><br><span class="line">species_u = penguins[<span class="string">&quot;species&quot;</span>].unique()</span><br><span class="line"></span><br><span class="line"><span class="comment"># plot 0: matplotlib</span></span><br><span class="line"><span class="keyword">for</span> i, s <span class="keyword">in</span> <span class="built_in">enumerate</span>(species_u):</span><br><span class="line">    axes[<span class="number">0</span>].scatter(penguins[<span class="string">&quot;bill_length_mm&quot;</span>].loc[penguins[<span class="string">&quot;species&quot;</span>] == s],</span><br><span class="line">                    penguins[<span class="string">&quot;bill_depth_mm&quot;</span>].loc[penguins[<span class="string">&quot;species&quot;</span>] == s],</span><br><span class="line">                    c = <span class="string">f&quot;C<span class="subst">&#123;i&#125;</span>&quot;</span>, label = s, alpha = <span class="number">0.3</span></span><br><span class="line">                    )</span><br><span class="line">axes[<span class="number">0</span>].legend(species_u, title = <span class="string">&quot;species&quot;</span>)</span><br><span class="line">axes[<span class="number">0</span>].set_xlabel(<span class="string">&quot;Bill Length (mm)&quot;</span>)</span><br><span class="line">axes[<span class="number">0</span>].set_ylabel(<span class="string">&quot;Bill Depth (mm)&quot;</span>)</span><br><span class="line"></span><br><span class="line">fig.tight_layout()</span><br></pre></td></tr></table></figure>


<p><img src="/images/seaborn_with_matplotlib/output_13_0.png" alt="png"></p>
<h2 id="plot-with-seaborn"><a href="#plot-with-seaborn" class="headerlink" title="plot with seaborn"></a>plot with seaborn</h2><blockquote>
<p><a target="_blank" rel="noopener" href="https://seaborn.pydata.org/generated/seaborn.scatterplot.html"><em>seaborn.scatterplot</em></a></p>
</blockquote>
<ul>
<li>이번엔 같은 plot을 seaborn으로 그려보자.</li>
<li>위 코드에 아래 세 줄만 추가한다.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">fig, axes = plt.subplots(ncols = <span class="number">2</span>, figsize = (<span class="number">8</span>, <span class="number">4</span>))</span><br><span class="line"></span><br><span class="line">species_u = penguins[<span class="string">&quot;species&quot;</span>].unique()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i, s <span class="keyword">in</span> <span class="built_in">enumerate</span>(species_u):</span><br><span class="line">    axes[<span class="number">0</span>].scatter(penguins[<span class="string">&quot;bill_length_mm&quot;</span>].loc[penguins[<span class="string">&quot;species&quot;</span>] == s],</span><br><span class="line">                    penguins[<span class="string">&quot;bill_depth_mm&quot;</span>].loc[penguins[<span class="string">&quot;species&quot;</span>] == s],</span><br><span class="line">                    c = <span class="string">f&quot;C<span class="subst">&#123;i&#125;</span>&quot;</span>, label = s, alpha = <span class="number">0.3</span></span><br><span class="line">                    )</span><br><span class="line">axes[<span class="number">0</span>].legend(species_u, title = <span class="string">&quot;species&quot;</span>)</span><br><span class="line">axes[<span class="number">0</span>].set_xlabel(<span class="string">&quot;Bill Length (mm)&quot;</span>)</span><br><span class="line">axes[<span class="number">0</span>].set_ylabel(<span class="string">&quot;Bill Depth (mm)&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># plot 1: seaborn</span></span><br><span class="line">sns.scatterplot(<span class="string">&quot;bill_length_mm&quot;</span>, <span class="string">&quot;bill_depth_mm&quot;</span>, hue=<span class="string">&quot;species&quot;</span>, data=penguins, alpha=<span class="number">0.3</span>, ax=axes[<span class="number">1</span>])</span><br><span class="line">axes[<span class="number">1</span>].set_xlabel(<span class="string">&quot;Bill Length (mm)&quot;</span>)</span><br><span class="line">axes[<span class="number">1</span>].set_ylabel(<span class="string">&quot;Bill Depth (mm)&quot;</span>)</span><br><span class="line"></span><br><span class="line">fig.tight_layout()</span><br></pre></td></tr></table></figure>


<p><img src="/images/seaborn_with_matplotlib/output_16_0.png" alt="png"></p>
<ul>
<li>단 3줄로 거의 동일한 그림이 나왔다.<ul>
<li>scatter plot의 점 크기가 살짝 작다.</li>
<li>label의 투명도가 살짝 다르다.</li>
</ul>
</li>
<li>seaborn 명령어 scatterplot() 를 그대로 사용했다.</li>
<li>x축과 y축 label도 바꾸었다.<ul>
<li><code>ax = axes[1]</code> 인자에서 볼 수 있듯, 존재하는 <code>axes</code>에 그림만 얹었다.</li>
<li>matplotlib 틀 + seaborn 그림 이므로, matplotlib 명령이 모두 통한다.</li>
</ul>
</li>
</ul>
<h2 id="matplotlib-seaborn-amp-seaborn-matplotlib"><a href="#matplotlib-seaborn-amp-seaborn-matplotlib" class="headerlink" title="matplotlib + seaborn &amp; seaborn + matplotlib"></a>matplotlib + seaborn &amp; seaborn + matplotlib</h2><ul>
<li>matplotlib와 seaborn이 자유롭게 섞일 수 있다.<ul>
<li>matplotlib 산점도 위에 seaborn 추세선을 얹을 수 있다.</li>
<li>seaborn 산점도 위에 matplotlib 중심점을 얹을 수 있다.</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">fig, axes = plt.subplots(ncols = <span class="number">2</span>, figsize = (<span class="number">8</span>, <span class="number">4</span>))</span><br><span class="line"></span><br><span class="line">species_u = penguins[<span class="string">&quot;species&quot;</span>].unique()</span><br><span class="line"></span><br><span class="line"><span class="comment"># plot 0: matplotlib + seaborn</span></span><br><span class="line"><span class="keyword">for</span> i, s <span class="keyword">in</span> <span class="built_in">enumerate</span>(species_u):</span><br><span class="line">    <span class="comment"># matplotlib 산점도</span></span><br><span class="line">    axes[<span class="number">0</span>].scatter(penguins[<span class="string">&quot;bill_length_mm&quot;</span>].loc[penguins[<span class="string">&quot;species&quot;</span>] == s],</span><br><span class="line">                    penguins[<span class="string">&quot;bill_depth_mm&quot;</span>].loc[penguins[<span class="string">&quot;species&quot;</span>] == s],</span><br><span class="line">                    c = <span class="string">f&quot;C<span class="subst">&#123;i&#125;</span>&quot;</span>, label = s, alpha = <span class="number">0.3</span></span><br><span class="line">                    )</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># seaborn 추세선</span></span><br><span class="line">    sns.regplot(<span class="string">&quot;bill_length_mm&quot;</span>, <span class="string">&quot;bill_depth_mm&quot;</span>, data = penguins.loc[penguins[<span class="string">&quot;species&quot;</span>] == s],</span><br><span class="line">                scatter = <span class="literal">False</span>, ax = axes[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">axes[<span class="number">0</span>].legend(species_u, title = <span class="string">&quot;species&quot;</span>)</span><br><span class="line">axes[<span class="number">0</span>].set_xlabel(<span class="string">&quot;Bill Length (mm)&quot;</span>)</span><br><span class="line">axes[<span class="number">0</span>].set_ylabel(<span class="string">&quot;Bill Depth (mm)&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># plot 1: seaborn + matplotlib</span></span><br><span class="line"><span class="comment"># seaborn 산점도</span></span><br><span class="line">sns.scatterplot(<span class="string">&quot;bill_length_mm&quot;</span>, <span class="string">&quot;bill_depth_mm&quot;</span>, hue = <span class="string">&quot;species&quot;</span>, data = penguins, alpha = <span class="number">0.3</span>, ax = axes[<span class="number">1</span>])</span><br><span class="line">axes[<span class="number">1</span>].set_xlabel(<span class="string">&quot;Bill Length (mm)&quot;</span>)</span><br><span class="line">axes[<span class="number">1</span>].set_ylabel(<span class="string">&quot;Bill Depth (mm)&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># matplotlib 중심점</span></span><br><span class="line"><span class="keyword">for</span> i, s <span class="keyword">in</span> <span class="built_in">enumerate</span>(species_u):</span><br><span class="line">  axes[<span class="number">1</span>].scatter(penguins[<span class="string">&quot;bill_length_mm&quot;</span>].loc[penguins[<span class="string">&quot;species&quot;</span>] == s].mean(),</span><br><span class="line">                  penguins[<span class="string">&quot;bill_depth_mm&quot;</span>].loc[penguins[<span class="string">&quot;species&quot;</span>] == s].mean(),</span><br><span class="line">                  c = <span class="string">f&quot;C<span class="subst">&#123;i&#125;</span>&quot;</span>, alpha = <span class="number">1</span>, marker = <span class="string">&quot;x&quot;</span>, s = <span class="number">100</span></span><br><span class="line">                  )</span><br><span class="line"></span><br><span class="line">fig.tight_layout()</span><br></pre></td></tr></table></figure>


<p><img src="/images/seaborn_with_matplotlib/output_20_0.png" alt="png"></p>
<h2 id="seaborn-seaborn-matplotlib"><a href="#seaborn-seaborn-matplotlib" class="headerlink" title="seaborn + seaborn + matplotlib"></a>seaborn + seaborn + matplotlib</h2><ul>
<li>안 될 이유가 없다.</li>
<li>seaborn <code>scatterplot</code> + seaborn <code>kdeplot</code> + matplotlib <code>text</code> 이다.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">fig, ax = plt.subplots(figsize = (<span class="number">6</span>, <span class="number">5</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># plot 0: scatter plot</span></span><br><span class="line">sns.scatterplot(<span class="string">&quot;bill_length_mm&quot;</span>, <span class="string">&quot;bill_depth_mm&quot;</span>, color = <span class="string">&quot;k&quot;</span>, data = penguins, alpha = <span class="number">0.3</span>, ax = ax, legend = <span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">ax.set_xlabel(<span class="string">&quot;Bill Length (mm)&quot;</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">&quot;Bill Depth (mm)&quot;</span>)</span><br><span class="line"></span><br><span class="line">fig.tight_layout()</span><br></pre></td></tr></table></figure>


<p><img src="/images/seaborn_with_matplotlib/output_23_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">fig, ax = plt.subplots(figsize = (<span class="number">6</span>, <span class="number">5</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># plot 0: scatter plot</span></span><br><span class="line">sns.scatterplot(<span class="string">&quot;bill_length_mm&quot;</span>, <span class="string">&quot;bill_depth_mm&quot;</span>, color = <span class="string">&quot;k&quot;</span>, data = penguins, alpha = <span class="number">0.3</span>, ax = ax, legend = <span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># plot 1: kde plot</span></span><br><span class="line">sns.kdeplot(<span class="string">&quot;bill_length_mm&quot;</span>, <span class="string">&quot;bill_depth_mm&quot;</span>, hue = <span class="string">&quot;species&quot;</span>, data = penguins, alpha = <span class="number">0.5</span>, ax = ax, legend = <span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">ax.set_xlabel(<span class="string">&quot;Bill Length (mm)&quot;</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">&quot;Bill Depth (mm)&quot;</span>)</span><br><span class="line"></span><br><span class="line">fig.tight_layout()</span><br></pre></td></tr></table></figure>


<p><img src="/images/seaborn_with_matplotlib/output_24_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">fig, ax = plt.subplots(figsize = (<span class="number">6</span>, <span class="number">5</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># plot 0: scatter plot</span></span><br><span class="line">sns.scatterplot(<span class="string">&quot;bill_length_mm&quot;</span>, <span class="string">&quot;bill_depth_mm&quot;</span>, color = <span class="string">&quot;k&quot;</span>, data = penguins, alpha = <span class="number">0.3</span>, ax = ax, legend = <span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># plot 1: kde plot</span></span><br><span class="line">sns.kdeplot(<span class="string">&quot;bill_length_mm&quot;</span>, <span class="string">&quot;bill_depth_mm&quot;</span>, hue = <span class="string">&quot;species&quot;</span>, data = penguins, alpha = <span class="number">0.5</span>, ax = ax, legend = <span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># text:</span></span><br><span class="line">species_u = penguins[<span class="string">&quot;species&quot;</span>].unique()</span><br><span class="line"><span class="keyword">for</span> i, s <span class="keyword">in</span> <span class="built_in">enumerate</span>(species_u):</span><br><span class="line">    ax.text(penguins[<span class="string">&quot;bill_length_mm&quot;</span>].loc[penguins[<span class="string">&quot;species&quot;</span>] == s].mean(),</span><br><span class="line">            penguins[<span class="string">&quot;bill_depth_mm&quot;</span>].loc[penguins[<span class="string">&quot;species&quot;</span>] == s].mean(),</span><br><span class="line">            s = s, fontdict = &#123;<span class="string">&quot;fontsize&quot;</span>: <span class="number">14</span>, <span class="string">&quot;fontweight&quot;</span>: <span class="string">&quot;bold&quot;</span>, <span class="string">&quot;color&quot;</span>: <span class="string">&quot;k&quot;</span>&#125;</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">ax.set_xlabel(<span class="string">&quot;Bill Length (mm)&quot;</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">&quot;Bill Depth (mm)&quot;</span>)</span><br><span class="line"></span><br><span class="line">fig.tight_layout()</span><br></pre></td></tr></table></figure>


<p><img src="/images/seaborn_with_matplotlib/output_25_0.png" alt="png"></p>
<h2 id="결론"><a href="#결론" class="headerlink" title="결론"></a>결론</h2><ul>
<li>seaborn을 matplotlib와 마음껏 섞어쓰자.</li>
<li>단, <strong>axes</strong>를 반환하는 명령어에 한해서다.</li>
<li>이런 명령어를 <strong>axes-lebel function</strong>이라고 한다.</li>
</ul>
<h1 id="seaborn-matplotlib을-이용한-jointplot-보완"><a href="#seaborn-matplotlib을-이용한-jointplot-보완" class="headerlink" title="seaborn + matplotlib을 이용한 jointplot 보완"></a>seaborn + matplotlib을 이용한 jointplot 보완</h1></div></article></div><nav class="pagination" role="navigation" aria-label="pagination"><div class="pagination-previous is-invisible is-hidden-mobile"><a href="/categories/study/page/0/">Previous</a></div><div class="pagination-next"><a href="/categories/study/page/2/">Next</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link is-current" href="/categories/study/">1</a></li><li><a class="pagination-link" href="/categories/study/page/2/">2</a></li></ul></nav></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/avatar.png" alt="Your name"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Your name</p><p class="is-size-6 is-block">Your title</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Your location</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">16</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Category</p><a href="/categories"><p class="title">1</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">0</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/ppoffice" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/ppoffice"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">Links</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://hexo.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Hexo</span></span><span class="level-right"><span class="level-item tag">hexo.io</span></span></a></li><li><a class="level is-mobile" href="https://bulma.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Bulma</span></span><span class="level-right"><span class="level-item tag">bulma.io</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/study/"><span class="level-start"><span class="level-item">study</span></span><span class="level-end"><span class="level-item tag">15</span></span></a></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2020-12-01T02:54:47.536Z">2020-12-01</time></p><p class="title"><a href="/2020/12/01/study/kaggle_how_to_choose_right_metric_for_evaluating_ml_model_vipulgandhi/">ML 모델 을 평가하기 위한 올바른 측정 항목을 선택하는 방법</a></p><p class="categories"><a href="/categories/study/">study</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2020-12-01T02:54:41.038Z">2020-12-01</time></p><p class="title"><a href="/2020/12/01/study/Tour_of_Evaluation_Metrics_for_Imbalanced_Classification/">불균형 분류에 대한 평가 지표 둘러보기 (번역)</a></p><p class="categories"><a href="/categories/study/">study</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2020-12-01T01:06:39.359Z">2020-12-01</time></p><p class="title"><a href="/2020/12/01/study/%EC%82%AC%EC%9D%B4%ED%82%B7%EB%9F%B0%EC%9C%BC%EB%A1%9C_%EC%8B%9C%EC%9E%91%ED%95%98%EB%8A%94_%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D/">파이썬 머신러닝 완벽가이드 2장</a></p><p class="categories"><a href="/categories/study/">study</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2020-12-01T01:06:14.981Z">2020-12-01</time></p><p class="title"><a href="/2020/12/01/study/kaggle_titanic-eda-simple-model-0-80622_mviola/">캐글 Titanic EDA + Simple Model [0.80622] 분석</a></p><p class="categories"><a href="/categories/study/">study</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2020-11-30T00:15:42.563Z">2020-11-30</time></p><p class="title"><a href="/2020/11/30/study/4.2%20%EA%B2%B0%EC%A0%95%20%ED%8A%B8%EB%A6%AC_Ver01/">파이썬 머신러닝 완벽가이드 4장</a></p><p class="categories"><a href="/categories/study/">study</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2020/12/"><span class="level-start"><span class="level-item">December 2020</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/11/"><span class="level-start"><span class="level-item">November 2020</span></span><span class="level-end"><span class="level-item tag">11</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/10/"><span class="level-start"><span class="level-item">October 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><!--!--><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">Subscribe for updates</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div><div class="card widget"><div class="card-content"><div class="notification is-danger">You need to set <code>client_id</code> and <code>slot_id</code> to show this AD unit. Please set it in <code>_config.yml</code>.</div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="제목" height="28"></a><p class="is-size-7"><span>&copy; 2020 JustY</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" async></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>