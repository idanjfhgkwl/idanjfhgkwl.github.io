{"pages":[],"posts":[{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2020/10/30/hello-world/"},{"title":"Chapter_1_2_Python_visualisation_seaborn","text":"1. Matplotlib &amp; Seaborn1-1. 기본 개요Matplotlib: 파이썬 표준 시각화 도구. 파이썬 그래프의 기본 토대. 객체지향 프로그래밍을 지원하므로 세세하게 꾸밀 수 있다. Seaborn: 파이썬 시각화 도구의 고급 버전. Matplotlib에 비해 비교적 단순한 인터페이스를 제공하기 때문에 초보자도 어렵지 않게 배울 수 있다. 1-2. matplotlib &amp; Seabon 설치설치방법은 윈도우 명령 프롬프트, MacOS, Linux 터미널에서 pip install matplotlib입력하면 되지만, 간혹 여러 환경에 따라 달라질 수 있으니 관련 싸이트에서 확인하기를 바란다. matplotlib 설치 방법: https://matplotlib.org/users/installing.html seaborn 설치 방법: https://seaborn.pydata.org/installing.html 2. 기본적인 시각화 문법 시각화 문법은 아래와 같다. 12import seaborn as snssns.name_of_graph(x, y, dataset, options) 우선 Sample 데이터를 불러와서 데이터를 확인해보자. 123456import seaborn as snsfrom tabulate import tabulatesns.set()tips = sns.load_dataset(&quot;tips&quot;)print(tabulate(tips.head(), tablefmt=&quot;pipe&quot;, headers=&quot;keys&quot;)) # Hugo 블로그 전용 위 데이터는 매우 간단한 테이블일 수 있지만, 다변량의 그래프를 하나의 이미지 안에서 어떤 형태로 그래프를 작성할 것인지 선택하는 것은 쉽지 않다. 123sns.relplot(x=&quot;total_bill&quot;, y=&quot;tip&quot;, col=&quot;time&quot;, hue=&quot;sex&quot;, style=&quot;smoker&quot;, size=&quot;size&quot;, data=tips); 소스코드에 대한 설명을 간단히 하면 아래와 같다. relplot은 다변량의 그래프를 작성할 때 사용한다. col 대신 row를 사용해도 된다. 여기에는 categorical(=범주형) 자료가 온다. (만약 값이 많으면..?) hue는 그래프에 표현되는 색상을 의미한다. style은 범주형 자료를 다르게 표현할 때 사용한다. (예: 동그라미, 별표 등) 대개 범주형 데이터를 지정한다. size 자료의 크기를 의미한다. 3. Grouped barplots123456789101112131415import seaborn as snssns.set_theme(style=&quot;whitegrid&quot;) # 축의 색상# 온라인 저장소에서 예제 데이터 세트를 로드한다.penguins = sns.load_dataset(&quot;penguins&quot;)# Draw a nested barplot by species and sexg = sns.catplot( data=penguins, kind=&quot;bar&quot;, x=&quot;species&quot;, y=&quot;body_mass_g&quot;, hue=&quot;sex&quot;, ci=&quot;sd&quot;, palette=&quot;dark&quot;, alpha=.6, height=6)g.despine(left=True)g.set_axis_labels(&quot;&quot;, &quot;Body mass (g)&quot;)g.legend.set_title(&quot;&quot;) 소스코드에 대한 설명을 간단히 하면 아래와 같다. seaborn.catplot: 하나의 수치형 변수와 하나 이상의 범주형 변수 간의 관계를 보여주는 그래프 kind: 그릴 플롯의 종류는 범주 형 좌표축 수준 플로팅 함수의 이름에 해당합니다. x, y, hue: 긴 형식의 데이터를 그리기위한 입력입니다. ci: 추정값 주위를 그리는 신뢰 구간의 크기입니다. “sd”인 경우 부트 스트랩을 건너 뛰고 관측 값의 표준 편차를 그립니다. palette: 다양한 수준의 hue변수 에 사용할 색상 입니다. alpha: ?? (그래프 투명도? 숫자가 작으면 연해진다.) height: 전체 플롯 세로 높이 (가로도 자동으로 조정된다?)","link":"/2020/11/02/study/kaggle_edu_Chapter_1_2_Python_visualisation_seaborn/"},{"title":"8장 기본 명령어","text":"8.1 SELECT테이블 전체를 검색하려면 *12SELECT *FROM TB_CUSTOMER; 필드가 많은 테이블에서 필요한 내용만 검색하려면 ,12345SELECT CUSTOMER_CD, CUSTOMER_NM, PHONE_NUMBER, EMAILFROM TB_CUSTOMER; 필드 제목을 한글로 바꾸려면 AS (생략가능)12345SELECT CUSTOMER_CD AS 고객코드, CUSTOMER_NM AS 고객명, PHONE_NUMBER AS 전화번호, EMAIL AS 이메일FROM TB_CUSTOMER; 수정한 게 안 올라간다","link":"/2020/11/02/study/sql_10minutes_ch08_basic_command_language/"},{"title":"캐글 타이타닉 분석","text":"출처: colab, 전태균님의 타이타닉 분석https://www.kaggle.com/daehungwak/guide-kor-dg 접기/펼치기 사전 준비Kaggle 데이터 불러오기Kaggle API 설치1!pip install kaggle Requirement already satisfied: kaggle in /usr/local/lib/python3.6/dist-packages (1.5.9) Requirement already satisfied: slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (0.0.1) Requirement already satisfied: urllib3 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.24.3) Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.0.1) Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.8.1) Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.41.1) Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle) (2020.6.20) Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.23.0) Requirement already satisfied: six&gt;=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.15.0) Requirement already satisfied: text-unidecode&gt;=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify-&gt;kaggle) (1.3) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;kaggle) (3.0.4) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;kaggle) (2.10) Kaggle Token 다운로드 아래 코드는 Kaggle API 토큰을 업로드 하는 코드이다. 12345678from google.colab import filesuploaded = files.upload()for fn in uploaded.keys(): print('uploaded file &quot;{name}&quot; with length {length} bytes'.format( name=fn, length=len(uploaded[fn]))) # kaggle.json을 아래 폴더로 옮긴 뒤, file을 사용할 수 있도록 권한을 부여한다. !mkdir -p ~/.kaggle/ &amp;&amp; mv kaggle.json ~/.kaggle/ &amp;&amp; chmod 600 ~/.kaggle/kaggle.json Upload widget is only available when the cell has been executed in the current browser session. Please rerun this cell to enable. Saving kaggle.json to kaggle.json uploaded file &quot;kaggle.json&quot; with length 63 bytes 실제로 kaggle.json 파일이 업로드 되었는지 확인 1ls -1ha ~/.kaggle/kaggle.json /root/.kaggle/kaggle.json 구글 드라이브 연동123456789101112from google.colab import drive # 패키지 불러오기 from os.path import join # 구글 드라이브 마운트ROOT = &quot;/content/drive&quot; # 드라이브 기본 경로print(ROOT) # print content of ROOT (Optional)drive.mount(ROOT) # 드라이브 기본 경로 # 프로젝트 파일 생성 및 다운받을 경로 이동MY_GOOGLE_DRIVE_PATH = 'My Drive/Colab Notebooks/python_basic/kaggle_titanic/data'PROJECT_PATH = join(ROOT, MY_GOOGLE_DRIVE_PATH)print(PROJECT_PATH) /content/drive Mounted at /content/drive /content/drive/My Drive/Colab Notebooks/python_basic/kaggle_titanic/data 1%cd &quot;{PROJECT_PATH}&quot; /content/drive/My Drive/Colab Notebooks/python_basic/kaggle_titanic/data kaggle competition list 불러오기 캐글 대회 목록 불러오기 1!kaggle competitions list Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.9 / client 1.5.4) ref deadline category reward teamCount userHasEntered --------------------------------------------- ------------------- --------------- --------- --------- -------------- contradictory-my-dear-watson 2030-07-01 23:59:00 Getting Started Prizes 134 False gan-getting-started 2030-07-01 23:59:00 Getting Started Prizes 161 False tpu-getting-started 2030-06-03 23:59:00 Getting Started Knowledge 292 False digit-recognizer 2030-01-01 00:00:00 Getting Started Knowledge 2248 False titanic 2030-01-01 00:00:00 Getting Started Knowledge 17260 True house-prices-advanced-regression-techniques 2030-01-01 00:00:00 Getting Started Knowledge 4325 True connectx 2030-01-01 00:00:00 Getting Started Knowledge 366 False nlp-getting-started 2030-01-01 00:00:00 Getting Started Knowledge 1130 False rock-paper-scissors 2021-02-01 23:59:00 Playground Prizes 226 False riiid-test-answer-prediction 2021-01-07 23:59:00 Featured $100,000 1491 False nfl-big-data-bowl-2021 2021-01-05 23:59:00 Analytics $100,000 0 False competitive-data-science-predict-future-sales 2020-12-31 23:59:00 Playground Kudos 9392 False halite-iv-playground-edition 2020-12-31 23:59:00 Playground Knowledge 44 False predict-volcanic-eruptions-ingv-oe 2020-12-28 23:59:00 Playground Swag 198 False hashcode-drone-delivery 2020-12-14 23:59:00 Playground Knowledge 80 False cdp-unlocking-climate-solutions 2020-12-02 23:59:00 Analytics $91,000 0 False lish-moa 2020-11-30 23:59:00 Research $30,000 3454 False google-football 2020-11-30 23:59:00 Featured $6,000 925 False conways-reverse-game-of-life-2020 2020-11-30 23:59:00 Playground Swag 132 False lyft-motion-prediction-autonomous-vehicles 2020-11-25 23:59:00 Featured $30,000 788 False Titanic: Machine Learning from Disaster 데이터셋 불러오기 타이타닉 대회 데이터를 가져오는 코드이다. 1!kaggle competitions download -c titanic Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.9 / client 1.5.4) Downloading train.csv to /content/drive/My Drive/Colab Notebooks/python_basic/kaggle_titanic/data 0% 0.00/59.8k [00:00&lt;?, ?B/s] 100% 59.8k/59.8k [00:00&lt;00:00, 7.40MB/s] Downloading gender_submission.csv to /content/drive/My Drive/Colab Notebooks/python_basic/kaggle_titanic/data 0% 0.00/3.18k [00:00&lt;?, ?B/s] 100% 3.18k/3.18k [00:00&lt;00:00, 439kB/s] Downloading test.csv to /content/drive/My Drive/Colab Notebooks/python_basic/kaggle_titanic/data 0% 0.00/28.0k [00:00&lt;?, ?B/s] 100% 28.0k/28.0k [00:00&lt;00:00, 3.95MB/s] 리눅스 명령어 ls는 경로(폴더) 내 모든 데이터 파일을 보여준다. 1!ls gender_submission.csv test.csv train.csv 캐글 데이터 수집 및 EDA우선 데이터를 수집하기에 앞서서 EDA에 관한 필수 패키지를 설치하자. 12345678import pandas as pd # 데이터 가공, 변환(dplyr)import pandas_profiling # 보고서 기능 # 아나콘다 할 때... 실습import numpy as np # 수치 연산 &amp; 배열, 행렬import matplotlib as mpl # 시각화import matplotlib.pyplot as plt # 시각화import seaborn as sns # 시각화from IPython.core.display import display, HTML 데이터 수집여기에서는 우선 test.csv &amp; train.csv 파일을 받도록 한다. 1234# 경로 변경 (프로젝트 파일 생성 및 다운받을 경로 이동)MY_GOOGLE_DRIVE_PATH = 'My Drive/Colab Notebooks/python_basic/kaggle_titanic'PROJECT_PATH = join(ROOT, MY_GOOGLE_DRIVE_PATH)print(PROJECT_PATH) /content/drive/My Drive/Colab Notebooks/python_basic/kaggle_titanic 1%cd &quot;{PROJECT_PATH}&quot; /content/drive/My Drive/Colab Notebooks/python_basic/kaggle_titanic 1!ls data source 123df_train = pd.read_csv('data/train.csv')df_test = pd.read_csv('data/test.csv')print(&quot;data import is done&quot;) data import is done 데이터 확인 Kaggle 데이터를 불러오면 우선 확인해야 하는 것은 데이터셋의 크기다. 변수의 갯수 수치형 변수 &amp; 범주형 변수의 개수 등을 파악해야 한다. Point 1 - train데이터에서 굳이 훈련데이터와 테스트 데이터를 구분할 필요는 없다. 보통 Kaggle에서는 테스트 데이터를 주기적으로 업데이트 해준다. Point 2 - 보통 test 데이터의 변수의 개수가 하나 더 작다. 1df_train.shape, df_test.shape ((891, 12), (418, 11)) 그 후 train데이터의 상위 5개의 데이터만 확인한다. 1display(df_train.head()) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked 0 1 0 3 Braund, Mr. Owen Harris male 22.0 1 0 A/5 21171 7.2500 NaN S 1 2 1 1 Cumings, Mrs. John Bradley (Florence Briggs Th... female 38.0 1 0 PC 17599 71.2833 C85 C 2 3 1 3 Heikkinen, Miss. Laina female 26.0 0 0 STON/O2. 3101282 7.9250 NaN S 3 4 1 1 Futrelle, Mrs. Jacques Heath (Lily May Peel) female 35.0 1 0 113803 53.1000 C123 S 4 5 0 3 Allen, Mr. William Henry male 35.0 0 0 373450 8.0500 NaN S 그 다음 확인해야 하는 것은 수치형 변수와 범주형 변수를 구분한다. 먼저 numerical_features를 구분하자. 데이터의 변수가 많아서, 일단 숫자형과 문자형으로 분리한 후, EDA를 하려고 한다. 아래 코드는 train데이터에서 숫자형 변수만 추출하는 코드이다. 123numeric_features = df_train.select_dtypes(include = [np.number]) # 수치형 데이터print(numeric_features.columns)print(&quot;The total number of include numeric features are: &quot;, len(numeric_features.columns)) Index(['PassengerId', 'Survived', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare'], dtype='object') The total number of include numeric features are: 7 numeric_features을 제외한 나머지 변수를 추출하자. (Categorical 등) 123categorical_features = df_train.select_dtypes(exclude = [np.number]) # 수치형이 아닌 데이터print(categorical_features.columns)print(&quot;The total number of exclude numeric features are: &quot;, len(categorical_features.columns)) Index(['Name', 'Sex', 'Ticket', 'Cabin', 'Embarked'], dtype='object') The total number of exclude numeric features are: 5 우선 전체 데이터는 891개 변수는 12개로 확인했다. 그 중 수치형 변수는 7개, 문자형 변수는 5개인 것으로 확인된다. 타이타닉 분석 따라하기 colab, 전태균님의 타이타닉 분석 타이타닉에 탑승한 사람들의 신상정보를 활용하여, 승선한 사람들의 생존여부를 예측하는 모델을 생성할 것이다. 12345678910111213141516import numpy as npimport pandas as pdimport matplotlib.pyplot as pltimport seaborn as sns# matplotlib 의 기본 scheme 말고 seaborn scheme 을 세팅하고,# 일일이 graph 의 font size 를 지정할 필요 없이 seaborn 의 font_scale 을 사용하면 편하다.plt.style.use('seaborn')sns.set(font_scale = 2.1)import missingno as msno# warnings 무시import warningswarnings.filterwarnings('ignore')%matplotlib inline 데이터셋 확인 대부분의 캐글 데이터들은 장 정제되어 있다. 하지만 가끔 null data가 존재한다. 이를 확인하고, 향후 수정한다. pandas는 파이썬에서 테이블화 된 데이터를 다루는 데 가장 최적화되어 있으며, 많이 쓰이는 라이브러리이다. pandas를 사용하여 Dataset의 간단한 통계적 분석부터, 복잡한 처리들을 간단한 메소드를 사용하여 해낼 수 있다. pandas는 파이썬으로 데이터 분석을 한다고 하면 능숙해져야 할 라이브러리이다. 1df_train.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked 0 1 0 3 Braund, Mr. Owen Harris male 22.0 1 0 A/5 21171 7.2500 NaN S 1 2 1 1 Cumings, Mrs. John Bradley (Florence Briggs Th... female 38.0 1 0 PC 17599 71.2833 C85 C 2 3 1 3 Heikkinen, Miss. Laina female 26.0 0 0 STON/O2. 3101282 7.9250 NaN S 3 4 1 1 Futrelle, Mrs. Jacques Heath (Lily May Peel) female 35.0 1 0 113803 53.1000 C123 S 4 5 0 3 Allen, Mr. William Henry male 35.0 0 0 373450 8.0500 NaN S 이 타이타닉에서 Feature는 Pclass(티켓의 클래스), Age(성별), SibSp(함께 탑승한 형제와 배우자의 수), Parch(함께 탑승한 부모, 아이의 수), Fare(탑승료) 이며, 예측하려는 target label은 Survived(생존여부) 이다. Feature Engineering은 머신러닝 알고리즘을 작동하기 위해 데이터에 대한 도메인 지식을 활용하여 특징(Feature)을 만들어내는 과정이다. 출처 또는 머신러닝 모델을 위한 데이터 테이블의 컬럼(특징)을 생성하거나 선택하는 작업을 의미한다. 간단히 정리하면, 모델의 성능을 높이기 위해, 모델에 입력할 데이터를 만들기 위해 주어진 초기 데이터로부터 특징을 가공하고 생성하는 전체 과정을 의미한다. Null data checkdescribe() 메소드를 쓰면 각 feature가 가진 통계치들을 반환해준다. 1df_train.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PassengerId Survived Pclass Age SibSp Parch Fare count 891.000000 891.000000 891.000000 714.000000 891.000000 891.000000 891.000000 mean 446.000000 0.383838 2.308642 29.699118 0.523008 0.381594 32.204208 std 257.353842 0.486592 0.836071 14.526497 1.102743 0.806057 49.693429 min 1.000000 0.000000 1.000000 0.420000 0.000000 0.000000 0.000000 25% 223.500000 0.000000 2.000000 20.125000 0.000000 0.000000 7.910400 50% 446.000000 0.000000 3.000000 28.000000 0.000000 0.000000 14.454200 75% 668.500000 1.000000 3.000000 38.000000 1.000000 0.000000 31.000000 max 891.000000 1.000000 3.000000 80.000000 8.000000 6.000000 512.329200 1df_test.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PassengerId Pclass Age SibSp Parch Fare count 418.000000 418.000000 332.000000 418.000000 418.000000 417.000000 mean 1100.500000 2.265550 30.272590 0.447368 0.392344 35.627188 std 120.810458 0.841838 14.181209 0.896760 0.981429 55.907576 min 892.000000 1.000000 0.170000 0.000000 0.000000 0.000000 25% 996.250000 1.000000 21.000000 0.000000 0.000000 7.895800 50% 1100.500000 3.000000 27.000000 0.000000 0.000000 14.454200 75% 1204.750000 3.000000 39.000000 1.000000 0.000000 31.500000 max 1309.000000 3.000000 76.000000 8.000000 9.000000 512.329200 이 테이블을 보면 PassengerId 숫자와 다른, null data가 존재하는 열(feature)이 있는 것 같다고 하는데 공부가 필요하다. 이를 좀 더 보기 편하도록 그래프로 시각화해서 살펴본다. 123for col in df_train.columns: msg = 'column: {:&gt;10}\\t Percent of NaN value: {:.2f}%'.format(col, 100 * (df_train[col].isnull().sum() / df_train[col].shape[0])) print(msg) column: PassengerId Percent of NaN value: 0.00% column: Survived Percent of NaN value: 0.00% column: Pclass Percent of NaN value: 0.00% column: Name Percent of NaN value: 0.00% column: Sex Percent of NaN value: 0.00% column: Age Percent of NaN value: 19.87% column: SibSp Percent of NaN value: 0.00% column: Parch Percent of NaN value: 0.00% column: Ticket Percent of NaN value: 0.00% column: Fare Percent of NaN value: 0.00% column: Cabin Percent of NaN value: 77.10% column: Embarked Percent of NaN value: 0.22% 123for col in df_test.columns: msg = 'column: {:&gt;10}\\t Percent of NaN value: {:.2f}%'.format(col, 100 * (df_test[col].isnull().sum() / df_test[col].shape[0])) print(msg) column: PassengerId Percent of NaN value: 0.00% column: Pclass Percent of NaN value: 0.00% column: Name Percent of NaN value: 0.00% column: Sex Percent of NaN value: 0.00% column: Age Percent of NaN value: 20.57% column: SibSp Percent of NaN value: 0.00% column: Parch Percent of NaN value: 0.00% column: Ticket Percent of NaN value: 0.00% column: Fare Percent of NaN value: 0.24% column: Cabin Percent of NaN value: 78.23% column: Embarked Percent of NaN value: 0.00% Train, Test 데이터셋에서 Age(둘 다 약 20%), Cabin(둘 다 약 80%), Embarked(Train만 0.22%) null data가 존재하는 것을 볼 수 있다. missingno(MSNO)라는 라이브러리를 사용하면 null data의 존재를 더 쉽게 볼 수 있다. 1msno.matrix(df = df_train.iloc[:, :], figsize = (8, 8), color = (0.8, 0.5, 0.2)) &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f0543ec17b8&gt; 1msno.bar(df = df_train.iloc[:, :], figsize = (8, 8), color = (0.8, 0.5, 0.2)) &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f0541623eb8&gt; 1msno.bar(df = df_test.iloc[:, :], figsize = (8, 8), color = (0.8, 0.5, 0.2)) &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f053fdb0c50&gt; Target label 확인 target label이 어떤 분포(distribution)를 가지고 있는 지 확인해봐야 한다. binary classification 문제의 경우에서, 1과 0의 분포가 어떠냐에 따라 모델의 평가 방법이 달라질 수 있다. 123456789f, ax = plt.subplots(1, 2, figsize = (18, 8))df_train['Survived'].value_counts().plot.pie(explode = [0, 0.1], autopct = '%1.1f%%', ax = ax[0], shadow = True)ax[0].set_title('Pie plot - Survived')ax[0].set_ylabel('')sns.countplot('Survived', data = df_train, ax = ax[1])ax[1].set_title('Count plot - Survived')plt.show() 38.4%가 살아남았다는 걸 알 수 있다. target label의 분포가 제법 균일(balanced)하다. 불균일한 경우, 예를 들어 100 중 1이 99, 0이 1개인 경우에는 만약 모델이 모든 것을 1이라 해도 정확도가 99%가 나오게 된다. 0을 찾는 문제라면 이 모델은 원하는 결과를 줄 수 없게 된다. EDA 탐색적 데이터 분석 Exploratory Data Analysis. 많은 데이터 안의 숨겨진 사실을 찾기 위해서는 적절한 시각화가 필요하다. 시각화 라이브러리는 matplotlib, seaborn, plotly 등이 있다. 특정 목적에 맞는 소스코드를 정리해서 필요할 때마다 참고하면 편하다. Pclass Pclass는 서수형 데이터(ordinal)이다. 카테고리이면서, 순서가 있는 데이터 타입이다. Pclass에 따른 생존률의 차이를 살펴보겠다. 엑셀의 피벗 차트와 유사한 작업을 하게 되는데, pandas dataframe엣는 groupby를 사용하면 쉽게 할 수 있다. 또한 pivot이라는 메소드도 있다. ‘Pclass’, “Survived’를 가져온 후, Pclass로 묶는다. 그러고 나면 각 Pclass마다 0과 1이 count가 되는데, 이를 평균 내면 각 Pclass별 생존률이 나온다. 아래와 같이 count()를 하면, 각 Pclass에 몇 명이 있는 지 확인할 수 있다. 1df_train[['Pclass', 'Survived']].groupby(['Pclass'], as_index = True).count() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Survived Pclass 1 216 2 184 3 491 아래와 같이 sum()을 하면, 216명 중 생존한(Survived = 1) 사람의 총합을 주게 된다. 1df_train[['Pclass', 'Survived']].groupby(['Pclass'], as_index = True).sum() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Survived Pclass 1 136 2 87 3 119 pandas의 crosstab을 사용하면 위 과정을 좀 더 수월하게 볼 수 있다. 1pd.crosstab(df_train['Pclass'], df_train['Survived'], margins = True).style.background_gradient(cmap = 'summer_r') 그룹화된 객체에 mean()을 하게 되면, 각 클래스별 생존률을 얻을 수 있다. 1df_train[['Pclass', 'Survived']].groupby(['Pclass'], as_index = True).mean().sort_values(by = 'Survived', ascending = False).plot.bar() &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f053fbfbe80&gt; Pclass가 좋을 수록(1st) 생존률이 높은 것을 확인할 수 있다. seaborn의 countplot을 이용하면 특정 label에 따른 개수를 확인해볼 수 있다. 12345678y_position = 1.02f, ax = plt.subplots(1, 2, figsize = (18, 8))df_train['Pclass'].value_counts().plot.bar(color = ['#CD7F32','#FFDF00','#D3D3D3'], ax = ax[0])ax[0].set_title('Number of Passengers By Pclass', y = y_position)ax[0].set_ylabel('Count')sns.countplot('Pclass', hue = 'Survived', data = df_train, ax = ax[1])ax[1].set_title('Pclass: Survived vs Dead', y = y_position)plt.show() Pclass가 높을 수록, 생존 확률이 높은 걸 확인할 수 있다? Pclass 1, 2, 3 순서대로 63%, 48%, 25% 이다. 생존에 Pclass가 큰 영향을 미친다고 생각해볼 수 있으며, 나중에 모델을 세울 때 이 feature를 사용하는 것이 좋을 것이라 판단할 수 있다. Sex 성별로 생존률이 어떻게 달라지는 지 확인해보겠다. 마찬가지로 pandas groupby와 seaborn countplot을 사용해서 시각화해본다. 123456f, ax = plt.subplots(1, 2, figsize = (18, 8))df_train[['Sex', 'Survived']].groupby(['Sex'], as_index = True).mean().plot.bar(ax = ax[0])ax[0].set_title('Survived vs Sex')sns.countplot('Sex', hue = 'Survived', data = df_train, ax = ax[1])ax[1].set_title('Sex: Survived vs Dead')plt.show() 여자가 생존할 확률이 높은 걸 확인할 수 있다. 1df_train[['Sex', &quot;Survived&quot;]].groupby(['Sex'], as_index = False).mean().sort_values(by = &quot;Survived&quot;, ascending = False) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Sex Survived 0 female 0.742038 1 male 0.188908 1pd.crosstab(df_train['Sex'], df_train['Survived'], margins = True).style.background_gradient(cmap = 'summer_r') Pclass와 마찬가지로, Sex도 예측 모델에 쓰일 중요한 feature 임을 알 수 있다. Both Sex and Pclass Sex, Pclass 2가지에 관하여 생존이 어떻게 달라지는 지 확인해 본다. seaborn의 factorplot을 이용하면, 손쉽게 3개의 차원으로 이루어진 그래프를 그릴 수 있다. 12sns.factorplot('Pclass', 'Survived', hue = 'Sex', data = df_train, size = 6, aspect = 1.5) &lt;seaborn.axisgrid.FacetGrid at 0x7f053fd72208&gt; 모든 클래스에서 여자가 살 확률이 남자보다 높은 걸 알 수 있다. 또한 남자, 여자 상관없이 클래스가 높을 수록 살 확률이 높다. hue = ‘Sex’ 대신 col = ‘Pclass’로 하면 아래와 같아진다. (column) 123sns.factorplot(x = 'Sex', y = &quot;Survived&quot;, col = 'Pclass', data = df_train, satureation = 5, size = 9, aspect = 1) &lt;seaborn.axisgrid.FacetGrid at 0x7f053fda8da0&gt; Age Age Feature를 살펴보자. 123print('제일 나이 많은 탑승객 : {:.1f} Years'.format(df_train['Age'].max()))print('제일 어린 탑승객 : {:.1f} Years'.format(df_train['Age'].min()))print('탑승객 평균 나이 : {:.1f} Years'.format(df_train['Age'].mean())) 제일 나이 많은 탑승객 : 80.0 Years 제일 어린 탑승객 : 0.4 Years 탑승객 평균 나이 : 29.7 Years 생존에 따른 Age의 히스토그램을 그려보겠다. 12345fig, ax = plt.subplots(1, 1, figsize = (9, 5))sns.kdeplot(df_train[df_train['Survived'] == 1]['Age'], ax = ax)sns.kdeplot(df_train[df_train['Survived'] == 0]['Age'], ax = ax)plt.legend(['Survived == 1', 'Survived == 0'])plt.show() 생존자 중 나이가 어린 경우가 많음을 볼 수 있다. 123456789# Age distribution withing classesplt.figure(figsize = (8, 6))df_train['Age'][df_train['Pclass'] == 1].plot(kind = 'kde')df_train['Age'][df_train['Pclass'] == 2].plot(kind = 'kde')df_train['Age'][df_train['Pclass'] == 3].plot(kind = 'kde')plt.xlabel('Age')plt.title('Age Distribution within classes')plt.legend(['1st Class', '2nd Class', '3rd Class']) &lt;matplotlib.legend.Legend at 0x7f053f7dd6a0&gt; Pclass가 높을 수록 나이 많은 사람의 비중이 커진다. 나이대가 변하면서 생존률이 어떻게 되는 지 보려고 한다. 나이 범위를 점점 넓혀가며, 생존률이 어떻게 되는 지 한 번 보자. 12345678910cummulate_survival_ratio = []for i in range(1, 80): cummulate_survival_ratio.append(df_train[df_train['Age'] &lt; i]['Survived'].sum() / len(df_train[df_train['Age'] &lt; i]['Survived']))plt.figure(figsize = (7, 7))plt.plot(cummulate_survival_ratio)plt.title('Survival rate change depending on range of Age', y = 1.02)plt.ylabel('Survival rate')plt.xlabel('Range of Age(0~x)')plt.show() 나이가 어릴 수록 생존률이 확실히 높은 것을 확인할 수 있다. Age는 중요한 Feature로 쓰일 수 있음을 확인했다. Pclass, Sex, Age Sex, Pclass, Age, Sruvived 모두에 대해서 보고 싶다면, 이를 쉽게 그려주는 seaborn의 biolinplot을 사용한다. x축은 우리가 나눠서 보고 싶어하는 case(Pclass, Sex)를 나타내고, y축은 보고 싶어하는 distribution(Age)이다. 12345678f, ax = plt.subplots(1, 2, figsize = (18,8))sns.violinplot(&quot;Pclass&quot;, &quot;Age&quot;, hue = &quot;Survived&quot;, data = df_train, scale = 'count', split = True, ax = ax[0])ax[0].set_title('Pclass and Age vs Survived')ax[0].set_yticks(range(0, 110, 10))sns.violinplot(&quot;Sex&quot;, &quot;Age&quot;, hue = &quot;Survived&quot;, data = df_train, scale = 'count', split = True, ax = ax[1])ax[1].set_title('Sex and Age vs Survived')ax[1].set_yticks(range(0, 110, 10))plt.show() 왼쪽 그림은 Pclass별로 Age의 분포가 어떻게 다른 지, 거기에 생존여부에 따라 구분한 그래프이다. 오른쪽 그림은 Sex, Survived에 따른 분포가 어떻게 다른 지 보여주는 그래프이다. Survived만 봤을 때, 모든 클래스에서 나이가 어릴 수록 생존을 많이 한 것을 볼 수 있다. 오른쪽 그림에서, 명확히 여자가 생존을 많이 한 것을 볼 수 있다. 여성과 아이를 먼저 챙겼다고 분석해볼 수 있다. EmbarkedFamily - SibSp(형제, 자매) + Parch(부모, 자녀)FareCabinTicket","link":"/2020/11/05/study/kaggle_titanic/"},{"title":"판다스 10분 완성","text":"출처: 데잇걸즈2, Pandas 10분 완성https://dataitgirls2.github.io/10minutes2pandas/ 접기/펼치기 판다스 10분 완성데잇걸즈210 minutes to pandasPandas Cheat SheetCookbook 123import pandas as pdimport numpy as npimport matplotlib.pyplot as plt 1) 객체 생성 (Object Creation)데이터 구조 소개 섹션 (Intro to data structures) pandas.Series()값을 가지고 있는 리스트를 통해 Series를 만들고, 정수로 만들어진 인덱스를 기본값으로 불러온다. 12s = pd.Series([1,3,5,np.nan,6,8])s 0 1.0 1 3.0 2 5.0 3 NaN 4 6.0 5 8.0 dtype: float64 pandas.date_range()DatetimeIndex 데이터프레임을 만든다. 12dates = pd.date_range('20130101', periods=6)dates DatetimeIndex(['2013-01-01', '2013-01-02', '2013-01-03', '2013-01-04', '2013-01-05', '2013-01-06'], dtype='datetime64[ns]', freq='D') pandas.DataFrame()레이블이 있는 열을 가지고 있는 numpy 배열을 전달하여 데이터프레임을 만든다. 12df = pd.DataFrame(np.random.randn(6,4), index=dates, columns=list('ABCD'))df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 2013-01-01 0.682032 -1.905166 -0.545421 0.238074 2013-01-02 0.120775 -0.572338 -2.020728 -0.542485 2013-01-03 -0.575200 -0.442130 -0.154726 -1.433836 2013-01-04 0.509850 0.801045 0.015410 -0.671559 2013-01-05 1.403769 1.191673 -1.332947 0.663252 2013-01-06 0.825406 1.283820 0.421918 -0.115307 Series와 같은 것으로 변환될 수 있는 객체들의 딕셔너리로 구성된 데이터프레임을 만든다. 1234567df2 = pd.DataFrame({'A' : 1., 'B' : pd.Timestamp('20130102'), 'C' : pd.Series(1,index=list(range(4)),dtype='float32'), 'D' : np.array([3] * 4,dtype='int32'), 'E' : pd.Categorical([&quot;test&quot;,&quot;train&quot;,&quot;test&quot;,&quot;train&quot;]), 'F' : 'foo' })df2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D E F 0 1.0 2013-01-02 1.0 3 test foo 1 1.0 2013-01-02 1.0 3 train foo 2 1.0 2013-01-02 1.0 3 test foo 3 1.0 2013-01-02 1.0 3 train foo .dtypes데이터프레임 결과물의 열은 다양한 데이터 타입으로 구성된다. 1df2.dtypes A float64 B datetime64[ns] C float32 D int32 E category F object dtype: object 2) 데이터 확인하기 (Viewing Data)Basic Section (Essential basic functionality) .head(), .tail()괄호() 안에 숫자가 들어간다면 처음/마지막 줄의 특정 줄을 불러올 수 있다. 숫자가 들어가지 않으면 기본값인 5줄로 처리된다. 1df.head() # 시작에서 처음 5줄 불러온다. .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 2013-01-01 0.682032 -1.905166 -0.545421 0.238074 2013-01-02 0.120775 -0.572338 -2.020728 -0.542485 2013-01-03 -0.575200 -0.442130 -0.154726 -1.433836 2013-01-04 0.509850 0.801045 0.015410 -0.671559 2013-01-05 1.403769 1.191673 -1.332947 0.663252 1df.tail(3) # 끝에서 마지막 3줄을 불러온다. .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 2013-01-04 0.509850 0.801045 0.015410 -0.671559 2013-01-05 1.403769 1.191673 -1.332947 0.663252 2013-01-06 0.825406 1.283820 0.421918 -0.115307 .index, .columns, .values인덱스, 열 그리고 numpy 데이터에 대한 세부 정보를 본다. 1df.index DatetimeIndex(['2013-01-01', '2013-01-02', '2013-01-03', '2013-01-04', '2013-01-05', '2013-01-06'], dtype='datetime64[ns]', freq='D') 1df.columns Index(['A', 'B', 'C', 'D'], dtype='object') 1df.values array([[ 0.68203244, -1.90516572, -0.54542119, 0.23807417], [ 0.12077523, -0.57233779, -2.02072777, -0.5424847 ], [-0.57519992, -0.44213025, -0.15472648, -1.43383629], [ 0.5098502 , 0.8010451 , 0.01541041, -0.67155854], [ 1.40376854, 1.1916732 , -1.33294702, 0.66325193], [ 0.82540573, 1.28382005, 0.42191767, -0.11530747]]) .describe()describe()는 데이터의 대략적인 통계적 정보 요약을 보여준다. 1df.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D count 6.000000 6.000000 6.000000 6.000000 mean 0.494439 0.059484 -0.602749 -0.310310 std 0.671655 1.252312 0.914322 0.739363 min -0.575200 -1.905166 -2.020728 -1.433836 25% 0.218044 -0.539786 -1.136066 -0.639290 50% 0.595941 0.179457 -0.350074 -0.328896 75% 0.789562 1.094016 -0.027124 0.149729 max 1.403769 1.283820 0.421918 0.663252 .T데이터를 전치한다.전치 행렬은 행과 열을 교환하여 얻는 행렬이다. 주대각선을 축으로 하는 반사 대칭을 가하여 얻는 행렬이다. 1df.T .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 2013-01-01 2013-01-02 2013-01-03 2013-01-04 2013-01-05 2013-01-06 A 0.682032 0.120775 -0.575200 0.509850 1.403769 0.825406 B -1.905166 -0.572338 -0.442130 0.801045 1.191673 1.283820 C -0.545421 -2.020728 -0.154726 0.015410 -1.332947 0.421918 D 0.238074 -0.542485 -1.433836 -0.671559 0.663252 -0.115307 .sort_index()축 별로 정렬한다. 1df.sort_index(axis=1, ascending=False) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } D C B A 2013-01-01 0.238074 -0.545421 -1.905166 0.682032 2013-01-02 -0.542485 -2.020728 -0.572338 0.120775 2013-01-03 -1.433836 -0.154726 -0.442130 -0.575200 2013-01-04 -0.671559 0.015410 0.801045 0.509850 2013-01-05 0.663252 -1.332947 1.191673 1.403769 2013-01-06 -0.115307 0.421918 1.283820 0.825406 .sort_values()값 별로 정렬한다. 1df.sort_values(by='B') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 2013-01-01 0.682032 -1.905166 -0.545421 0.238074 2013-01-02 0.120775 -0.572338 -2.020728 -0.542485 2013-01-03 -0.575200 -0.442130 -0.154726 -1.433836 2013-01-04 0.509850 0.801045 0.015410 -0.671559 2013-01-05 1.403769 1.191673 -1.332947 0.663252 2013-01-06 0.825406 1.283820 0.421918 -0.115307 3) 선택 (Selection)데이터 인덱싱 및 선택 문서 (Indexing and selecting data)다중 인덱싱 / 심화 인덱싱 (MultiIndex / advanced indexing) 3-1) 데이터 얻기 (Getting)df.A 와 동일한 Series를 생성하는 단일 열을 선택한다. 1df['A'] 2013-01-01 0.682032 2013-01-02 0.120775 2013-01-03 -0.575200 2013-01-04 0.509850 2013-01-05 1.403769 2013-01-06 0.825406 Freq: D, Name: A, dtype: float64 행을 분할하는 [ ]를 통해 선택한다. 1df[0:3] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 2013-01-01 0.682032 -1.905166 -0.545421 0.238074 2013-01-02 0.120775 -0.572338 -2.020728 -0.542485 2013-01-03 -0.575200 -0.442130 -0.154726 -1.433836 1df['20130102':'20130104'] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 2013-01-02 0.120775 -0.572338 -2.020728 -0.542485 2013-01-03 -0.575200 -0.442130 -0.154726 -1.433836 2013-01-04 0.509850 0.801045 0.015410 -0.671559 3-2) Label 을 통한 선택 (Selection by Label)Label을 통한 선택 (Indexing and selecting data)라벨을 사용하여 횡단면을 얻는다. 1df.loc[dates[0]] # 2013-01-01 가로 데이터 A 0.682032 B -1.905166 C -0.545421 D 0.238074 Name: 2013-01-01 00:00:00, dtype: float64 라벨을 사용하여 여러 축(의 데이터)을 얻는다. 1df.loc[:,['A','B']] # 맨 처음 &quot;:&quot;는 인덱스 범위인 것 같은데 아무것도 안 적으면 모든 데이터를 말하는 것 같다. .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B 2013-01-01 0.682032 -1.905166 2013-01-02 0.120775 -0.572338 2013-01-03 -0.575200 -0.442130 2013-01-04 0.509850 0.801045 2013-01-05 1.403769 1.191673 2013-01-06 0.825406 1.283820 양쪽 종단점을 포함한 라벨 슬라이싱을 본다. 1df.loc['20130102':'20130104', ['A','B']] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B 2013-01-02 0.120775 -0.572338 2013-01-03 -0.575200 -0.442130 2013-01-04 0.509850 0.801045 반환되는 객체의 차원를 줄인다. 1df.loc['20130102',['A','B']] A 0.120775 B -0.572338 Name: 2013-01-02 00:00:00, dtype: float64 스칼라 값을 얻는다.스칼라란 선형대수학에서 선형공간을 정의 할 때, 선형공간의 원소와 스칼라 곱을 하는 체의 원소이다. 1df.loc[dates[0],'A'] 0.6820324363584261 스칼라 값을 더 빠르게 구하는 방법이다. 1df.at[dates[0],'A'] 0.6820324363584261 3-3) 위치로 선택하기 (Selection by Position)위치로 선택하기 (Indexing and selecting data)넘겨받은 정수의 위치를 기준으로 선택한다. 1df.iloc[3] # 2013-01-04 데이터 A 0.509850 B 0.801045 C 0.015410 D -0.671559 Name: 2013-01-04 00:00:00, dtype: float64 정수로 표기된 슬라이스들을 통해, numpy / python과 유사하게 작동한다. 1df.iloc[3:5,0:2] # 3:5 -&gt; 3 이상 5 미만? .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B 2013-01-04 0.509850 0.801045 2013-01-05 1.403769 1.191673 정수로 표기된 위치값의 리스트들을 통해, numpy / python의 스타일과 유사해진다. 1df.iloc[[1,2,4],[0,2]] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A C 2013-01-02 0.120775 -2.020728 2013-01-03 -0.575200 -0.154726 2013-01-05 1.403769 -1.332947 명시적으로 행을 나누고자 하는 경우이다. 1df.iloc[1:3,:] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 2013-01-02 0.120775 -0.572338 -2.020728 -0.542485 2013-01-03 -0.575200 -0.442130 -0.154726 -1.433836 명시적으로 열을 나누고자 하는 경우이다. 1df.iloc[:,1:3] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } B C 2013-01-01 -1.905166 -0.545421 2013-01-02 -0.572338 -2.020728 2013-01-03 -0.442130 -0.154726 2013-01-04 0.801045 0.015410 2013-01-05 1.191673 -1.332947 2013-01-06 1.283820 0.421918 명시적으로 (특정한) 값을 얻고자 하는 경우이다. 1df.iloc[1,1] # 2013-01-02의 B 데이터 -0.5723377908969021 스칼라 값을 빠르게 얻는 방법이다. 1df.iat[1,1] -0.5723377908969021 3-4) Boolean Indexing데이터를 선택하기 위해 단일 열의 값을 사용한다. 1df[df.A &gt; 0] # A값 음수인 2013-01-03 데이터 빠진다. .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 2013-01-01 0.682032 -1.905166 -0.545421 0.238074 2013-01-02 0.120775 -0.572338 -2.020728 -0.542485 2013-01-04 0.509850 0.801045 0.015410 -0.671559 2013-01-05 1.403769 1.191673 -1.332947 0.663252 2013-01-06 0.825406 1.283820 0.421918 -0.115307 Boolean 조건을 충족하는 데이터프레임에서 값을 선택한다. 1df[df &gt; 0] # 음수는 NaN 처리된다. .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 2013-01-01 0.682032 NaN NaN 0.238074 2013-01-02 0.120775 NaN NaN NaN 2013-01-03 NaN NaN NaN NaN 2013-01-04 0.509850 0.801045 0.015410 NaN 2013-01-05 1.403769 1.191673 NaN 0.663252 2013-01-06 0.825406 1.283820 0.421918 NaN 필터링을 위한 메소드 isin()을 사용한다. 123df2 = df.copy()df2['E'] = ['one', 'one', 'two', 'three', 'four', 'three']df2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D E 2013-01-01 0.682032 -1.905166 -0.545421 0.238074 one 2013-01-02 0.120775 -0.572338 -2.020728 -0.542485 one 2013-01-03 -0.575200 -0.442130 -0.154726 -1.433836 two 2013-01-04 0.509850 0.801045 0.015410 -0.671559 three 2013-01-05 1.403769 1.191673 -1.332947 0.663252 four 2013-01-06 0.825406 1.283820 0.421918 -0.115307 three 1df2[df2['E'].isin(['two','four'])] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D E 2013-01-03 -0.575200 -0.442130 -0.154726 -1.433836 two 2013-01-05 1.403769 1.191673 -1.332947 0.663252 four 3-5) 설정 (Setting)새 열을 설정하면 데이터가 인덱스 별로 자동 정렬된다. 12s1 = pd.Series([1,2,3,4,5,6], index=pd.date_range('20130102', periods=6))s1 2013-01-02 1 2013-01-03 2 2013-01-04 3 2013-01-05 4 2013-01-06 5 2013-01-07 6 Freq: D, dtype: int64 123456789101112df['F'] = s1# 라벨에 의해 값을 설정한다.df.at[dates[0],'A'] = 0# 위치에 의해 값을 설정한다.df.iat[0,1] = 0# Numpy 배열을 사용한 할당에 의해 값을 설정한다.df.loc[:,'D'] = np.array([5] * len(df))df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D F 2013-01-01 0.000000 0.000000 -0.545421 5 NaN 2013-01-02 0.120775 -0.572338 -2.020728 5 1.0 2013-01-03 -0.575200 -0.442130 -0.154726 5 2.0 2013-01-04 0.509850 0.801045 0.015410 5 3.0 2013-01-05 1.403769 1.191673 -1.332947 5 4.0 2013-01-06 0.825406 1.283820 0.421918 5 5.0 where 연산을 설정한다. 123df2 = df.copy()df2[df2 &gt; 0] = -df2df2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D F 2013-01-01 0.000000 0.000000 -0.545421 -5 NaN 2013-01-02 -0.120775 -0.572338 -2.020728 -5 -1.0 2013-01-03 -0.575200 -0.442130 -0.154726 -5 -2.0 2013-01-04 -0.509850 -0.801045 -0.015410 -5 -3.0 2013-01-05 -1.403769 -1.191673 -1.332947 -5 -4.0 2013-01-06 -0.825406 -1.283820 -0.421918 -5 -5.0 4) 결측치 (Missing Data)Missing data section (Working with missing data)Pandas는 결측치를 표현하기 위해 주로 np.nan 값을 사용한다.Reindexing으로 지정된 축 상의 인덱스를 변경/추가/삭제할 수 있다.Reindexing은 데이터의 복사본을 반환한다. 123df1 = df.reindex(index=dates[0:4], columns=list(df.columns) + ['E'])df1.loc[dates[0]:dates[1],'E'] = 1df1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D F E 2013-01-01 0.000000 0.000000 -0.545421 5 NaN 1.0 2013-01-02 0.120775 -0.572338 -2.020728 5 1.0 1.0 2013-01-03 -0.575200 -0.442130 -0.154726 5 2.0 NaN 2013-01-04 0.509850 0.801045 0.015410 5 3.0 NaN 결측치를 가지고 있는 행들을 지운다. 1df1.dropna(how='any') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D F E 2013-01-02 0.120775 -0.572338 -2.020728 5 1.0 1.0 결측치를 채워 넣는다. 1df1.fillna(value=5) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D F E 2013-01-01 0.000000 0.000000 -0.545421 5 5.0 1.0 2013-01-02 0.120775 -0.572338 -2.020728 5 1.0 1.0 2013-01-03 -0.575200 -0.442130 -0.154726 5 2.0 5.0 2013-01-04 0.509850 0.801045 0.015410 5 3.0 5.0 nan인 값에 boolean을 통한 표식을 얻는다.데이터프레임의 모든 값이 boolean 형태로 표시되도록 하며, nan인 값에만 True가 표시되게 하는 함수이다. 1pd.isna(df1) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D F E 2013-01-01 False False False False True False 2013-01-02 False False False False False False 2013-01-03 False False False False False True 2013-01-04 False False False False False True 5) 연산 (Operation)이진 (Binary) 연산의 기본 섹션 (Flexible binary operations) 5-1) 통계 (Stats)12일반적으로 결측치를 제외한 후 연산된다.기술통계를 수행한다. 1df.mean() A 0.380767 B 0.377012 C -0.602749 D 5.000000 F 3.000000 dtype: float64 다른 축에서 동일한 연산을 수행한다. 1df.mean(1) 2013-01-01 1.113645 2013-01-02 0.705542 2013-01-03 1.165589 2013-01-04 1.865261 2013-01-05 2.052499 2013-01-06 2.506229 Freq: D, dtype: float64 정렬이 필요하며, 차원이 다른 객체로 연산해보겠다.pandas는 지정된 차원을 따라 자동으로 브로드 캐스팅된다.broadcast란 numpy에서 유래한 용어로, n차원이나 스칼라 값으로 연산을 수행할 때 도출되는 결과의 규칙을 설명하는 것을 의미합니다. 12s = pd.Series([1,3,5,np.nan,6,8], index=dates).shift(2)s 2013-01-01 NaN 2013-01-02 NaN 2013-01-03 1.0 2013-01-04 3.0 2013-01-05 5.0 2013-01-06 NaN Freq: D, dtype: float64 1df.sub(s, axis='index') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D F 2013-01-01 NaN NaN NaN NaN NaN 2013-01-02 NaN NaN NaN NaN NaN 2013-01-03 -1.575200 -1.442130 -1.154726 4.0 1.0 2013-01-04 -2.490150 -2.198955 -2.984590 2.0 0.0 2013-01-05 -3.596231 -3.808327 -6.332947 0.0 -1.0 2013-01-06 NaN NaN NaN NaN NaN 5-2) 적용 (Apply)데이터에 함수를 적용한다. 1df.apply(np.cumsum) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D F 2013-01-01 0.000000 0.000000 -0.545421 5 NaN 2013-01-02 0.120775 -0.572338 -2.566149 10 1.0 2013-01-03 -0.454425 -1.014468 -2.720875 15 3.0 2013-01-04 0.055426 -0.213423 -2.705465 20 6.0 2013-01-05 1.459194 0.978250 -4.038412 25 10.0 2013-01-06 2.284600 2.262070 -3.616494 30 15.0 1df.apply(lambda x: x.max() - x.min()) A 1.978968 B 1.856158 C 2.442645 D 0.000000 F 4.000000 dtype: float64 5-3) 히스토그래밍 (Histogramming)히스토그래밍과 이산화 (Value counts (histogramming) / mode) 12s = pd.Series(np.random.randint(0, 7, size=10))s 0 3 1 1 2 1 3 1 4 2 5 1 6 2 7 6 8 3 9 0 dtype: int64 1s.value_counts() 1 4 3 2 2 2 6 1 0 1 dtype: int64 5-4) 문자열 메소드 (String Methods)벡터화된 문자열 메소드 (String methods) Series는 다음의 코드와 같이 문자열 처리 메소드 모음 (set)을 가지고 있다.이 모음은 배열의 각 요소를 쉽게 조작할 수 있도록 만들어주는 문자열의 속성에 포함되어 있다. 문자열의 패턴 일치 확인은 기본적으로 정규 표현식을 사용하며, 몇몇 경우에는 항상 정규 표현식을 사용함에 유의한다. 12s = pd.Series(['A', 'B', 'C', 'Aaba', 'Baca', np.nan, 'CABA', 'dog', 'cat'])s.str.lower() 0 a 1 b 2 c 3 aaba 4 baca 5 NaN 6 caba 7 dog 8 cat dtype: object 6) 병합 (Merge)6-1) 연결 (Concat)데이터베이스 스타일 결합 (Database-style DataFrame or named Series joining/merging)결합 (join) / 병합 (merge) 형태의 연산에 대한 인덱스, 관계 대수 기능을 위한 다양한 형태의 논리를 포함한 Series, 데이터프레임, Panel 객체를 손쉽게 결합할 수 있도록 하는 다양한 기능을 pandas 에서 제공한다.concat()으로 pandas 객체를 연결한다. 12df = pd.DataFrame(np.random.randn(10, 4))df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 3 0 1.279807 -1.153900 -0.582429 2.259070 1 0.532136 -0.575209 -0.042748 0.484629 2 0.531710 -0.147753 0.678086 0.120756 3 -0.533762 -0.226234 -0.250227 1.775457 4 2.162538 -0.116354 -0.521884 0.673141 5 -0.057809 -0.355782 1.148990 0.041617 6 0.765778 -0.145401 -0.604436 -1.779851 7 0.512580 0.055678 -0.070639 0.009846 8 -0.360689 1.153401 -0.377757 0.762205 9 1.038979 -0.550957 -0.740130 -0.839612 12pieces = [df[:3], df[3:7], df[7:]]pd.concat(pieces) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 3 0 1.279807 -1.153900 -0.582429 2.259070 1 0.532136 -0.575209 -0.042748 0.484629 2 0.531710 -0.147753 0.678086 0.120756 3 -0.533762 -0.226234 -0.250227 1.775457 4 2.162538 -0.116354 -0.521884 0.673141 5 -0.057809 -0.355782 1.148990 0.041617 6 0.765778 -0.145401 -0.604436 -1.779851 7 0.512580 0.055678 -0.070639 0.009846 8 -0.360689 1.153401 -0.377757 0.762205 9 1.038979 -0.550957 -0.740130 -0.839612 6-2) 결합 (Join)SQL 방식으로 병합한다. 12left = pd.DataFrame({'key': ['foo', 'foo'], 'lval': [1, 2]})left .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } key lval 0 foo 1 1 foo 2 12right = pd.DataFrame({'key': ['foo', 'foo'], 'rval': [4, 5]})right .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } key rval 0 foo 4 1 foo 5 1pd.merge(left, right, on= 'key') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } key lval rval 0 foo 1 4 1 foo 1 5 2 foo 2 4 3 foo 2 5 다른 예시이다. 12left = pd.DataFrame({'key' : ['foo', 'bar'], 'lval' : [1, 2]})left .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } key lval 0 foo 1 1 bar 2 12right = pd.DataFrame({'key': ['foo', 'bar'], 'rval': [4, 5]})right .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } key rval 0 foo 4 1 bar 5 1pd.merge(left, right, on= 'key') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } key lval rval 0 foo 1 4 1 bar 2 5 6-3) 추가 (Append)Appending rows to a DataFrame데이터프레임에 행을 추가한다. 12df = pd.DataFrame(np.random.randn(8, 4), columns=['A', 'B', 'C', 'D'])df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 0 0.061291 1.294748 1.034918 1.133735 1 0.350266 0.612054 -0.184834 -0.349107 2 0.161551 1.173609 1.759626 -0.416150 3 0.208048 2.164734 -0.798397 1.449171 4 -0.777391 -1.097352 1.458079 2.725382 5 -1.336344 1.087275 0.883582 -0.455446 6 0.937276 -1.917829 0.637797 -1.077501 7 0.721358 2.959415 0.398118 -0.245919 12s = df.iloc[3]df.append(s, ignore_index=True) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 0 0.061291 1.294748 1.034918 1.133735 1 0.350266 0.612054 -0.184834 -0.349107 2 0.161551 1.173609 1.759626 -0.416150 3 0.208048 2.164734 -0.798397 1.449171 4 -0.777391 -1.097352 1.458079 2.725382 5 -1.336344 1.087275 0.883582 -0.455446 6 0.937276 -1.917829 0.637797 -1.077501 7 0.721358 2.959415 0.398118 -0.245919 8 0.208048 2.164734 -0.798397 1.449171 7) 그룹화 (Grouping)그룹화(Group by: split-apply-combine)는 다음 단계 중 하나 이상을 포함하는 과정을 가리킨다. 몇몇 기준에 따라 여러 그룹으로 데이터를 분할 (splitting) 각 그룹에 독립적으로 함수를 적용 (applying) 결과물들을 하나의 데이터 구조로 결합 (combining) 12345678df = pd.DataFrame( { 'A' : ['foo', 'bar', 'foo', 'bar', 'foo', 'bar', 'foo', 'foo'], 'B' : ['one', 'one', 'two', 'three', 'two', 'two', 'one', 'three'], 'C' : np.random.randn(8), 'D' : np.random.randn(8) })df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 0 foo one -0.531629 -0.841219 1 bar one 0.847370 -0.702148 2 foo two -3.025496 -0.458174 3 bar three -0.317416 -1.938912 4 foo two -0.128090 -0.880979 5 bar two -1.918498 0.091863 6 foo one -1.142854 0.159617 7 foo three -1.046234 0.334159 생성된 데이터프레임을 그룹화한 후 각 그룹에 sum() 함수를 적용한다. 1df.groupby('A').sum() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } C D A bar -1.388545 -2.549197 foo -5.874302 -1.686597 여러 열을 기준으로 그룹화하면 계층적 인덱스가 형성된다.여기에도 sum 함수를 적용할 수 있다. 1df.groupby(['A','B']).sum() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } C D A B bar one 0.847370 -0.702148 three -0.317416 -1.938912 two -1.918498 0.091863 foo one -1.674483 -0.681602 three -1.046234 0.334159 two -3.153585 -1.339153 8) 변형 (Reshaping)계층적 인덱싱 (Advanced indexing with hierarchical index)변형 (Reshaping by stacking and unstacking) 8-1) 스택 (Stack)12345678tuples = list(zip(*[['bar', 'bar', 'baz', 'baz', 'foo', 'foo', 'qux', 'qux'], ['one', 'two', 'one', 'two', 'one', 'two', 'one', 'two']]))index = pd.MultiIndex.from_tuples(tuples, names=['first', 'second'])df = pd.DataFrame(np.random.randn(8, 2), index=index, columns=['A', 'B'])df2 = df[:4]df2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B first second bar one 0.779699 -0.960938 two 1.442232 -0.931635 baz one -0.185774 -0.529952 two -0.794478 0.736231 stack() 메소드는 데이터프레임 열들의 계층을 “압축”한다. 12stacked = df2.stack()stacked first second bar one A 0.779699 B -0.960938 two A 1.442232 B -0.931635 baz one A -0.185774 B -0.529952 two A -0.794478 B 0.736231 dtype: float64 “Stack된” 데이터프레임 또는 (MultiIndex를 인덱스로 사용하는) Series인 경우, stack()의 역 연산은 unstack()이며, 기본적으로 마지막 계층을 unstack합니다. 1stacked.unstack() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B first second bar one 0.779699 -0.960938 two 1.442232 -0.931635 baz one -0.185774 -0.529952 two -0.794478 0.736231 1stacked.unstack(1) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } second one two first bar A 0.779699 1.442232 B -0.960938 -0.931635 baz A -0.185774 -0.794478 B -0.529952 0.736231 1stacked.unstack(0) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } first bar baz second one A 0.779699 -0.185774 B -0.960938 -0.529952 two A 1.442232 -0.794478 B -0.931635 0.736231 8-2) 피벗 테이블 (Pivot Tables)Reshaping by pivoting DataFrame objects 123456df = pd.DataFrame({'A' : ['one', 'one', 'two', 'three'] * 3, 'B' : ['A', 'B', 'C'] * 4, 'C' : ['foo', 'foo', 'foo', 'bar', 'bar', 'bar'] * 2, 'D' : np.random.randn(12), 'E' : np.random.randn(12)})df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D E 0 one A foo -0.388590 -0.092924 1 one B foo 0.096758 -1.404417 2 two C foo 0.235439 -1.441384 3 three A bar 1.644634 -1.404461 4 one B bar -0.137766 -0.732040 5 one C bar -0.385584 0.876041 6 two A foo -1.144518 1.714938 7 three B foo -0.287536 0.984071 8 one C foo 0.670096 -0.093942 9 one A bar -0.540043 -0.589907 10 two B bar 1.116471 1.360887 11 three C bar -0.751042 0.641450 1pd.pivot_table(df, values='D', index=['A', 'B'], columns=['C']) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } C bar foo A B one A -0.540043 -0.388590 B -0.137766 0.096758 C -0.385584 0.670096 three A 1.644634 NaN B NaN -0.287536 C -0.751042 NaN two A NaN -1.144518 B 1.116471 NaN C NaN 0.235439 9) 시계열 (Time Series)시계열 (Time series / date functionality)Pandas는 자주 일어나는 변환 (예시 : 5분마다 일어나는 데이터에 대한 2차 데이터 변환) 사이에 수행하는 리샘플링 연산을 위한 간단하고, 강력하며, 효율적인 함수를 제공한다. 이는 재무 (금융) 응용에서 매우 일반적이지만 이에 국한되지는 않는다. 123rng = pd.date_range('1/1/2012', periods=100, freq='S')ts = pd.Series(np.random.randint(0, 500, len(rng)), index=rng)ts.resample('5Min').sum() 2012-01-01 27558 Freq: 5T, dtype: int64 시간대를 표현한다. 123rng = pd.date_range('3/6/2012 00:00', periods=5, freq='D')ts = pd.Series(np.random.randn(len(rng)), rng)ts 2012-03-06 1.212536 2012-03-07 0.751280 2012-03-08 1.779794 2012-03-09 -0.981566 2012-03-10 0.161867 Freq: D, dtype: float64 12ts_utc = ts.tz_localize('UTC')ts_utc 2012-03-06 00:00:00+00:00 1.212536 2012-03-07 00:00:00+00:00 0.751280 2012-03-08 00:00:00+00:00 1.779794 2012-03-09 00:00:00+00:00 -0.981566 2012-03-10 00:00:00+00:00 0.161867 Freq: D, dtype: float64 다른 시간대로 변환한다. 1ts_utc.tz_convert('US/Eastern') 2012-03-05 19:00:00-05:00 1.212536 2012-03-06 19:00:00-05:00 0.751280 2012-03-07 19:00:00-05:00 1.779794 2012-03-08 19:00:00-05:00 -0.981566 2012-03-09 19:00:00-05:00 0.161867 Freq: D, dtype: float64 시간 표현 ↔ 기간 표현으로 변환한다. 123rng = pd.date_range('1/1/2012', periods=5, freq='M')ts = pd.Series(np.random.randn(len(rng)), index=rng)ts 2012-01-31 -1.147594 2012-02-29 2.187776 2012-03-31 -0.567851 2012-04-30 1.944352 2012-05-31 -0.638937 Freq: M, dtype: float64 12ps = ts.to_period()ps 2012-01 -1.147594 2012-02 2.187776 2012-03 -0.567851 2012-04 1.944352 2012-05 -0.638937 Freq: M, dtype: float64 1ps.to_timestamp() 2012-01-01 -1.147594 2012-02-01 2.187776 2012-03-01 -0.567851 2012-04-01 1.944352 2012-05-01 -0.638937 Freq: MS, dtype: float64 기간 ↔ 시간 변환은 편리한 산술 기능들을 사용할 수 있도록 만들어준다. 다음 예제에서, 우리는 11월에 끝나는 연말 결산의 분기별 빈도를 분기말 익월의 월말일 오전 9시로 변환한다. 1234prng = pd.period_range('1990Q1', '2000Q4', freq='Q-NOV')ts = pd.Series(np.random.randn(len(prng)), prng)ts.index = (prng.asfreq('M', 'e') + 1).asfreq('H', 's') + 9ts.head() 1990-03-01 09:00 0.079946 1990-06-01 09:00 -0.277186 1990-09-01 09:00 -0.848088 1990-12-01 09:00 -0.181953 1991-03-01 09:00 -1.110753 Freq: H, dtype: float64 10) 범주화 (Categoricals)범주형 소개 (Categorical data)API 문서 (API reference)Pandas는 데이터프레임 내에 범주형 데이터를 포함할 수 있다. 가공하지 않은 성적을 범주형 데이터로 변환한다. 123df = pd.DataFrame({&quot;id&quot;:[1,2,3,4,5,6], &quot;raw_grade&quot;:['a', 'b', 'b', 'a', 'a', 'e']})df[&quot;grade&quot;] = df[&quot;raw_grade&quot;].astype(&quot;category&quot;)df[&quot;grade&quot;] 0 a 1 b 2 b 3 a 4 a 5 e Name: grade, dtype: category Categories (3, object): ['a', 'b', 'e'] 범주에 더 의미 있는 이름을 붙여준다. (Series.cat.categories로 할당하는 것이 적합하다). 1df[&quot;grade&quot;].cat.categories = [&quot;very good&quot;, &quot;good&quot;, &quot;very bad&quot;] 범주의 순서를 바꾸고 동시에 누락된 범주를 추가한다. (Series.cat에 속하는 메소드는 기본적으로 새로운 Series를 반환한다). 12df[&quot;grade&quot;] = df[&quot;grade&quot;].cat.set_categories([&quot;very bad&quot;, &quot;bad&quot;, &quot;medium&quot;, &quot;good&quot;, &quot;very good&quot;])df[&quot;grade&quot;] 0 very good 1 good 2 good 3 very good 4 very good 5 very bad Name: grade, dtype: category Categories (5, object): ['very bad', 'bad', 'medium', 'good', 'very good'] 정렬은 사전 순서가 아닌, 해당 범주에서 지정된 순서대로 배열한다.(very bad, bad, medium, good, very good 의 순서로 기재되어 있기 때문에 정렬 결과도 해당 순서대로 배열된다.) 1df.sort_values(by=&quot;grade&quot;) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id raw_grade grade 5 6 e very bad 1 2 b good 2 3 b good 0 1 a very good 3 4 a very good 4 5 a very good 범주의 열을 기준으로 그룹화하면 빈 범주도 표시된다. 1df.groupby(&quot;grade&quot;).size() grade very bad 1 bad 0 medium 0 good 2 very good 3 dtype: int64 11) 그래프 (Plotting)Visualization 123ts = pd.Series(np.random.randn(1000), index=pd.date_range('1/1/2000', periods=1000))ts = ts.cumsum()ts.plot() &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f2486b26fd0&gt; 데이터프레임에서 plot() 메소드는 라벨이 존재하는 모든 열을 그릴 때 편리하다. 1234df = pd.DataFrame(np.random.randn(1000, 4), index=ts.index, columns=['A', 'B', 'C', 'D']) df = df.cumsum()plt.figure(); df.plot(); plt.legend(loc='best') &lt;matplotlib.legend.Legend at 0x7f24869c8b38&gt; &lt;Figure size 432x288 with 0 Axes&gt; 12) 데이터 입/출력 (Getting Data In/Out)12-1) CSVcsv 파일에 쓴다. 1df.to_csv('foo.csv') csv 파일을 읽는다. 1pd.read_csv('foo.csv') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Unnamed: 0 A B C D 0 2000-01-01 0.360804 1.476213 -1.265585 -0.240420 1 2000-01-02 0.152304 2.910805 -0.935624 0.640792 2 2000-01-03 0.149786 3.056196 -0.101461 0.915478 3 2000-01-04 0.448929 1.637972 -0.117874 -0.625844 4 2000-01-05 0.942632 1.786469 -0.512949 -2.706819 ... ... ... ... ... ... 995 2002-09-22 -8.745917 28.871050 5.372952 -34.693096 996 2002-09-23 -8.355598 28.768257 5.388912 -33.545690 997 2002-09-24 -9.838548 28.876310 7.481758 -33.658828 998 2002-09-25 -10.267756 29.122170 7.903836 -33.454289 999 2002-09-26 -9.947559 28.628743 6.611331 -32.886421 1000 rows × 5 columns 12-2) HDF5HDF5 Store에 쓰고 읽어온다. 12df.to_hdf('foo.h5','df')pd.read_hdf('foo.h5','df') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } A B C D 2000-01-01 0.360804 1.476213 -1.265585 -0.240420 2000-01-02 0.152304 2.910805 -0.935624 0.640792 2000-01-03 0.149786 3.056196 -0.101461 0.915478 2000-01-04 0.448929 1.637972 -0.117874 -0.625844 2000-01-05 0.942632 1.786469 -0.512949 -2.706819 ... ... ... ... ... 2002-09-22 -8.745917 28.871050 5.372952 -34.693096 2002-09-23 -8.355598 28.768257 5.388912 -33.545690 2002-09-24 -9.838548 28.876310 7.481758 -33.658828 2002-09-25 -10.267756 29.122170 7.903836 -33.454289 2002-09-26 -9.947559 28.628743 6.611331 -32.886421 1000 rows × 4 columns 12-3) Excel엑셀 파일(MS Excel)에 쓴다. 1df.to_excel('foo.xlsx', sheet_name='Sheet1') 엑셀 파일(MS Excel)을 읽어온다. 1pd.read_excel('foo.xlsx', 'Sheet1', index_col=None, na_values=['NA']) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Unnamed: 0 A B C D 0 2000-01-01 0.360804 1.476213 -1.265585 -0.240420 1 2000-01-02 0.152304 2.910805 -0.935624 0.640792 2 2000-01-03 0.149786 3.056196 -0.101461 0.915478 3 2000-01-04 0.448929 1.637972 -0.117874 -0.625844 4 2000-01-05 0.942632 1.786469 -0.512949 -2.706819 ... ... ... ... ... ... 995 2002-09-22 -8.745917 28.871050 5.372952 -34.693096 996 2002-09-23 -8.355598 28.768257 5.388912 -33.545690 997 2002-09-24 -9.838548 28.876310 7.481758 -33.658828 998 2002-09-25 -10.267756 29.122170 7.903836 -33.454289 999 2002-09-26 -9.947559 28.628743 6.611331 -32.886421 1000 rows × 5 columns 13) 잡았다! (Gotchas)연산 수행 시 다음과 같은 예외 상황을 볼 수도 있다. 12if pd.Series([False, True, False]): print(&quot;I was true&quot;) --------------------------------------------------------------------------- ValueError Traceback (most recent call last) &lt;ipython-input-135-5c782b38cd2f&gt; in &lt;module&gt;() ----&gt; 1 if pd.Series([False, True, False]): 2 print(&quot;I was true&quot;) /usr/local/lib/python3.6/dist-packages/pandas/core/generic.py in __nonzero__(self) 1328 def __nonzero__(self): 1329 raise ValueError( -&gt; 1330 f&quot;The truth value of a {type(self).__name__} is ambiguous. &quot; 1331 &quot;Use a.empty, a.bool(), a.item(), a.any() or a.all().&quot; 1332 ) ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all(). 이러한 경우에는 any(), all(), empty 등을 사용해서 무엇을 원하는지를 선택 (반영)해주어야 한다. 12if pd.Series([False, True, False])is not None: print(&quot;I was not None&quot;) I was not None","link":"/2020/11/04/study/pandas_10minutes/"},{"title":"Seaborn with Matplotlib","text":"출처: Pega Devloghttps://jehyunlee.github.io/ 접기/펼치기 seaborn + matplotlib출처: Pega Devlog seaborn은 matplotlib을 쉽고 아름답게 쓰고자 만들어졌다. 따라서 seaborn의 결과물은 당연히 matplotlib의 결과물이다. 그러나 간혹 seaborn이 그린 그림의 폰트, 색상에 접근이 되지 않아서 난처하다. seaborn의 구조를 잘 이해하지 못하면 해결도 어렵다. v0.11 기준으로 seaborn에는 다음과 같은 함수들이 있다. matplotlib의 출력물은 figure와 axes(축)만을 반환한다. seaborn의 명령어 중 axes를 반환하는 것들은 matplotlib와 섞어 쓰기 좋다. 먼저 matplotlib의 객체 지향 object oriented interface를 사용해서 그림의 틀을 만든 뒤, 특정 axes에 seaborn을 삽입하면 된다. 결론적으로, 하고 싶은 것이 다 된다. Load data 예제로 사용할 펭귄 데이터를 불러온다. (이 데이터는 seaborn에 내장되어 있다.) 12345678910import pandas as pdimport matplotlib.pyplot as pltimport seaborn as sns# Pandas에서 나오는 경고문 무시import warningswarnings.filterwarnings('ignore')penguins = sns.load_dataset(&quot;penguins&quot;)penguins.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex 0 Adelie Torgersen 39.1 18.7 181.0 3750.0 Male 1 Adelie Torgersen 39.5 17.4 186.0 3800.0 Female 2 Adelie Torgersen 40.3 18.0 195.0 3250.0 Female 3 Adelie Torgersen NaN NaN NaN NaN NaN 4 Adelie Torgersen 36.7 19.3 193.0 3450.0 Female figure and axes matplotlib으로 도화지(figure)를 깔고, 축공간(axes)을 만든다. 1 * 2 축공간을 구성한다. 12fig, axes = plt.subplots(ncols = 2, figsize = (8, 4))fig.tight_layout() plot with matplotlib matplotlib 기능을 이용해서 산점도를 그린다. x축은 부리 길이(bill length) y축은 부리 위 아래 두께(bill depth) 색상은 종(species)으로 한다. (Adelie, Chinstrap, Gentoo가 있다.) 두 축공간 중 왼쪽에만 그린다. 123456789101112131415fig, axes = plt.subplots(ncols = 2, figsize = (8, 4))species_u = penguins[&quot;species&quot;].unique()# plot 0: matplotlibfor i, s in enumerate(species_u): axes[0].scatter(penguins[&quot;bill_length_mm&quot;].loc[penguins[&quot;species&quot;] == s], penguins[&quot;bill_depth_mm&quot;].loc[penguins[&quot;species&quot;] == s], c = f&quot;C{i}&quot;, label = s, alpha = 0.3 )axes[0].legend(species_u, title = &quot;species&quot;)axes[0].set_xlabel(&quot;Bill Length (mm)&quot;)axes[0].set_ylabel(&quot;Bill Depth (mm)&quot;)fig.tight_layout() plot with seaborn seaborn.scatterplot 이번엔 같은 plot을 seaborn으로 그려보자. 위 코드에 아래 세 줄만 추가한다. 12345678910111213141516171819fig, axes = plt.subplots(ncols = 2, figsize = (8, 4))species_u = penguins[&quot;species&quot;].unique()for i, s in enumerate(species_u): axes[0].scatter(penguins[&quot;bill_length_mm&quot;].loc[penguins[&quot;species&quot;] == s], penguins[&quot;bill_depth_mm&quot;].loc[penguins[&quot;species&quot;] == s], c = f&quot;C{i}&quot;, label = s, alpha = 0.3 )axes[0].legend(species_u, title = &quot;species&quot;)axes[0].set_xlabel(&quot;Bill Length (mm)&quot;)axes[0].set_ylabel(&quot;Bill Depth (mm)&quot;)# plot 1: seabornsns.scatterplot(&quot;bill_length_mm&quot;, &quot;bill_depth_mm&quot;, hue=&quot;species&quot;, data=penguins, alpha=0.3, ax=axes[1])axes[1].set_xlabel(&quot;Bill Length (mm)&quot;)axes[1].set_ylabel(&quot;Bill Depth (mm)&quot;)fig.tight_layout() 단 3줄로 거의 동일한 그림이 나왔다. scatter plot의 점 크기가 살짝 작다. label의 투명도가 살짝 다르다. seaborn 명령어 scatterplot() 를 그대로 사용했다. x축과 y축 label도 바꾸었다. ax = axes[1] 인자에서 볼 수 있듯, 존재하는 axes에 그림만 얹었다. matplotlib 틀 + seaborn 그림 이므로, matplotlib 명령이 모두 통한다. matplotlib + seaborn &amp; seaborn + matplotlib matplotlib와 seaborn이 자유롭게 섞일 수 있다. matplotlib 산점도 위에 seaborn 추세선을 얹을 수 있다. seaborn 산점도 위에 matplotlib 중심점을 얹을 수 있다. 12345678910111213141516171819202122232425262728293031323334fig, axes = plt.subplots(ncols = 2, figsize = (8, 4))species_u = penguins[&quot;species&quot;].unique()# plot 0: matplotlib + seabornfor i, s in enumerate(species_u): # matplotlib 산점도 axes[0].scatter(penguins[&quot;bill_length_mm&quot;].loc[penguins[&quot;species&quot;] == s], penguins[&quot;bill_depth_mm&quot;].loc[penguins[&quot;species&quot;] == s], c = f&quot;C{i}&quot;, label = s, alpha = 0.3 ) # seaborn 추세선 sns.regplot(&quot;bill_length_mm&quot;, &quot;bill_depth_mm&quot;, data = penguins.loc[penguins[&quot;species&quot;] == s], scatter = False, ax = axes[0])axes[0].legend(species_u, title = &quot;species&quot;)axes[0].set_xlabel(&quot;Bill Length (mm)&quot;)axes[0].set_ylabel(&quot;Bill Depth (mm)&quot;)# plot 1: seaborn + matplotlib# seaborn 산점도sns.scatterplot(&quot;bill_length_mm&quot;, &quot;bill_depth_mm&quot;, hue = &quot;species&quot;, data = penguins, alpha = 0.3, ax = axes[1])axes[1].set_xlabel(&quot;Bill Length (mm)&quot;)axes[1].set_ylabel(&quot;Bill Depth (mm)&quot;)# matplotlib 중심점for i, s in enumerate(species_u): axes[1].scatter(penguins[&quot;bill_length_mm&quot;].loc[penguins[&quot;species&quot;] == s].mean(), penguins[&quot;bill_depth_mm&quot;].loc[penguins[&quot;species&quot;] == s].mean(), c = f&quot;C{i}&quot;, alpha = 1, marker = &quot;x&quot;, s = 100 )fig.tight_layout() seaborn + seaborn + matplotlib 안 될 이유가 없다. seaborn scatterplot + seaborn kdeplot + matplotlib text 이다. 123456789fig, ax = plt.subplots(figsize = (6, 5))# plot 0: scatter plotsns.scatterplot(&quot;bill_length_mm&quot;, &quot;bill_depth_mm&quot;, color = &quot;k&quot;, data = penguins, alpha = 0.3, ax = ax, legend = False)ax.set_xlabel(&quot;Bill Length (mm)&quot;)ax.set_ylabel(&quot;Bill Depth (mm)&quot;)fig.tight_layout() 123456789101112fig, ax = plt.subplots(figsize = (6, 5))# plot 0: scatter plotsns.scatterplot(&quot;bill_length_mm&quot;, &quot;bill_depth_mm&quot;, color = &quot;k&quot;, data = penguins, alpha = 0.3, ax = ax, legend = False)# plot 1: kde plotsns.kdeplot(&quot;bill_length_mm&quot;, &quot;bill_depth_mm&quot;, hue = &quot;species&quot;, data = penguins, alpha = 0.5, ax = ax, legend = False)ax.set_xlabel(&quot;Bill Length (mm)&quot;)ax.set_ylabel(&quot;Bill Depth (mm)&quot;)fig.tight_layout() 1234567891011121314151617181920fig, ax = plt.subplots(figsize = (6, 5))# plot 0: scatter plotsns.scatterplot(&quot;bill_length_mm&quot;, &quot;bill_depth_mm&quot;, color = &quot;k&quot;, data = penguins, alpha = 0.3, ax = ax, legend = False)# plot 1: kde plotsns.kdeplot(&quot;bill_length_mm&quot;, &quot;bill_depth_mm&quot;, hue = &quot;species&quot;, data = penguins, alpha = 0.5, ax = ax, legend = False)# text:species_u = penguins[&quot;species&quot;].unique()for i, s in enumerate(species_u): ax.text(penguins[&quot;bill_length_mm&quot;].loc[penguins[&quot;species&quot;] == s].mean(), penguins[&quot;bill_depth_mm&quot;].loc[penguins[&quot;species&quot;] == s].mean(), s = s, fontdict = {&quot;fontsize&quot;: 14, &quot;fontweight&quot;: &quot;bold&quot;, &quot;color&quot;: &quot;k&quot;} )ax.set_xlabel(&quot;Bill Length (mm)&quot;)ax.set_ylabel(&quot;Bill Depth (mm)&quot;)fig.tight_layout() 결론 seaborn을 matplotlib와 마음껏 섞어쓰자. 단, axes를 반환하는 명령어에 한해서다. 이런 명령어를 axes-lebel function이라고 한다. seaborn + matplotlib을 이용한 jointplot 보완","link":"/2020/11/05/study/seaborn_with_matplotlib/"},{"title":"캐글 Home Credit Default Risk 분석","text":"출처: Will Koehrsen, Introduction to Manual Feature Engineering, 캐글, 2018.08.01 접기/펼치기 사전 준비Kaggle 데이터 불러오기Kaggle API 설치1!pip install kaggle Requirement already satisfied: kaggle in /usr/local/lib/python3.6/dist-packages (1.5.9) Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle) (2020.6.20) Requirement already satisfied: urllib3 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.24.3) Requirement already satisfied: six&gt;=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.15.0) Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.41.1) Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.8.1) Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.23.0) Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.0.1) Requirement already satisfied: slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (0.0.1) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;kaggle) (3.0.4) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;kaggle) (2.10) Requirement already satisfied: text-unidecode&gt;=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify-&gt;kaggle) (1.3) Kaggle Token 다운로드12345678from google.colab import filesuploaded = files.upload()for fn in uploaded.keys(): print('uploaded file &quot;{name}&quot; with length {length} bytes'.format( name=fn, length=len(uploaded[fn]))) # kaggle.json을 아래 폴더로 옮긴 뒤, file을 사용할 수 있도록 권한을 부여한다. !mkdir -p ~/.kaggle/ &amp;&amp; mv kaggle.json ~/.kaggle/ &amp;&amp; chmod 600 ~/.kaggle/kaggle.json Upload widget is only available when the cell has been executed in the current browser session. Please rerun this cell to enable. Saving kaggle.json to kaggle.json uploaded file &quot;kaggle.json&quot; with length 63 bytes 1ls -1ha ~/.kaggle/kaggle.json /root/.kaggle/kaggle.json 구글 드라이브 연동123456789101112from google.colab import drive # 패키지 불러오기 from os.path import join # 구글 드라이브 마운트ROOT = &quot;/content/drive&quot; # 드라이브 기본 경로print(ROOT) # print content of ROOT (Optional)drive.mount(ROOT) # 드라이브 기본 경로 # 프로젝트 파일 생성 및 다운받을 경로 이동MY_GOOGLE_DRIVE_PATH = 'My Drive/Colab Notebooks/python_basic/kaggle_home-credit-default-risk/data'PROJECT_PATH = join(ROOT, MY_GOOGLE_DRIVE_PATH)print(PROJECT_PATH) /content/drive Mounted at /content/drive /content/drive/My Drive/Colab Notebooks/python_basic/kaggle_home-credit-default-risk/data 1%cd &quot;{PROJECT_PATH}&quot; /content/drive/My Drive/Colab Notebooks/python_basic/kaggle_home-credit-default-risk/data Kaggle Competitions list 불러오기1!kaggle competitions list Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.9 / client 1.5.4) ref deadline category reward teamCount userHasEntered --------------------------------------------- ------------------- --------------- --------- --------- -------------- contradictory-my-dear-watson 2030-07-01 23:59:00 Getting Started Prizes 134 False gan-getting-started 2030-07-01 23:59:00 Getting Started Prizes 161 False tpu-getting-started 2030-06-03 23:59:00 Getting Started Knowledge 292 False digit-recognizer 2030-01-01 00:00:00 Getting Started Knowledge 2248 False titanic 2030-01-01 00:00:00 Getting Started Knowledge 17266 True house-prices-advanced-regression-techniques 2030-01-01 00:00:00 Getting Started Knowledge 4327 True connectx 2030-01-01 00:00:00 Getting Started Knowledge 366 False nlp-getting-started 2030-01-01 00:00:00 Getting Started Knowledge 1130 False rock-paper-scissors 2021-02-01 23:59:00 Playground Prizes 232 False riiid-test-answer-prediction 2021-01-07 23:59:00 Featured $100,000 1491 False nfl-big-data-bowl-2021 2021-01-05 23:59:00 Analytics $100,000 0 False competitive-data-science-predict-future-sales 2020-12-31 23:59:00 Playground Kudos 9393 False halite-iv-playground-edition 2020-12-31 23:59:00 Playground Knowledge 44 False predict-volcanic-eruptions-ingv-oe 2020-12-28 23:59:00 Playground Swag 198 False hashcode-drone-delivery 2020-12-14 23:59:00 Playground Knowledge 80 False cdp-unlocking-climate-solutions 2020-12-02 23:59:00 Analytics $91,000 0 False lish-moa 2020-11-30 23:59:00 Research $30,000 3454 False google-football 2020-11-30 23:59:00 Featured $6,000 925 False conways-reverse-game-of-life-2020 2020-11-30 23:59:00 Playground Swag 133 False lyft-motion-prediction-autonomous-vehicles 2020-11-25 23:59:00 Featured $30,000 788 False Home Credit Default Risk 데이터셋 불러오기1!kaggle competitions download -c home-credit-default-risk Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.9 / client 1.5.4) Downloading installments_payments.csv.zip to /content/drive/My Drive/Colab Notebooks/python_basic/kaggle_home-credit-default-risk/data 98% 266M/271M [00:02&lt;00:00, 123MB/s] 100% 271M/271M [00:02&lt;00:00, 104MB/s] Downloading previous_application.csv.zip to /content/drive/My Drive/Colab Notebooks/python_basic/kaggle_home-credit-default-risk/data 98% 75.0M/76.3M [00:00&lt;00:00, 81.3MB/s] 100% 76.3M/76.3M [00:00&lt;00:00, 89.9MB/s] Downloading application_test.csv.zip to /content/drive/My Drive/Colab Notebooks/python_basic/kaggle_home-credit-default-risk/data 100% 5.81M/5.81M [00:00&lt;00:00, 60.2MB/s] Downloading bureau.csv.zip to /content/drive/My Drive/Colab Notebooks/python_basic/kaggle_home-credit-default-risk/data 71% 26.0M/36.8M [00:00&lt;00:00, 49.6MB/s] 100% 36.8M/36.8M [00:00&lt;00:00, 74.7MB/s] Downloading sample_submission.csv to /content/drive/My Drive/Colab Notebooks/python_basic/kaggle_home-credit-default-risk/data 0% 0.00/524k [00:00&lt;?, ?B/s] 100% 524k/524k [00:00&lt;00:00, 34.4MB/s] Downloading POS_CASH_balance.csv.zip to /content/drive/My Drive/Colab Notebooks/python_basic/kaggle_home-credit-default-risk/data 99% 107M/109M [00:01&lt;00:00, 95.7MB/s] 100% 109M/109M [00:01&lt;00:00, 90.6MB/s] Downloading credit_card_balance.csv.zip to /content/drive/My Drive/Colab Notebooks/python_basic/kaggle_home-credit-default-risk/data 89% 86.0M/96.7M [00:01&lt;00:00, 71.8MB/s] 100% 96.7M/96.7M [00:01&lt;00:00, 76.9MB/s] Downloading HomeCredit_columns_description.csv to /content/drive/My Drive/Colab Notebooks/python_basic/kaggle_home-credit-default-risk/data 0% 0.00/36.5k [00:00&lt;?, ?B/s] 100% 36.5k/36.5k [00:00&lt;00:00, 4.81MB/s] Downloading application_train.csv.zip to /content/drive/My Drive/Colab Notebooks/python_basic/kaggle_home-credit-default-risk/data 83% 30.0M/36.1M [00:00&lt;00:00, 37.6MB/s] 100% 36.1M/36.1M [00:00&lt;00:00, 52.1MB/s] Downloading bureau_balance.csv.zip to /content/drive/My Drive/Colab Notebooks/python_basic/kaggle_home-credit-default-risk/data 99% 56.0M/56.8M [00:01&lt;00:00, 27.5MB/s] 100% 56.8M/56.8M [00:01&lt;00:00, 47.0MB/s] 1!ls application_test.csv.zip HomeCredit_columns_description.csv application_train.csv.zip installments_payments.csv.zip bureau_balance.csv.zip POS_CASH_balance.csv.zip bureau.csv.zip previous_application.csv.zip credit_card_balance.csv.zip sample_submission.csv zip 파일 압축 풀기 참고 12345678!unzip application_test.csv.zip!unzip application_train.csv.zip!unzip bureau_balance.csv.zip!unzip bureau.csv.zip!unzip credit_card_balance.csv.zip!unzip installments_payments.csv.zip!unzip POS_CASH_balance.csv.zip!unzip previous_application.csv.zip Archive: application_test.csv.zip inflating: application_test.csv Archive: application_train.csv.zip inflating: application_train.csv Archive: bureau_balance.csv.zip inflating: bureau_balance.csv Archive: bureau.csv.zip inflating: bureau.csv Archive: credit_card_balance.csv.zip inflating: credit_card_balance.csv Archive: installments_payments.csv.zip inflating: installments_payments.csv Archive: POS_CASH_balance.csv.zip inflating: POS_CASH_balance.csv Archive: previous_application.csv.zip inflating: previous_application.csv 1!ls application_test.csv credit_card_balance.csv.zip application_test.csv.zip HomeCredit_columns_description.csv application_train.csv installments_payments.csv application_train.csv.zip installments_payments.csv.zip bureau_balance.csv POS_CASH_balance.csv bureau_balance.csv.zip POS_CASH_balance.csv.zip bureau.csv previous_application.csv bureau.csv.zip previous_application.csv.zip credit_card_balance.csv sample_submission.csv 압축파일 삭제하기 참고 12345678!rm application_test.csv.zip!rm application_train.csv.zip!rm bureau_balance.csv.zip!rm bureau.csv.zip!rm credit_card_balance.csv.zip!rm installments_payments.csv.zip!rm POS_CASH_balance.csv.zip!rm previous_application.csv.zip 1!ls application_test.csv HomeCredit_columns_description.csv application_train.csv installments_payments.csv bureau_balance.csv POS_CASH_balance.csv bureau.csv previous_application.csv credit_card_balance.csv sample_submission.csv Introduction: Manual Feature Engineering bureau.csv: ‘Home Credit’에 제출된 고객(Client)의 다른 금융기관에서의 과거의 대출 기록. (각각의 대출 기록은 각각의 열로 정리되어 있다.) bureau_balance.csv: 과거 대출들의 월별 데이터. (각 월별 데이터는 각각의 열로 정리되어 있다.) Manual(수동화된) Feature Engineering은 지루한 과정일 수 있다. 이것은 많은 사람들이 자동화된 Feature Engineering 기능을 활용하는 주된 이유이다. 대출 및 채무 불이행의 주된 원인에 대한 지식을 갖추는데는 한계가 있기 때문에, 최종 학습용 데이터프레임에서 가능한 많은 정보들을 얻는 데 주안점을 두었다. 이 커널은 어떤 Feature가 중요한 지를 결정하는 것에 있어서, 사람보다 모델이 고르도록 하는 접근방식을 취한다. 기본적으로 이러한 접근방식에서는 최대한 많은 Feature를 만들고, 모델은 이러한 Feature를 전부 활용한다. 수작업(Manual) Feature Engineering의 각 과정은 많은 양의 Pandas 코드와 약간의 인내심, 특히 데이터 처리에 있어서 많은 인내심을 필요로 한다. Feature Engineering은 여전히 전처리 작업을 필요로 한다. 12345678910111213141516171819# 데이터 처리import pandas as pdimport numpy as np# 시각화import matplotlib.pyplot as pltimport seaborn as sns# Pandas에서 나오는 경고문 무시import warningswarnings.filterwarnings('ignore')# 원본# plt.style.use('fivethirtyeight')# matplotlib 의 기본 scheme 말고 seaborn scheme 을 세팅하고,# 일일이 graph 의 font size 를 지정할 필요 없이 seaborn 의 font_scale 을 사용하면 편하다.plt.style.use('seaborn')sns.set(font_scale = 2.1) 예시: 고객의 이전 대출 수량 파악Counts of a client’s previous loans먼저 고객의 과거 타 금융기관에서의 대출 수량을 간단히 파악하고자 한다. 이 과정은 이 커널에서 반복적으로 사용되는 아래의 pandas 명령어를 포함한다. groupby: Column값에 따라 데이터프레임을 그룹화. 이 과정에서는 SK_ID_CURR Column의 값에 따라 고객별로 데이터프레임을 그룹화 agg: 그룹화된 데이터의 평균 등을 계산. ‘grouped_df.mean()’을 통해 직접 평균을 계산하거나, agg 명령어와 리스트를 활용하여 평균, 최대값, 최소값, 합계 등을 계산 (grouped_df.agg([mean, max, min, sum])). merge: 집계된(aggregated) 값을 해당 고객과 매칭. SK_ID_CURR Column을 활용하여 집계된 값을 원본 트레이닝 데이터로 병합하고, 해당값이 없을 경우에는 NaN값을 입력. 또한 rename 명령어를 통해 Column을 딕셔너리(dict)를 활용하여 변경한다. 이러한 방식은 생성된 변수를 계속해서 추적하는 데 유용하다. 123# bureau 파일 읽기bureau = pd.read_csv('bureau.csv')bureau.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } SK_ID_CURR SK_ID_BUREAU CREDIT_ACTIVE CREDIT_CURRENCY DAYS_CREDIT CREDIT_DAY_OVERDUE DAYS_CREDIT_ENDDATE DAYS_ENDDATE_FACT AMT_CREDIT_MAX_OVERDUE CNT_CREDIT_PROLONG AMT_CREDIT_SUM AMT_CREDIT_SUM_DEBT AMT_CREDIT_SUM_LIMIT AMT_CREDIT_SUM_OVERDUE CREDIT_TYPE DAYS_CREDIT_UPDATE AMT_ANNUITY 0 215354 5714462 Closed currency 1 -497 0 -153.0 -153.0 NaN 0 91323.0 0.0 NaN 0.0 Consumer credit -131 NaN 1 215354 5714463 Active currency 1 -208 0 1075.0 NaN NaN 0 225000.0 171342.0 NaN 0.0 Credit card -20 NaN 2 215354 5714464 Active currency 1 -203 0 528.0 NaN NaN 0 464323.5 NaN NaN 0.0 Consumer credit -16 NaN 3 215354 5714465 Active currency 1 -203 0 NaN NaN NaN 0 90000.0 NaN NaN 0.0 Credit card -16 NaN 4 215354 5714466 Active currency 1 -629 0 1197.0 NaN 77674.5 0 2700000.0 NaN NaN 0.0 Consumer credit -21 NaN 고객 아이디(SK_ID_CURR)를 기준으로 groupby 실행한다. 이전 대출 횟수를 파악하고, Column 이름을 변경한다. 12previous_loan_counts = bureau.groupby('SK_ID_CURR', as_index = False)['SK_ID_BUREAU'].count().rename(columns = {'SK_ID_BUREAU': 'previous_loan_counts'})previous_loan_counts.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } SK_ID_CURR previous_loan_counts 0 100001 7 1 100002 8 2 100003 4 3 100004 2 4 100005 3 훈련용 데이터프레임과 병합(Join)한다. 12train = pd.read_csv('application_train.csv')train = train.merge(previous_loan_counts, on = 'SK_ID_CURR', how = 'left') NaN 값에 0 을 대입한다. 12train['previous_loan_counts'] = train['previous_loan_counts'].fillna(0)train.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } SK_ID_CURR TARGET NAME_CONTRACT_TYPE CODE_GENDER FLAG_OWN_CAR FLAG_OWN_REALTY CNT_CHILDREN AMT_INCOME_TOTAL AMT_CREDIT AMT_ANNUITY AMT_GOODS_PRICE NAME_TYPE_SUITE NAME_INCOME_TYPE NAME_EDUCATION_TYPE NAME_FAMILY_STATUS NAME_HOUSING_TYPE REGION_POPULATION_RELATIVE DAYS_BIRTH DAYS_EMPLOYED DAYS_REGISTRATION DAYS_ID_PUBLISH OWN_CAR_AGE FLAG_MOBIL FLAG_EMP_PHONE FLAG_WORK_PHONE FLAG_CONT_MOBILE FLAG_PHONE FLAG_EMAIL OCCUPATION_TYPE CNT_FAM_MEMBERS REGION_RATING_CLIENT REGION_RATING_CLIENT_W_CITY WEEKDAY_APPR_PROCESS_START HOUR_APPR_PROCESS_START REG_REGION_NOT_LIVE_REGION REG_REGION_NOT_WORK_REGION LIVE_REGION_NOT_WORK_REGION REG_CITY_NOT_LIVE_CITY REG_CITY_NOT_WORK_CITY LIVE_CITY_NOT_WORK_CITY ... LIVINGAREA_MEDI NONLIVINGAPARTMENTS_MEDI NONLIVINGAREA_MEDI FONDKAPREMONT_MODE HOUSETYPE_MODE TOTALAREA_MODE WALLSMATERIAL_MODE EMERGENCYSTATE_MODE OBS_30_CNT_SOCIAL_CIRCLE DEF_30_CNT_SOCIAL_CIRCLE OBS_60_CNT_SOCIAL_CIRCLE DEF_60_CNT_SOCIAL_CIRCLE DAYS_LAST_PHONE_CHANGE FLAG_DOCUMENT_2 FLAG_DOCUMENT_3 FLAG_DOCUMENT_4 FLAG_DOCUMENT_5 FLAG_DOCUMENT_6 FLAG_DOCUMENT_7 FLAG_DOCUMENT_8 FLAG_DOCUMENT_9 FLAG_DOCUMENT_10 FLAG_DOCUMENT_11 FLAG_DOCUMENT_12 FLAG_DOCUMENT_13 FLAG_DOCUMENT_14 FLAG_DOCUMENT_15 FLAG_DOCUMENT_16 FLAG_DOCUMENT_17 FLAG_DOCUMENT_18 FLAG_DOCUMENT_19 FLAG_DOCUMENT_20 FLAG_DOCUMENT_21 AMT_REQ_CREDIT_BUREAU_HOUR AMT_REQ_CREDIT_BUREAU_DAY AMT_REQ_CREDIT_BUREAU_WEEK AMT_REQ_CREDIT_BUREAU_MON AMT_REQ_CREDIT_BUREAU_QRT AMT_REQ_CREDIT_BUREAU_YEAR previous_loan_counts 0 100002 1 Cash loans M N Y 0 202500.0 406597.5 24700.5 351000.0 Unaccompanied Working Secondary / secondary special Single / not married House / apartment 0.018801 -9461 -637 -3648.0 -2120 NaN 1 1 0 1 1 0 Laborers 1.0 2 2 WEDNESDAY 10 0 0 0 0 0 0 ... 0.0193 0.0000 0.00 reg oper account block of flats 0.0149 Stone, brick No 2.0 2.0 2.0 2.0 -1134.0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.0 0.0 0.0 0.0 0.0 1.0 8.0 1 100003 0 Cash loans F N N 0 270000.0 1293502.5 35698.5 1129500.0 Family State servant Higher education Married House / apartment 0.003541 -16765 -1188 -1186.0 -291 NaN 1 1 0 1 1 0 Core staff 2.0 1 1 MONDAY 11 0 0 0 0 0 0 ... 0.0558 0.0039 0.01 reg oper account block of flats 0.0714 Block No 1.0 0.0 1.0 0.0 -828.0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.0 0.0 0.0 0.0 0.0 0.0 4.0 2 100004 0 Revolving loans M Y Y 0 67500.0 135000.0 6750.0 135000.0 Unaccompanied Working Secondary / secondary special Single / not married House / apartment 0.010032 -19046 -225 -4260.0 -2531 26.0 1 1 1 1 1 0 Laborers 1.0 2 2 MONDAY 9 0 0 0 0 0 0 ... NaN NaN NaN NaN NaN NaN NaN NaN 0.0 0.0 0.0 0.0 -815.0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.0 0.0 0.0 0.0 0.0 0.0 2.0 3 100006 0 Cash loans F N Y 0 135000.0 312682.5 29686.5 297000.0 Unaccompanied Working Secondary / secondary special Civil marriage House / apartment 0.008019 -19005 -3039 -9833.0 -2437 NaN 1 1 0 1 0 0 Laborers 2.0 2 2 WEDNESDAY 17 0 0 0 0 0 0 ... NaN NaN NaN NaN NaN NaN NaN NaN 2.0 0.0 2.0 0.0 -617.0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 NaN NaN NaN NaN NaN NaN 0.0 4 100007 0 Cash loans M N Y 0 121500.0 513000.0 21865.5 513000.0 Unaccompanied Working Secondary / secondary special Single / not married House / apartment 0.028663 -19932 -3038 -4311.0 -3458 NaN 1 1 0 1 0 0 Core staff 1.0 2 2 THURSDAY 11 0 0 0 0 1 1 ... NaN NaN NaN NaN NaN NaN NaN NaN 0.0 0.0 0.0 0.0 -1106.0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 5 rows × 123 columns 맨 오른쪽 컬럼에 새롭게 만들어진 previous_loan_counts 컬럼을 확일할 수 있다. R Value 를 활용한 변수 유용성 평가Assessing Usefulness of New Variable with r value 새롭게 생성된 Column의 변수가 유용한 지 판단하기 위해서, 우선 목표값(target)과 해당 변수간의 피어슨 상관계수를 계산하고자 한다. 두 변수 사이의 선형관계(linear relationship)는 -1(완벽하게 음의 선형관계)에서부터 +1(완벽히 양의 선형관계) 사이의 값으로 표현된다. R Value가 변수의 유용성을 평가하기 위한 최선을 방식은 아니지만, 머신러닝 모델을 발전시키는 데 효과가 있을 지에 대한 대략적인 정보를 줄 수는 있다. 목표값에 대한 r-value가 커질수록, 해당 변수가 목표값에 영향을 끼칠 가능성이 높아진다. 그러므로 목표값에 대해 가장 큰 r-value의 절대값을 가지는 변수를 찾고자 한다. 또한 커널밀도추정그래프를 활용하여 목표값과의 상관관계를 시각적으로 살펴볼 것이다. 커널밀도추정그래프Kernal Density Estimate Plots 커널밀도추정그래프는 단일 변수의 분포를 보여준다. 히스토그램을 부드럽게 한 것으로 생각해보면 될 것이다. 범주형 변수의 값 차이에 따른 분포의 차이를 보기 위해, 카테고리에 따라 색을 다르게 칠하도록 하겠다. 예를 들어, target 값이 0인지 1인지에 따라 색을 다르게 칠한 previous_loan_count의 커널밀도추정그래프를 그릴 수 있다. 이러한 그래프는 대출을 상환한 그룹(target == 0)과 그렇지 못한 그룹(target == 1)의 분포에 있어 차이점을 보여줄 것이다. 이는 변수들이 머신러닝 모델과 관련성을 가지는 지를 보여줄 수 있는 지표로 활용될 수 있다. 원본 소스코드에 있던 df.ix 는 더 이상 지원하지 않아서 대신 df.loc를 사용한다. 1234567891011121314151617181920212223242526272829303132333435363738# 변수의 분포에 대한 그래프 target값에 따라 색을 달리하여 작성한다.def kde_target(var_name, df): ''' Args input: var_name = str, 변수가 되는 Column df: DataFrame, 대상 데이터 프레임 return: None ''' # 새롭게 생성된 변수와 target간의 상관계수를 계산한다. corr = df['TARGET'].corr(df[var_name]) # 대출을 상환한 그룹(0)과 그렇지 않은 그룹(1)의 중간값(media)을 계산한다. avg_repaid = df.loc[df['TARGET'] == 0, var_name].median() avg_not_repaid = df.loc[df['TARGET'] == 1, var_name].median() plt.figure(figsize = (12, 6)) # target값에 따라 색을 달리하여 그래프 작성 sns.kdeplot(df.loc[df['TARGET'] == 0, var_name], label = 'TARGET == 0') sns.kdeplot(df.loc[df['TARGET'] == 1, var_name], label = 'TARGET == 1') # 그래프 라벨링 plt.xlabel(var_name); plt.ylabel('Density'); plt.title('%s Distribution' % var_name) plt.legend(); # 상관계수 출력 print('The correlation between %s and the TARGET is %0.4f' % (var_name, corr)) # 중간값 출력 print('Median value for loan that was not repaid = %0.4f' % avg_not_repaid) print('Median value for loan that was repaid = %0.4f' % avg_repaid) Random Forest 및 Gradient Boosting Machine에 의해 가장 중요한 변수로 판명된 EXT_SOURCE_3를 활용하여 테스트하도록 하겠다. 1kde_target('EXT_SOURCE_3', train) The correlation between EXT_SOURCE_3 and the TARGET is -0.1789 Median value for loan that was not repaid = 0.3791 Median value for loan that was repaid = 0.5460 새로운 변수 previous_loan_counts를 살펴보겠다. 1kde_target('previous_loan_counts', train) The correlation between previous_loan_counts and the TARGET is -0.0100 Median value for loan that was not repaid = 3.0000 Median value for loan that was repaid = 4.0000 이 그래프를 보면 상관계수가 너무 작고, target값에 따른 분포의 차이도 거의 없는 걸 확인할 수 있다. 이를 통해, 새롭게 생성된 변수(previous_loan_counts Distribution)가 중요하지 않음을 알 수 있다. 이제 bureau 데이터프레임으로부터 몇 개의 변수를 새롭게 생성해보도록 하겠다. bureau 데이터프레임의 모든 수치형 변수로부터 평균, 최소, 최대값을 가져올 예정이다. 수치 데이터의 대표값을 계산Aggregating Numeric Columns 여기서 ‘대표값을 계산한다’는 것은 agg를 활용하여 데이터프레임의 평균, 최대값, 최소값, 합계 등을 구하는 것으로 정의한다. bureau 데이터 프레임 안의 수치형 변수를 활용하기 위해, 모든 수치 데이터 Column의 대표값을 계산할 것이다. 이를 위해 고객 ID별로 그룹화를 수행하고, 그룹화된 데이터프레임의 대표값들을 agg를 Putting the Functions TogetherFeature Engineering OutcomesModelingResults","link":"/2020/11/05/study/kaggle_home_credit_default_risk/"},{"title":"Hello Coding 한입에 쏙 파이썬","text":"출처: 김왼손, 김태간, 『Hello Coding 한입에 쏙 파이썬』, 한빛미디어, 2018.06 접기/펼치기 12# 코드 1-1 ‘Hello World!’를 출력하는 코드print('Hello World!') Hello World! 12# 코드 1-2 print()를 잘못 사용한 코드Print('Hello World!') --------------------------------------------------------------------------- NameError Traceback (most recent call last) &lt;ipython-input-2-18101e960d2d&gt; in &lt;module&gt;() 1 # 코드 1-2 print()를 잘못 사용한 코드 ----&gt; 2 Print('Hello World!') NameError: name 'Print' is not defined 12# 코드 1-3 ‘토끼야 안녕!’을 출력하는 코드print('토끼야 안녕!') 토끼야 안녕! 1234# 코드 2-1 다양한 숫자를 출력하는 코드print(1)print(-2)print(3.14) 1 -2 3.14 12345# 코드 2-2 더하기, 빼기, 곱하기, 나누기를 하는 코드print(1 + 2)print(3 - 2)print(2 * 4)print(6 / 3) 3 1 8 2.0 1234# 코드 2-3 제곱, 몫, 나머지를 구하는 코드print(5 ** 2)print(5 // 2)print(5 % 2) 25 2 1 12345# 코드 2-4 다양한 연산을 하는 코드print(3 + 7)print(6 * 3)print(4 ** 2)print(9 % 5) 10 18 16 4 12345# 코드 2-5 문자열을 출력하는 코드print('Hello World!')print('3.14')print('토끼야 안녕!')print(&quot;토끼야 안녕!&quot;) Hello World! 3.14 토끼야 안녕! 토끼야 안녕! 123# 코드 2-6 문자열 + 연산을 하는 코드print('토끼' + '야 안녕!')print('다람쥐' + '야 안녕!') 토끼야 안녕! 다람쥐야 안녕! 12# 코드 2-7 문자열 * 연산을 하는 코드print('데굴' * 2) 데굴데굴 12# 코드 2-8 색깔 문자를 연결하는 코드print('빨' + '주' + '노' + '초' + '파' + '남' + '보') 빨주노초파남보 123# 코드 2-9 변수에 저장한 문자열을 출력하는 코드rainbow = '빨주노초파남보'print(rainbow) 빨주노초파남보 1234567# 코드 2-10 변수에 저장한 값을 변경하는 코드count = 0print(count)count = 1print(count)count = count + 1print(count) 0 1 2 1234567# 코드 2-11 변수를 사용해 음료의 총 금액을 계산하는 코드coffee = 4100juice = 4600tea = 3900print(coffee * 3 + juice * 2 + tea * 1)print(coffee * 4 + juice * 3 + tea * 3)print(coffee * 1 + juice * 1 + tea * 2) 25400 41900 16500 1234# 코드 2-12 주석으로 내용을 설명하는 코드coffee = 4100 # 커피의 가격juice = 4600 # 주스의 가격tea = 3900 # 홍차의 가격 123# 코드 2-13 주석으로 문장을 제외하는 코드print('토끼야 안녕!')# print('토끼야 안녕!') 토끼야 안녕! 1234567891011# 코드 3-1 여러 개의 변수에 사탕을 저장하는 코드candy0 = '딸기맛'print(candy0)candy1 = '레몬맛'print(candy1)candy2 = '수박맛'print(candy2)candy3 = '박하맛'print(candy3)candy4 = '우유맛'print(candy4) 딸기맛 레몬맛 수박맛 박하맛 우유맛 123# 코드 3-2 여러 개의 사탕을 하나의 변수에 저장하는 코드candies = ['딸기맛', '레몬맛', '수박맛', '박하맛', '우유맛']print(candies) ['딸기맛', '레몬맛', '수박맛', '박하맛', '우유맛'] 1234567# 코드 3-3 리스트를 만드는 코드my_list1 = []print(my_list1)my_list2 = [1, -2, 3.14]print(my_list2)my_list3 = ['앨리스', 10, [1.0, 1.2]]print(my_list3) [] [1, -2, 3.14] ['앨리스', 10, [1.0, 1.2]] 12345678# 코드 3-4 리스트에 값을 추가하는 코드clovers = []clovers.append('클로버1')print(clovers)clovers.append('하트2')print(clovers)clovers.append('클로버3')print(clovers) ['클로버1'] ['클로버1', '하트2'] ['클로버1', '하트2', '클로버3'] 123456# 코드 3-5 리스트의 값에 접근하는 코드clovers = ['클로버1', '하트2', '클로버3']print(clovers[1])clovers[1] = '클로버2'print(clovers[1])print(clovers[3]) 하트2 클로버2 --------------------------------------------------------------------------- IndexError Traceback (most recent call last) &lt;ipython-input-21-670b859913d2&gt; in &lt;module&gt;() 4 clovers[1] = '클로버2' 5 print(clovers[1]) ----&gt; 6 print(clovers[3]) IndexError: list index out of range 123456# 코드 3-6 리스트에서 값을 제거하는 코드clovers = ['클로버1', '클로버2', '클로버3']print(clovers[1])del clovers[1]print(clovers)print(clovers[1]) 클로버2 ['클로버1', '클로버3'] 클로버3 12345678# 코드 3-7 사탕을 추가하고 제거하는 코드candies = ['딸기맛', '레몬맛', '수박맛', '박하맛', '우유맛']print(candies)candies.append('콜라맛')candies.append('포도맛')print(candies)del candies[3]print(candies) ['딸기맛', '레몬맛', '수박맛', '박하맛', '우유맛'] ['딸기맛', '레몬맛', '수박맛', '박하맛', '우유맛', '콜라맛', '포도맛'] ['딸기맛', '레몬맛', '수박맛', '우유맛', '콜라맛', '포도맛'] 12345# 코드 3-8 리스트에서 여러 개의 값을 가져오는 코드week = ['월', '화', '수', '목', '금', '토', '일']print(week)print(week[2:5])print(week) ['월', '화', '수', '목', '금', '토', '일'] ['수', '목', '금'] ['월', '화', '수', '목', '금', '토', '일'] 12345678# 코드 3-9 리스트에서 값을 가져오는 코드candies = ['딸기맛', '레몬맛', '수박맛', '우유맛', '콜라맛', '포도맛']cat_candy = candies[0]print('체셔고양이에게는', cat_candy, '사탕을 줘요.')duck_candy = candies[1]print('오리에게는', duck_candy, '사탕을 줘요.')dodo_candies = candies[3:6]print('도도새에게는', dodo_candies, '사탕을 줘요.') 체셔고양이에게는 딸기맛 사탕을 줘요. 오리에게는 레몬맛 사탕을 줘요. 도도새에게는 ['우유맛', '콜라맛', '포도맛'] 사탕을 줘요. 1234# 코드 3-10 리스트의 문자열을 정렬하는 코드animals = ['체셔고양이', '오리', '도도새']animals.sort()print(animals) ['도도새', '오리', '체셔고양이'] 1234# 코드 3-11 리스트에서 특정 값의 개수를 세는 코드cards = ['하트', '클로버', '하트', '다이아']print(cards.count('하트'))print(cards.count('클로버')) 2 1 12345678# 코드 4-1 거북이 100마리에게 인사하는 코드print('안녕 거북이 0')print('안녕 거북이 1')# .# .# .print('안녕 거북이 98')print('안녕 거북이 99') 안녕 거북이 0 안녕 거북이 1 안녕 거북이 98 안녕 거북이 99 123# 코드 4-2 거북이 5,000마리에게 인사하는 코드for num in range(5000): print('안녕 거북이', num) \u001b[1;30;43m스트리밍 출력 내용이 길어서 마지막 5000줄이 삭제되었습니다.\u001b[0m 안녕 거북이 0 안녕 거북이 1 안녕 거북이 2 안녕 거북이 3 안녕 거북이 4 안녕 거북이 5 안녕 거북이 6 안녕 거북이 7 안녕 거북이 8 안녕 거북이 9 안녕 거북이 10 안녕 거북이 11 안녕 거북이 12 안녕 거북이 13 안녕 거북이 14 안녕 거북이 15 안녕 거북이 16 안녕 거북이 17 안녕 거북이 18 안녕 거북이 19 안녕 거북이 20 안녕 거북이 21 안녕 거북이 22 안녕 거북이 23 안녕 거북이 24 안녕 거북이 25 안녕 거북이 26 안녕 거북이 27 안녕 거북이 28 안녕 거북이 29 안녕 거북이 30 안녕 거북이 31 안녕 거북이 32 안녕 거북이 33 안녕 거북이 34 안녕 거북이 35 안녕 거북이 36 안녕 거북이 37 안녕 거북이 38 안녕 거북이 39 안녕 거북이 40 안녕 거북이 41 안녕 거북이 42 안녕 거북이 43 안녕 거북이 44 안녕 거북이 45 안녕 거북이 46 안녕 거북이 47 안녕 거북이 48 안녕 거북이 49 안녕 거북이 50 안녕 거북이 51 안녕 거북이 52 안녕 거북이 53 안녕 거북이 54 안녕 거북이 55 안녕 거북이 56 안녕 거북이 57 안녕 거북이 58 안녕 거북이 59 안녕 거북이 60 안녕 거북이 61 안녕 거북이 62 안녕 거북이 63 안녕 거북이 64 안녕 거북이 65 안녕 거북이 66 안녕 거북이 67 안녕 거북이 68 안녕 거북이 69 안녕 거북이 70 안녕 거북이 71 안녕 거북이 72 안녕 거북이 73 안녕 거북이 74 안녕 거북이 75 안녕 거북이 76 안녕 거북이 77 안녕 거북이 78 안녕 거북이 79 안녕 거북이 80 안녕 거북이 81 안녕 거북이 82 안녕 거북이 83 안녕 거북이 84 안녕 거북이 85 안녕 거북이 86 안녕 거북이 87 안녕 거북이 88 안녕 거북이 89 안녕 거북이 90 안녕 거북이 91 안녕 거북이 92 안녕 거북이 93 안녕 거북이 94 안녕 거북이 95 안녕 거북이 96 안녕 거북이 97 안녕 거북이 98 안녕 거북이 99 안녕 거북이 100 안녕 거북이 101 안녕 거북이 102 안녕 거북이 103 안녕 거북이 104 안녕 거북이 105 안녕 거북이 106 안녕 거북이 107 안녕 거북이 108 안녕 거북이 109 안녕 거북이 110 안녕 거북이 111 안녕 거북이 112 안녕 거북이 113 안녕 거북이 114 안녕 거북이 115 안녕 거북이 116 안녕 거북이 117 안녕 거북이 118 안녕 거북이 119 안녕 거북이 120 안녕 거북이 121 안녕 거북이 122 안녕 거북이 123 안녕 거북이 124 안녕 거북이 125 안녕 거북이 126 안녕 거북이 127 안녕 거북이 128 안녕 거북이 129 안녕 거북이 130 안녕 거북이 131 안녕 거북이 132 안녕 거북이 133 안녕 거북이 134 안녕 거북이 135 안녕 거북이 136 안녕 거북이 137 안녕 거북이 138 안녕 거북이 139 안녕 거북이 140 안녕 거북이 141 안녕 거북이 142 안녕 거북이 143 안녕 거북이 144 안녕 거북이 145 안녕 거북이 146 안녕 거북이 147 안녕 거북이 148 안녕 거북이 149 안녕 거북이 150 안녕 거북이 151 안녕 거북이 152 안녕 거북이 153 안녕 거북이 154 안녕 거북이 155 안녕 거북이 156 안녕 거북이 157 안녕 거북이 158 안녕 거북이 159 안녕 거북이 160 안녕 거북이 161 안녕 거북이 162 안녕 거북이 163 안녕 거북이 164 안녕 거북이 165 안녕 거북이 166 안녕 거북이 167 안녕 거북이 168 안녕 거북이 169 안녕 거북이 170 안녕 거북이 171 안녕 거북이 172 안녕 거북이 173 안녕 거북이 174 안녕 거북이 175 안녕 거북이 176 안녕 거북이 177 안녕 거북이 178 안녕 거북이 179 안녕 거북이 180 안녕 거북이 181 안녕 거북이 182 안녕 거북이 183 안녕 거북이 184 안녕 거북이 185 안녕 거북이 186 안녕 거북이 187 안녕 거북이 188 안녕 거북이 189 안녕 거북이 190 안녕 거북이 191 안녕 거북이 192 안녕 거북이 193 안녕 거북이 194 안녕 거북이 195 안녕 거북이 196 안녕 거북이 197 안녕 거북이 198 안녕 거북이 199 안녕 거북이 200 안녕 거북이 201 안녕 거북이 202 안녕 거북이 203 안녕 거북이 204 안녕 거북이 205 안녕 거북이 206 안녕 거북이 207 안녕 거북이 208 안녕 거북이 209 안녕 거북이 210 안녕 거북이 211 안녕 거북이 212 안녕 거북이 213 안녕 거북이 214 안녕 거북이 215 안녕 거북이 216 안녕 거북이 217 안녕 거북이 218 안녕 거북이 219 안녕 거북이 220 안녕 거북이 221 안녕 거북이 222 안녕 거북이 223 안녕 거북이 224 안녕 거북이 225 안녕 거북이 226 안녕 거북이 227 안녕 거북이 228 안녕 거북이 229 안녕 거북이 230 안녕 거북이 231 안녕 거북이 232 안녕 거북이 233 안녕 거북이 234 안녕 거북이 235 안녕 거북이 236 안녕 거북이 237 안녕 거북이 238 안녕 거북이 239 안녕 거북이 240 안녕 거북이 241 안녕 거북이 242 안녕 거북이 243 안녕 거북이 244 안녕 거북이 245 안녕 거북이 246 안녕 거북이 247 안녕 거북이 248 안녕 거북이 249 안녕 거북이 250 안녕 거북이 251 안녕 거북이 252 안녕 거북이 253 안녕 거북이 254 안녕 거북이 255 안녕 거북이 256 안녕 거북이 257 안녕 거북이 258 안녕 거북이 259 안녕 거북이 260 안녕 거북이 261 안녕 거북이 262 안녕 거북이 263 안녕 거북이 264 안녕 거북이 265 안녕 거북이 266 안녕 거북이 267 안녕 거북이 268 안녕 거북이 269 안녕 거북이 270 안녕 거북이 271 안녕 거북이 272 안녕 거북이 273 안녕 거북이 274 안녕 거북이 275 안녕 거북이 276 안녕 거북이 277 안녕 거북이 278 안녕 거북이 279 안녕 거북이 280 안녕 거북이 281 안녕 거북이 282 안녕 거북이 283 안녕 거북이 284 안녕 거북이 285 안녕 거북이 286 안녕 거북이 287 안녕 거북이 288 안녕 거북이 289 안녕 거북이 290 안녕 거북이 291 안녕 거북이 292 안녕 거북이 293 안녕 거북이 294 안녕 거북이 295 안녕 거북이 296 안녕 거북이 297 안녕 거북이 298 안녕 거북이 299 안녕 거북이 300 안녕 거북이 301 안녕 거북이 302 안녕 거북이 303 안녕 거북이 304 안녕 거북이 305 안녕 거북이 306 안녕 거북이 307 안녕 거북이 308 안녕 거북이 309 안녕 거북이 310 안녕 거북이 311 안녕 거북이 312 안녕 거북이 313 안녕 거북이 314 안녕 거북이 315 안녕 거북이 316 안녕 거북이 317 안녕 거북이 318 안녕 거북이 319 안녕 거북이 320 안녕 거북이 321 안녕 거북이 322 안녕 거북이 323 안녕 거북이 324 안녕 거북이 325 안녕 거북이 326 안녕 거북이 327 안녕 거북이 328 안녕 거북이 329 안녕 거북이 330 안녕 거북이 331 안녕 거북이 332 안녕 거북이 333 안녕 거북이 334 안녕 거북이 335 안녕 거북이 336 안녕 거북이 337 안녕 거북이 338 안녕 거북이 339 안녕 거북이 340 안녕 거북이 341 안녕 거북이 342 안녕 거북이 343 안녕 거북이 344 안녕 거북이 345 안녕 거북이 346 안녕 거북이 347 안녕 거북이 348 안녕 거북이 349 안녕 거북이 350 안녕 거북이 351 안녕 거북이 352 안녕 거북이 353 안녕 거북이 354 안녕 거북이 355 안녕 거북이 356 안녕 거북이 357 안녕 거북이 358 안녕 거북이 359 안녕 거북이 360 안녕 거북이 361 안녕 거북이 362 안녕 거북이 363 안녕 거북이 364 안녕 거북이 365 안녕 거북이 366 안녕 거북이 367 안녕 거북이 368 안녕 거북이 369 안녕 거북이 370 안녕 거북이 371 안녕 거북이 372 안녕 거북이 373 안녕 거북이 374 안녕 거북이 375 안녕 거북이 376 안녕 거북이 377 안녕 거북이 378 안녕 거북이 379 안녕 거북이 380 안녕 거북이 381 안녕 거북이 382 안녕 거북이 383 안녕 거북이 384 안녕 거북이 385 안녕 거북이 386 안녕 거북이 387 안녕 거북이 388 안녕 거북이 389 안녕 거북이 390 안녕 거북이 391 안녕 거북이 392 안녕 거북이 393 안녕 거북이 394 안녕 거북이 395 안녕 거북이 396 안녕 거북이 397 안녕 거북이 398 안녕 거북이 399 안녕 거북이 400 안녕 거북이 401 안녕 거북이 402 안녕 거북이 403 안녕 거북이 404 안녕 거북이 405 안녕 거북이 406 안녕 거북이 407 안녕 거북이 408 안녕 거북이 409 안녕 거북이 410 안녕 거북이 411 안녕 거북이 412 안녕 거북이 413 안녕 거북이 414 안녕 거북이 415 안녕 거북이 416 안녕 거북이 417 안녕 거북이 418 안녕 거북이 419 안녕 거북이 420 안녕 거북이 421 안녕 거북이 422 안녕 거북이 423 안녕 거북이 424 안녕 거북이 425 안녕 거북이 426 안녕 거북이 427 안녕 거북이 428 안녕 거북이 429 안녕 거북이 430 안녕 거북이 431 안녕 거북이 432 안녕 거북이 433 안녕 거북이 434 안녕 거북이 435 안녕 거북이 436 안녕 거북이 437 안녕 거북이 438 안녕 거북이 439 안녕 거북이 440 안녕 거북이 441 안녕 거북이 442 안녕 거북이 443 안녕 거북이 444 안녕 거북이 445 안녕 거북이 446 안녕 거북이 447 안녕 거북이 448 안녕 거북이 449 안녕 거북이 450 안녕 거북이 451 안녕 거북이 452 안녕 거북이 453 안녕 거북이 454 안녕 거북이 455 안녕 거북이 456 안녕 거북이 457 안녕 거북이 458 안녕 거북이 459 안녕 거북이 460 안녕 거북이 461 안녕 거북이 462 안녕 거북이 463 안녕 거북이 464 안녕 거북이 465 안녕 거북이 466 안녕 거북이 467 안녕 거북이 468 안녕 거북이 469 안녕 거북이 470 안녕 거북이 471 안녕 거북이 472 안녕 거북이 473 안녕 거북이 474 안녕 거북이 475 안녕 거북이 476 안녕 거북이 477 안녕 거북이 478 안녕 거북이 479 안녕 거북이 480 안녕 거북이 481 안녕 거북이 482 안녕 거북이 483 안녕 거북이 484 안녕 거북이 485 안녕 거북이 486 안녕 거북이 487 안녕 거북이 488 안녕 거북이 489 안녕 거북이 490 안녕 거북이 491 안녕 거북이 492 안녕 거북이 493 안녕 거북이 494 안녕 거북이 495 안녕 거북이 496 안녕 거북이 497 안녕 거북이 498 안녕 거북이 499 안녕 거북이 500 안녕 거북이 501 안녕 거북이 502 안녕 거북이 503 안녕 거북이 504 안녕 거북이 505 안녕 거북이 506 안녕 거북이 507 안녕 거북이 508 안녕 거북이 509 안녕 거북이 510 안녕 거북이 511 안녕 거북이 512 안녕 거북이 513 안녕 거북이 514 안녕 거북이 515 안녕 거북이 516 안녕 거북이 517 안녕 거북이 518 안녕 거북이 519 안녕 거북이 520 안녕 거북이 521 안녕 거북이 522 안녕 거북이 523 안녕 거북이 524 안녕 거북이 525 안녕 거북이 526 안녕 거북이 527 안녕 거북이 528 안녕 거북이 529 안녕 거북이 530 안녕 거북이 531 안녕 거북이 532 안녕 거북이 533 안녕 거북이 534 안녕 거북이 535 안녕 거북이 536 안녕 거북이 537 안녕 거북이 538 안녕 거북이 539 안녕 거북이 540 안녕 거북이 541 안녕 거북이 542 안녕 거북이 543 안녕 거북이 544 안녕 거북이 545 안녕 거북이 546 안녕 거북이 547 안녕 거북이 548 안녕 거북이 549 안녕 거북이 550 안녕 거북이 551 안녕 거북이 552 안녕 거북이 553 안녕 거북이 554 안녕 거북이 555 안녕 거북이 556 안녕 거북이 557 안녕 거북이 558 안녕 거북이 559 안녕 거북이 560 안녕 거북이 561 안녕 거북이 562 안녕 거북이 563 안녕 거북이 564 안녕 거북이 565 안녕 거북이 566 안녕 거북이 567 안녕 거북이 568 안녕 거북이 569 안녕 거북이 570 안녕 거북이 571 안녕 거북이 572 안녕 거북이 573 안녕 거북이 574 안녕 거북이 575 안녕 거북이 576 안녕 거북이 577 안녕 거북이 578 안녕 거북이 579 안녕 거북이 580 안녕 거북이 581 안녕 거북이 582 안녕 거북이 583 안녕 거북이 584 안녕 거북이 585 안녕 거북이 586 안녕 거북이 587 안녕 거북이 588 안녕 거북이 589 안녕 거북이 590 안녕 거북이 591 안녕 거북이 592 안녕 거북이 593 안녕 거북이 594 안녕 거북이 595 안녕 거북이 596 안녕 거북이 597 안녕 거북이 598 안녕 거북이 599 안녕 거북이 600 안녕 거북이 601 안녕 거북이 602 안녕 거북이 603 안녕 거북이 604 안녕 거북이 605 안녕 거북이 606 안녕 거북이 607 안녕 거북이 608 안녕 거북이 609 안녕 거북이 610 안녕 거북이 611 안녕 거북이 612 안녕 거북이 613 안녕 거북이 614 안녕 거북이 615 안녕 거북이 616 안녕 거북이 617 안녕 거북이 618 안녕 거북이 619 안녕 거북이 620 안녕 거북이 621 안녕 거북이 622 안녕 거북이 623 안녕 거북이 624 안녕 거북이 625 안녕 거북이 626 안녕 거북이 627 안녕 거북이 628 안녕 거북이 629 안녕 거북이 630 안녕 거북이 631 안녕 거북이 632 안녕 거북이 633 안녕 거북이 634 안녕 거북이 635 안녕 거북이 636 안녕 거북이 637 안녕 거북이 638 안녕 거북이 639 안녕 거북이 640 안녕 거북이 641 안녕 거북이 642 안녕 거북이 643 안녕 거북이 644 안녕 거북이 645 안녕 거북이 646 안녕 거북이 647 안녕 거북이 648 안녕 거북이 649 안녕 거북이 650 안녕 거북이 651 안녕 거북이 652 안녕 거북이 653 안녕 거북이 654 안녕 거북이 655 안녕 거북이 656 안녕 거북이 657 안녕 거북이 658 안녕 거북이 659 안녕 거북이 660 안녕 거북이 661 안녕 거북이 662 안녕 거북이 663 안녕 거북이 664 안녕 거북이 665 안녕 거북이 666 안녕 거북이 667 안녕 거북이 668 안녕 거북이 669 안녕 거북이 670 안녕 거북이 671 안녕 거북이 672 안녕 거북이 673 안녕 거북이 674 안녕 거북이 675 안녕 거북이 676 안녕 거북이 677 안녕 거북이 678 안녕 거북이 679 안녕 거북이 680 안녕 거북이 681 안녕 거북이 682 안녕 거북이 683 안녕 거북이 684 안녕 거북이 685 안녕 거북이 686 안녕 거북이 687 안녕 거북이 688 안녕 거북이 689 안녕 거북이 690 안녕 거북이 691 안녕 거북이 692 안녕 거북이 693 안녕 거북이 694 안녕 거북이 695 안녕 거북이 696 안녕 거북이 697 안녕 거북이 698 안녕 거북이 699 안녕 거북이 700 안녕 거북이 701 안녕 거북이 702 안녕 거북이 703 안녕 거북이 704 안녕 거북이 705 안녕 거북이 706 안녕 거북이 707 안녕 거북이 708 안녕 거북이 709 안녕 거북이 710 안녕 거북이 711 안녕 거북이 712 안녕 거북이 713 안녕 거북이 714 안녕 거북이 715 안녕 거북이 716 안녕 거북이 717 안녕 거북이 718 안녕 거북이 719 안녕 거북이 720 안녕 거북이 721 안녕 거북이 722 안녕 거북이 723 안녕 거북이 724 안녕 거북이 725 안녕 거북이 726 안녕 거북이 727 안녕 거북이 728 안녕 거북이 729 안녕 거북이 730 안녕 거북이 731 안녕 거북이 732 안녕 거북이 733 안녕 거북이 734 안녕 거북이 735 안녕 거북이 736 안녕 거북이 737 안녕 거북이 738 안녕 거북이 739 안녕 거북이 740 안녕 거북이 741 안녕 거북이 742 안녕 거북이 743 안녕 거북이 744 안녕 거북이 745 안녕 거북이 746 안녕 거북이 747 안녕 거북이 748 안녕 거북이 749 안녕 거북이 750 안녕 거북이 751 안녕 거북이 752 안녕 거북이 753 안녕 거북이 754 안녕 거북이 755 안녕 거북이 756 안녕 거북이 757 안녕 거북이 758 안녕 거북이 759 안녕 거북이 760 안녕 거북이 761 안녕 거북이 762 안녕 거북이 763 안녕 거북이 764 안녕 거북이 765 안녕 거북이 766 안녕 거북이 767 안녕 거북이 768 안녕 거북이 769 안녕 거북이 770 안녕 거북이 771 안녕 거북이 772 안녕 거북이 773 안녕 거북이 774 안녕 거북이 775 안녕 거북이 776 안녕 거북이 777 안녕 거북이 778 안녕 거북이 779 안녕 거북이 780 안녕 거북이 781 안녕 거북이 782 안녕 거북이 783 안녕 거북이 784 안녕 거북이 785 안녕 거북이 786 안녕 거북이 787 안녕 거북이 788 안녕 거북이 789 안녕 거북이 790 안녕 거북이 791 안녕 거북이 792 안녕 거북이 793 안녕 거북이 794 안녕 거북이 795 안녕 거북이 796 안녕 거북이 797 안녕 거북이 798 안녕 거북이 799 안녕 거북이 800 안녕 거북이 801 안녕 거북이 802 안녕 거북이 803 안녕 거북이 804 안녕 거북이 805 안녕 거북이 806 안녕 거북이 807 안녕 거북이 808 안녕 거북이 809 안녕 거북이 810 안녕 거북이 811 안녕 거북이 812 안녕 거북이 813 안녕 거북이 814 안녕 거북이 815 안녕 거북이 816 안녕 거북이 817 안녕 거북이 818 안녕 거북이 819 안녕 거북이 820 안녕 거북이 821 안녕 거북이 822 안녕 거북이 823 안녕 거북이 824 안녕 거북이 825 안녕 거북이 826 안녕 거북이 827 안녕 거북이 828 안녕 거북이 829 안녕 거북이 830 안녕 거북이 831 안녕 거북이 832 안녕 거북이 833 안녕 거북이 834 안녕 거북이 835 안녕 거북이 836 안녕 거북이 837 안녕 거북이 838 안녕 거북이 839 안녕 거북이 840 안녕 거북이 841 안녕 거북이 842 안녕 거북이 843 안녕 거북이 844 안녕 거북이 845 안녕 거북이 846 안녕 거북이 847 안녕 거북이 848 안녕 거북이 849 안녕 거북이 850 안녕 거북이 851 안녕 거북이 852 안녕 거북이 853 안녕 거북이 854 안녕 거북이 855 안녕 거북이 856 안녕 거북이 857 안녕 거북이 858 안녕 거북이 859 안녕 거북이 860 안녕 거북이 861 안녕 거북이 862 안녕 거북이 863 안녕 거북이 864 안녕 거북이 865 안녕 거북이 866 안녕 거북이 867 안녕 거북이 868 안녕 거북이 869 안녕 거북이 870 안녕 거북이 871 안녕 거북이 872 안녕 거북이 873 안녕 거북이 874 안녕 거북이 875 안녕 거북이 876 안녕 거북이 877 안녕 거북이 878 안녕 거북이 879 안녕 거북이 880 안녕 거북이 881 안녕 거북이 882 안녕 거북이 883 안녕 거북이 884 안녕 거북이 885 안녕 거북이 886 안녕 거북이 887 안녕 거북이 888 안녕 거북이 889 안녕 거북이 890 안녕 거북이 891 안녕 거북이 892 안녕 거북이 893 안녕 거북이 894 안녕 거북이 895 안녕 거북이 896 안녕 거북이 897 안녕 거북이 898 안녕 거북이 899 안녕 거북이 900 안녕 거북이 901 안녕 거북이 902 안녕 거북이 903 안녕 거북이 904 안녕 거북이 905 안녕 거북이 906 안녕 거북이 907 안녕 거북이 908 안녕 거북이 909 안녕 거북이 910 안녕 거북이 911 안녕 거북이 912 안녕 거북이 913 안녕 거북이 914 안녕 거북이 915 안녕 거북이 916 안녕 거북이 917 안녕 거북이 918 안녕 거북이 919 안녕 거북이 920 안녕 거북이 921 안녕 거북이 922 안녕 거북이 923 안녕 거북이 924 안녕 거북이 925 안녕 거북이 926 안녕 거북이 927 안녕 거북이 928 안녕 거북이 929 안녕 거북이 930 안녕 거북이 931 안녕 거북이 932 안녕 거북이 933 안녕 거북이 934 안녕 거북이 935 안녕 거북이 936 안녕 거북이 937 안녕 거북이 938 안녕 거북이 939 안녕 거북이 940 안녕 거북이 941 안녕 거북이 942 안녕 거북이 943 안녕 거북이 944 안녕 거북이 945 안녕 거북이 946 안녕 거북이 947 안녕 거북이 948 안녕 거북이 949 안녕 거북이 950 안녕 거북이 951 안녕 거북이 952 안녕 거북이 953 안녕 거북이 954 안녕 거북이 955 안녕 거북이 956 안녕 거북이 957 안녕 거북이 958 안녕 거북이 959 안녕 거북이 960 안녕 거북이 961 안녕 거북이 962 안녕 거북이 963 안녕 거북이 964 안녕 거북이 965 안녕 거북이 966 안녕 거북이 967 안녕 거북이 968 안녕 거북이 969 안녕 거북이 970 안녕 거북이 971 안녕 거북이 972 안녕 거북이 973 안녕 거북이 974 안녕 거북이 975 안녕 거북이 976 안녕 거북이 977 안녕 거북이 978 안녕 거북이 979 안녕 거북이 980 안녕 거북이 981 안녕 거북이 982 안녕 거북이 983 안녕 거북이 984 안녕 거북이 985 안녕 거북이 986 안녕 거북이 987 안녕 거북이 988 안녕 거북이 989 안녕 거북이 990 안녕 거북이 991 안녕 거북이 992 안녕 거북이 993 안녕 거북이 994 안녕 거북이 995 안녕 거북이 996 안녕 거북이 997 안녕 거북이 998 안녕 거북이 999 안녕 거북이 1000 안녕 거북이 1001 안녕 거북이 1002 안녕 거북이 1003 안녕 거북이 1004 안녕 거북이 1005 안녕 거북이 1006 안녕 거북이 1007 안녕 거북이 1008 안녕 거북이 1009 안녕 거북이 1010 안녕 거북이 1011 안녕 거북이 1012 안녕 거북이 1013 안녕 거북이 1014 안녕 거북이 1015 안녕 거북이 1016 안녕 거북이 1017 안녕 거북이 1018 안녕 거북이 1019 안녕 거북이 1020 안녕 거북이 1021 안녕 거북이 1022 안녕 거북이 1023 안녕 거북이 1024 안녕 거북이 1025 안녕 거북이 1026 안녕 거북이 1027 안녕 거북이 1028 안녕 거북이 1029 안녕 거북이 1030 안녕 거북이 1031 안녕 거북이 1032 안녕 거북이 1033 안녕 거북이 1034 안녕 거북이 1035 안녕 거북이 1036 안녕 거북이 1037 안녕 거북이 1038 안녕 거북이 1039 안녕 거북이 1040 안녕 거북이 1041 안녕 거북이 1042 안녕 거북이 1043 안녕 거북이 1044 안녕 거북이 1045 안녕 거북이 1046 안녕 거북이 1047 안녕 거북이 1048 안녕 거북이 1049 안녕 거북이 1050 안녕 거북이 1051 안녕 거북이 1052 안녕 거북이 1053 안녕 거북이 1054 안녕 거북이 1055 안녕 거북이 1056 안녕 거북이 1057 안녕 거북이 1058 안녕 거북이 1059 안녕 거북이 1060 안녕 거북이 1061 안녕 거북이 1062 안녕 거북이 1063 안녕 거북이 1064 안녕 거북이 1065 안녕 거북이 1066 안녕 거북이 1067 안녕 거북이 1068 안녕 거북이 1069 안녕 거북이 1070 안녕 거북이 1071 안녕 거북이 1072 안녕 거북이 1073 안녕 거북이 1074 안녕 거북이 1075 안녕 거북이 1076 안녕 거북이 1077 안녕 거북이 1078 안녕 거북이 1079 안녕 거북이 1080 안녕 거북이 1081 안녕 거북이 1082 안녕 거북이 1083 안녕 거북이 1084 안녕 거북이 1085 안녕 거북이 1086 안녕 거북이 1087 안녕 거북이 1088 안녕 거북이 1089 안녕 거북이 1090 안녕 거북이 1091 안녕 거북이 1092 안녕 거북이 1093 안녕 거북이 1094 안녕 거북이 1095 안녕 거북이 1096 안녕 거북이 1097 안녕 거북이 1098 안녕 거북이 1099 안녕 거북이 1100 안녕 거북이 1101 안녕 거북이 1102 안녕 거북이 1103 안녕 거북이 1104 안녕 거북이 1105 안녕 거북이 1106 안녕 거북이 1107 안녕 거북이 1108 안녕 거북이 1109 안녕 거북이 1110 안녕 거북이 1111 안녕 거북이 1112 안녕 거북이 1113 안녕 거북이 1114 안녕 거북이 1115 안녕 거북이 1116 안녕 거북이 1117 안녕 거북이 1118 안녕 거북이 1119 안녕 거북이 1120 안녕 거북이 1121 안녕 거북이 1122 안녕 거북이 1123 안녕 거북이 1124 안녕 거북이 1125 안녕 거북이 1126 안녕 거북이 1127 안녕 거북이 1128 안녕 거북이 1129 안녕 거북이 1130 안녕 거북이 1131 안녕 거북이 1132 안녕 거북이 1133 안녕 거북이 1134 안녕 거북이 1135 안녕 거북이 1136 안녕 거북이 1137 안녕 거북이 1138 안녕 거북이 1139 안녕 거북이 1140 안녕 거북이 1141 안녕 거북이 1142 안녕 거북이 1143 안녕 거북이 1144 안녕 거북이 1145 안녕 거북이 1146 안녕 거북이 1147 안녕 거북이 1148 안녕 거북이 1149 안녕 거북이 1150 안녕 거북이 1151 안녕 거북이 1152 안녕 거북이 1153 안녕 거북이 1154 안녕 거북이 1155 안녕 거북이 1156 안녕 거북이 1157 안녕 거북이 1158 안녕 거북이 1159 안녕 거북이 1160 안녕 거북이 1161 안녕 거북이 1162 안녕 거북이 1163 안녕 거북이 1164 안녕 거북이 1165 안녕 거북이 1166 안녕 거북이 1167 안녕 거북이 1168 안녕 거북이 1169 안녕 거북이 1170 안녕 거북이 1171 안녕 거북이 1172 안녕 거북이 1173 안녕 거북이 1174 안녕 거북이 1175 안녕 거북이 1176 안녕 거북이 1177 안녕 거북이 1178 안녕 거북이 1179 안녕 거북이 1180 안녕 거북이 1181 안녕 거북이 1182 안녕 거북이 1183 안녕 거북이 1184 안녕 거북이 1185 안녕 거북이 1186 안녕 거북이 1187 안녕 거북이 1188 안녕 거북이 1189 안녕 거북이 1190 안녕 거북이 1191 안녕 거북이 1192 안녕 거북이 1193 안녕 거북이 1194 안녕 거북이 1195 안녕 거북이 1196 안녕 거북이 1197 안녕 거북이 1198 안녕 거북이 1199 안녕 거북이 1200 안녕 거북이 1201 안녕 거북이 1202 안녕 거북이 1203 안녕 거북이 1204 안녕 거북이 1205 안녕 거북이 1206 안녕 거북이 1207 안녕 거북이 1208 안녕 거북이 1209 안녕 거북이 1210 안녕 거북이 1211 안녕 거북이 1212 안녕 거북이 1213 안녕 거북이 1214 안녕 거북이 1215 안녕 거북이 1216 안녕 거북이 1217 안녕 거북이 1218 안녕 거북이 1219 안녕 거북이 1220 안녕 거북이 1221 안녕 거북이 1222 안녕 거북이 1223 안녕 거북이 1224 안녕 거북이 1225 안녕 거북이 1226 안녕 거북이 1227 안녕 거북이 1228 안녕 거북이 1229 안녕 거북이 1230 안녕 거북이 1231 안녕 거북이 1232 안녕 거북이 1233 안녕 거북이 1234 안녕 거북이 1235 안녕 거북이 1236 안녕 거북이 1237 안녕 거북이 1238 안녕 거북이 1239 안녕 거북이 1240 안녕 거북이 1241 안녕 거북이 1242 안녕 거북이 1243 안녕 거북이 1244 안녕 거북이 1245 안녕 거북이 1246 안녕 거북이 1247 안녕 거북이 1248 안녕 거북이 1249 안녕 거북이 1250 안녕 거북이 1251 안녕 거북이 1252 안녕 거북이 1253 안녕 거북이 1254 안녕 거북이 1255 안녕 거북이 1256 안녕 거북이 1257 안녕 거북이 1258 안녕 거북이 1259 안녕 거북이 1260 안녕 거북이 1261 안녕 거북이 1262 안녕 거북이 1263 안녕 거북이 1264 안녕 거북이 1265 안녕 거북이 1266 안녕 거북이 1267 안녕 거북이 1268 안녕 거북이 1269 안녕 거북이 1270 안녕 거북이 1271 안녕 거북이 1272 안녕 거북이 1273 안녕 거북이 1274 안녕 거북이 1275 안녕 거북이 1276 안녕 거북이 1277 안녕 거북이 1278 안녕 거북이 1279 안녕 거북이 1280 안녕 거북이 1281 안녕 거북이 1282 안녕 거북이 1283 안녕 거북이 1284 안녕 거북이 1285 안녕 거북이 1286 안녕 거북이 1287 안녕 거북이 1288 안녕 거북이 1289 안녕 거북이 1290 안녕 거북이 1291 안녕 거북이 1292 안녕 거북이 1293 안녕 거북이 1294 안녕 거북이 1295 안녕 거북이 1296 안녕 거북이 1297 안녕 거북이 1298 안녕 거북이 1299 안녕 거북이 1300 안녕 거북이 1301 안녕 거북이 1302 안녕 거북이 1303 안녕 거북이 1304 안녕 거북이 1305 안녕 거북이 1306 안녕 거북이 1307 안녕 거북이 1308 안녕 거북이 1309 안녕 거북이 1310 안녕 거북이 1311 안녕 거북이 1312 안녕 거북이 1313 안녕 거북이 1314 안녕 거북이 1315 안녕 거북이 1316 안녕 거북이 1317 안녕 거북이 1318 안녕 거북이 1319 안녕 거북이 1320 안녕 거북이 1321 안녕 거북이 1322 안녕 거북이 1323 안녕 거북이 1324 안녕 거북이 1325 안녕 거북이 1326 안녕 거북이 1327 안녕 거북이 1328 안녕 거북이 1329 안녕 거북이 1330 안녕 거북이 1331 안녕 거북이 1332 안녕 거북이 1333 안녕 거북이 1334 안녕 거북이 1335 안녕 거북이 1336 안녕 거북이 1337 안녕 거북이 1338 안녕 거북이 1339 안녕 거북이 1340 안녕 거북이 1341 안녕 거북이 1342 안녕 거북이 1343 안녕 거북이 1344 안녕 거북이 1345 안녕 거북이 1346 안녕 거북이 1347 안녕 거북이 1348 안녕 거북이 1349 안녕 거북이 1350 안녕 거북이 1351 안녕 거북이 1352 안녕 거북이 1353 안녕 거북이 1354 안녕 거북이 1355 안녕 거북이 1356 안녕 거북이 1357 안녕 거북이 1358 안녕 거북이 1359 안녕 거북이 1360 안녕 거북이 1361 안녕 거북이 1362 안녕 거북이 1363 안녕 거북이 1364 안녕 거북이 1365 안녕 거북이 1366 안녕 거북이 1367 안녕 거북이 1368 안녕 거북이 1369 안녕 거북이 1370 안녕 거북이 1371 안녕 거북이 1372 안녕 거북이 1373 안녕 거북이 1374 안녕 거북이 1375 안녕 거북이 1376 안녕 거북이 1377 안녕 거북이 1378 안녕 거북이 1379 안녕 거북이 1380 안녕 거북이 1381 안녕 거북이 1382 안녕 거북이 1383 안녕 거북이 1384 안녕 거북이 1385 안녕 거북이 1386 안녕 거북이 1387 안녕 거북이 1388 안녕 거북이 1389 안녕 거북이 1390 안녕 거북이 1391 안녕 거북이 1392 안녕 거북이 1393 안녕 거북이 1394 안녕 거북이 1395 안녕 거북이 1396 안녕 거북이 1397 안녕 거북이 1398 안녕 거북이 1399 안녕 거북이 1400 안녕 거북이 1401 안녕 거북이 1402 안녕 거북이 1403 안녕 거북이 1404 안녕 거북이 1405 안녕 거북이 1406 안녕 거북이 1407 안녕 거북이 1408 안녕 거북이 1409 안녕 거북이 1410 안녕 거북이 1411 안녕 거북이 1412 안녕 거북이 1413 안녕 거북이 1414 안녕 거북이 1415 안녕 거북이 1416 안녕 거북이 1417 안녕 거북이 1418 안녕 거북이 1419 안녕 거북이 1420 안녕 거북이 1421 안녕 거북이 1422 안녕 거북이 1423 안녕 거북이 1424 안녕 거북이 1425 안녕 거북이 1426 안녕 거북이 1427 안녕 거북이 1428 안녕 거북이 1429 안녕 거북이 1430 안녕 거북이 1431 안녕 거북이 1432 안녕 거북이 1433 안녕 거북이 1434 안녕 거북이 1435 안녕 거북이 1436 안녕 거북이 1437 안녕 거북이 1438 안녕 거북이 1439 안녕 거북이 1440 안녕 거북이 1441 안녕 거북이 1442 안녕 거북이 1443 안녕 거북이 1444 안녕 거북이 1445 안녕 거북이 1446 안녕 거북이 1447 안녕 거북이 1448 안녕 거북이 1449 안녕 거북이 1450 안녕 거북이 1451 안녕 거북이 1452 안녕 거북이 1453 안녕 거북이 1454 안녕 거북이 1455 안녕 거북이 1456 안녕 거북이 1457 안녕 거북이 1458 안녕 거북이 1459 안녕 거북이 1460 안녕 거북이 1461 안녕 거북이 1462 안녕 거북이 1463 안녕 거북이 1464 안녕 거북이 1465 안녕 거북이 1466 안녕 거북이 1467 안녕 거북이 1468 안녕 거북이 1469 안녕 거북이 1470 안녕 거북이 1471 안녕 거북이 1472 안녕 거북이 1473 안녕 거북이 1474 안녕 거북이 1475 안녕 거북이 1476 안녕 거북이 1477 안녕 거북이 1478 안녕 거북이 1479 안녕 거북이 1480 안녕 거북이 1481 안녕 거북이 1482 안녕 거북이 1483 안녕 거북이 1484 안녕 거북이 1485 안녕 거북이 1486 안녕 거북이 1487 안녕 거북이 1488 안녕 거북이 1489 안녕 거북이 1490 안녕 거북이 1491 안녕 거북이 1492 안녕 거북이 1493 안녕 거북이 1494 안녕 거북이 1495 안녕 거북이 1496 안녕 거북이 1497 안녕 거북이 1498 안녕 거북이 1499 안녕 거북이 1500 안녕 거북이 1501 안녕 거북이 1502 안녕 거북이 1503 안녕 거북이 1504 안녕 거북이 1505 안녕 거북이 1506 안녕 거북이 1507 안녕 거북이 1508 안녕 거북이 1509 안녕 거북이 1510 안녕 거북이 1511 안녕 거북이 1512 안녕 거북이 1513 안녕 거북이 1514 안녕 거북이 1515 안녕 거북이 1516 안녕 거북이 1517 안녕 거북이 1518 안녕 거북이 1519 안녕 거북이 1520 안녕 거북이 1521 안녕 거북이 1522 안녕 거북이 1523 안녕 거북이 1524 안녕 거북이 1525 안녕 거북이 1526 안녕 거북이 1527 안녕 거북이 1528 안녕 거북이 1529 안녕 거북이 1530 안녕 거북이 1531 안녕 거북이 1532 안녕 거북이 1533 안녕 거북이 1534 안녕 거북이 1535 안녕 거북이 1536 안녕 거북이 1537 안녕 거북이 1538 안녕 거북이 1539 안녕 거북이 1540 안녕 거북이 1541 안녕 거북이 1542 안녕 거북이 1543 안녕 거북이 1544 안녕 거북이 1545 안녕 거북이 1546 안녕 거북이 1547 안녕 거북이 1548 안녕 거북이 1549 안녕 거북이 1550 안녕 거북이 1551 안녕 거북이 1552 안녕 거북이 1553 안녕 거북이 1554 안녕 거북이 1555 안녕 거북이 1556 안녕 거북이 1557 안녕 거북이 1558 안녕 거북이 1559 안녕 거북이 1560 안녕 거북이 1561 안녕 거북이 1562 안녕 거북이 1563 안녕 거북이 1564 안녕 거북이 1565 안녕 거북이 1566 안녕 거북이 1567 안녕 거북이 1568 안녕 거북이 1569 안녕 거북이 1570 안녕 거북이 1571 안녕 거북이 1572 안녕 거북이 1573 안녕 거북이 1574 안녕 거북이 1575 안녕 거북이 1576 안녕 거북이 1577 안녕 거북이 1578 안녕 거북이 1579 안녕 거북이 1580 안녕 거북이 1581 안녕 거북이 1582 안녕 거북이 1583 안녕 거북이 1584 안녕 거북이 1585 안녕 거북이 1586 안녕 거북이 1587 안녕 거북이 1588 안녕 거북이 1589 안녕 거북이 1590 안녕 거북이 1591 안녕 거북이 1592 안녕 거북이 1593 안녕 거북이 1594 안녕 거북이 1595 안녕 거북이 1596 안녕 거북이 1597 안녕 거북이 1598 안녕 거북이 1599 안녕 거북이 1600 안녕 거북이 1601 안녕 거북이 1602 안녕 거북이 1603 안녕 거북이 1604 안녕 거북이 1605 안녕 거북이 1606 안녕 거북이 1607 안녕 거북이 1608 안녕 거북이 1609 안녕 거북이 1610 안녕 거북이 1611 안녕 거북이 1612 안녕 거북이 1613 안녕 거북이 1614 안녕 거북이 1615 안녕 거북이 1616 안녕 거북이 1617 안녕 거북이 1618 안녕 거북이 1619 안녕 거북이 1620 안녕 거북이 1621 안녕 거북이 1622 안녕 거북이 1623 안녕 거북이 1624 안녕 거북이 1625 안녕 거북이 1626 안녕 거북이 1627 안녕 거북이 1628 안녕 거북이 1629 안녕 거북이 1630 안녕 거북이 1631 안녕 거북이 1632 안녕 거북이 1633 안녕 거북이 1634 안녕 거북이 1635 안녕 거북이 1636 안녕 거북이 1637 안녕 거북이 1638 안녕 거북이 1639 안녕 거북이 1640 안녕 거북이 1641 안녕 거북이 1642 안녕 거북이 1643 안녕 거북이 1644 안녕 거북이 1645 안녕 거북이 1646 안녕 거북이 1647 안녕 거북이 1648 안녕 거북이 1649 안녕 거북이 1650 안녕 거북이 1651 안녕 거북이 1652 안녕 거북이 1653 안녕 거북이 1654 안녕 거북이 1655 안녕 거북이 1656 안녕 거북이 1657 안녕 거북이 1658 안녕 거북이 1659 안녕 거북이 1660 안녕 거북이 1661 안녕 거북이 1662 안녕 거북이 1663 안녕 거북이 1664 안녕 거북이 1665 안녕 거북이 1666 안녕 거북이 1667 안녕 거북이 1668 안녕 거북이 1669 안녕 거북이 1670 안녕 거북이 1671 안녕 거북이 1672 안녕 거북이 1673 안녕 거북이 1674 안녕 거북이 1675 안녕 거북이 1676 안녕 거북이 1677 안녕 거북이 1678 안녕 거북이 1679 안녕 거북이 1680 안녕 거북이 1681 안녕 거북이 1682 안녕 거북이 1683 안녕 거북이 1684 안녕 거북이 1685 안녕 거북이 1686 안녕 거북이 1687 안녕 거북이 1688 안녕 거북이 1689 안녕 거북이 1690 안녕 거북이 1691 안녕 거북이 1692 안녕 거북이 1693 안녕 거북이 1694 안녕 거북이 1695 안녕 거북이 1696 안녕 거북이 1697 안녕 거북이 1698 안녕 거북이 1699 안녕 거북이 1700 안녕 거북이 1701 안녕 거북이 1702 안녕 거북이 1703 안녕 거북이 1704 안녕 거북이 1705 안녕 거북이 1706 안녕 거북이 1707 안녕 거북이 1708 안녕 거북이 1709 안녕 거북이 1710 안녕 거북이 1711 안녕 거북이 1712 안녕 거북이 1713 안녕 거북이 1714 안녕 거북이 1715 안녕 거북이 1716 안녕 거북이 1717 안녕 거북이 1718 안녕 거북이 1719 안녕 거북이 1720 안녕 거북이 1721 안녕 거북이 1722 안녕 거북이 1723 안녕 거북이 1724 안녕 거북이 1725 안녕 거북이 1726 안녕 거북이 1727 안녕 거북이 1728 안녕 거북이 1729 안녕 거북이 1730 안녕 거북이 1731 안녕 거북이 1732 안녕 거북이 1733 안녕 거북이 1734 안녕 거북이 1735 안녕 거북이 1736 안녕 거북이 1737 안녕 거북이 1738 안녕 거북이 1739 안녕 거북이 1740 안녕 거북이 1741 안녕 거북이 1742 안녕 거북이 1743 안녕 거북이 1744 안녕 거북이 1745 안녕 거북이 1746 안녕 거북이 1747 안녕 거북이 1748 안녕 거북이 1749 안녕 거북이 1750 안녕 거북이 1751 안녕 거북이 1752 안녕 거북이 1753 안녕 거북이 1754 안녕 거북이 1755 안녕 거북이 1756 안녕 거북이 1757 안녕 거북이 1758 안녕 거북이 1759 안녕 거북이 1760 안녕 거북이 1761 안녕 거북이 1762 안녕 거북이 1763 안녕 거북이 1764 안녕 거북이 1765 안녕 거북이 1766 안녕 거북이 1767 안녕 거북이 1768 안녕 거북이 1769 안녕 거북이 1770 안녕 거북이 1771 안녕 거북이 1772 안녕 거북이 1773 안녕 거북이 1774 안녕 거북이 1775 안녕 거북이 1776 안녕 거북이 1777 안녕 거북이 1778 안녕 거북이 1779 안녕 거북이 1780 안녕 거북이 1781 안녕 거북이 1782 안녕 거북이 1783 안녕 거북이 1784 안녕 거북이 1785 안녕 거북이 1786 안녕 거북이 1787 안녕 거북이 1788 안녕 거북이 1789 안녕 거북이 1790 안녕 거북이 1791 안녕 거북이 1792 안녕 거북이 1793 안녕 거북이 1794 안녕 거북이 1795 안녕 거북이 1796 안녕 거북이 1797 안녕 거북이 1798 안녕 거북이 1799 안녕 거북이 1800 안녕 거북이 1801 안녕 거북이 1802 안녕 거북이 1803 안녕 거북이 1804 안녕 거북이 1805 안녕 거북이 1806 안녕 거북이 1807 안녕 거북이 1808 안녕 거북이 1809 안녕 거북이 1810 안녕 거북이 1811 안녕 거북이 1812 안녕 거북이 1813 안녕 거북이 1814 안녕 거북이 1815 안녕 거북이 1816 안녕 거북이 1817 안녕 거북이 1818 안녕 거북이 1819 안녕 거북이 1820 안녕 거북이 1821 안녕 거북이 1822 안녕 거북이 1823 안녕 거북이 1824 안녕 거북이 1825 안녕 거북이 1826 안녕 거북이 1827 안녕 거북이 1828 안녕 거북이 1829 안녕 거북이 1830 안녕 거북이 1831 안녕 거북이 1832 안녕 거북이 1833 안녕 거북이 1834 안녕 거북이 1835 안녕 거북이 1836 안녕 거북이 1837 안녕 거북이 1838 안녕 거북이 1839 안녕 거북이 1840 안녕 거북이 1841 안녕 거북이 1842 안녕 거북이 1843 안녕 거북이 1844 안녕 거북이 1845 안녕 거북이 1846 안녕 거북이 1847 안녕 거북이 1848 안녕 거북이 1849 안녕 거북이 1850 안녕 거북이 1851 안녕 거북이 1852 안녕 거북이 1853 안녕 거북이 1854 안녕 거북이 1855 안녕 거북이 1856 안녕 거북이 1857 안녕 거북이 1858 안녕 거북이 1859 안녕 거북이 1860 안녕 거북이 1861 안녕 거북이 1862 안녕 거북이 1863 안녕 거북이 1864 안녕 거북이 1865 안녕 거북이 1866 안녕 거북이 1867 안녕 거북이 1868 안녕 거북이 1869 안녕 거북이 1870 안녕 거북이 1871 안녕 거북이 1872 안녕 거북이 1873 안녕 거북이 1874 안녕 거북이 1875 안녕 거북이 1876 안녕 거북이 1877 안녕 거북이 1878 안녕 거북이 1879 안녕 거북이 1880 안녕 거북이 1881 안녕 거북이 1882 안녕 거북이 1883 안녕 거북이 1884 안녕 거북이 1885 안녕 거북이 1886 안녕 거북이 1887 안녕 거북이 1888 안녕 거북이 1889 안녕 거북이 1890 안녕 거북이 1891 안녕 거북이 1892 안녕 거북이 1893 안녕 거북이 1894 안녕 거북이 1895 안녕 거북이 1896 안녕 거북이 1897 안녕 거북이 1898 안녕 거북이 1899 안녕 거북이 1900 안녕 거북이 1901 안녕 거북이 1902 안녕 거북이 1903 안녕 거북이 1904 안녕 거북이 1905 안녕 거북이 1906 안녕 거북이 1907 안녕 거북이 1908 안녕 거북이 1909 안녕 거북이 1910 안녕 거북이 1911 안녕 거북이 1912 안녕 거북이 1913 안녕 거북이 1914 안녕 거북이 1915 안녕 거북이 1916 안녕 거북이 1917 안녕 거북이 1918 안녕 거북이 1919 안녕 거북이 1920 안녕 거북이 1921 안녕 거북이 1922 안녕 거북이 1923 안녕 거북이 1924 안녕 거북이 1925 안녕 거북이 1926 안녕 거북이 1927 안녕 거북이 1928 안녕 거북이 1929 안녕 거북이 1930 안녕 거북이 1931 안녕 거북이 1932 안녕 거북이 1933 안녕 거북이 1934 안녕 거북이 1935 안녕 거북이 1936 안녕 거북이 1937 안녕 거북이 1938 안녕 거북이 1939 안녕 거북이 1940 안녕 거북이 1941 안녕 거북이 1942 안녕 거북이 1943 안녕 거북이 1944 안녕 거북이 1945 안녕 거북이 1946 안녕 거북이 1947 안녕 거북이 1948 안녕 거북이 1949 안녕 거북이 1950 안녕 거북이 1951 안녕 거북이 1952 안녕 거북이 1953 안녕 거북이 1954 안녕 거북이 1955 안녕 거북이 1956 안녕 거북이 1957 안녕 거북이 1958 안녕 거북이 1959 안녕 거북이 1960 안녕 거북이 1961 안녕 거북이 1962 안녕 거북이 1963 안녕 거북이 1964 안녕 거북이 1965 안녕 거북이 1966 안녕 거북이 1967 안녕 거북이 1968 안녕 거북이 1969 안녕 거북이 1970 안녕 거북이 1971 안녕 거북이 1972 안녕 거북이 1973 안녕 거북이 1974 안녕 거북이 1975 안녕 거북이 1976 안녕 거북이 1977 안녕 거북이 1978 안녕 거북이 1979 안녕 거북이 1980 안녕 거북이 1981 안녕 거북이 1982 안녕 거북이 1983 안녕 거북이 1984 안녕 거북이 1985 안녕 거북이 1986 안녕 거북이 1987 안녕 거북이 1988 안녕 거북이 1989 안녕 거북이 1990 안녕 거북이 1991 안녕 거북이 1992 안녕 거북이 1993 안녕 거북이 1994 안녕 거북이 1995 안녕 거북이 1996 안녕 거북이 1997 안녕 거북이 1998 안녕 거북이 1999 안녕 거북이 2000 안녕 거북이 2001 안녕 거북이 2002 안녕 거북이 2003 안녕 거북이 2004 안녕 거북이 2005 안녕 거북이 2006 안녕 거북이 2007 안녕 거북이 2008 안녕 거북이 2009 안녕 거북이 2010 안녕 거북이 2011 안녕 거북이 2012 안녕 거북이 2013 안녕 거북이 2014 안녕 거북이 2015 안녕 거북이 2016 안녕 거북이 2017 안녕 거북이 2018 안녕 거북이 2019 안녕 거북이 2020 안녕 거북이 2021 안녕 거북이 2022 안녕 거북이 2023 안녕 거북이 2024 안녕 거북이 2025 안녕 거북이 2026 안녕 거북이 2027 안녕 거북이 2028 안녕 거북이 2029 안녕 거북이 2030 안녕 거북이 2031 안녕 거북이 2032 안녕 거북이 2033 안녕 거북이 2034 안녕 거북이 2035 안녕 거북이 2036 안녕 거북이 2037 안녕 거북이 2038 안녕 거북이 2039 안녕 거북이 2040 안녕 거북이 2041 안녕 거북이 2042 안녕 거북이 2043 안녕 거북이 2044 안녕 거북이 2045 안녕 거북이 2046 안녕 거북이 2047 안녕 거북이 2048 안녕 거북이 2049 안녕 거북이 2050 안녕 거북이 2051 안녕 거북이 2052 안녕 거북이 2053 안녕 거북이 2054 안녕 거북이 2055 안녕 거북이 2056 안녕 거북이 2057 안녕 거북이 2058 안녕 거북이 2059 안녕 거북이 2060 안녕 거북이 2061 안녕 거북이 2062 안녕 거북이 2063 안녕 거북이 2064 안녕 거북이 2065 안녕 거북이 2066 안녕 거북이 2067 안녕 거북이 2068 안녕 거북이 2069 안녕 거북이 2070 안녕 거북이 2071 안녕 거북이 2072 안녕 거북이 2073 안녕 거북이 2074 안녕 거북이 2075 안녕 거북이 2076 안녕 거북이 2077 안녕 거북이 2078 안녕 거북이 2079 안녕 거북이 2080 안녕 거북이 2081 안녕 거북이 2082 안녕 거북이 2083 안녕 거북이 2084 안녕 거북이 2085 안녕 거북이 2086 안녕 거북이 2087 안녕 거북이 2088 안녕 거북이 2089 안녕 거북이 2090 안녕 거북이 2091 안녕 거북이 2092 안녕 거북이 2093 안녕 거북이 2094 안녕 거북이 2095 안녕 거북이 2096 안녕 거북이 2097 안녕 거북이 2098 안녕 거북이 2099 안녕 거북이 2100 안녕 거북이 2101 안녕 거북이 2102 안녕 거북이 2103 안녕 거북이 2104 안녕 거북이 2105 안녕 거북이 2106 안녕 거북이 2107 안녕 거북이 2108 안녕 거북이 2109 안녕 거북이 2110 안녕 거북이 2111 안녕 거북이 2112 안녕 거북이 2113 안녕 거북이 2114 안녕 거북이 2115 안녕 거북이 2116 안녕 거북이 2117 안녕 거북이 2118 안녕 거북이 2119 안녕 거북이 2120 안녕 거북이 2121 안녕 거북이 2122 안녕 거북이 2123 안녕 거북이 2124 안녕 거북이 2125 안녕 거북이 2126 안녕 거북이 2127 안녕 거북이 2128 안녕 거북이 2129 안녕 거북이 2130 안녕 거북이 2131 안녕 거북이 2132 안녕 거북이 2133 안녕 거북이 2134 안녕 거북이 2135 안녕 거북이 2136 안녕 거북이 2137 안녕 거북이 2138 안녕 거북이 2139 안녕 거북이 2140 안녕 거북이 2141 안녕 거북이 2142 안녕 거북이 2143 안녕 거북이 2144 안녕 거북이 2145 안녕 거북이 2146 안녕 거북이 2147 안녕 거북이 2148 안녕 거북이 2149 안녕 거북이 2150 안녕 거북이 2151 안녕 거북이 2152 안녕 거북이 2153 안녕 거북이 2154 안녕 거북이 2155 안녕 거북이 2156 안녕 거북이 2157 안녕 거북이 2158 안녕 거북이 2159 안녕 거북이 2160 안녕 거북이 2161 안녕 거북이 2162 안녕 거북이 2163 안녕 거북이 2164 안녕 거북이 2165 안녕 거북이 2166 안녕 거북이 2167 안녕 거북이 2168 안녕 거북이 2169 안녕 거북이 2170 안녕 거북이 2171 안녕 거북이 2172 안녕 거북이 2173 안녕 거북이 2174 안녕 거북이 2175 안녕 거북이 2176 안녕 거북이 2177 안녕 거북이 2178 안녕 거북이 2179 안녕 거북이 2180 안녕 거북이 2181 안녕 거북이 2182 안녕 거북이 2183 안녕 거북이 2184 안녕 거북이 2185 안녕 거북이 2186 안녕 거북이 2187 안녕 거북이 2188 안녕 거북이 2189 안녕 거북이 2190 안녕 거북이 2191 안녕 거북이 2192 안녕 거북이 2193 안녕 거북이 2194 안녕 거북이 2195 안녕 거북이 2196 안녕 거북이 2197 안녕 거북이 2198 안녕 거북이 2199 안녕 거북이 2200 안녕 거북이 2201 안녕 거북이 2202 안녕 거북이 2203 안녕 거북이 2204 안녕 거북이 2205 안녕 거북이 2206 안녕 거북이 2207 안녕 거북이 2208 안녕 거북이 2209 안녕 거북이 2210 안녕 거북이 2211 안녕 거북이 2212 안녕 거북이 2213 안녕 거북이 2214 안녕 거북이 2215 안녕 거북이 2216 안녕 거북이 2217 안녕 거북이 2218 안녕 거북이 2219 안녕 거북이 2220 안녕 거북이 2221 안녕 거북이 2222 안녕 거북이 2223 안녕 거북이 2224 안녕 거북이 2225 안녕 거북이 2226 안녕 거북이 2227 안녕 거북이 2228 안녕 거북이 2229 안녕 거북이 2230 안녕 거북이 2231 안녕 거북이 2232 안녕 거북이 2233 안녕 거북이 2234 안녕 거북이 2235 안녕 거북이 2236 안녕 거북이 2237 안녕 거북이 2238 안녕 거북이 2239 안녕 거북이 2240 안녕 거북이 2241 안녕 거북이 2242 안녕 거북이 2243 안녕 거북이 2244 안녕 거북이 2245 안녕 거북이 2246 안녕 거북이 2247 안녕 거북이 2248 안녕 거북이 2249 안녕 거북이 2250 안녕 거북이 2251 안녕 거북이 2252 안녕 거북이 2253 안녕 거북이 2254 안녕 거북이 2255 안녕 거북이 2256 안녕 거북이 2257 안녕 거북이 2258 안녕 거북이 2259 안녕 거북이 2260 안녕 거북이 2261 안녕 거북이 2262 안녕 거북이 2263 안녕 거북이 2264 안녕 거북이 2265 안녕 거북이 2266 안녕 거북이 2267 안녕 거북이 2268 안녕 거북이 2269 안녕 거북이 2270 안녕 거북이 2271 안녕 거북이 2272 안녕 거북이 2273 안녕 거북이 2274 안녕 거북이 2275 안녕 거북이 2276 안녕 거북이 2277 안녕 거북이 2278 안녕 거북이 2279 안녕 거북이 2280 안녕 거북이 2281 안녕 거북이 2282 안녕 거북이 2283 안녕 거북이 2284 안녕 거북이 2285 안녕 거북이 2286 안녕 거북이 2287 안녕 거북이 2288 안녕 거북이 2289 안녕 거북이 2290 안녕 거북이 2291 안녕 거북이 2292 안녕 거북이 2293 안녕 거북이 2294 안녕 거북이 2295 안녕 거북이 2296 안녕 거북이 2297 안녕 거북이 2298 안녕 거북이 2299 안녕 거북이 2300 안녕 거북이 2301 안녕 거북이 2302 안녕 거북이 2303 안녕 거북이 2304 안녕 거북이 2305 안녕 거북이 2306 안녕 거북이 2307 안녕 거북이 2308 안녕 거북이 2309 안녕 거북이 2310 안녕 거북이 2311 안녕 거북이 2312 안녕 거북이 2313 안녕 거북이 2314 안녕 거북이 2315 안녕 거북이 2316 안녕 거북이 2317 안녕 거북이 2318 안녕 거북이 2319 안녕 거북이 2320 안녕 거북이 2321 안녕 거북이 2322 안녕 거북이 2323 안녕 거북이 2324 안녕 거북이 2325 안녕 거북이 2326 안녕 거북이 2327 안녕 거북이 2328 안녕 거북이 2329 안녕 거북이 2330 안녕 거북이 2331 안녕 거북이 2332 안녕 거북이 2333 안녕 거북이 2334 안녕 거북이 2335 안녕 거북이 2336 안녕 거북이 2337 안녕 거북이 2338 안녕 거북이 2339 안녕 거북이 2340 안녕 거북이 2341 안녕 거북이 2342 안녕 거북이 2343 안녕 거북이 2344 안녕 거북이 2345 안녕 거북이 2346 안녕 거북이 2347 안녕 거북이 2348 안녕 거북이 2349 안녕 거북이 2350 안녕 거북이 2351 안녕 거북이 2352 안녕 거북이 2353 안녕 거북이 2354 안녕 거북이 2355 안녕 거북이 2356 안녕 거북이 2357 안녕 거북이 2358 안녕 거북이 2359 안녕 거북이 2360 안녕 거북이 2361 안녕 거북이 2362 안녕 거북이 2363 안녕 거북이 2364 안녕 거북이 2365 안녕 거북이 2366 안녕 거북이 2367 안녕 거북이 2368 안녕 거북이 2369 안녕 거북이 2370 안녕 거북이 2371 안녕 거북이 2372 안녕 거북이 2373 안녕 거북이 2374 안녕 거북이 2375 안녕 거북이 2376 안녕 거북이 2377 안녕 거북이 2378 안녕 거북이 2379 안녕 거북이 2380 안녕 거북이 2381 안녕 거북이 2382 안녕 거북이 2383 안녕 거북이 2384 안녕 거북이 2385 안녕 거북이 2386 안녕 거북이 2387 안녕 거북이 2388 안녕 거북이 2389 안녕 거북이 2390 안녕 거북이 2391 안녕 거북이 2392 안녕 거북이 2393 안녕 거북이 2394 안녕 거북이 2395 안녕 거북이 2396 안녕 거북이 2397 안녕 거북이 2398 안녕 거북이 2399 안녕 거북이 2400 안녕 거북이 2401 안녕 거북이 2402 안녕 거북이 2403 안녕 거북이 2404 안녕 거북이 2405 안녕 거북이 2406 안녕 거북이 2407 안녕 거북이 2408 안녕 거북이 2409 안녕 거북이 2410 안녕 거북이 2411 안녕 거북이 2412 안녕 거북이 2413 안녕 거북이 2414 안녕 거북이 2415 안녕 거북이 2416 안녕 거북이 2417 안녕 거북이 2418 안녕 거북이 2419 안녕 거북이 2420 안녕 거북이 2421 안녕 거북이 2422 안녕 거북이 2423 안녕 거북이 2424 안녕 거북이 2425 안녕 거북이 2426 안녕 거북이 2427 안녕 거북이 2428 안녕 거북이 2429 안녕 거북이 2430 안녕 거북이 2431 안녕 거북이 2432 안녕 거북이 2433 안녕 거북이 2434 안녕 거북이 2435 안녕 거북이 2436 안녕 거북이 2437 안녕 거북이 2438 안녕 거북이 2439 안녕 거북이 2440 안녕 거북이 2441 안녕 거북이 2442 안녕 거북이 2443 안녕 거북이 2444 안녕 거북이 2445 안녕 거북이 2446 안녕 거북이 2447 안녕 거북이 2448 안녕 거북이 2449 안녕 거북이 2450 안녕 거북이 2451 안녕 거북이 2452 안녕 거북이 2453 안녕 거북이 2454 안녕 거북이 2455 안녕 거북이 2456 안녕 거북이 2457 안녕 거북이 2458 안녕 거북이 2459 안녕 거북이 2460 안녕 거북이 2461 안녕 거북이 2462 안녕 거북이 2463 안녕 거북이 2464 안녕 거북이 2465 안녕 거북이 2466 안녕 거북이 2467 안녕 거북이 2468 안녕 거북이 2469 안녕 거북이 2470 안녕 거북이 2471 안녕 거북이 2472 안녕 거북이 2473 안녕 거북이 2474 안녕 거북이 2475 안녕 거북이 2476 안녕 거북이 2477 안녕 거북이 2478 안녕 거북이 2479 안녕 거북이 2480 안녕 거북이 2481 안녕 거북이 2482 안녕 거북이 2483 안녕 거북이 2484 안녕 거북이 2485 안녕 거북이 2486 안녕 거북이 2487 안녕 거북이 2488 안녕 거북이 2489 안녕 거북이 2490 안녕 거북이 2491 안녕 거북이 2492 안녕 거북이 2493 안녕 거북이 2494 안녕 거북이 2495 안녕 거북이 2496 안녕 거북이 2497 안녕 거북이 2498 안녕 거북이 2499 안녕 거북이 2500 안녕 거북이 2501 안녕 거북이 2502 안녕 거북이 2503 안녕 거북이 2504 안녕 거북이 2505 안녕 거북이 2506 안녕 거북이 2507 안녕 거북이 2508 안녕 거북이 2509 안녕 거북이 2510 안녕 거북이 2511 안녕 거북이 2512 안녕 거북이 2513 안녕 거북이 2514 안녕 거북이 2515 안녕 거북이 2516 안녕 거북이 2517 안녕 거북이 2518 안녕 거북이 2519 안녕 거북이 2520 안녕 거북이 2521 안녕 거북이 2522 안녕 거북이 2523 안녕 거북이 2524 안녕 거북이 2525 안녕 거북이 2526 안녕 거북이 2527 안녕 거북이 2528 안녕 거북이 2529 안녕 거북이 2530 안녕 거북이 2531 안녕 거북이 2532 안녕 거북이 2533 안녕 거북이 2534 안녕 거북이 2535 안녕 거북이 2536 안녕 거북이 2537 안녕 거북이 2538 안녕 거북이 2539 안녕 거북이 2540 안녕 거북이 2541 안녕 거북이 2542 안녕 거북이 2543 안녕 거북이 2544 안녕 거북이 2545 안녕 거북이 2546 안녕 거북이 2547 안녕 거북이 2548 안녕 거북이 2549 안녕 거북이 2550 안녕 거북이 2551 안녕 거북이 2552 안녕 거북이 2553 안녕 거북이 2554 안녕 거북이 2555 안녕 거북이 2556 안녕 거북이 2557 안녕 거북이 2558 안녕 거북이 2559 안녕 거북이 2560 안녕 거북이 2561 안녕 거북이 2562 안녕 거북이 2563 안녕 거북이 2564 안녕 거북이 2565 안녕 거북이 2566 안녕 거북이 2567 안녕 거북이 2568 안녕 거북이 2569 안녕 거북이 2570 안녕 거북이 2571 안녕 거북이 2572 안녕 거북이 2573 안녕 거북이 2574 안녕 거북이 2575 안녕 거북이 2576 안녕 거북이 2577 안녕 거북이 2578 안녕 거북이 2579 안녕 거북이 2580 안녕 거북이 2581 안녕 거북이 2582 안녕 거북이 2583 안녕 거북이 2584 안녕 거북이 2585 안녕 거북이 2586 안녕 거북이 2587 안녕 거북이 2588 안녕 거북이 2589 안녕 거북이 2590 안녕 거북이 2591 안녕 거북이 2592 안녕 거북이 2593 안녕 거북이 2594 안녕 거북이 2595 안녕 거북이 2596 안녕 거북이 2597 안녕 거북이 2598 안녕 거북이 2599 안녕 거북이 2600 안녕 거북이 2601 안녕 거북이 2602 안녕 거북이 2603 안녕 거북이 2604 안녕 거북이 2605 안녕 거북이 2606 안녕 거북이 2607 안녕 거북이 2608 안녕 거북이 2609 안녕 거북이 2610 안녕 거북이 2611 안녕 거북이 2612 안녕 거북이 2613 안녕 거북이 2614 안녕 거북이 2615 안녕 거북이 2616 안녕 거북이 2617 안녕 거북이 2618 안녕 거북이 2619 안녕 거북이 2620 안녕 거북이 2621 안녕 거북이 2622 안녕 거북이 2623 안녕 거북이 2624 안녕 거북이 2625 안녕 거북이 2626 안녕 거북이 2627 안녕 거북이 2628 안녕 거북이 2629 안녕 거북이 2630 안녕 거북이 2631 안녕 거북이 2632 안녕 거북이 2633 안녕 거북이 2634 안녕 거북이 2635 안녕 거북이 2636 안녕 거북이 2637 안녕 거북이 2638 안녕 거북이 2639 안녕 거북이 2640 안녕 거북이 2641 안녕 거북이 2642 안녕 거북이 2643 안녕 거북이 2644 안녕 거북이 2645 안녕 거북이 2646 안녕 거북이 2647 안녕 거북이 2648 안녕 거북이 2649 안녕 거북이 2650 안녕 거북이 2651 안녕 거북이 2652 안녕 거북이 2653 안녕 거북이 2654 안녕 거북이 2655 안녕 거북이 2656 안녕 거북이 2657 안녕 거북이 2658 안녕 거북이 2659 안녕 거북이 2660 안녕 거북이 2661 안녕 거북이 2662 안녕 거북이 2663 안녕 거북이 2664 안녕 거북이 2665 안녕 거북이 2666 안녕 거북이 2667 안녕 거북이 2668 안녕 거북이 2669 안녕 거북이 2670 안녕 거북이 2671 안녕 거북이 2672 안녕 거북이 2673 안녕 거북이 2674 안녕 거북이 2675 안녕 거북이 2676 안녕 거북이 2677 안녕 거북이 2678 안녕 거북이 2679 안녕 거북이 2680 안녕 거북이 2681 안녕 거북이 2682 안녕 거북이 2683 안녕 거북이 2684 안녕 거북이 2685 안녕 거북이 2686 안녕 거북이 2687 안녕 거북이 2688 안녕 거북이 2689 안녕 거북이 2690 안녕 거북이 2691 안녕 거북이 2692 안녕 거북이 2693 안녕 거북이 2694 안녕 거북이 2695 안녕 거북이 2696 안녕 거북이 2697 안녕 거북이 2698 안녕 거북이 2699 안녕 거북이 2700 안녕 거북이 2701 안녕 거북이 2702 안녕 거북이 2703 안녕 거북이 2704 안녕 거북이 2705 안녕 거북이 2706 안녕 거북이 2707 안녕 거북이 2708 안녕 거북이 2709 안녕 거북이 2710 안녕 거북이 2711 안녕 거북이 2712 안녕 거북이 2713 안녕 거북이 2714 안녕 거북이 2715 안녕 거북이 2716 안녕 거북이 2717 안녕 거북이 2718 안녕 거북이 2719 안녕 거북이 2720 안녕 거북이 2721 안녕 거북이 2722 안녕 거북이 2723 안녕 거북이 2724 안녕 거북이 2725 안녕 거북이 2726 안녕 거북이 2727 안녕 거북이 2728 안녕 거북이 2729 안녕 거북이 2730 안녕 거북이 2731 안녕 거북이 2732 안녕 거북이 2733 안녕 거북이 2734 안녕 거북이 2735 안녕 거북이 2736 안녕 거북이 2737 안녕 거북이 2738 안녕 거북이 2739 안녕 거북이 2740 안녕 거북이 2741 안녕 거북이 2742 안녕 거북이 2743 안녕 거북이 2744 안녕 거북이 2745 안녕 거북이 2746 안녕 거북이 2747 안녕 거북이 2748 안녕 거북이 2749 안녕 거북이 2750 안녕 거북이 2751 안녕 거북이 2752 안녕 거북이 2753 안녕 거북이 2754 안녕 거북이 2755 안녕 거북이 2756 안녕 거북이 2757 안녕 거북이 2758 안녕 거북이 2759 안녕 거북이 2760 안녕 거북이 2761 안녕 거북이 2762 안녕 거북이 2763 안녕 거북이 2764 안녕 거북이 2765 안녕 거북이 2766 안녕 거북이 2767 안녕 거북이 2768 안녕 거북이 2769 안녕 거북이 2770 안녕 거북이 2771 안녕 거북이 2772 안녕 거북이 2773 안녕 거북이 2774 안녕 거북이 2775 안녕 거북이 2776 안녕 거북이 2777 안녕 거북이 2778 안녕 거북이 2779 안녕 거북이 2780 안녕 거북이 2781 안녕 거북이 2782 안녕 거북이 2783 안녕 거북이 2784 안녕 거북이 2785 안녕 거북이 2786 안녕 거북이 2787 안녕 거북이 2788 안녕 거북이 2789 안녕 거북이 2790 안녕 거북이 2791 안녕 거북이 2792 안녕 거북이 2793 안녕 거북이 2794 안녕 거북이 2795 안녕 거북이 2796 안녕 거북이 2797 안녕 거북이 2798 안녕 거북이 2799 안녕 거북이 2800 안녕 거북이 2801 안녕 거북이 2802 안녕 거북이 2803 안녕 거북이 2804 안녕 거북이 2805 안녕 거북이 2806 안녕 거북이 2807 안녕 거북이 2808 안녕 거북이 2809 안녕 거북이 2810 안녕 거북이 2811 안녕 거북이 2812 안녕 거북이 2813 안녕 거북이 2814 안녕 거북이 2815 안녕 거북이 2816 안녕 거북이 2817 안녕 거북이 2818 안녕 거북이 2819 안녕 거북이 2820 안녕 거북이 2821 안녕 거북이 2822 안녕 거북이 2823 안녕 거북이 2824 안녕 거북이 2825 안녕 거북이 2826 안녕 거북이 2827 안녕 거북이 2828 안녕 거북이 2829 안녕 거북이 2830 안녕 거북이 2831 안녕 거북이 2832 안녕 거북이 2833 안녕 거북이 2834 안녕 거북이 2835 안녕 거북이 2836 안녕 거북이 2837 안녕 거북이 2838 안녕 거북이 2839 안녕 거북이 2840 안녕 거북이 2841 안녕 거북이 2842 안녕 거북이 2843 안녕 거북이 2844 안녕 거북이 2845 안녕 거북이 2846 안녕 거북이 2847 안녕 거북이 2848 안녕 거북이 2849 안녕 거북이 2850 안녕 거북이 2851 안녕 거북이 2852 안녕 거북이 2853 안녕 거북이 2854 안녕 거북이 2855 안녕 거북이 2856 안녕 거북이 2857 안녕 거북이 2858 안녕 거북이 2859 안녕 거북이 2860 안녕 거북이 2861 안녕 거북이 2862 안녕 거북이 2863 안녕 거북이 2864 안녕 거북이 2865 안녕 거북이 2866 안녕 거북이 2867 안녕 거북이 2868 안녕 거북이 2869 안녕 거북이 2870 안녕 거북이 2871 안녕 거북이 2872 안녕 거북이 2873 안녕 거북이 2874 안녕 거북이 2875 안녕 거북이 2876 안녕 거북이 2877 안녕 거북이 2878 안녕 거북이 2879 안녕 거북이 2880 안녕 거북이 2881 안녕 거북이 2882 안녕 거북이 2883 안녕 거북이 2884 안녕 거북이 2885 안녕 거북이 2886 안녕 거북이 2887 안녕 거북이 2888 안녕 거북이 2889 안녕 거북이 2890 안녕 거북이 2891 안녕 거북이 2892 안녕 거북이 2893 안녕 거북이 2894 안녕 거북이 2895 안녕 거북이 2896 안녕 거북이 2897 안녕 거북이 2898 안녕 거북이 2899 안녕 거북이 2900 안녕 거북이 2901 안녕 거북이 2902 안녕 거북이 2903 안녕 거북이 2904 안녕 거북이 2905 안녕 거북이 2906 안녕 거북이 2907 안녕 거북이 2908 안녕 거북이 2909 안녕 거북이 2910 안녕 거북이 2911 안녕 거북이 2912 안녕 거북이 2913 안녕 거북이 2914 안녕 거북이 2915 안녕 거북이 2916 안녕 거북이 2917 안녕 거북이 2918 안녕 거북이 2919 안녕 거북이 2920 안녕 거북이 2921 안녕 거북이 2922 안녕 거북이 2923 안녕 거북이 2924 안녕 거북이 2925 안녕 거북이 2926 안녕 거북이 2927 안녕 거북이 2928 안녕 거북이 2929 안녕 거북이 2930 안녕 거북이 2931 안녕 거북이 2932 안녕 거북이 2933 안녕 거북이 2934 안녕 거북이 2935 안녕 거북이 2936 안녕 거북이 2937 안녕 거북이 2938 안녕 거북이 2939 안녕 거북이 2940 안녕 거북이 2941 안녕 거북이 2942 안녕 거북이 2943 안녕 거북이 2944 안녕 거북이 2945 안녕 거북이 2946 안녕 거북이 2947 안녕 거북이 2948 안녕 거북이 2949 안녕 거북이 2950 안녕 거북이 2951 안녕 거북이 2952 안녕 거북이 2953 안녕 거북이 2954 안녕 거북이 2955 안녕 거북이 2956 안녕 거북이 2957 안녕 거북이 2958 안녕 거북이 2959 안녕 거북이 2960 안녕 거북이 2961 안녕 거북이 2962 안녕 거북이 2963 안녕 거북이 2964 안녕 거북이 2965 안녕 거북이 2966 안녕 거북이 2967 안녕 거북이 2968 안녕 거북이 2969 안녕 거북이 2970 안녕 거북이 2971 안녕 거북이 2972 안녕 거북이 2973 안녕 거북이 2974 안녕 거북이 2975 안녕 거북이 2976 안녕 거북이 2977 안녕 거북이 2978 안녕 거북이 2979 안녕 거북이 2980 안녕 거북이 2981 안녕 거북이 2982 안녕 거북이 2983 안녕 거북이 2984 안녕 거북이 2985 안녕 거북이 2986 안녕 거북이 2987 안녕 거북이 2988 안녕 거북이 2989 안녕 거북이 2990 안녕 거북이 2991 안녕 거북이 2992 안녕 거북이 2993 안녕 거북이 2994 안녕 거북이 2995 안녕 거북이 2996 안녕 거북이 2997 안녕 거북이 2998 안녕 거북이 2999 안녕 거북이 3000 안녕 거북이 3001 안녕 거북이 3002 안녕 거북이 3003 안녕 거북이 3004 안녕 거북이 3005 안녕 거북이 3006 안녕 거북이 3007 안녕 거북이 3008 안녕 거북이 3009 안녕 거북이 3010 안녕 거북이 3011 안녕 거북이 3012 안녕 거북이 3013 안녕 거북이 3014 안녕 거북이 3015 안녕 거북이 3016 안녕 거북이 3017 안녕 거북이 3018 안녕 거북이 3019 안녕 거북이 3020 안녕 거북이 3021 안녕 거북이 3022 안녕 거북이 3023 안녕 거북이 3024 안녕 거북이 3025 안녕 거북이 3026 안녕 거북이 3027 안녕 거북이 3028 안녕 거북이 3029 안녕 거북이 3030 안녕 거북이 3031 안녕 거북이 3032 안녕 거북이 3033 안녕 거북이 3034 안녕 거북이 3035 안녕 거북이 3036 안녕 거북이 3037 안녕 거북이 3038 안녕 거북이 3039 안녕 거북이 3040 안녕 거북이 3041 안녕 거북이 3042 안녕 거북이 3043 안녕 거북이 3044 안녕 거북이 3045 안녕 거북이 3046 안녕 거북이 3047 안녕 거북이 3048 안녕 거북이 3049 안녕 거북이 3050 안녕 거북이 3051 안녕 거북이 3052 안녕 거북이 3053 안녕 거북이 3054 안녕 거북이 3055 안녕 거북이 3056 안녕 거북이 3057 안녕 거북이 3058 안녕 거북이 3059 안녕 거북이 3060 안녕 거북이 3061 안녕 거북이 3062 안녕 거북이 3063 안녕 거북이 3064 안녕 거북이 3065 안녕 거북이 3066 안녕 거북이 3067 안녕 거북이 3068 안녕 거북이 3069 안녕 거북이 3070 안녕 거북이 3071 안녕 거북이 3072 안녕 거북이 3073 안녕 거북이 3074 안녕 거북이 3075 안녕 거북이 3076 안녕 거북이 3077 안녕 거북이 3078 안녕 거북이 3079 안녕 거북이 3080 안녕 거북이 3081 안녕 거북이 3082 안녕 거북이 3083 안녕 거북이 3084 안녕 거북이 3085 안녕 거북이 3086 안녕 거북이 3087 안녕 거북이 3088 안녕 거북이 3089 안녕 거북이 3090 안녕 거북이 3091 안녕 거북이 3092 안녕 거북이 3093 안녕 거북이 3094 안녕 거북이 3095 안녕 거북이 3096 안녕 거북이 3097 안녕 거북이 3098 안녕 거북이 3099 안녕 거북이 3100 안녕 거북이 3101 안녕 거북이 3102 안녕 거북이 3103 안녕 거북이 3104 안녕 거북이 3105 안녕 거북이 3106 안녕 거북이 3107 안녕 거북이 3108 안녕 거북이 3109 안녕 거북이 3110 안녕 거북이 3111 안녕 거북이 3112 안녕 거북이 3113 안녕 거북이 3114 안녕 거북이 3115 안녕 거북이 3116 안녕 거북이 3117 안녕 거북이 3118 안녕 거북이 3119 안녕 거북이 3120 안녕 거북이 3121 안녕 거북이 3122 안녕 거북이 3123 안녕 거북이 3124 안녕 거북이 3125 안녕 거북이 3126 안녕 거북이 3127 안녕 거북이 3128 안녕 거북이 3129 안녕 거북이 3130 안녕 거북이 3131 안녕 거북이 3132 안녕 거북이 3133 안녕 거북이 3134 안녕 거북이 3135 안녕 거북이 3136 안녕 거북이 3137 안녕 거북이 3138 안녕 거북이 3139 안녕 거북이 3140 안녕 거북이 3141 안녕 거북이 3142 안녕 거북이 3143 안녕 거북이 3144 안녕 거북이 3145 안녕 거북이 3146 안녕 거북이 3147 안녕 거북이 3148 안녕 거북이 3149 안녕 거북이 3150 안녕 거북이 3151 안녕 거북이 3152 안녕 거북이 3153 안녕 거북이 3154 안녕 거북이 3155 안녕 거북이 3156 안녕 거북이 3157 안녕 거북이 3158 안녕 거북이 3159 안녕 거북이 3160 안녕 거북이 3161 안녕 거북이 3162 안녕 거북이 3163 안녕 거북이 3164 안녕 거북이 3165 안녕 거북이 3166 안녕 거북이 3167 안녕 거북이 3168 안녕 거북이 3169 안녕 거북이 3170 안녕 거북이 3171 안녕 거북이 3172 안녕 거북이 3173 안녕 거북이 3174 안녕 거북이 3175 안녕 거북이 3176 안녕 거북이 3177 안녕 거북이 3178 안녕 거북이 3179 안녕 거북이 3180 안녕 거북이 3181 안녕 거북이 3182 안녕 거북이 3183 안녕 거북이 3184 안녕 거북이 3185 안녕 거북이 3186 안녕 거북이 3187 안녕 거북이 3188 안녕 거북이 3189 안녕 거북이 3190 안녕 거북이 3191 안녕 거북이 3192 안녕 거북이 3193 안녕 거북이 3194 안녕 거북이 3195 안녕 거북이 3196 안녕 거북이 3197 안녕 거북이 3198 안녕 거북이 3199 안녕 거북이 3200 안녕 거북이 3201 안녕 거북이 3202 안녕 거북이 3203 안녕 거북이 3204 안녕 거북이 3205 안녕 거북이 3206 안녕 거북이 3207 안녕 거북이 3208 안녕 거북이 3209 안녕 거북이 3210 안녕 거북이 3211 안녕 거북이 3212 안녕 거북이 3213 안녕 거북이 3214 안녕 거북이 3215 안녕 거북이 3216 안녕 거북이 3217 안녕 거북이 3218 안녕 거북이 3219 안녕 거북이 3220 안녕 거북이 3221 안녕 거북이 3222 안녕 거북이 3223 안녕 거북이 3224 안녕 거북이 3225 안녕 거북이 3226 안녕 거북이 3227 안녕 거북이 3228 안녕 거북이 3229 안녕 거북이 3230 안녕 거북이 3231 안녕 거북이 3232 안녕 거북이 3233 안녕 거북이 3234 안녕 거북이 3235 안녕 거북이 3236 안녕 거북이 3237 안녕 거북이 3238 안녕 거북이 3239 안녕 거북이 3240 안녕 거북이 3241 안녕 거북이 3242 안녕 거북이 3243 안녕 거북이 3244 안녕 거북이 3245 안녕 거북이 3246 안녕 거북이 3247 안녕 거북이 3248 안녕 거북이 3249 안녕 거북이 3250 안녕 거북이 3251 안녕 거북이 3252 안녕 거북이 3253 안녕 거북이 3254 안녕 거북이 3255 안녕 거북이 3256 안녕 거북이 3257 안녕 거북이 3258 안녕 거북이 3259 안녕 거북이 3260 안녕 거북이 3261 안녕 거북이 3262 안녕 거북이 3263 안녕 거북이 3264 안녕 거북이 3265 안녕 거북이 3266 안녕 거북이 3267 안녕 거북이 3268 안녕 거북이 3269 안녕 거북이 3270 안녕 거북이 3271 안녕 거북이 3272 안녕 거북이 3273 안녕 거북이 3274 안녕 거북이 3275 안녕 거북이 3276 안녕 거북이 3277 안녕 거북이 3278 안녕 거북이 3279 안녕 거북이 3280 안녕 거북이 3281 안녕 거북이 3282 안녕 거북이 3283 안녕 거북이 3284 안녕 거북이 3285 안녕 거북이 3286 안녕 거북이 3287 안녕 거북이 3288 안녕 거북이 3289 안녕 거북이 3290 안녕 거북이 3291 안녕 거북이 3292 안녕 거북이 3293 안녕 거북이 3294 안녕 거북이 3295 안녕 거북이 3296 안녕 거북이 3297 안녕 거북이 3298 안녕 거북이 3299 안녕 거북이 3300 안녕 거북이 3301 안녕 거북이 3302 안녕 거북이 3303 안녕 거북이 3304 안녕 거북이 3305 안녕 거북이 3306 안녕 거북이 3307 안녕 거북이 3308 안녕 거북이 3309 안녕 거북이 3310 안녕 거북이 3311 안녕 거북이 3312 안녕 거북이 3313 안녕 거북이 3314 안녕 거북이 3315 안녕 거북이 3316 안녕 거북이 3317 안녕 거북이 3318 안녕 거북이 3319 안녕 거북이 3320 안녕 거북이 3321 안녕 거북이 3322 안녕 거북이 3323 안녕 거북이 3324 안녕 거북이 3325 안녕 거북이 3326 안녕 거북이 3327 안녕 거북이 3328 안녕 거북이 3329 안녕 거북이 3330 안녕 거북이 3331 안녕 거북이 3332 안녕 거북이 3333 안녕 거북이 3334 안녕 거북이 3335 안녕 거북이 3336 안녕 거북이 3337 안녕 거북이 3338 안녕 거북이 3339 안녕 거북이 3340 안녕 거북이 3341 안녕 거북이 3342 안녕 거북이 3343 안녕 거북이 3344 안녕 거북이 3345 안녕 거북이 3346 안녕 거북이 3347 안녕 거북이 3348 안녕 거북이 3349 안녕 거북이 3350 안녕 거북이 3351 안녕 거북이 3352 안녕 거북이 3353 안녕 거북이 3354 안녕 거북이 3355 안녕 거북이 3356 안녕 거북이 3357 안녕 거북이 3358 안녕 거북이 3359 안녕 거북이 3360 안녕 거북이 3361 안녕 거북이 3362 안녕 거북이 3363 안녕 거북이 3364 안녕 거북이 3365 안녕 거북이 3366 안녕 거북이 3367 안녕 거북이 3368 안녕 거북이 3369 안녕 거북이 3370 안녕 거북이 3371 안녕 거북이 3372 안녕 거북이 3373 안녕 거북이 3374 안녕 거북이 3375 안녕 거북이 3376 안녕 거북이 3377 안녕 거북이 3378 안녕 거북이 3379 안녕 거북이 3380 안녕 거북이 3381 안녕 거북이 3382 안녕 거북이 3383 안녕 거북이 3384 안녕 거북이 3385 안녕 거북이 3386 안녕 거북이 3387 안녕 거북이 3388 안녕 거북이 3389 안녕 거북이 3390 안녕 거북이 3391 안녕 거북이 3392 안녕 거북이 3393 안녕 거북이 3394 안녕 거북이 3395 안녕 거북이 3396 안녕 거북이 3397 안녕 거북이 3398 안녕 거북이 3399 안녕 거북이 3400 안녕 거북이 3401 안녕 거북이 3402 안녕 거북이 3403 안녕 거북이 3404 안녕 거북이 3405 안녕 거북이 3406 안녕 거북이 3407 안녕 거북이 3408 안녕 거북이 3409 안녕 거북이 3410 안녕 거북이 3411 안녕 거북이 3412 안녕 거북이 3413 안녕 거북이 3414 안녕 거북이 3415 안녕 거북이 3416 안녕 거북이 3417 안녕 거북이 3418 안녕 거북이 3419 안녕 거북이 3420 안녕 거북이 3421 안녕 거북이 3422 안녕 거북이 3423 안녕 거북이 3424 안녕 거북이 3425 안녕 거북이 3426 안녕 거북이 3427 안녕 거북이 3428 안녕 거북이 3429 안녕 거북이 3430 안녕 거북이 3431 안녕 거북이 3432 안녕 거북이 3433 안녕 거북이 3434 안녕 거북이 3435 안녕 거북이 3436 안녕 거북이 3437 안녕 거북이 3438 안녕 거북이 3439 안녕 거북이 3440 안녕 거북이 3441 안녕 거북이 3442 안녕 거북이 3443 안녕 거북이 3444 안녕 거북이 3445 안녕 거북이 3446 안녕 거북이 3447 안녕 거북이 3448 안녕 거북이 3449 안녕 거북이 3450 안녕 거북이 3451 안녕 거북이 3452 안녕 거북이 3453 안녕 거북이 3454 안녕 거북이 3455 안녕 거북이 3456 안녕 거북이 3457 안녕 거북이 3458 안녕 거북이 3459 안녕 거북이 3460 안녕 거북이 3461 안녕 거북이 3462 안녕 거북이 3463 안녕 거북이 3464 안녕 거북이 3465 안녕 거북이 3466 안녕 거북이 3467 안녕 거북이 3468 안녕 거북이 3469 안녕 거북이 3470 안녕 거북이 3471 안녕 거북이 3472 안녕 거북이 3473 안녕 거북이 3474 안녕 거북이 3475 안녕 거북이 3476 안녕 거북이 3477 안녕 거북이 3478 안녕 거북이 3479 안녕 거북이 3480 안녕 거북이 3481 안녕 거북이 3482 안녕 거북이 3483 안녕 거북이 3484 안녕 거북이 3485 안녕 거북이 3486 안녕 거북이 3487 안녕 거북이 3488 안녕 거북이 3489 안녕 거북이 3490 안녕 거북이 3491 안녕 거북이 3492 안녕 거북이 3493 안녕 거북이 3494 안녕 거북이 3495 안녕 거북이 3496 안녕 거북이 3497 안녕 거북이 3498 안녕 거북이 3499 안녕 거북이 3500 안녕 거북이 3501 안녕 거북이 3502 안녕 거북이 3503 안녕 거북이 3504 안녕 거북이 3505 안녕 거북이 3506 안녕 거북이 3507 안녕 거북이 3508 안녕 거북이 3509 안녕 거북이 3510 안녕 거북이 3511 안녕 거북이 3512 안녕 거북이 3513 안녕 거북이 3514 안녕 거북이 3515 안녕 거북이 3516 안녕 거북이 3517 안녕 거북이 3518 안녕 거북이 3519 안녕 거북이 3520 안녕 거북이 3521 안녕 거북이 3522 안녕 거북이 3523 안녕 거북이 3524 안녕 거북이 3525 안녕 거북이 3526 안녕 거북이 3527 안녕 거북이 3528 안녕 거북이 3529 안녕 거북이 3530 안녕 거북이 3531 안녕 거북이 3532 안녕 거북이 3533 안녕 거북이 3534 안녕 거북이 3535 안녕 거북이 3536 안녕 거북이 3537 안녕 거북이 3538 안녕 거북이 3539 안녕 거북이 3540 안녕 거북이 3541 안녕 거북이 3542 안녕 거북이 3543 안녕 거북이 3544 안녕 거북이 3545 안녕 거북이 3546 안녕 거북이 3547 안녕 거북이 3548 안녕 거북이 3549 안녕 거북이 3550 안녕 거북이 3551 안녕 거북이 3552 안녕 거북이 3553 안녕 거북이 3554 안녕 거북이 3555 안녕 거북이 3556 안녕 거북이 3557 안녕 거북이 3558 안녕 거북이 3559 안녕 거북이 3560 안녕 거북이 3561 안녕 거북이 3562 안녕 거북이 3563 안녕 거북이 3564 안녕 거북이 3565 안녕 거북이 3566 안녕 거북이 3567 안녕 거북이 3568 안녕 거북이 3569 안녕 거북이 3570 안녕 거북이 3571 안녕 거북이 3572 안녕 거북이 3573 안녕 거북이 3574 안녕 거북이 3575 안녕 거북이 3576 안녕 거북이 3577 안녕 거북이 3578 안녕 거북이 3579 안녕 거북이 3580 안녕 거북이 3581 안녕 거북이 3582 안녕 거북이 3583 안녕 거북이 3584 안녕 거북이 3585 안녕 거북이 3586 안녕 거북이 3587 안녕 거북이 3588 안녕 거북이 3589 안녕 거북이 3590 안녕 거북이 3591 안녕 거북이 3592 안녕 거북이 3593 안녕 거북이 3594 안녕 거북이 3595 안녕 거북이 3596 안녕 거북이 3597 안녕 거북이 3598 안녕 거북이 3599 안녕 거북이 3600 안녕 거북이 3601 안녕 거북이 3602 안녕 거북이 3603 안녕 거북이 3604 안녕 거북이 3605 안녕 거북이 3606 안녕 거북이 3607 안녕 거북이 3608 안녕 거북이 3609 안녕 거북이 3610 안녕 거북이 3611 안녕 거북이 3612 안녕 거북이 3613 안녕 거북이 3614 안녕 거북이 3615 안녕 거북이 3616 안녕 거북이 3617 안녕 거북이 3618 안녕 거북이 3619 안녕 거북이 3620 안녕 거북이 3621 안녕 거북이 3622 안녕 거북이 3623 안녕 거북이 3624 안녕 거북이 3625 안녕 거북이 3626 안녕 거북이 3627 안녕 거북이 3628 안녕 거북이 3629 안녕 거북이 3630 안녕 거북이 3631 안녕 거북이 3632 안녕 거북이 3633 안녕 거북이 3634 안녕 거북이 3635 안녕 거북이 3636 안녕 거북이 3637 안녕 거북이 3638 안녕 거북이 3639 안녕 거북이 3640 안녕 거북이 3641 안녕 거북이 3642 안녕 거북이 3643 안녕 거북이 3644 안녕 거북이 3645 안녕 거북이 3646 안녕 거북이 3647 안녕 거북이 3648 안녕 거북이 3649 안녕 거북이 3650 안녕 거북이 3651 안녕 거북이 3652 안녕 거북이 3653 안녕 거북이 3654 안녕 거북이 3655 안녕 거북이 3656 안녕 거북이 3657 안녕 거북이 3658 안녕 거북이 3659 안녕 거북이 3660 안녕 거북이 3661 안녕 거북이 3662 안녕 거북이 3663 안녕 거북이 3664 안녕 거북이 3665 안녕 거북이 3666 안녕 거북이 3667 안녕 거북이 3668 안녕 거북이 3669 안녕 거북이 3670 안녕 거북이 3671 안녕 거북이 3672 안녕 거북이 3673 안녕 거북이 3674 안녕 거북이 3675 안녕 거북이 3676 안녕 거북이 3677 안녕 거북이 3678 안녕 거북이 3679 안녕 거북이 3680 안녕 거북이 3681 안녕 거북이 3682 안녕 거북이 3683 안녕 거북이 3684 안녕 거북이 3685 안녕 거북이 3686 안녕 거북이 3687 안녕 거북이 3688 안녕 거북이 3689 안녕 거북이 3690 안녕 거북이 3691 안녕 거북이 3692 안녕 거북이 3693 안녕 거북이 3694 안녕 거북이 3695 안녕 거북이 3696 안녕 거북이 3697 안녕 거북이 3698 안녕 거북이 3699 안녕 거북이 3700 안녕 거북이 3701 안녕 거북이 3702 안녕 거북이 3703 안녕 거북이 3704 안녕 거북이 3705 안녕 거북이 3706 안녕 거북이 3707 안녕 거북이 3708 안녕 거북이 3709 안녕 거북이 3710 안녕 거북이 3711 안녕 거북이 3712 안녕 거북이 3713 안녕 거북이 3714 안녕 거북이 3715 안녕 거북이 3716 안녕 거북이 3717 안녕 거북이 3718 안녕 거북이 3719 안녕 거북이 3720 안녕 거북이 3721 안녕 거북이 3722 안녕 거북이 3723 안녕 거북이 3724 안녕 거북이 3725 안녕 거북이 3726 안녕 거북이 3727 안녕 거북이 3728 안녕 거북이 3729 안녕 거북이 3730 안녕 거북이 3731 안녕 거북이 3732 안녕 거북이 3733 안녕 거북이 3734 안녕 거북이 3735 안녕 거북이 3736 안녕 거북이 3737 안녕 거북이 3738 안녕 거북이 3739 안녕 거북이 3740 안녕 거북이 3741 안녕 거북이 3742 안녕 거북이 3743 안녕 거북이 3744 안녕 거북이 3745 안녕 거북이 3746 안녕 거북이 3747 안녕 거북이 3748 안녕 거북이 3749 안녕 거북이 3750 안녕 거북이 3751 안녕 거북이 3752 안녕 거북이 3753 안녕 거북이 3754 안녕 거북이 3755 안녕 거북이 3756 안녕 거북이 3757 안녕 거북이 3758 안녕 거북이 3759 안녕 거북이 3760 안녕 거북이 3761 안녕 거북이 3762 안녕 거북이 3763 안녕 거북이 3764 안녕 거북이 3765 안녕 거북이 3766 안녕 거북이 3767 안녕 거북이 3768 안녕 거북이 3769 안녕 거북이 3770 안녕 거북이 3771 안녕 거북이 3772 안녕 거북이 3773 안녕 거북이 3774 안녕 거북이 3775 안녕 거북이 3776 안녕 거북이 3777 안녕 거북이 3778 안녕 거북이 3779 안녕 거북이 3780 안녕 거북이 3781 안녕 거북이 3782 안녕 거북이 3783 안녕 거북이 3784 안녕 거북이 3785 안녕 거북이 3786 안녕 거북이 3787 안녕 거북이 3788 안녕 거북이 3789 안녕 거북이 3790 안녕 거북이 3791 안녕 거북이 3792 안녕 거북이 3793 안녕 거북이 3794 안녕 거북이 3795 안녕 거북이 3796 안녕 거북이 3797 안녕 거북이 3798 안녕 거북이 3799 안녕 거북이 3800 안녕 거북이 3801 안녕 거북이 3802 안녕 거북이 3803 안녕 거북이 3804 안녕 거북이 3805 안녕 거북이 3806 안녕 거북이 3807 안녕 거북이 3808 안녕 거북이 3809 안녕 거북이 3810 안녕 거북이 3811 안녕 거북이 3812 안녕 거북이 3813 안녕 거북이 3814 안녕 거북이 3815 안녕 거북이 3816 안녕 거북이 3817 안녕 거북이 3818 안녕 거북이 3819 안녕 거북이 3820 안녕 거북이 3821 안녕 거북이 3822 안녕 거북이 3823 안녕 거북이 3824 안녕 거북이 3825 안녕 거북이 3826 안녕 거북이 3827 안녕 거북이 3828 안녕 거북이 3829 안녕 거북이 3830 안녕 거북이 3831 안녕 거북이 3832 안녕 거북이 3833 안녕 거북이 3834 안녕 거북이 3835 안녕 거북이 3836 안녕 거북이 3837 안녕 거북이 3838 안녕 거북이 3839 안녕 거북이 3840 안녕 거북이 3841 안녕 거북이 3842 안녕 거북이 3843 안녕 거북이 3844 안녕 거북이 3845 안녕 거북이 3846 안녕 거북이 3847 안녕 거북이 3848 안녕 거북이 3849 안녕 거북이 3850 안녕 거북이 3851 안녕 거북이 3852 안녕 거북이 3853 안녕 거북이 3854 안녕 거북이 3855 안녕 거북이 3856 안녕 거북이 3857 안녕 거북이 3858 안녕 거북이 3859 안녕 거북이 3860 안녕 거북이 3861 안녕 거북이 3862 안녕 거북이 3863 안녕 거북이 3864 안녕 거북이 3865 안녕 거북이 3866 안녕 거북이 3867 안녕 거북이 3868 안녕 거북이 3869 안녕 거북이 3870 안녕 거북이 3871 안녕 거북이 3872 안녕 거북이 3873 안녕 거북이 3874 안녕 거북이 3875 안녕 거북이 3876 안녕 거북이 3877 안녕 거북이 3878 안녕 거북이 3879 안녕 거북이 3880 안녕 거북이 3881 안녕 거북이 3882 안녕 거북이 3883 안녕 거북이 3884 안녕 거북이 3885 안녕 거북이 3886 안녕 거북이 3887 안녕 거북이 3888 안녕 거북이 3889 안녕 거북이 3890 안녕 거북이 3891 안녕 거북이 3892 안녕 거북이 3893 안녕 거북이 3894 안녕 거북이 3895 안녕 거북이 3896 안녕 거북이 3897 안녕 거북이 3898 안녕 거북이 3899 안녕 거북이 3900 안녕 거북이 3901 안녕 거북이 3902 안녕 거북이 3903 안녕 거북이 3904 안녕 거북이 3905 안녕 거북이 3906 안녕 거북이 3907 안녕 거북이 3908 안녕 거북이 3909 안녕 거북이 3910 안녕 거북이 3911 안녕 거북이 3912 안녕 거북이 3913 안녕 거북이 3914 안녕 거북이 3915 안녕 거북이 3916 안녕 거북이 3917 안녕 거북이 3918 안녕 거북이 3919 안녕 거북이 3920 안녕 거북이 3921 안녕 거북이 3922 안녕 거북이 3923 안녕 거북이 3924 안녕 거북이 3925 안녕 거북이 3926 안녕 거북이 3927 안녕 거북이 3928 안녕 거북이 3929 안녕 거북이 3930 안녕 거북이 3931 안녕 거북이 3932 안녕 거북이 3933 안녕 거북이 3934 안녕 거북이 3935 안녕 거북이 3936 안녕 거북이 3937 안녕 거북이 3938 안녕 거북이 3939 안녕 거북이 3940 안녕 거북이 3941 안녕 거북이 3942 안녕 거북이 3943 안녕 거북이 3944 안녕 거북이 3945 안녕 거북이 3946 안녕 거북이 3947 안녕 거북이 3948 안녕 거북이 3949 안녕 거북이 3950 안녕 거북이 3951 안녕 거북이 3952 안녕 거북이 3953 안녕 거북이 3954 안녕 거북이 3955 안녕 거북이 3956 안녕 거북이 3957 안녕 거북이 3958 안녕 거북이 3959 안녕 거북이 3960 안녕 거북이 3961 안녕 거북이 3962 안녕 거북이 3963 안녕 거북이 3964 안녕 거북이 3965 안녕 거북이 3966 안녕 거북이 3967 안녕 거북이 3968 안녕 거북이 3969 안녕 거북이 3970 안녕 거북이 3971 안녕 거북이 3972 안녕 거북이 3973 안녕 거북이 3974 안녕 거북이 3975 안녕 거북이 3976 안녕 거북이 3977 안녕 거북이 3978 안녕 거북이 3979 안녕 거북이 3980 안녕 거북이 3981 안녕 거북이 3982 안녕 거북이 3983 안녕 거북이 3984 안녕 거북이 3985 안녕 거북이 3986 안녕 거북이 3987 안녕 거북이 3988 안녕 거북이 3989 안녕 거북이 3990 안녕 거북이 3991 안녕 거북이 3992 안녕 거북이 3993 안녕 거북이 3994 안녕 거북이 3995 안녕 거북이 3996 안녕 거북이 3997 안녕 거북이 3998 안녕 거북이 3999 안녕 거북이 4000 안녕 거북이 4001 안녕 거북이 4002 안녕 거북이 4003 안녕 거북이 4004 안녕 거북이 4005 안녕 거북이 4006 안녕 거북이 4007 안녕 거북이 4008 안녕 거북이 4009 안녕 거북이 4010 안녕 거북이 4011 안녕 거북이 4012 안녕 거북이 4013 안녕 거북이 4014 안녕 거북이 4015 안녕 거북이 4016 안녕 거북이 4017 안녕 거북이 4018 안녕 거북이 4019 안녕 거북이 4020 안녕 거북이 4021 안녕 거북이 4022 안녕 거북이 4023 안녕 거북이 4024 안녕 거북이 4025 안녕 거북이 4026 안녕 거북이 4027 안녕 거북이 4028 안녕 거북이 4029 안녕 거북이 4030 안녕 거북이 4031 안녕 거북이 4032 안녕 거북이 4033 안녕 거북이 4034 안녕 거북이 4035 안녕 거북이 4036 안녕 거북이 4037 안녕 거북이 4038 안녕 거북이 4039 안녕 거북이 4040 안녕 거북이 4041 안녕 거북이 4042 안녕 거북이 4043 안녕 거북이 4044 안녕 거북이 4045 안녕 거북이 4046 안녕 거북이 4047 안녕 거북이 4048 안녕 거북이 4049 안녕 거북이 4050 안녕 거북이 4051 안녕 거북이 4052 안녕 거북이 4053 안녕 거북이 4054 안녕 거북이 4055 안녕 거북이 4056 안녕 거북이 4057 안녕 거북이 4058 안녕 거북이 4059 안녕 거북이 4060 안녕 거북이 4061 안녕 거북이 4062 안녕 거북이 4063 안녕 거북이 4064 안녕 거북이 4065 안녕 거북이 4066 안녕 거북이 4067 안녕 거북이 4068 안녕 거북이 4069 안녕 거북이 4070 안녕 거북이 4071 안녕 거북이 4072 안녕 거북이 4073 안녕 거북이 4074 안녕 거북이 4075 안녕 거북이 4076 안녕 거북이 4077 안녕 거북이 4078 안녕 거북이 4079 안녕 거북이 4080 안녕 거북이 4081 안녕 거북이 4082 안녕 거북이 4083 안녕 거북이 4084 안녕 거북이 4085 안녕 거북이 4086 안녕 거북이 4087 안녕 거북이 4088 안녕 거북이 4089 안녕 거북이 4090 안녕 거북이 4091 안녕 거북이 4092 안녕 거북이 4093 안녕 거북이 4094 안녕 거북이 4095 안녕 거북이 4096 안녕 거북이 4097 안녕 거북이 4098 안녕 거북이 4099 안녕 거북이 4100 안녕 거북이 4101 안녕 거북이 4102 안녕 거북이 4103 안녕 거북이 4104 안녕 거북이 4105 안녕 거북이 4106 안녕 거북이 4107 안녕 거북이 4108 안녕 거북이 4109 안녕 거북이 4110 안녕 거북이 4111 안녕 거북이 4112 안녕 거북이 4113 안녕 거북이 4114 안녕 거북이 4115 안녕 거북이 4116 안녕 거북이 4117 안녕 거북이 4118 안녕 거북이 4119 안녕 거북이 4120 안녕 거북이 4121 안녕 거북이 4122 안녕 거북이 4123 안녕 거북이 4124 안녕 거북이 4125 안녕 거북이 4126 안녕 거북이 4127 안녕 거북이 4128 안녕 거북이 4129 안녕 거북이 4130 안녕 거북이 4131 안녕 거북이 4132 안녕 거북이 4133 안녕 거북이 4134 안녕 거북이 4135 안녕 거북이 4136 안녕 거북이 4137 안녕 거북이 4138 안녕 거북이 4139 안녕 거북이 4140 안녕 거북이 4141 안녕 거북이 4142 안녕 거북이 4143 안녕 거북이 4144 안녕 거북이 4145 안녕 거북이 4146 안녕 거북이 4147 안녕 거북이 4148 안녕 거북이 4149 안녕 거북이 4150 안녕 거북이 4151 안녕 거북이 4152 안녕 거북이 4153 안녕 거북이 4154 안녕 거북이 4155 안녕 거북이 4156 안녕 거북이 4157 안녕 거북이 4158 안녕 거북이 4159 안녕 거북이 4160 안녕 거북이 4161 안녕 거북이 4162 안녕 거북이 4163 안녕 거북이 4164 안녕 거북이 4165 안녕 거북이 4166 안녕 거북이 4167 안녕 거북이 4168 안녕 거북이 4169 안녕 거북이 4170 안녕 거북이 4171 안녕 거북이 4172 안녕 거북이 4173 안녕 거북이 4174 안녕 거북이 4175 안녕 거북이 4176 안녕 거북이 4177 안녕 거북이 4178 안녕 거북이 4179 안녕 거북이 4180 안녕 거북이 4181 안녕 거북이 4182 안녕 거북이 4183 안녕 거북이 4184 안녕 거북이 4185 안녕 거북이 4186 안녕 거북이 4187 안녕 거북이 4188 안녕 거북이 4189 안녕 거북이 4190 안녕 거북이 4191 안녕 거북이 4192 안녕 거북이 4193 안녕 거북이 4194 안녕 거북이 4195 안녕 거북이 4196 안녕 거북이 4197 안녕 거북이 4198 안녕 거북이 4199 안녕 거북이 4200 안녕 거북이 4201 안녕 거북이 4202 안녕 거북이 4203 안녕 거북이 4204 안녕 거북이 4205 안녕 거북이 4206 안녕 거북이 4207 안녕 거북이 4208 안녕 거북이 4209 안녕 거북이 4210 안녕 거북이 4211 안녕 거북이 4212 안녕 거북이 4213 안녕 거북이 4214 안녕 거북이 4215 안녕 거북이 4216 안녕 거북이 4217 안녕 거북이 4218 안녕 거북이 4219 안녕 거북이 4220 안녕 거북이 4221 안녕 거북이 4222 안녕 거북이 4223 안녕 거북이 4224 안녕 거북이 4225 안녕 거북이 4226 안녕 거북이 4227 안녕 거북이 4228 안녕 거북이 4229 안녕 거북이 4230 안녕 거북이 4231 안녕 거북이 4232 안녕 거북이 4233 안녕 거북이 4234 안녕 거북이 4235 안녕 거북이 4236 안녕 거북이 4237 안녕 거북이 4238 안녕 거북이 4239 안녕 거북이 4240 안녕 거북이 4241 안녕 거북이 4242 안녕 거북이 4243 안녕 거북이 4244 안녕 거북이 4245 안녕 거북이 4246 안녕 거북이 4247 안녕 거북이 4248 안녕 거북이 4249 안녕 거북이 4250 안녕 거북이 4251 안녕 거북이 4252 안녕 거북이 4253 안녕 거북이 4254 안녕 거북이 4255 안녕 거북이 4256 안녕 거북이 4257 안녕 거북이 4258 안녕 거북이 4259 안녕 거북이 4260 안녕 거북이 4261 안녕 거북이 4262 안녕 거북이 4263 안녕 거북이 4264 안녕 거북이 4265 안녕 거북이 4266 안녕 거북이 4267 안녕 거북이 4268 안녕 거북이 4269 안녕 거북이 4270 안녕 거북이 4271 안녕 거북이 4272 안녕 거북이 4273 안녕 거북이 4274 안녕 거북이 4275 안녕 거북이 4276 안녕 거북이 4277 안녕 거북이 4278 안녕 거북이 4279 안녕 거북이 4280 안녕 거북이 4281 안녕 거북이 4282 안녕 거북이 4283 안녕 거북이 4284 안녕 거북이 4285 안녕 거북이 4286 안녕 거북이 4287 안녕 거북이 4288 안녕 거북이 4289 안녕 거북이 4290 안녕 거북이 4291 안녕 거북이 4292 안녕 거북이 4293 안녕 거북이 4294 안녕 거북이 4295 안녕 거북이 4296 안녕 거북이 4297 안녕 거북이 4298 안녕 거북이 4299 안녕 거북이 4300 안녕 거북이 4301 안녕 거북이 4302 안녕 거북이 4303 안녕 거북이 4304 안녕 거북이 4305 안녕 거북이 4306 안녕 거북이 4307 안녕 거북이 4308 안녕 거북이 4309 안녕 거북이 4310 안녕 거북이 4311 안녕 거북이 4312 안녕 거북이 4313 안녕 거북이 4314 안녕 거북이 4315 안녕 거북이 4316 안녕 거북이 4317 안녕 거북이 4318 안녕 거북이 4319 안녕 거북이 4320 안녕 거북이 4321 안녕 거북이 4322 안녕 거북이 4323 안녕 거북이 4324 안녕 거북이 4325 안녕 거북이 4326 안녕 거북이 4327 안녕 거북이 4328 안녕 거북이 4329 안녕 거북이 4330 안녕 거북이 4331 안녕 거북이 4332 안녕 거북이 4333 안녕 거북이 4334 안녕 거북이 4335 안녕 거북이 4336 안녕 거북이 4337 안녕 거북이 4338 안녕 거북이 4339 안녕 거북이 4340 안녕 거북이 4341 안녕 거북이 4342 안녕 거북이 4343 안녕 거북이 4344 안녕 거북이 4345 안녕 거북이 4346 안녕 거북이 4347 안녕 거북이 4348 안녕 거북이 4349 안녕 거북이 4350 안녕 거북이 4351 안녕 거북이 4352 안녕 거북이 4353 안녕 거북이 4354 안녕 거북이 4355 안녕 거북이 4356 안녕 거북이 4357 안녕 거북이 4358 안녕 거북이 4359 안녕 거북이 4360 안녕 거북이 4361 안녕 거북이 4362 안녕 거북이 4363 안녕 거북이 4364 안녕 거북이 4365 안녕 거북이 4366 안녕 거북이 4367 안녕 거북이 4368 안녕 거북이 4369 안녕 거북이 4370 안녕 거북이 4371 안녕 거북이 4372 안녕 거북이 4373 안녕 거북이 4374 안녕 거북이 4375 안녕 거북이 4376 안녕 거북이 4377 안녕 거북이 4378 안녕 거북이 4379 안녕 거북이 4380 안녕 거북이 4381 안녕 거북이 4382 안녕 거북이 4383 안녕 거북이 4384 안녕 거북이 4385 안녕 거북이 4386 안녕 거북이 4387 안녕 거북이 4388 안녕 거북이 4389 안녕 거북이 4390 안녕 거북이 4391 안녕 거북이 4392 안녕 거북이 4393 안녕 거북이 4394 안녕 거북이 4395 안녕 거북이 4396 안녕 거북이 4397 안녕 거북이 4398 안녕 거북이 4399 안녕 거북이 4400 안녕 거북이 4401 안녕 거북이 4402 안녕 거북이 4403 안녕 거북이 4404 안녕 거북이 4405 안녕 거북이 4406 안녕 거북이 4407 안녕 거북이 4408 안녕 거북이 4409 안녕 거북이 4410 안녕 거북이 4411 안녕 거북이 4412 안녕 거북이 4413 안녕 거북이 4414 안녕 거북이 4415 안녕 거북이 4416 안녕 거북이 4417 안녕 거북이 4418 안녕 거북이 4419 안녕 거북이 4420 안녕 거북이 4421 안녕 거북이 4422 안녕 거북이 4423 안녕 거북이 4424 안녕 거북이 4425 안녕 거북이 4426 안녕 거북이 4427 안녕 거북이 4428 안녕 거북이 4429 안녕 거북이 4430 안녕 거북이 4431 안녕 거북이 4432 안녕 거북이 4433 안녕 거북이 4434 안녕 거북이 4435 안녕 거북이 4436 안녕 거북이 4437 안녕 거북이 4438 안녕 거북이 4439 안녕 거북이 4440 안녕 거북이 4441 안녕 거북이 4442 안녕 거북이 4443 안녕 거북이 4444 안녕 거북이 4445 안녕 거북이 4446 안녕 거북이 4447 안녕 거북이 4448 안녕 거북이 4449 안녕 거북이 4450 안녕 거북이 4451 안녕 거북이 4452 안녕 거북이 4453 안녕 거북이 4454 안녕 거북이 4455 안녕 거북이 4456 안녕 거북이 4457 안녕 거북이 4458 안녕 거북이 4459 안녕 거북이 4460 안녕 거북이 4461 안녕 거북이 4462 안녕 거북이 4463 안녕 거북이 4464 안녕 거북이 4465 안녕 거북이 4466 안녕 거북이 4467 안녕 거북이 4468 안녕 거북이 4469 안녕 거북이 4470 안녕 거북이 4471 안녕 거북이 4472 안녕 거북이 4473 안녕 거북이 4474 안녕 거북이 4475 안녕 거북이 4476 안녕 거북이 4477 안녕 거북이 4478 안녕 거북이 4479 안녕 거북이 4480 안녕 거북이 4481 안녕 거북이 4482 안녕 거북이 4483 안녕 거북이 4484 안녕 거북이 4485 안녕 거북이 4486 안녕 거북이 4487 안녕 거북이 4488 안녕 거북이 4489 안녕 거북이 4490 안녕 거북이 4491 안녕 거북이 4492 안녕 거북이 4493 안녕 거북이 4494 안녕 거북이 4495 안녕 거북이 4496 안녕 거북이 4497 안녕 거북이 4498 안녕 거북이 4499 안녕 거북이 4500 안녕 거북이 4501 안녕 거북이 4502 안녕 거북이 4503 안녕 거북이 4504 안녕 거북이 4505 안녕 거북이 4506 안녕 거북이 4507 안녕 거북이 4508 안녕 거북이 4509 안녕 거북이 4510 안녕 거북이 4511 안녕 거북이 4512 안녕 거북이 4513 안녕 거북이 4514 안녕 거북이 4515 안녕 거북이 4516 안녕 거북이 4517 안녕 거북이 4518 안녕 거북이 4519 안녕 거북이 4520 안녕 거북이 4521 안녕 거북이 4522 안녕 거북이 4523 안녕 거북이 4524 안녕 거북이 4525 안녕 거북이 4526 안녕 거북이 4527 안녕 거북이 4528 안녕 거북이 4529 안녕 거북이 4530 안녕 거북이 4531 안녕 거북이 4532 안녕 거북이 4533 안녕 거북이 4534 안녕 거북이 4535 안녕 거북이 4536 안녕 거북이 4537 안녕 거북이 4538 안녕 거북이 4539 안녕 거북이 4540 안녕 거북이 4541 안녕 거북이 4542 안녕 거북이 4543 안녕 거북이 4544 안녕 거북이 4545 안녕 거북이 4546 안녕 거북이 4547 안녕 거북이 4548 안녕 거북이 4549 안녕 거북이 4550 안녕 거북이 4551 안녕 거북이 4552 안녕 거북이 4553 안녕 거북이 4554 안녕 거북이 4555 안녕 거북이 4556 안녕 거북이 4557 안녕 거북이 4558 안녕 거북이 4559 안녕 거북이 4560 안녕 거북이 4561 안녕 거북이 4562 안녕 거북이 4563 안녕 거북이 4564 안녕 거북이 4565 안녕 거북이 4566 안녕 거북이 4567 안녕 거북이 4568 안녕 거북이 4569 안녕 거북이 4570 안녕 거북이 4571 안녕 거북이 4572 안녕 거북이 4573 안녕 거북이 4574 안녕 거북이 4575 안녕 거북이 4576 안녕 거북이 4577 안녕 거북이 4578 안녕 거북이 4579 안녕 거북이 4580 안녕 거북이 4581 안녕 거북이 4582 안녕 거북이 4583 안녕 거북이 4584 안녕 거북이 4585 안녕 거북이 4586 안녕 거북이 4587 안녕 거북이 4588 안녕 거북이 4589 안녕 거북이 4590 안녕 거북이 4591 안녕 거북이 4592 안녕 거북이 4593 안녕 거북이 4594 안녕 거북이 4595 안녕 거북이 4596 안녕 거북이 4597 안녕 거북이 4598 안녕 거북이 4599 안녕 거북이 4600 안녕 거북이 4601 안녕 거북이 4602 안녕 거북이 4603 안녕 거북이 4604 안녕 거북이 4605 안녕 거북이 4606 안녕 거북이 4607 안녕 거북이 4608 안녕 거북이 4609 안녕 거북이 4610 안녕 거북이 4611 안녕 거북이 4612 안녕 거북이 4613 안녕 거북이 4614 안녕 거북이 4615 안녕 거북이 4616 안녕 거북이 4617 안녕 거북이 4618 안녕 거북이 4619 안녕 거북이 4620 안녕 거북이 4621 안녕 거북이 4622 안녕 거북이 4623 안녕 거북이 4624 안녕 거북이 4625 안녕 거북이 4626 안녕 거북이 4627 안녕 거북이 4628 안녕 거북이 4629 안녕 거북이 4630 안녕 거북이 4631 안녕 거북이 4632 안녕 거북이 4633 안녕 거북이 4634 안녕 거북이 4635 안녕 거북이 4636 안녕 거북이 4637 안녕 거북이 4638 안녕 거북이 4639 안녕 거북이 4640 안녕 거북이 4641 안녕 거북이 4642 안녕 거북이 4643 안녕 거북이 4644 안녕 거북이 4645 안녕 거북이 4646 안녕 거북이 4647 안녕 거북이 4648 안녕 거북이 4649 안녕 거북이 4650 안녕 거북이 4651 안녕 거북이 4652 안녕 거북이 4653 안녕 거북이 4654 안녕 거북이 4655 안녕 거북이 4656 안녕 거북이 4657 안녕 거북이 4658 안녕 거북이 4659 안녕 거북이 4660 안녕 거북이 4661 안녕 거북이 4662 안녕 거북이 4663 안녕 거북이 4664 안녕 거북이 4665 안녕 거북이 4666 안녕 거북이 4667 안녕 거북이 4668 안녕 거북이 4669 안녕 거북이 4670 안녕 거북이 4671 안녕 거북이 4672 안녕 거북이 4673 안녕 거북이 4674 안녕 거북이 4675 안녕 거북이 4676 안녕 거북이 4677 안녕 거북이 4678 안녕 거북이 4679 안녕 거북이 4680 안녕 거북이 4681 안녕 거북이 4682 안녕 거북이 4683 안녕 거북이 4684 안녕 거북이 4685 안녕 거북이 4686 안녕 거북이 4687 안녕 거북이 4688 안녕 거북이 4689 안녕 거북이 4690 안녕 거북이 4691 안녕 거북이 4692 안녕 거북이 4693 안녕 거북이 4694 안녕 거북이 4695 안녕 거북이 4696 안녕 거북이 4697 안녕 거북이 4698 안녕 거북이 4699 안녕 거북이 4700 안녕 거북이 4701 안녕 거북이 4702 안녕 거북이 4703 안녕 거북이 4704 안녕 거북이 4705 안녕 거북이 4706 안녕 거북이 4707 안녕 거북이 4708 안녕 거북이 4709 안녕 거북이 4710 안녕 거북이 4711 안녕 거북이 4712 안녕 거북이 4713 안녕 거북이 4714 안녕 거북이 4715 안녕 거북이 4716 안녕 거북이 4717 안녕 거북이 4718 안녕 거북이 4719 안녕 거북이 4720 안녕 거북이 4721 안녕 거북이 4722 안녕 거북이 4723 안녕 거북이 4724 안녕 거북이 4725 안녕 거북이 4726 안녕 거북이 4727 안녕 거북이 4728 안녕 거북이 4729 안녕 거북이 4730 안녕 거북이 4731 안녕 거북이 4732 안녕 거북이 4733 안녕 거북이 4734 안녕 거북이 4735 안녕 거북이 4736 안녕 거북이 4737 안녕 거북이 4738 안녕 거북이 4739 안녕 거북이 4740 안녕 거북이 4741 안녕 거북이 4742 안녕 거북이 4743 안녕 거북이 4744 안녕 거북이 4745 안녕 거북이 4746 안녕 거북이 4747 안녕 거북이 4748 안녕 거북이 4749 안녕 거북이 4750 안녕 거북이 4751 안녕 거북이 4752 안녕 거북이 4753 안녕 거북이 4754 안녕 거북이 4755 안녕 거북이 4756 안녕 거북이 4757 안녕 거북이 4758 안녕 거북이 4759 안녕 거북이 4760 안녕 거북이 4761 안녕 거북이 4762 안녕 거북이 4763 안녕 거북이 4764 안녕 거북이 4765 안녕 거북이 4766 안녕 거북이 4767 안녕 거북이 4768 안녕 거북이 4769 안녕 거북이 4770 안녕 거북이 4771 안녕 거북이 4772 안녕 거북이 4773 안녕 거북이 4774 안녕 거북이 4775 안녕 거북이 4776 안녕 거북이 4777 안녕 거북이 4778 안녕 거북이 4779 안녕 거북이 4780 안녕 거북이 4781 안녕 거북이 4782 안녕 거북이 4783 안녕 거북이 4784 안녕 거북이 4785 안녕 거북이 4786 안녕 거북이 4787 안녕 거북이 4788 안녕 거북이 4789 안녕 거북이 4790 안녕 거북이 4791 안녕 거북이 4792 안녕 거북이 4793 안녕 거북이 4794 안녕 거북이 4795 안녕 거북이 4796 안녕 거북이 4797 안녕 거북이 4798 안녕 거북이 4799 안녕 거북이 4800 안녕 거북이 4801 안녕 거북이 4802 안녕 거북이 4803 안녕 거북이 4804 안녕 거북이 4805 안녕 거북이 4806 안녕 거북이 4807 안녕 거북이 4808 안녕 거북이 4809 안녕 거북이 4810 안녕 거북이 4811 안녕 거북이 4812 안녕 거북이 4813 안녕 거북이 4814 안녕 거북이 4815 안녕 거북이 4816 안녕 거북이 4817 안녕 거북이 4818 안녕 거북이 4819 안녕 거북이 4820 안녕 거북이 4821 안녕 거북이 4822 안녕 거북이 4823 안녕 거북이 4824 안녕 거북이 4825 안녕 거북이 4826 안녕 거북이 4827 안녕 거북이 4828 안녕 거북이 4829 안녕 거북이 4830 안녕 거북이 4831 안녕 거북이 4832 안녕 거북이 4833 안녕 거북이 4834 안녕 거북이 4835 안녕 거북이 4836 안녕 거북이 4837 안녕 거북이 4838 안녕 거북이 4839 안녕 거북이 4840 안녕 거북이 4841 안녕 거북이 4842 안녕 거북이 4843 안녕 거북이 4844 안녕 거북이 4845 안녕 거북이 4846 안녕 거북이 4847 안녕 거북이 4848 안녕 거북이 4849 안녕 거북이 4850 안녕 거북이 4851 안녕 거북이 4852 안녕 거북이 4853 안녕 거북이 4854 안녕 거북이 4855 안녕 거북이 4856 안녕 거북이 4857 안녕 거북이 4858 안녕 거북이 4859 안녕 거북이 4860 안녕 거북이 4861 안녕 거북이 4862 안녕 거북이 4863 안녕 거북이 4864 안녕 거북이 4865 안녕 거북이 4866 안녕 거북이 4867 안녕 거북이 4868 안녕 거북이 4869 안녕 거북이 4870 안녕 거북이 4871 안녕 거북이 4872 안녕 거북이 4873 안녕 거북이 4874 안녕 거북이 4875 안녕 거북이 4876 안녕 거북이 4877 안녕 거북이 4878 안녕 거북이 4879 안녕 거북이 4880 안녕 거북이 4881 안녕 거북이 4882 안녕 거북이 4883 안녕 거북이 4884 안녕 거북이 4885 안녕 거북이 4886 안녕 거북이 4887 안녕 거북이 4888 안녕 거북이 4889 안녕 거북이 4890 안녕 거북이 4891 안녕 거북이 4892 안녕 거북이 4893 안녕 거북이 4894 안녕 거북이 4895 안녕 거북이 4896 안녕 거북이 4897 안녕 거북이 4898 안녕 거북이 4899 안녕 거북이 4900 안녕 거북이 4901 안녕 거북이 4902 안녕 거북이 4903 안녕 거북이 4904 안녕 거북이 4905 안녕 거북이 4906 안녕 거북이 4907 안녕 거북이 4908 안녕 거북이 4909 안녕 거북이 4910 안녕 거북이 4911 안녕 거북이 4912 안녕 거북이 4913 안녕 거북이 4914 안녕 거북이 4915 안녕 거북이 4916 안녕 거북이 4917 안녕 거북이 4918 안녕 거북이 4919 안녕 거북이 4920 안녕 거북이 4921 안녕 거북이 4922 안녕 거북이 4923 안녕 거북이 4924 안녕 거북이 4925 안녕 거북이 4926 안녕 거북이 4927 안녕 거북이 4928 안녕 거북이 4929 안녕 거북이 4930 안녕 거북이 4931 안녕 거북이 4932 안녕 거북이 4933 안녕 거북이 4934 안녕 거북이 4935 안녕 거북이 4936 안녕 거북이 4937 안녕 거북이 4938 안녕 거북이 4939 안녕 거북이 4940 안녕 거북이 4941 안녕 거북이 4942 안녕 거북이 4943 안녕 거북이 4944 안녕 거북이 4945 안녕 거북이 4946 안녕 거북이 4947 안녕 거북이 4948 안녕 거북이 4949 안녕 거북이 4950 안녕 거북이 4951 안녕 거북이 4952 안녕 거북이 4953 안녕 거북이 4954 안녕 거북이 4955 안녕 거북이 4956 안녕 거북이 4957 안녕 거북이 4958 안녕 거북이 4959 안녕 거북이 4960 안녕 거북이 4961 안녕 거북이 4962 안녕 거북이 4963 안녕 거북이 4964 안녕 거북이 4965 안녕 거북이 4966 안녕 거북이 4967 안녕 거북이 4968 안녕 거북이 4969 안녕 거북이 4970 안녕 거북이 4971 안녕 거북이 4972 안녕 거북이 4973 안녕 거북이 4974 안녕 거북이 4975 안녕 거북이 4976 안녕 거북이 4977 안녕 거북이 4978 안녕 거북이 4979 안녕 거북이 4980 안녕 거북이 4981 안녕 거북이 4982 안녕 거북이 4983 안녕 거북이 4984 안녕 거북이 4985 안녕 거북이 4986 안녕 거북이 4987 안녕 거북이 4988 안녕 거북이 4989 안녕 거북이 4990 안녕 거북이 4991 안녕 거북이 4992 안녕 거북이 4993 안녕 거북이 4994 안녕 거북이 4995 안녕 거북이 4996 안녕 거북이 4997 안녕 거북이 4998 안녕 거북이 4999 123# 코드 4-3 리스트의 숫자를 차례대로 출력하는 코드for num in [0, 1, 2]: print(num) 0 1 2 1234# 코드 4-4 리스트의 문자열을 차례대로 출력하는 코드characters = ['앨리스', '도도새', '3월토끼']for character in characters: print(character) 앨리스 도도새 3월토끼 1234# 코드 4-5 모든 신하에게 퇴장 명령을 내리는 코드players = ['공작부인', '흰 토끼', '하트잭', '모자장수']for player in players: print(player, '퇴장!') 공작부인 퇴장! 흰 토끼 퇴장! 하트잭 퇴장! 모자장수 퇴장! 123# 코드 4-6 문자열의 문자를 하나씩 출력하는 코드for letter in '체셔고양이': print(letter) 체 셔 고 양 이 12345# 코드 4-7 리스트의 숫자를 차례대로 출력하는 코드nums = [0, 1, 2]for num in nums: print(num)print(nums) 0 1 2 [0, 1, 2] 12345# 코드 4-8 리스트의 숫자를 차례대로 출력하는 코드nums = [0, 1, 2]for num in nums: print(num) print(nums) 0 [0, 1, 2] 1 [0, 1, 2] 2 [0, 1, 2] 123# 코드 4-9 0부터 2까지 차례로 출력하는 코드for num in range(3): print(num) 0 1 2 123# 코드 4-10 구구단 2단을 출력하는 코드for y in range(1, 10): print(2, 'x', y, '=', 2 * y) 2 x 1 = 2 2 x 2 = 4 2 x 3 = 6 2 x 4 = 8 2 x 5 = 10 2 x 6 = 12 2 x 7 = 14 2 x 8 = 16 2 x 9 = 18 12345# 코드 4-11 모든 ‘하얀장미’를 ‘빨간장미’로 바꾸는 코드roses = ['하얀장미', '하얀장미', '하얀장미']for i in range(3): roses[i] = '빨간장미'print(roses) ['빨간장미', '빨간장미', '빨간장미'] 123# 코드 5-1 True와 False를 출력하는 코드print(True)print(False) True False 123# 코드 5-2 값을 비교하는 코드print(1 &lt; 2)print(2 &lt; 1) True False 123# 코드 5-3 값을 비교하는 코드print(2 &gt; 1)print(2 &gt; 2) True False 123# 코드 5-4 값을 비교하는 코드print(1 &lt;= 1)print(2 &lt;= 1) True False 123# 코드 5-5 값을 비교하는 코드print(1 &gt;= 1)print(1 &gt;= 2) True False 123# 코드 5-6 값을 비교하는 코드print(1 == 1)print(2 == 1) True False 123# 코드 5-7 값을 비교하는 코드print(1 != 2)print(2 != 2) True False 123# 코드 5-8 조건에 따라 출력하는 코드if True: print('참입니다.') 참입니다. 123# 코드 5-9 조건에 따라 출력하는 코드if False: print('참입니다.') 1234# 코드 5-10 점수에 따라 합격여부를 출력하는 코드score = 90if score &gt; 80: print('합격입니다.') 합격입니다. 123456# 코드 5-11 점수에 따라 합격여부를 출력하는 코드score = 60if score &gt; 80: print('합격입니다.')else: print('불합격입니다.') 불합격입니다. 12345678910# 코드 5-12 점수에 따라 학점을 출력하는 코드score = 75if 80 &lt; score &lt;= 100: print('학점은 A입니다.')elif 60 &lt; score &lt;= 80: print('학점은 B입니다.')elif 40 &lt; score &lt;= 60: print('학점은 C입니다.')else: print('학점은 F입니다.') 학점은 B입니다. 1234567891011# 코드 5-13 총 입장료를 계산하는 코드total_price = 0ages = [22, 21, 17, 32, 4, 28, 19, 8]for age in ages: if age &gt;= 20: total_price = total_price + 8000 elif age &gt;= 10: total_price = total_price + 5000 else: total_price = total_price + 2500print('총 입장료는', total_price, '원입니다.') 총 입장료는 47000 원입니다. 12345# 코드 5-14 여러 조건을 판단하는 코드games = 12points = 25if games &gt;= 10 and points &gt;= 20: print('MVP로 선정되었습니다.') MVP로 선정되었습니다. 12345# 코드 5-15 and의 결과를 확인하는 코드print(True and True)print(True and False)print(False and True)print(False and False) True False False False 12345# 코드 5-16 or의 결과를 확인하는 코드print(True or True)print(True or False)print(False or True)print(False or False) True True True False 123# 코드 5-17 not의 결과를 확인하는 코드print(not True)print(not False) False True 12345# 코드 5-18 인상착의로 범인을 잡아내는 코드suspects = [['거위', '새', '암컷'], ['푸들', '개', '수컷'], ['비글', '개', '암컷']]for suspect in suspects: if suspect[1] == '개' and suspect[2] == '암컷': print('범인은', suspect[0], '입니다.') 범인은 비글 입니다. 123456789101112131415161718192021222324252627282930import randomrps = ['가위', '바위', '보']while True: player = input('가위/바위/보/끝: ') computer = random.choice(rps) if player == '끝': break print(player, computer) if player == computer: print('비겼어요!') elif player == '가위': if computer == '바위': print('졌어요!') else: print('이겼어요!') elif player == '바위': if computer == '보': print('졌어요!') else: print('이겼어요!') elif player == '보': if computer == '가위': print('졌어요!') else: print('이겼어요!') 가위/바위/보/끝: 가위 가위 가위 비겼어요! 가위/바위/보/끝: 바위 바위 보 졌어요! 가위/바위/보/끝: 보 보 가위 졌어요! 가위/바위/보/끝: 끝 1234567# 코드 7-1 튜플을 만드는 코드my_tuple1 = ()print(my_tuple1)my_tuple2 = (1, -2, 3.14)print(my_tuple2)my_tuple3 = '앨리스', 10, 1.0, 1.2print(my_tuple3) () (1, -2, 3.14) ('앨리스', 10, 1.0, 1.2) 12345# 코드 7-2 값이 한 개인 튜플을 만드는 코드my_int = (1)print(type(my_int))my_tuple = (1,)print(type(my_tuple)) &lt;class 'int'&gt; &lt;class 'tuple'&gt; 123# 코드 7-3 튜플에서 값을 가져오는 코드clovers = ('클로버1', '하트2', '클로버3')print(clovers[1]) 하트2 123# 코드 7-4 튜플의 값을 변경하려는 코드clovers = ('클로버1', '하트2', '클로버3')clovers[1] = '클로버2' --------------------------------------------------------------------------- TypeError Traceback (most recent call last) &lt;ipython-input-68-84a1a3ec1095&gt; in &lt;module&gt;() 1 # 코드 7-4 튜플의 값을 변경하려는 코드 2 clovers = ('클로버1', '하트2', '클로버3') ----&gt; 3 clovers[1] = '클로버2' TypeError: 'tuple' object does not support item assignment 123456# 코드 7-5 패킹과 언패킹을 하는 코드clovers = '클로버1', '클로버2', '클로버3'print(clovers)alice_blue = (240, 248, 255)r, g, b = alice_blueprint('R:', r, 'G:', g, 'B:', b) ('클로버1', '클로버2', '클로버3') R: 240 G: 248 B: 255 123456# 코드 7-6 두 변수의 값을 서로 교환하는 코드dodo = '박하맛'alice = '딸기맛'print('도도새:', dodo, '앨리스:', alice)dodo, alice = alice, dodoprint('도도새:', dodo, '앨리스:', alice) 도도새: 박하맛 앨리스: 딸기맛 도도새: 딸기맛 앨리스: 박하맛 1234567# 코드 7-7 딕셔너리를 만드는 코드my_dict1 = {}print(my_dict1)my_dict2 = {0: 1, 1: -2, 2: 3.14}print(my_dict2)my_dict3 = {'이름': '앨리스', '나이': 10, '시력': [1.0, 1.2]}print(my_dict3) {} {0: 1, 1: -2, 2: 3.14} {'이름': '앨리스', '나이': 10, '시력': [1.0, 1.2]} 12345# 코드 7-8 딕셔너리에 키-값을 추가하는 코드clover = {'나이': 27, '직업': '병사'}print(clover)clover['번호'] = 9print(clover) {'나이': 27, '직업': '병사'} {'나이': 27, '직업': '병사', '번호': 9} 123456# 코드 7-9 딕셔너리의 값에 접근하는 코드clover = {'나이': 27, '직업': '병사', '번호': 9}print(clover['번호'])clover['번호'] = 6print(clover['번호'])print(clover.get('번호')) 9 6 6 12345# 코드 7-10 딕셔너리에서 키-값을 제거하는 코드clover = {'나이': 27, '직업': '병사', '번호': 6}print(clover)del clover['나이']print(clover) {'나이': 27, '직업': '병사', '번호': 6} {'직업': '병사', '번호': 6} 123456789# 코드 7-11 라면 주문을 추가/수정/취소하는 코드order = {'스페이드1': '비빔라면', '다이아2': '매운라면'}print(order)order['클로버3'] = '카레라면'print(order)order['다이아2'] = '짜장라면'print(order)del order['스페이드1']print(order) {'스페이드1': '비빔라면', '다이아2': '매운라면'} {'스페이드1': '비빔라면', '다이아2': '매운라면', '클로버3': '카레라면'} {'스페이드1': '비빔라면', '다이아2': '짜장라면', '클로버3': '카레라면'} {'다이아2': '짜장라면', '클로버3': '카레라면'} 12345678910# 코드 8-1 모든 카드 병사에게 유죄 판결을 내리는 코드print('하트 1 유죄!')print('하트 2 유죄!')print('하트 3 유죄!')print('클로버 1 유죄!')print('클로버 2 유죄!')print('클로버 3 유죄!')print('스페이드 1 유죄!')print('스페이드 2 유죄!')print('스페이드 3 유죄!') 하트 1 유죄! 하트 2 유죄! 하트 3 유죄! 클로버 1 유죄! 클로버 2 유죄! 클로버 3 유죄! 스페이드 1 유죄! 스페이드 2 유죄! 스페이드 3 유죄! 12345# 코드 8-2 함수를 사용해 문자열을 출력하는 코드def my_func(): print('토끼야 안녕!') my_func() 토끼야 안녕! 12345# 코드 8-3 함수를 사용해 두 개의 숫자를 더하는 코드def add(num1, num2): return num1 + num2 print(add(2, 3)) 5 12345# 코드 8-4 함수를 사용해 두 개의 숫자를 더하고 곱하는 코드def add_mul(num1, num2): return num1 + num2, num1 * num2print(add_mul(2, 3)) (5, 6) 123456789# 코드 8-5 함수를 사용해 판결을 내리는 코드def judge_cards(name): print(name, '1 유죄!') print(name, '2 유죄!') print(name, '3 유죄!')judge_cards('하트')judge_cards('클로버')judge_cards('스페이드') 하트 1 유죄! 하트 2 유죄! 하트 3 유죄! 클로버 1 유죄! 클로버 2 유죄! 클로버 3 유죄! 스페이드 1 유죄! 스페이드 2 유죄! 스페이드 3 유죄! 1234# 코드 8-6 하나의 값을 임의로 선택하는 코드import randomanimals = ['체셔고양이', '오리', '도도새']print(random.choice(animals)) 도도새 1234# 코드 8-7 여러 개의 값을 임의로 선택하는 코드import randomanimals = ['체셔고양이', '오리', '도도새']print(random.sample(animals, 2)) ['체셔고양이', '오리'] 123# 코드 8-8 지정한 범위에서 임의의 정수를 선택하는 코드import randomprint(random.randint(5, 10)) 5 12345# 코드 8-9 임의로 카드 하나를 뽑는 코드import randomcards = ['하트', '클로버', '스페이드']chosen_card = random.choice(cards)print(chosen_card, '유죄!') 클로버 유죄! 1help('modules') Please wait a moment while I gather a list of all available modules... /usr/local/lib/python3.6/dist-packages/IPython/kernel/__init__.py:13: ShimWarning: The `IPython.kernel` package has been deprecated since IPython 4.0.You should import from ipykernel or jupyter_client instead. &quot;You should import from ipykernel or jupyter_client instead.&quot;, ShimWarning) /usr/local/lib/python3.6/dist-packages/datascience/tables.py:17: MatplotlibDeprecationWarning: The 'warn' parameter of use() is deprecated since Matplotlib 3.1 and will be removed in 3.3. If any parameter follows 'warn', they should be pass as keyword, not positionally. /usr/local/lib/python3.6/dist-packages/datascience/util.py:10: MatplotlibDeprecationWarning: The 'warn' parameter of use() is deprecated since Matplotlib 3.1 and will be removed in 3.3. If any parameter follows 'warn', they should be pass as keyword, not positionally. /usr/local/lib/python3.6/dist-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use &quot;pip install psycopg2-binary&quot; instead. For details see: &lt;http://initd.org/psycopg/docs/install.html#binary-install-from-pypi&gt;. /usr/local/lib/python3.6/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/). /usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API. /usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.datasets.base module is deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.datasets. Anything that cannot be imported from sklearn.datasets is now part of the private API. /usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.ensemble.bagging module is deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API. /usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.ensemble.base module is deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API. /usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.ensemble.forest module is deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API. /usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+. /usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.utils.testing module is deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.utils. Anything that cannot be imported from sklearn.utils is now part of the private API. /usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.metrics.classification module is deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API. Downloading http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2 to /root/mlxtend_data/shape_predictor_68_face_landmarks.dat.bz2 /usr/lib/python3.6/pkgutil.py:92: UserWarning: The DICOM readers are highly experimental, unstable, and only work for Siemens time-series at the moment Please use with caution. We would be grateful for your help in improving them /usr/local/lib/python3.6/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available. Exception ignored in: &lt;_io.FileIO name='/usr/local/lib/python3.6/dist-packages/theano/gof/c_code/lazylinker_c.c' mode='rb' closefd=True&gt; ResourceWarning: unclosed file &lt;_io.TextIOWrapper name='/usr/local/lib/python3.6/dist-packages/theano/gof/c_code/lazylinker_c.c' mode='r' encoding='UTF-8'&gt; WARNING:pystan:This submodule contains experimental code, please use with caution Cython concurrent knnimpute regex IPython configparser korean_lunar_calendar reprlib OpenGL contextlib lib2to3 requests PIL contextlib2 libfuturize requests_oauthlib ScreenResolution convertdate libpasteurize resampy __future__ copy librosa resource _ast copyreg lightgbm retrying _asyncio coverage linecache rlcompleter _bisect coveralls llvmlite rmagic _blake2 crcmod lmdb rpy2 _bootlocale crypt locale rsa _bz2 csv logging runpy _cffi_backend ctypes lsb_release samples _codecs cufflinks lucid sched _codecs_cn curses lunarcalendar scipy _codecs_hk cv2 lxml scs _codecs_iso2022 cvxopt lzma seaborn _codecs_jp cvxpy macpath secrets _codecs_kr cycler macurl2path select _codecs_tw cymem mailbox selectors _collections cython mailcap send2trash _collections_abc cythonmagic markdown setuptools _compat_pickle daft markupsafe setuptools_git _compression dask marshal shapely _crypt dataclasses math shelve _csv datascience matplotlib shlex _ctypes datetime matplotlib_venn shutil _ctypes_test dateutil mimetypes signal _curses dbm missingno simplegeneric _curses_panel dbus mistune site _cvxcore debugpy mizani sitecustomize _datetime decimal mlxtend six _dbm decorator mmap skimage _dbus_bindings defusedxml modulefinder sklearn _dbus_glib_bindings descartes more_itertools sklearn_pandas _decimal difflib moviepy slugify _distutils_hack dill mpmath smart_open _dummy_thread dis msgpack smtpd _ecos distributed multiprocess smtplib _elementtree distutils multiprocessing sndhdr _foo django multitasking snowballstemmer _functools dlib murmurhash socket _hashlib docopt music21 socketserver _heapq docs natsort socks _imp doctest nbclient sockshandler _io docutils nbconvert softwareproperties _json dopamine nbformat sortedcontainers _locale dot_parser nest_asyncio spacy _lsprof dummy_threading netrc sphinx _lzma easy_install networkx spwd _markupbase easydict nibabel sql _md5 ecos nis sqlalchemy _multibytecodec editdistance nisext sqlite3 _multiprocessing ee nltk sqlparse _opcode email nntplib sre_compile _operator en_core_web_sm notebook sre_constants _osx_support encodings np_utils sre_parse _pickle entrypoints ntpath srsly _plotly_future_ enum nturl2path ssl _plotly_utils ephem numba stat _posixsubprocess errno numbergen statistics _pydecimal et_xmlfile numbers statsmodels _pyio examples numexpr storemagic _pyrsistent_version fa2 numpy string _pytest fancyimpute nvidia_smi stringprep _random fastai oauth2client struct _rinterface_cffi_abi fastdtw oauthlib subprocess _rinterface_cffi_api fastprogress ogr sunau _scs_direct fastrlock okgrade symbol _scs_indirect faulthandler onnx_chainer sympy _scs_python fbprophet opcode sympyprinting _sha1 fcntl openpyxl symtable _sha256 feather operator sys _sha3 filecmp opt_einsum sysconfig _sha512 fileinput optparse syslog _signal filelock os tables _sitebuiltins firebase_admin osgeo tabnanny _socket fix_yahoo_finance osqp tabulate _sqlite3 flask osqppurepy tarfile _sre fnmatch osr tblib _ssl folium ossaudiodev telnetlib _stat formatter packaging tempfile _string fractions palettable tensorboard _strptime ftplib pandas tensorboard_plugin_wit _struct functools pandas_datareader tensorboardcolab _symtable future pandas_gbq tensorflow _sysconfigdata_m_linux_x86_64-linux-gnu gast pandas_profiling tensorflow_addons _testbuffer gc pandocfilters tensorflow_datasets _testcapi gdal panel tensorflow_estimator _testimportmultiple gdalconst param tensorflow_gcs_config _testmultiphase gdalnumeric parser tensorflow_hub _thread gdown parso tensorflow_metadata _threading_local genericpath past tensorflow_privacy _tkinter gensim pasta tensorflow_probability _tracemalloc geographiclib pathlib termcolor _warnings geopy patsy terminado _weakref getopt pdb termios _weakrefset getpass pexpect test abc gettext pickle testpath absl gi pickleshare tests aifc gin pickletools text_unidecode alabaster github2pypi pip textblob albumentations glob pipes textgenrnn altair glob2 piptools textwrap antigravity gnm pkg_resources theano apiclient google_auth_httplib2 pkgutil thinc apt google_auth_oauthlib plac this apt_inst google_drive_downloader plac_core threading apt_pkg googleapiclient plac_ext tifffile aptsources googlesearch plac_tk time argon2 graphviz platform timeit argparse gridfs plistlib tkinter array grp plotly tlz asgiref grpc plotlywidget token ast gspread plotnine tokenize astor gspread_dataframe pluggy toml astropy gym poplib toolz astunparse gzip portpicker torch async_generator h5py posix torchsummary asynchat hashlib posixpath torchtext asyncio heapdict pprint torchvision asyncore heapq prefetch_generator tornado atari_py hmac preshed tqdm atexit holidays prettytable trace atomicwrites holoviews profile traceback attr html progressbar tracemalloc audioop html5lib prometheus_client traitlets audioread http promise tree autograd httpimport prompt_toolkit tty autoreload httplib2 pstats turtle babel httplib2shim psutil tweepy backcall humanize psycopg2 typeguard base64 hyperopt pty types bdb ideep4py ptyprocess typing bin idna pvectorc typing_extensions binascii image pwd tzlocal binhex imageio py umap bisect imagesize py_compile unicodedata bleach imaplib pyarrow unittest blis imblearn pyasn1 uritemplate bokeh imgaug pyasn1_modules urllib boost imghdr pyclbr urllib3 bottleneck imp pycocotools uu branca importlib pycparser uuid bs4 importlib_metadata pyct vega_datasets bson importlib_resources pydata_google_auth venv builtins imutils pydoc vis bz2 inflect pydoc_data warnings cProfile iniconfig pydot wasabi cachecontrol inspect pydot_ng wave cachetools intervaltree pydotplus wcwidth caffe2 io pydrive weakref calendar ipaddress pyemd webbrowser catalogue ipykernel pyexpat webencodings certifi ipykernel_launcher pyglet werkzeug cffi ipython_genutils pygments wheel cgi ipywidgets pygtkcompat widgetsnbextension cgitb itertools pylab wordcloud chainer itsdangerous pymc3 wrapt chainermn jax pymeeus wsgiref chainerx jaxlib pymongo xarray chardet jdcal pymystem3 xdrlib chess jedi pynvml xgboost chunk jieba pyparsing xkit click jinja2 pyrsistent xlrd client joblib pysndfile xlwt cloudpickle jpeg4py pystan xml cmake json pytest xmlrpc cmath jsonschema python_utils xxlimited cmd jupyter pytz xxsubtype cmdstanpy jupyter_client pyviz_comms yaml code jupyter_console pywt yellowbrick codecs jupyter_core pyximport zict codeop jupyterlab_pygments qtconsole zipapp colab kaggle qtpy zipfile collections kapre queue zipimport colorlover keras quopri zipp colorsys keras_preprocessing random zlib community keyword re zmq compileall kiwisolver readline Enter any module name to get more help. Or, type &quot;modules spam&quot; to search for modules whose name or summary contain the string &quot;spam&quot;.","link":"/2020/11/12/study/hello_coding_python/"},{"title":"캐글 카사바 잎 질병 분류 파파고 번역","text":"출처: Yaroslav Isaienkov, Cassava Leaf Disease - Exploratory Data Analysis, 캐글, 2020.11.29https://www.kaggle.com/ihelon/cassava-leaf-disease-exploratory-data-analysis 접기/펼치기 구글 연동 (생략)1!pip install kaggle 12345678from google.colab import filesuploaded = files.upload()for fn in uploaded.keys(): print('uploaded file &quot;{name}&quot; with length {length} bytes'.format( name=fn, length=len(uploaded[fn]))) # kaggle.json을 아래 폴더로 옮긴 뒤, file을 사용할 수 있도록 권한을 부여한다. !mkdir -p ~/.kaggle/ &amp;&amp; mv kaggle.json ~/.kaggle/ &amp;&amp; chmod 600 ~/.kaggle/kaggle.json 1ls -1ha ~/.kaggle/kaggle.json 123456789101112from google.colab import drive # 패키지 불러오기 from os.path import join # 구글 드라이브 마운트ROOT = &quot;/content/drive&quot; # 드라이브 기본 경로print(ROOT) # print content of ROOT (Optional)drive.mount(ROOT) # 드라이브 기본 경로 # 프로젝트 파일 생성 및 다운받을 경로 이동MY_GOOGLE_DRIVE_PATH = 'My Drive/Colab Notebooks/python_basic/kaggle_cassava-leaf-disease-classification/data'PROJECT_PATH = join(ROOT, MY_GOOGLE_DRIVE_PATH)print(PROJECT_PATH) 1%cd &quot;{PROJECT_PATH}&quot; 카사바 잎 질병 분류Cassava Leaf Disease Classificationhttps://www.kaggle.com/c/cassava-leaf-disease-classification이미지에 존재하는 질병 유형 식별Identify the type of disease present on a Cassava Leaf image Description아프리카에서 두 번째로 많은 탄수화물을 공급하고 있는 카사바는 가혹한 조건을 견뎌낼 수 있기 때문에 소작농들이 재배하는 주요 식량안보 작물이다.As the second-largest provider of carbohydrates in Africa, cassava is a key food security crop grown by smallholder farmers because it can withstand harsh conditions.아프리카 사하라 사막 이남의 가정 농장의 80% 이상이 이 녹농 뿌리를 기르고 있지만 바이러스성 질병은 수확량이 저조한 주요 원인이다.At least 80% of household farms in Sub-Saharan Africa grow this starchy root, but viral diseases are major sources of poor yields.데이터 과학의 도움으로, 일반적인 질병들이 치료될 수 있도록 식별하는 것이 가능할 수도 있다.With the help of data science, it may be possible to identify common diseases so they can be treated. 기존의 질병감지 방법은 농업인이 정부출연 농업전문가의 도움을 받아 식물을 육안으로 검사하고 진단할 수 있도록 해야 한다.Existing methods of disease detection require farmers to solicit the help of government-funded agricultural experts to visually inspect and diagnose the plants.이것은 노동집약적이고, 공급량이 적고, 비용이 많이 드는 것으로 고통받고 있다.This suffers from being labor-intensive, low-supply and costly.추가적인 도전으로서, 아프리카 농부들은 낮은 대역폭의 모바일 퀄리티 카메라에만 접근할 수 있기 때문에 농부들을 위한 효과적인 해결책은 상당한 제약 조건 하에서 좋은 성과를 거두어야 한다.As an added challenge, effective solutions for farmers must perform well under significant constraints, since African farmers may only have access to mobile-quality cameras with low-bandwidth. 이번 대회에서는 우간다의 정기 조사 때 수집한 21,367개의 라벨 이미지 데이터 세트를 소개한다.In this competition, we introduce a dataset of 21,367 labeled images collected during a regular survey in Uganda.대부분의 이미지는 그들의 정원을 사진 찍는 농부들로부터 크라우드소싱되었고, 캄팔라 소재 마케레대학의 AI 연구소와 협력하여 국립작물자원연구소(NaCRRI)의 전문가들이 주석을 달았다.Most images were crowdsourced from farmers taking photos of their gardens, and annotated by experts at the National Crops Resources Research Institute (NaCRRI) in collaboration with the AI lab at Makerere University, Kampala.이것은 농부들이 실생활에서 진단해야 할 것을 가장 현실적으로 나타내는 형식이다.This is in a format that most realistically represents what farmers would need to diagnose in real life. Data Description비교적 저렴한 카메라의 사진을 사용하여 카사바 공장의 문제점을 식별할 수 있는가?Can you identify a problem with a cassava plant using a photo from a relatively inexpensive camera?이 대회는 많은 아프리카 국가들의 식량 공급에 물질적인 해를 끼치는 여러 질병들을 구별하는 것에 도전할 것이다.This competition will challenge you to distinguish between several diseases that cause material harm to the food supply of many African countries.어떤 경우에는 더 이상의 확산을 막기 위해 감염된 식물을 태우는 것이 주요 치료법인데, 이것은 농부들에게 꽤 유용한 빠른 자동 전환이 될 수 있다.In some cases the main remedy is to burn the infected plants to prevent further spread, which can make a rapid automated turnaround quite useful to the farmers. Files[train/test]_images the image files.테스트 이미지의 전체 세트는 노트북이 채점을 위해 제출되었을 때만 사용할 수 있다.The full set of test images will only be available to your notebook when it is submitted for scoring.테스트 세트에서 약 15,000개의 이미지를 볼 수 있을 것으로 예상한다.Expect to see roughly 15,000 images in the test set.train.csv image_id the image file name. label 질병의 ID 코드 (the ID code for the disease) sample_submission.csv공개된 테스트 세트 내용을 고려할 때 적절한 형식의 샘플 제출.A properly formatted sample submission, given the disclosed test set content. image_id the image file name. label 질병의 예상 ID 코드 (the predicted ID code for the disease) [train/test]_tfrecordstfrecord 형식의 이미지 파일the image files in tfrecord format.label_num_to_disease_map.json각 질병 코드와 실제 질병 이름 간의 매핑.The mapping between each disease code and the real disease name. 카사바 잎 질병 - EDA(탐색적 데이터 분석)Cassava Leaf Disease - Exploratory Data Analysishttps://www.kaggle.com/ihelon/cassava-leaf-disease-exploratory-data-analysis Cassava Leaf 질병 분류 과제를 위한 빠른 탐색 데이터 분석Quick Exploratory Data Analysis for Cassava Leaf Disease Classification challenge이 대회는 많은 아프리카 국가들의 식량 공급에 물질적인 해를 끼치는 여러 질병들을 구별하는 것에 도전할 것이다.This competition will challenge you to distinguish between several diseases that cause material harm to the food supply of many African countries. 어떤 경우에는 더 이상의 확산을 막기 위해 감염된 식물을 태우는 것이 주요 치료법인데, 이것은 농부들에게 꽤 유용한 빠른 자동 전환이 될 수 있다.In some cases the main remedy is to burn the infected plants to prevent further spread, which can make a rapid automated turnaround quite useful to the farmers. Overview12345678910import osimport jsonimport numpy as npimport pandas as pdimport seaborn as snimport matplotlib.pyplot as pltimport cv2import albumentations as Afrom sklearn import metrics as sk_metrics 1BASE_DIR = &quot;../input/cassava-leaf-disease-classification/&quot; 이번 대회에는 5개의 수업이 있다: 4개의 질병과 1개의 건강클래스 번호와 클래스 이름 간의 매핑은 파일 label_num_to_disease_map.json에서 찾을 수 있다. In this competition we have 5 classes: 4 diseases and 1 healthyWe can find the mapping between the class number and its name in the file label_num_to_disease_map.json 1234with open(os.path.join(PROJECT_PATH, &quot;label_num_to_disease_map.json&quot;)) as file: map_classes = json.loads(file.read()) print(json.dumps(map_classes, indent=4)) { &quot;0&quot;: &quot;Cassava Bacterial Blight (CBB)&quot;, &quot;1&quot;: &quot;Cassava Brown Streak Disease (CBSD)&quot;, &quot;2&quot;: &quot;Cassava Green Mottle (CGM)&quot;, &quot;3&quot;: &quot;Cassava Mosaic Disease (CMD)&quot;, &quot;4&quot;: &quot;Healthy&quot; } 12input_files = os.listdir(os.path.join(BASE_DIR, &quot;train_images&quot;))print(f&quot;Number of train images: {len(input_files)}&quot;) 이하 용량이 커서 결과 나중에 처음 300개의 이미지 치수를 살펴봅시다.아래에서 볼 수 있듯이 모든 이미지는 크기가 동일하다(600, 800, 3)Let’s take a look at the dimensions of the first 300 imagesAs you can see below, all images are the same size (600, 800, 3) 123456img_shapes = {}for image_name in os.listdir(os.path.join(BASE_DIR, &quot;train_images&quot;))[:300]: image = cv2.imread(os.path.join(BASE_DIR, &quot;train_images&quot;, image_name)) img_shapes[image.shape] = img_shapes.get(image.shape, 0) + 1print(img_shapes) 교육 데이터 프레임을 로드하고 실제 클래스 이름이 포함된 열을 추가합시다.Let’s load the training dataframe and add a column with the real class name to it. 12345df_train = pd.read_csv(os.path.join(BASE_DIR, &quot;train.csv&quot;))df_train[&quot;class_name&quot;] = df_train[&quot;label&quot;].astype(str).map(map_classes)df_train 각 반의 사진 수를 살펴보자.Let’s look at the number of pictures in each class. 12plt.figure(figsize=(8, 4))sn.countplot(y=&quot;class_name&quot;, data=df_train); 우리가 알 수 있듯이 데이터 집합은 상당히 큰 불균형을 가지고 있다.As we can see, the dataset has a fairly large imbalance. 일반 시각화 (General Visualization)12345678910111213def visualize_batch(image_ids, labels): plt.figure(figsize=(16, 12)) for ind, (image_id, label) in enumerate(zip(image_ids, labels)): plt.subplot(3, 3, ind + 1) image = cv2.imread(os.path.join(BASE_DIR, &quot;train_images&quot;, image_id)) image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) plt.imshow(image) plt.title(f&quot;Class: {label}&quot;, fontsize=12) plt.axis(&quot;off&quot;) plt.show() 12345tmp_df = df_train.sample(9)image_ids = tmp_df[&quot;image_id&quot;].valueslabels = tmp_df[&quot;class_name&quot;].valuesvisualize_batch(image_ids, labels) Cassava Bacterial Blight (CBB)The image from discussion: Cassava Lead Diseases: Overview 12345678tmp_df = df_train[df_train[&quot;label&quot;] == 0]print(f&quot;Total train images for class 0: {tmp_df.shape[0]}&quot;)tmp_df = tmp_df.sample(9)image_ids = tmp_df[&quot;image_id&quot;].valueslabels = tmp_df[&quot;label&quot;].valuesvisualize_batch(image_ids, labels) Cassava Brown Streak Disease (CBSD)The image from discussion: Cassava Lead Diseases: Overview 12345678tmp_df = df_train[df_train[&quot;label&quot;] == 1]print(f&quot;Total train images for class 1: {tmp_df.shape[0]}&quot;)tmp_df = tmp_df.sample(9)image_ids = tmp_df[&quot;image_id&quot;].valueslabels = tmp_df[&quot;label&quot;].valuesvisualize_batch(image_ids, labels) Cassava Green Mottle (CGM)The image from discussion: Cassava Lead Diseases: Overview 12345678tmp_df = df_train[df_train[&quot;label&quot;] == 2]print(f&quot;Total train images for class 2: {tmp_df.shape[0]}&quot;)tmp_df = tmp_df.sample(9)image_ids = tmp_df[&quot;image_id&quot;].valueslabels = tmp_df[&quot;label&quot;].valuesvisualize_batch(image_ids, labels) Cassava Mosaic Disease (CMD)The image from discussion: Cassava Lead Diseases: Overview 12345678tmp_df = df_train[df_train[&quot;label&quot;] == 3]print(f&quot;Total train images for class 3: {tmp_df.shape[0]}&quot;)tmp_df = tmp_df.sample(9)image_ids = tmp_df[&quot;image_id&quot;].valueslabels = tmp_df[&quot;label&quot;].valuesvisualize_batch(image_ids, labels) Healthy12345678tmp_df = df_train[df_train[&quot;label&quot;] == 4]print(f&quot;Total train images for class 4: {tmp_df.shape[0]}&quot;)tmp_df = tmp_df.sample(9)image_ids = tmp_df[&quot;image_id&quot;].valueslabels = tmp_df[&quot;label&quot;].valuesvisualize_batch(image_ids, labels) 확대 예제 (Augmentation Examples)이미지 증가는 기존 교육 사례에서 새로운 교육 사례를 만드는 과정이다.Image augmentation is a process of creating new training examples from the existing ones. 새로운 샘플을 만들려면 원본 이미지를 약간 변경하십시오.To make a new sample, you slightly change the original image. 예를 들어, 새로운 이미지를 조금 더 밝게 만들 수 있다.For instance, you could make a new image a little brighter; 원본 이미지에서 조각을 잘라낼 수 있다.you could cut a piece from the original image; 원래 이미지를 미러링하는 등의 방법으로 새로운 이미지를 만들 수 있다.you could make a new image by mirroring the original one, etc. source The image from the Albumentations Documentation 1234567891011121314151617181920def plot_augmentation(image_id, transform): plt.figure(figsize=(16, 4)) img = cv2.imread(os.path.join(BASE_DIR, &quot;train_images&quot;, image_id)) img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) plt.subplot(1, 3, 1) plt.imshow(img) plt.axis(&quot;off&quot;) plt.subplot(1, 3, 2) x = transform(image=img)[&quot;image&quot;] plt.imshow(x) plt.axis(&quot;off&quot;) plt.subplot(1, 3, 3) x = transform(image=img)[&quot;image&quot;] plt.imshow(x) plt.axis(&quot;off&quot;) plt.show() 우리는 몇몇 수업의 수가 상당히 제한되어 있기 때문에, 우리는 증강을 사용할 수 있다.Since we have a fairly limited number of some classes, we can use augmentation 이 절은 연금술 라이브러리를 사용한 증축의 예를 보여준다.This section shows examples of augmentation using the albumentations library 아래 예제는 지정학적 가장자리 보완과 함께 회전-시프트-척도 확대를 사용한다.The example below uses rotate-shift-scale augmentation with specular edge complementation. 이런 종류의 사진을 보면, 이런 증가는 꽤 자연스러워 보인다.For this kind of pictures, this augmentation looks quite natural. 12345678910transform_shift_scale_rotate = A.ShiftScaleRotate( p=1.0, shift_limit=(-0.3, 0.3), scale_limit=(-0.1, 0.1), rotate_limit=(-180, 180), interpolation=0, border_mode=4, )plot_augmentation(&quot;1003442061.jpg&quot;, transform_shift_scale_rotate) 또 다른 유용한 증가는 ThoughDropout일 수 있다.Another useful augmentation could be CoarseDropout. 이 확대 덕분에, 당신은 모델의 수명을 복잡하게 만들 수 있어서 그녀가 이미지의 일부 세부사항을 너무 자세히 보지 않도록 할 수 있다.Thanks to this augmentation, you can complicate the life of the model so that she does not look too closely at some of the details of the image. 아래의 예를 보자.Let’s look at the example below: 1234567891011transform_coarse_dropout = A.CoarseDropout( p=1.0, max_holes=100, max_height=50, max_width=50, min_holes=30, min_height=20, min_width=20,)plot_augmentation(&quot;1003442061.jpg&quot;, transform_coarse_dropout) 우리는 두 개 이상의 증강을 하나의 과정으로 구성할 수 있다.We can compose two or more augmentations into one process. 예를 들어 shift-scale-rotate 및 ThoughDropout을 일관되게 사용합시다.For example, let’s use shift-scale-rotate and CoarseDropout consistently: 123456789transform = A.Compose( transforms=[ transform_shift_scale_rotate, transform_coarse_dropout, ], p=1.0,)plot_augmentation(&quot;1003442061.jpg&quot;, transform) Submission Example제출 템플릿 로드Load the submission template 12df_sub = pd.read_csv(&quot;../input/cassava-leaf-disease-classification/sample_submission.csv&quot;, index_col=0)df_sub 제출 파일에서 하나의 파일만 볼 수 있기 때문에As we can see only one file in the submission file 1os.listdir(os.path.join(BASE_DIR, &quot;test_images&quot;)) 코드 대회인데다 시험 데이터가 숨겨져 있기 때문이다.This is because it is a Code Competition, and the test data is hidden 노트북에서 볼 수 없는 테스트 데이터 집합으로 작업을 수정해야 함Your notebook should correct working with unseen test dataset 테스트 이미지의 전체 세트는 노트북이 채점을 위해 제출되었을 때만 사용할 수 있다.The full set of test images will only be available to your notebook when it is submitted for scoring. 테스트 세트에서 약 15,000개의 이미지를 볼 수 있을 것으로 예상한다.Expect to see roughly 15,000 images in the test set. 이 경기의 척도는 정확성이다.The metric of this competition is Accuracy. 정확도 - 총 표본 수에 대해 올바르게 예측된 표본 수의 비율Accuracy - the ratio of the number of samples predicted correctly to the total number of samples 모든 예에 대해 하나의 클래스만 선택하면 교육 세트의 정확도를 계산해 봅시다.Let’s calculate the accuracy on a training set if we select only one class for all examples. 1234for pred_class in range(0, 5): y_true = df_train[&quot;label&quot;].values y_pred = np.full_like(y_true, pred_class) print(f&quot;accuracy score (predict {pred_class}): {sk_metrics.accuracy_score(y_true, y_pred):.3f}&quot;) 우리는 계층의 불균형이 크기 때문에 가장 빈번한 수업을 예측하면 이 경우 정확도가 더 크다.Since we have a large imbalance of classes, if we predict the most frequent class, then our accuracy is greater in this case 테스트 세트에 있는 모든 이미지의 라벨로 가장 인기 있는 트레이닝 세트의 클래스를 선택하자.Let’s choose the most popular class of training set as the label for all images in test set 1df_sub[&quot;label&quot;] = 3 그리고 나서 제출 파일에 결과를 쓰세요.And then write result to the submission file 1df_sub.to_csv(&quot;submission.csv&quot;) 평가를 위해 결과를 제출하면 공용 라이더보드(열차 위는 0.615)에서 0.614의 정확도를 얻는다.If you submit the result for evaluation, you will get an accuracy of 0.614 on a public liderboard (on the train it is 0.615). 이는 공공 시험 분포에 대한 계층의 불균형도 있음을 나타낼 수 있다.This may indicate that there is also an imbalance of classes on the public test distribution. WORK IN PROGRESS…","link":"/2020/11/26/study/kaggle_cassava_leaf_disease_classification/"},{"title":"PyQt5 Tutorial - PyQt5 기초 (Basics)","text":"출처: 김민휘, PyQt5 Tutorial - 파이썬으로 만드는 나만의 GUI 프로그램, Codetorialhttps://codetorial.net/pyqt5/basics/opening.html 접기/펼치기 PyQt5 기초 (Basics)창 띄우기출처: Codetorial 예제123456789101112131415161718192021import sysfrom PyQt5.QtWidgets import QApplication, QWidgetclass MyApp(QWidget): def __init__(self): super().__init__() self.initUI() def initUI(self): self.setWindowTitle('My First Application') self.move(300, 300) self.resize(400, 200) self.show()if __name__ == '__main__': app = QApplication(sys.argv) ex = MyApp() sys.exit(app.exec_()) 결과(Windows7 환경에서 실행) 설명12import sysfrom PyQt5.QtWidgets import QApplication, QWidget 기본적인 UI 구성요소를 제공하는 위젯(클래스)은 PyQt5.QtWidgets 모듈에 포함돼 있다. QtWidgets 공식 문서 1234567891011class MyApp(QWidget): def __init__(self): super().__init__() self.initUI() def initUI(self): self.setWindowTitle('My First Application') self.move(300, 300) self.resize(400, 200) self.show() self MyApp 객체를 말한다. setWindowTitle() 타이틀바에 나타나는 창의 제목을 설정한다. move() 위젯을 스크린의 x = 300px, y = 300px의 위치로 이동시킨다. resize() 위젯의 크기를 너비 400px, 높이 200px로 조절한다. show() 위젯을 스크린에 보여준다. 1234if __name__ == '__main__': app = QApplication(sys.argv) ex = MyApp() sys.exit(app.exec_()) if __name__ == '__main__':__name__은 현재 모듈의 이름이 저장되는 내장 변수이다. 예를 들어 ‘test.py’라는 코드를 import해서 예제 코드를 실행하면 __name__은 ‘test’가 된다. 그렇지 않고 코드를 직접 실행한다면 __name__은 __main__이 된다. 이 한 줄의 코드를 통해 프로그램이 직접 실행되는 지, 모듈을 통해 실행되는 지 확인할 수 있다. app = QApplication(sys.argv)모든 PyQt5 어플리케이션은 어플리케이션 객체를 생성해야 한다. QApplication 공식 문서 어플리케이션 아이콘 넣기출처: Codetorial 예제12345678910111213141516171819202122import sysfrom PyQt5.QtWidgets import QApplication, QWidgetfrom PyQt5.QtGui import QIconclass MyApp(QWidget): def __init__(self): super().__init__() self.initUI() def initUI(self): self.setWindowTitle('Icon') self.setWindowIcon(QIcon('image/web.png')) self.setGeometry(300, 300, 300, 200) self.show()if __name__ == '__main__': app = QApplication(sys.argv) ex = MyApp() sys.exit(app.exec_()) 결과 설명1self.setWindowIcon(QIcon('image/web.png')) setWindowIcon() 메소드는 어플리케이션 아이콘을 설정하도록 한다.이를 위해서 QIcon 객체를 생성했고, QIcon()에 보여질 이미지를 입력한다. (경로 확인) 1self.setGeometry(300, 300, 300, 200) setGeometry() 메소드는 창의 위치와 크기를 설정한다.앞의 두 매개변수는 창의 x, y 위치를 결정하고, 뒤의 두 매개변수는 각각 창의 너비와 높이를 결정한다. 이 메소드는 창 띄우기 예제에서 사용했던 move()와 resize() 메서드를 하나로 합쳐놓은 것과 같다. 창 닫기출처: Codetorial 예제1234567891011121314151617181920212223242526import sysfrom PyQt5.QtWidgets import QApplication, QWidget, QPushButtonfrom PyQt5.QtCore import QCoreApplicationclass MyApp(QWidget): def __init__(self): super().__init__() self.initUI() def initUI(self): btn = QPushButton('Quit', self) btn.move(50, 50) btn.resize(btn.sizeHint()) btn.clicked.connect(QCoreApplication.instance().quit) self.setWindowTitle('Quit Button') self.setGeometry(300, 300, 300, 200) self.show()if __name__ == '__main__': app = QApplication(sys.argv) ex = MyApp() sys.exit(app.exec_()) 설명1btn = QPushButton('Quit', self) 푸시버튼을 하나 만든다. 이 btn은 QPushButton 클래스의 인스턴스이다.첫번째 파라미터에는 버튼에 표시될 텍스트(Quit)를 입력하고, 두번째 파라미터에는 버튼이 위치할 부모 위젯(self)을 입력한다. 1btn.clicked.connect(QCoreApplication.instance().quit) PyQt5에서의 이벤트 처리는 시그널과 슬롯 메커니즘으로 이루어진다. btn을 클릭하면 clicked 시그널이 만들어진다. instance() 메소드는 현재 인스턴스를 반환한다. clicked 시그널은 어플리케이션을 종료하는 quit() 메소드에 연결된다. 이렇게 두 객체 발신자와 수신자(Sender &amp; Receiver) 간에 커뮤니케이션이 이루어잔다. 이 예제에서 발신자는 푸시버튼(btn)이고, 수신자는 어플리케이션 객체(app)이다. 툴팁 나타내기","link":"/2020/11/26/study/pyqt5_ex03/"},{"title":"캐글 Titanic EDA + Simple Model [0.80622] 분석","text":"출처: Massimiliano Viola, Titanic EDA + Simple Model [0.80622], 캐글, 2020-09-10https://www.kaggle.com/mviola/titanic-eda-simple-model-0-80622 접기/펼치기 Titanic EDA + Simple Model [0.80622]Introductionhttps://www.kaggle.com/mviola/titanic-eda-simple-model-0-80622이 노트북은 Titanic : Machine Learning from Disaster 대회에 대한 저의 첫 번째 접근 방식을 다룹니다 ( 자세한 내용은 여기 에서 확인 ).초심자로서 제 목표는 기본 기능 엔지니어링과 간단한 모델로 80 % 이상의 점수를 얻는 것이었고 결국 큰 만족과 노력으로 그것을 만들었습니다.지나치게 복잡하지 않고 똑같이하고 싶은 분들을 위해이 과정을 안내하고 어떤 식 으로든 도움을 드리고자합니다.시작하자! Importing packages and data표준 모듈을로드하고 데이터를 살펴 보는 것으로 시작합니다. 1234567891011121314import pandas as pd import numpy as nppd.plotting.register_matplotlib_converters()import matplotlib.pyplot as plt%matplotlib inlineimport seaborn as snssns.set_style('dark')from sklearn.impute import SimpleImputerfrom sklearn.preprocessing import OneHotEncoderfrom sklearn.compose import ColumnTransformerfrom sklearn.pipeline import Pipelinefrom sklearn.ensemble import RandomForestClassifierfrom sklearn.model_selection import cross_val_scoreprint('Setup complete') Setup complete 1!pip install kaggle Requirement already satisfied: kaggle in /usr/local/lib/python3.6/dist-packages (1.5.9) Requirement already satisfied: six&gt;=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.15.0) Requirement already satisfied: slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (0.0.1) Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.8.1) Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.0.1) Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.41.1) Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.23.0) Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle) (2020.11.8) Requirement already satisfied: urllib3 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.24.3) Requirement already satisfied: text-unidecode&gt;=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify-&gt;kaggle) (1.3) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;kaggle) (3.0.4) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;kaggle) (2.10) 12345678from google.colab import filesuploaded = files.upload()for fn in uploaded.keys(): print('uploaded file &quot;{name}&quot; with length {length} bytes'.format( name=fn, length=len(uploaded[fn]))) # kaggle.json을 아래 폴더로 옮긴 뒤, file을 사용할 수 있도록 권한을 부여한다. !mkdir -p ~/.kaggle/ &amp;&amp; mv kaggle.json ~/.kaggle/ &amp;&amp; chmod 600 ~/.kaggle/kaggle.json Upload widget is only available when the cell has been executed in the current browser session. Please rerun this cell to enable. Saving kaggle.json to kaggle.json uploaded file &quot;kaggle.json&quot; with length 63 bytes 1ls -1ha ~/.kaggle/kaggle.json /root/.kaggle/kaggle.json 123456789101112from google.colab import drive # 패키지 불러오기 from os.path import join # 구글 드라이브 마운트ROOT = &quot;/content/drive&quot; # 드라이브 기본 경로print(ROOT) # print content of ROOT (Optional)drive.mount(ROOT) # 드라이브 기본 경로 # 프로젝트 파일 생성 및 다운받을 경로 이동MY_GOOGLE_DRIVE_PATH = 'My Drive/Colab Notebooks/python_basic/kaggle_titanic-eda-simple-model-0-80622_mviola/data'PROJECT_PATH = join(ROOT, MY_GOOGLE_DRIVE_PATH)print(PROJECT_PATH) /content/drive Mounted at /content/drive /content/drive/My Drive/Colab Notebooks/python_basic/kaggle_titanic-eda-simple-model-0-80622_mviola/data 1%cd &quot;{PROJECT_PATH}&quot; /content/drive/My Drive/Colab Notebooks/python_basic/kaggle_titanic-eda-simple-model-0-80622_mviola/data 1!kaggle competitions list Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.9 / client 1.5.4) ref deadline category reward teamCount userHasEntered --------------------------------------------- ------------------- --------------- --------- --------- -------------- contradictory-my-dear-watson 2030-07-01 23:59:00 Getting Started Prizes 82 False gan-getting-started 2030-07-01 23:59:00 Getting Started Prizes 177 False tpu-getting-started 2030-06-03 23:59:00 Getting Started Knowledge 269 False digit-recognizer 2030-01-01 00:00:00 Getting Started Knowledge 2401 False titanic 2030-01-01 00:00:00 Getting Started Knowledge 18309 True house-prices-advanced-regression-techniques 2030-01-01 00:00:00 Getting Started Knowledge 4648 True connectx 2030-01-01 00:00:00 Getting Started Knowledge 425 False nlp-getting-started 2030-01-01 00:00:00 Getting Started Knowledge 1270 False competitive-data-science-predict-future-sales 2022-12-31 23:59:00 Playground Kudos 9679 False jane-street-market-prediction 2021-02-22 23:59:00 Featured $100,000 519 False cassava-leaf-disease-classification 2021-02-18 23:59:00 Research $18,000 726 True rfcx-species-audio-detection 2021-02-17 23:59:00 Research $15,000 239 False rock-paper-scissors 2021-02-01 23:59:00 Playground Prizes 932 False hubmap-kidney-segmentation 2021-02-01 23:59:00 Research $60,000 155 False riiid-test-answer-prediction 2021-01-07 23:59:00 Featured $100,000 2149 False kaggle-survey-2020 2021-01-06 23:59:00 Analytics $30,000 0 False nfl-big-data-bowl-2021 2021-01-05 23:59:00 Analytics $100,000 0 False nfl-impact-detection 2021-01-04 23:59:00 Featured $75,000 56 False halite-iv-playground-edition 2020-12-31 23:59:00 Playground Knowledge 53 False predict-volcanic-eruptions-ingv-oe 2020-12-28 23:59:00 Playground Swag 345 False 1!kaggle competitions download -c titanic Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.9 / client 1.5.4) Downloading train.csv to /content/drive/My Drive/Colab Notebooks/python_basic/kaggle_titanic-eda-simple-model-0-80622_mviola/data 0% 0.00/59.8k [00:00&lt;?, ?B/s] 100% 59.8k/59.8k [00:00&lt;00:00, 7.92MB/s] Downloading test.csv to /content/drive/My Drive/Colab Notebooks/python_basic/kaggle_titanic-eda-simple-model-0-80622_mviola/data 0% 0.00/28.0k [00:00&lt;?, ?B/s] 100% 28.0k/28.0k [00:00&lt;00:00, 3.90MB/s] Downloading gender_submission.csv to /content/drive/My Drive/Colab Notebooks/python_basic/kaggle_titanic-eda-simple-model-0-80622_mviola/data 0% 0.00/3.18k [00:00&lt;?, ?B/s] 100% 3.18k/3.18k [00:00&lt;00:00, 451kB/s] 1!ls gender_submission.csv test.csv train.csv 123# Load and display train datatrain_data = pd.read_csv('train.csv')train_data.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked 0 1 0 3 Braund, Mr. Owen Harris male 22.0 1 0 A/5 21171 7.2500 NaN S 1 2 1 1 Cumings, Mrs. John Bradley (Florence Briggs Th... female 38.0 1 0 PC 17599 71.2833 C85 C 2 3 1 3 Heikkinen, Miss. Laina female 26.0 0 0 STON/O2. 3101282 7.9250 NaN S 3 4 1 1 Futrelle, Mrs. Jacques Heath (Lily May Peel) female 35.0 1 0 113803 53.1000 C123 S 4 5 0 3 Allen, Mr. William Henry male 35.0 0 0 373450 8.0500 NaN S 그런 다음 훈련 및 테스트 데이터 모두에서 결 측값을 확인합니다. 1train_data.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 891 entries, 0 to 890 Data columns (total 12 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 PassengerId 891 non-null int64 1 Survived 891 non-null int64 2 Pclass 891 non-null int64 3 Name 891 non-null object 4 Sex 891 non-null object 5 Age 714 non-null float64 6 SibSp 891 non-null int64 7 Parch 891 non-null int64 8 Ticket 891 non-null object 9 Fare 891 non-null float64 10 Cabin 204 non-null object 11 Embarked 889 non-null object dtypes: float64(2), int64(5), object(5) memory usage: 83.7+ KB 123# Load and display test datatest_data = pd.read_csv('test.csv')test_data.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PassengerId Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked 0 892 3 Kelly, Mr. James male 34.5 0 0 330911 7.8292 NaN Q 1 893 3 Wilkes, Mrs. James (Ellen Needs) female 47.0 1 0 363272 7.0000 NaN S 2 894 2 Myles, Mr. Thomas Francis male 62.0 0 0 240276 9.6875 NaN Q 3 895 3 Wirz, Mr. Albert male 27.0 0 0 315154 8.6625 NaN S 4 896 3 Hirvonen, Mrs. Alexander (Helga E Lindqvist) female 22.0 1 1 3101298 12.2875 NaN S 1test_data.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 418 entries, 0 to 417 Data columns (total 11 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 PassengerId 418 non-null int64 1 Pclass 418 non-null int64 2 Name 418 non-null object 3 Sex 418 non-null object 4 Age 332 non-null float64 5 SibSp 418 non-null int64 6 Parch 418 non-null int64 7 Ticket 418 non-null object 8 Fare 417 non-null float64 9 Cabin 91 non-null object 10 Embarked 418 non-null object dtypes: float64(2), int64(4), object(5) memory usage: 36.0+ KB Age, Cabin및의 값 Embarked은 기차 데이터에서 누락되고 Age, Fare및의 값 Cabin은 테스트 데이터에서 누락되었습니다.필요한 경우 나중에 처리합니다.마지막으로, 우리가 무엇을 예측할 것인지에 대한 아이디어를 얻기 위해 목표에 집중합니다. 얼마나 많은 승객이 살아남 았는지 봅시다. 1train_data['Survived'].value_counts(normalize=True) 0 0.616162 1 0.383838 Name: Survived, dtype: float64 1g = sns.countplot(y=train_data['Survived']).set_title('Survivors and deads count') 훈련 데이터에서 승객의 약 38.4 %만이 재난에서 살아 남았습니다. 이것은 우리가 염두에 두어야 할 중요한 가치입니다. Feature analysis and creation이 섹션의 목표는 모델링 부분에서보다 정확한 기능 선택을 수행하기 위해 데이터에 대한 일반적인 이해를 얻는 것입니다.따라서 승객의 생존 여부를 예측하는 데있어 그 중요성을 결정하기 위해 한 번에 하나의 기능을 탐색 할 것입니다. Sex승객의 약 65 %는 남성이고 나머지 35 %는 여성이었습니다.여기서 주목해야 할 중요한 점은 여성의 생존율이 남성의 생존율의 4 배라는 점이며 이것이 Sex가장 유익한 특징 중 하나입니다.성별 제출 자체 점수가 0.76555 인 것은 아닙니다! 1234fig, axarr = plt.subplots(1, 2, figsize=(12,6))a = sns.countplot(train_data['Sex'], ax=axarr[0]).set_title('Passengers count by sex')axarr[1].set_title('Survival rate by sex')b = sns.barplot(x='Sex', y='Survived', data=train_data, ax=axarr[1]).set_ylabel('Survival rate') /usr/local/lib/python3.6/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. FutureWarning Pclass배에는 세 개의 클래스가 있었고 플롯에서 우리는 세 번째 클래스의 승객 수가 1 등석과 2 등석의 승객 수를 합친 것보다 더 많음을 알 수 있습니다.그러나 등급별 생존율은 동일하지 않습니다. 1 등석 승객의 60 % 이상과 2 등석 승객의 약 절반이 구조 된 반면 3 등석 승객의 75 %는 재난에서 살아남지 못했습니다.이러한 이유로 이것은 확실히 고려해야 할 중요한 측면입니다. 1train_data.groupby('Pclass').Survived.mean() Pclass 1 0.629630 2 0.472826 3 0.242363 Name: Survived, dtype: float64 1234fig, axarr = plt.subplots(1,2,figsize=(12,6))a = sns.countplot(x='Pclass', hue='Survived', data=train_data, ax=axarr[0]).set_title('Survivors and deads count by class')axarr[1].set_title('Survival rate by class')b = sns.barplot(x='Pclass', y='Survived', data=train_data, ax=axarr[1]).set_ylabel('Survival rate') 우리는 또한 생존에 의해 속도를 볼 수 Sex와 Pclass매우 인상적이다 : 비율이 세 번째 수준의 여성이 50 %로 떨어진다 동안 구출 첫 번째 클래스 및 두 번째 클래스 여성은 각각 97 %와 92 %였다.그럼에도 불구하고 이것은 일류 남성의 37 % 생존율보다 여전히 높습니다. 1train_data.groupby(['Pclass', 'Sex']).Survived.mean() Pclass Sex 1 female 0.968085 male 0.368852 2 female 0.921053 male 0.157407 3 female 0.500000 male 0.135447 Name: Survived, dtype: float64 12plt.title('Survival rate by sex and class')g = sns.barplot(x='Pclass', y='Survived', hue='Sex', data=train_data).set_ylabel('Survival rate') Age이 열에는 많은 결 측값이 포함되어 있지만 훈련 데이터에서 평균 연령이 30 세 미만임을 알 수 있습니다.다음은 일반적으로 생존자와 사망자에 대한 연령 분포의 플롯입니다. 12345678fig, axarr = plt.subplots(1,2,figsize=(12,6))axarr[0].set_title('Age distribution')f = sns.distplot(train_data['Age'], color='g', bins=40, ax=axarr[0])axarr[1].set_title('Age distribution for the two subpopulations')g = sns.kdeplot(train_data['Age'].loc[train_data['Survived'] == 1], shade= True, ax=axarr[1], label='Survived').set_xlabel('Age')g = sns.kdeplot(train_data['Age'].loc[train_data['Survived'] == 0], shade=True, ax=axarr[1], label='Not Survived') /usr/local/lib/python3.6/dist-packages/seaborn/distributions.py:2551: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) 첫 번째 모습에서, 사이의 관계 Age와 Survived이 나타납니다 매우 명확하지 : 우리는 살아 사람들을 위해 젊은 승객에 해당하는 피크가 있다는 것을 확실히 알 수 있지만, 그 외에도 나머지는 매우 유익하지 않습니다.우리도 고려해 보면이 기능을 더 잘 이해할 수 Sex있습니다. 이제 많은 수의 남성 생존자가 12 년 미만을 보냈고 여성 그룹에는 특별한 속성이 없다는 것이 더 분명해졌습니다. 12plt.figure(figsize=(8,5))g = sns.swarmplot(y='Sex', x='Age', hue='Survived', data=train_data).set_title('Survived by age and sex') 살펴볼 또 다른 흥미로운 점은 Age, Pclass및 간의 관계 Survived입니다.우리는 Pclass매우 명확한 수평 패턴이 없기 때문에 그 영향이 중요하다고 생각합니다.또한 1 등석에는 아이들이 많지 않다는 것을 알 수 있습니다. 12plt.figure(figsize=(8,5))h = sns.swarmplot(x='Pclass', y='Age', hue='Survived', data=train_data).set_title('Survived by age and class') 이 모든 플롯 후에 나는 Age모델에서 의 중요성에 대해 확신하지 못합니다 . 나는 그것을 사용하지 않을 생각이지만 나중에 보게 될 것이라고 생각합니다. Fare설명에서 Fare분포가 양으로 치우친 것을 볼 수 있습니다. 데이터의 75 %는 31 미만이고 최대는 512입니다.이 기능을 더 잘 이해하기 위해 여기서 가장 간단한 아이디어는 사 분위수를 사용하여 요금 범위를 만드는 것입니다.처음 보면 운임이 높을수록 생존 가능성이 높아진다는 것을 알 수 있습니다. 1train_data.Fare.describe() count 891.000000 mean 32.204208 std 49.693429 min 0.000000 25% 7.910400 50% 14.454200 75% 31.000000 max 512.329200 Name: Fare, dtype: float64 12345fig, axarr = plt.subplots(1,2,figsize=(12,6))f = sns.distplot(train_data.Fare, color='g', ax=axarr[0]).set_title('Fare distribution')fare_ranges = pd.qcut(train_data.Fare, 4, labels = ['Low', 'Mid', 'High', 'Very high'])axarr[1].set_title('Survival rate by fare category')g = sns.barplot(x=fare_ranges, y=train_data.Survived, ax=axarr[1]).set_ylabel('Survival rate') /usr/local/lib/python3.6/dist-packages/seaborn/distributions.py:2551: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) 그러나 모델링에 관해서는 이러한 운임 범주가 상당히 부족하여 전혀 도움이되지 않았습니다.아래의 더 자세한 플롯을 살펴보면 예를 들어 요금이 200에서 300 사이 인 모든 남성이 사망 한 것을 볼 수 있습니다.이런 이유로 Fare너무 많은 정보를 잃지 않도록 기능을 그대로 두었습니다 . 트리의 더 깊은 수준에서 더 차별적 인 관계가 열리고 좋은 그룹 탐지기가 될 수 있습니다. 123plt.figure(figsize=(8,5))# 이 플롯에서 운임이 500 초과인 이상 값 3개를 제외했습니다.a = sns.swarmplot(x='Sex', y='Fare', hue='Survived', data=train_data.loc[train_data.Fare&lt;500]).set_title('Survived by fare and sex') /usr/local/lib/python3.6/dist-packages/seaborn/categorical.py:1296: UserWarning: 53.4% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot. warnings.warn(msg, UserWarning) /usr/local/lib/python3.6/dist-packages/seaborn/categorical.py:1296: UserWarning: 21.1% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot. warnings.warn(msg, UserWarning) 여기 에서 Erik의 커널을 본 후이 기능을 충분히 분석하지 않았 음을 상기 시켰습니다.설명을 인쇄 할 때의 최소값 Fare이 0이고 조금 이상하다는 사실도 알아 차렸어야합니다 .이 정보가 정확합니까? 이 승객이 누구인지 봅시다. 1train_data.loc[train_data.Fare==0] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked 179 180 0 3 Leonard, Mr. Lionel male 36.0 0 0 LINE 0.0 NaN S 263 264 0 1 Harrison, Mr. William male 40.0 0 0 112059 0.0 B94 S 271 272 1 3 Tornquist, Mr. William Henry male 25.0 0 0 LINE 0.0 NaN S 277 278 0 2 Parkes, Mr. Francis \"Frank\" male NaN 0 0 239853 0.0 NaN S 302 303 0 3 Johnson, Mr. William Cahoone Jr male 19.0 0 0 LINE 0.0 NaN S 413 414 0 2 Cunningham, Mr. Alfred Fleming male NaN 0 0 239853 0.0 NaN S 466 467 0 2 Campbell, Mr. William male NaN 0 0 239853 0.0 NaN S 481 482 0 2 Frost, Mr. Anthony Wood \"Archie\" male NaN 0 0 239854 0.0 NaN S 597 598 0 3 Johnson, Mr. Alfred male 49.0 0 0 LINE 0.0 NaN S 633 634 0 1 Parr, Mr. William Henry Marsh male NaN 0 0 112052 0.0 NaN S 674 675 0 2 Watson, Mr. Ennis Hastings male NaN 0 0 239856 0.0 NaN S 732 733 0 2 Knight, Mr. Robert J male NaN 0 0 239855 0.0 NaN S 806 807 0 1 Andrews, Mr. Thomas Jr male 39.0 0 0 112050 0.0 A36 S 815 816 0 1 Fry, Mr. Richard male NaN 0 0 112058 0.0 B102 S 822 823 0 1 Reuchlin, Jonkheer. John George male 38.0 0 0 19972 0.0 NaN S 그들 중 일부는 1 등석 또는 2 등석 승객이기 때문에 내 모델을 혼동시킬 수있는 제로 요금을 제거하기로 결정했습니다.이 함수의 도움으로에 대해 0 값을 만날 때마다 null 값을 설정합니다 Fare.그것들은 나중에 모델을 훈련 할 때 전가 될 것입니다. 123456789def remove_zero_fares(row): if row.Fare == 0: row.Fare = np.NaN return row# Apply the functiontrain_data = train_data.apply(remove_zero_fares, axis=1)test_data = test_data.apply(remove_zero_fares, axis=1)# Check if it did the jobprint('Number of zero-Fares: {:d}'.format(train_data.loc[train_data.Fare==0].shape[0])) Number of zero-Fares: 0 EmbarkedEmbarked승객이 어디에서 탑승했는지 알려줍니다.세 가지 가능한 값이 있습니다 : Southampton, Cherbourg 및 Queenstown.교육 데이터에서 70 % 이상의 사람들이 Southampton에서 탑승했으며 Cherbourg에서 20 % 미만, 나머지는 Queenstown에서 탑승했습니다.탑승 지점으로 생존자를 세어 보면 Cherbourg에서 출발 한 사람들이 사망 한 사람들보다 더 많은 사람들이 살아 남았다는 것을 알 수 있습니다. 1234fig, axarr = plt.subplots(1,2,figsize=(12,6))sns.countplot(train_data['Embarked'], ax=axarr[0]).set_title('Passengers count by boarding point')p = sns.countplot(x = 'Embarked', hue = 'Survived', data = train_data, ax=axarr[1]).set_title('Survivors and deads count by boarding point') /usr/local/lib/python3.6/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. FutureWarning 승객의 탑승 지점이 생존 기회를 바꿀 수있을 것이라고 예상하지 않기 때문에 아마도 퀸스 타운과 사우 샘프 턴이 아닌 Cherbourg에서 온 승객의 1 등석 및 2 등석 승객 비율이 더 높기 때문일 것입니다.이를 확인하기 위해 다른 착수 지점에 대한 클래스 분포를 확인합니다. 1g = sns.countplot(data=train_data, x='Embarked', hue='Pclass').set_title('Pclass count by embarking point') 그 주장은 정확하고 왜 그 생존율이 그렇게 높은지 정당화합니다.이 기능은 트리의 더 깊은 수준에서 그룹을 감지하는 데 유용 할 수 있으며 이것이 제가 그것을 유지하는 유일한 이유입니다. NameName열은 우리가 성씨을 사용하여 가족 그룹을 식별 할 수 예를 들어 같은 유용한 정보가 포함되어 있습니다.그러나이 노트북에서는 승객의 직함 만 추출하여 열차 및 테스트 데이터 모두에 대한 새로운 기능을 생성했습니다. 12train_data['Title'] = train_data['Name'].apply(lambda x: x.split(',')[1].split('.')[0].strip())test_data['Title'] = test_data['Name'].apply(lambda x: x.split(',')[1].split('.')[0].strip()) 1train_data['Title'].value_counts() Mr 517 Miss 182 Mrs 125 Master 40 Dr 7 Rev 6 Mlle 2 Col 2 Major 2 Mme 1 Lady 1 Capt 1 the Countess 1 Don 1 Jonkheer 1 Ms 1 Sir 1 Name: Title, dtype: int64 1test_data['Title'].value_counts() Mr 240 Miss 78 Mrs 72 Master 21 Col 2 Rev 2 Dr 1 Dona 1 Ms 1 Name: Title, dtype: int64 타이틀의 분포를 살펴보면 정말 저주파 타이틀을 더 큰 그룹으로 옮기는 것이 편리 할 수 ​​있습니다.이를 분석 한 후 모든 희귀 여성 타이틀을 미스로, 모든 희귀 남성 타이틀을 Mr. 123456# 희귀 여성 타이틀 대체train_data['Title'].replace(['Mme', 'Ms', 'Lady', 'Mlle', 'the Countess', 'Dona'], 'Miss', inplace=True)test_data['Title'].replace(['Mme', 'Ms', 'Lady', 'Mlle', 'the Countess', 'Dona'], 'Miss', inplace=True)# Substitute rare male titlestrain_data['Title'].replace(['Major', 'Col', 'Capt', 'Don', 'Sir', 'Jonkheer'], 'Mr', inplace=True)test_data['Title'].replace(['Major', 'Col', 'Capt', 'Don', 'Sir', 'Jonkheer'], 'Mr', inplace=True) 다음은 최종 결과입니다. 대부분의 경우 생존율이 평균 생존율보다 상당히 높거나 낮은 것으로 나타나기 때문에이 새로운 기능에 대해 상대적으로 높은 기대를 가지고 있습니다. 1train_data.groupby('Title').Survived.mean() Title Dr 0.428571 Master 0.575000 Miss 0.707447 Mr 0.160000 Mrs 0.792000 Rev 0.000000 Name: Survived, dtype: float64 12plt.title('Survival rate by Title')g = sns.barplot(x='Title', y='Survived', data=train_data).set_ylabel('Survival rate') Cabin and Ticket이 Cabin기능은 결 측값이 많기 때문에 다소 문제가 있습니다.나는 그것이 우리 모델에 너무 많은 도움이 될 것이라고 기대하지 않으므로 그것을 분석조차하지 않습니다.다른 한편으로 올바르게 설계된 Ticket컬럼은 가족 그룹을 찾는 가장 좋은 방법이지만이 노트북에 대해 선택한 접근 방식은 아닙니다 (다른 노트북에서 시도해 볼 것입니다).잠재력을 완전히 알고 삭제하는 것이 아쉽기 때문에 두 개의 새로운 열을 만들기로 결정했습니다. 하나는 티켓 처음 두 글자이고 다른 하나는 티켓 길이입니다. 123456# 처음 두 글자 추출train_data['Ticket_lett'] = train_data.Ticket.apply(lambda x: x[:2])test_data['Ticket_lett'] = test_data.Ticket.apply(lambda x: x[:2])# 티켓 길이 계산train_data['Ticket_len'] = train_data.Ticket.apply(lambda x: len(x))test_data['Ticket_len'] = test_data.Ticket.apply(lambda x: len(x)) 이것은 우리 모델에 약간 도움이 될 것이므로 여기에서 괜찮다고 생각합니다. SibSpSibSp타이타닉 호에 탑승 한 사람의 형제 자매 또는 배우자의 수입니다.90 % 이상의 사람들이 혼자 여행하거나 형제 자매 또는 배우자와 함께 여행 한 것으로 나타났습니다.서로 다른 범주 간의 생존율은 다소 혼란 스럽지만 혼자 여행하거나 형제가 2 명 이상인 사람들의 생존 가능성이 낮다는 것을 알 수 있습니다.또한, 5 ~ 8 명의 형제 자매가있는 대가족 중 누구도 살아남을 수 없었 음을 알 수 있습니다. 1234fig, axarr = plt.subplots(1,2,figsize=(12,6))a = sns.countplot(train_data['SibSp'], ax=axarr[0]).set_title('Passengers count by SibSp')axarr[1].set_title('Survival rate by SibSp')b = sns.barplot(x='SibSp', y='Survived', data=train_data, ax=axarr[1]).set_ylabel('Survival rate') /usr/local/lib/python3.6/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. FutureWarning Parch받는 유사 SibSp열이 기능은 부모 또는 각 승객이 함께 여행 한 아이의 수를 포함합니다.여기에서 우리는 같은 결론을 내립니다. SibSp작은 가족이 더 큰 가족과 혼자 여행하는 승객보다 생존 할 기회가 더 많다는 것을 다시 한 번 볼 수 있습니다. 1234fig, axarr = plt.subplots(1,2,figsize=(12,6))a = sns.countplot(train_data['Parch'], ax=axarr[0]).set_title('Passengers count by Parch')axarr[1].set_title('Survival rate by Parch')b = sns.barplot(x='Parch', y='Survived', data=train_data, ax=axarr[1]).set_ylabel('Survival rate') /usr/local/lib/python3.6/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. FutureWarning Family type약해 보이는 두 개의 예측 변수가 있기 때문에 우리가 할 수있는 한 가지는 이들을 결합하여 더 강한 예측 변수를 얻는 것입니다. 및의 경우 두 변수를 결합하여 , 및 1 (누가 승객 자신) 의 합인 가족 크기 특성을 얻을 수 있습니다.SibSpParchSibSpParch 123# 새로운 Fam_size 컬럼 생성train_data['Fam_size'] = train_data['SibSp'] + train_data['Parch'] + 1test_data['Fam_size'] = test_data['SibSp'] + test_data['Parch'] + 1 가족 규모별로 생존율을 도표화하면 혼자있는 사람들이 최대 4 개 구성 요소의 가족보다 생존 확률이 낮았고, 대가족의 경우 생존율이 떨어지고 궁극적으로 매우 큰 가족의 경우 0이되는 것이 분명합니다. 12plt.title('Survival rate by family size')g = sns.barplot(x='Fam_size', y='Survived', data=train_data).set_ylabel('Survival rate') 이전 트렌드를 더 요약하기 위해 마지막 기능으로 가족 규모에 대해 4 개의 그룹을 만들었습니다. 123# 4 개 그룹 생성train_data['Fam_type'] = pd.cut(train_data.Fam_size, [0,1,4,7,11], labels=['Solo', 'Small', 'Big', 'Very big'])test_data['Fam_type'] = pd.cut(test_data.Fam_size, [0,1,4,7,11], labels=['Solo', 'Small', 'Big', 'Very big']) 최종 결과는 다음과 같습니다. 좋은 패턴을 발견 한 것 같습니다. 12plt.title('Survival rate by family type')g = sns.barplot(x=train_data.Fam_type, y=train_data.Survived).set_ylabel('Survival rate') 이러한 모든 고려 사항이 끝나면 마침내 모든 것을 간단하고 매우 효율적인 모델에 통합 할 때입니다. Modeling사용할 기능을 선택하고 대상을 격리하는 것으로 시작합니다.말씀 드렸듯이 저는 고려하지 않을 것이며 Cabin, Age청년 인 관련 정보가 마스터 제목에 암호화되어 있으므로 결국 제외 했습니다.나는 또한 칼럼을 Sex고려할 때 유용 하지 않기 때문에 사용하지 않았습니다 Title. 성인 남성과 어린 아이들은 같은 성별을 가지고 있지만 이전에 본 것과 정말 다른 범주이므로 알고리즘을 혼동하고 싶지 않습니다.열을 추출하지 않으면 매우 중요하므로 모델 Title을 입력 Sex하는 것을 잊지 마십시오 ! 1234y = train_data['Survived']features = ['Pclass', 'Fare', 'Title', 'Embarked', 'Fam_type', 'Ticket_len', 'Ticket_lett']X = train_data[features]X.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Pclass Fare Title Embarked Fam_type Ticket_len Ticket_lett 0 3 7.2500 Mr S Small 9 A/ 1 1 71.2833 Mrs C Small 8 PC 2 3 7.9250 Miss S Solo 16 ST 3 1 53.1000 Mrs S Small 6 11 4 3 8.0500 Mr S Solo 6 37 EDA에서 학습 및 테스트 데이터 모두에 누락 된 값이 있고 처리 할 여러 범주 형 변수가 있다는 것을 기억하기 때문에 파이프 라인을 사용하여 모든 작업을 단순화하기로 결정했습니다. 1234567891011121314151617181920212223242526272829numerical_cols = ['Fare']categorical_cols = ['Pclass', 'Title', 'Embarked', 'Fam_type', 'Ticket_len', 'Ticket_lett']# 수치 데이터 전처리numerical_transformer = SimpleImputer(strategy='median')# 범주 형 데이터 전처리categorical_transformer = Pipeline(steps=[ ('imputer', SimpleImputer(strategy='most_frequent')), ('onehot', OneHotEncoder(handle_unknown='ignore'))])# 숫자 및 범주 데이터에 대한 번들 전처리preprocessor = ColumnTransformer( transformers=[ ('num', numerical_transformer, numerical_cols), ('cat', categorical_transformer, categorical_cols) ])# 번들 전처리 및 모델링 코드titanic_pipeline = Pipeline(steps=[ ('preprocessor', preprocessor), ('model', RandomForestClassifier(random_state=0, n_estimators=500, max_depth=5))])# 훈련 데이터 전처리, 모델 적합titanic_pipeline.fit(X,y)print('Cross validation score: {:.3f}'.format(cross_val_score(titanic_pipeline, X, y, cv=10).mean())) Cross validation score: 0.826 이제 테스트 데이터에서 predict 메서드를 호출하기 만하면 예측을 수행 할 준비가되었습니다. 12X_test = test_data[features]X_test.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Pclass Fare Title Embarked Fam_type Ticket_len Ticket_lett 0 3 7.8292 Mr Q Solo 6 33 1 3 7.0000 Mrs S Small 6 36 2 2 9.6875 Mr Q Solo 6 24 3 3 8.6625 Mr S Solo 6 31 4 3 12.2875 Mrs S Small 7 31 12# 테스트 데이터 전처리, 예측 얻기predictions = titanic_pipeline.predict(X_test) 지금해야 할 일은 제출 파일로 변환하는 것입니다! 1234output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})output.to_csv('my_submission.csv', index=False)files.download(&quot;my_submission.csv&quot;) # 구글 코랩 다운로드 추가 https://stackoverflow.com/questions/49394737/exporting-data-from-google-colab-to-local-machineprint('Your submission was successfully saved!') &lt;IPython.core.display.Javascript object&gt; &lt;IPython.core.display.Javascript object&gt; Your submission was successfully saved! Conclusion이것은 저의 첫 Kaggle 대회였으며 한 번에 한 단계 씩 순위표를 오르는 것은 정말 멋진 여정이었습니다.아래에서 2020 년 9 월 Titanic LB 점수의 히스토그램을 확인할 수 있습니다. 현재이 노트북은 상위 4 %에 속합니다. 이것은 매우 좋은 점수이지만 영리한 그룹화 접근 방식과 모델 앙상블 링으로 분류기를 개선 할 수 있습니다.당신은이 작업을 수행하는 방법에 관심이 있다면, 여기 당신이 도달 0.82775가 (검사 할 수 있음을 내 다른 노트북 찾을 수 이 지금은 최고 점수의 몇 가지 아이디어를 가지고뿐만 아니라 아웃).지금은이 노트북이 유용했거나 마음에 들었다면 알려주세요. 정말 감사하겠습니다!이 대회에 행운을 빕니다. 다음 대회에서 만나요.","link":"/2020/12/01/study/kaggle_titanic-eda-simple-model-0-80622_mviola/"},{"title":"ML 모델 을 평가하기 위한 올바른 측정 항목을 선택하는 방법","text":"출처: Vipul Gandhi, How to Choose Right Metric for Evaluating ML Model, 캐글, 2020.01.06https://www.kaggle.com/vipulgandhi/how-to-choose-right-metric-for-evaluating-ml-model 접기/펼치기 ML 모델 을 평가하기 위한 올바른 측정 항목을 선택하는 방법(How to Choose Right Metric for Evaluating ML Model) 도입부https://www.kaggle.com/vipulgandhi/how-to-choose-right-metric-for-evaluating-ml-model 이 Scikit-learn 페이지는 훌륭한 참조를 제공합니다. 일반적인 기능 엔지니어링, 선택, 모델 구현을 수행하고 확률 또는 클래스 형태로 출력을 얻은 후 다음 단계는 테스트 데이터 세트를 사용하여 일부 메트릭을 기반으로 모델이 얼마나 효과적인지 확인하는 것입니다. 메트릭은 모델의 성능을 설명합니다.모델은 정확도 _ 점수라는 메트릭을 사용하여 평가할 때 만족스러운 결과를 제공 할 수 있지만 logarithmic_loss와 같은 다른 메트릭 또는 다른 이러한 메트릭에 대해 평가할 때 좋지 않은 결과를 제공 할 수 있습니다. 따라서 기계 학습 모델을 평가하기 위해 올바른 메트릭을 선택하는 것이 매우 중요합니다.측정 항목 선택은 기계 학습 알고리즘의 성능을 측정하고 비교하는 방법에 영향을줍니다. 결과에서 다른 특성의 중요성에 가중치를 부여하는 방법에 영향을줍니다.분류 메트릭 정확성. 대수 손실. ROC, AUC. 혼란 매트릭스. 분류 보고서. 회귀 지표 평균 절대 오차. 평균 제곱 오차. 평균 제곱근 오차. 루트 평균 제곱 로그 오류. R 광장. R 제곱을 조정했습니다. 분류 문제에서는 , 우리는 (자신이 생성 출력의 종류에 따라) 알고리즘의 두 가지 유형을 사용 클래스 출력 : SVM 및 KNN과 같은 알고리즘은 클래스 출력을 생성합니다. 예를 들어, 이진 분류 문제에서 출력은 0 또는 1입니다. SKLearn의 / 기타 알고리즘은 이러한 클래스 출력을 확률로 변환 할 수 있습니다. 확률 출력 : 로지스틱 회귀, 랜덤 포레스트, 그라디언트 부스팅, Adaboost 등과 같은 알고리즘은 확률 출력을 제공합니다. 확률 출력은 임계 확률을 생성하여 클래스 출력으로 변환 할 수 있습니다. 회귀 문제에서 출력은 본질적으로 항상 연속적이며 추가 처리가 필요하지 않습니다. 분류 메트릭(Classification Metrices) 데이터 세트 : 피마 인디언 당뇨병 예측. 평가 알고리즘 : 로지스틱 회귀, SGDClassifier, RandomForestClassifier. 123456789101112from google.colab import drive # 패키지 불러오기 from os.path import join # 구글 드라이브 마운트ROOT = &quot;/content/drive&quot; # 드라이브 기본 경로print(ROOT) # print content of ROOT (Optional)drive.mount(ROOT) # 드라이브 기본 경로 # 프로젝트 파일 생성 및 다운받을 경로 이동MY_GOOGLE_DRIVE_PATH = 'My Drive/Colab Notebooks/python_basic/kaggle_how-to-choose-right-metric-for-evaluating-ml-model_vipulgandhi/data'PROJECT_PATH = join(ROOT, MY_GOOGLE_DRIVE_PATH)print(PROJECT_PATH) /content/drive Mounted at /content/drive /content/drive/My Drive/Colab Notebooks/python_basic/kaggle_how-to-choose-right-metric-for-evaluating-ml-model_vipulgandhi/data 1%cd &quot;{PROJECT_PATH}&quot; /content/drive/My Drive/Colab Notebooks/python_basic/kaggle_how-to-choose-right-metric-for-evaluating-ml-model_vipulgandhi/data 1!ls diabetes.csv 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152import pandas as pdfrom sklearn import metricsfrom sklearn.ensemble import RandomForestClassifierfrom sklearn.linear_model import LogisticRegression, SGDClassifierfrom sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, StratifiedKFoldimport matplotlib.pyplot as pltimport warningswarnings.filterwarnings('ignore')diabetes_data = pd.read_csv('diabetes.csv')X = diabetes_data.drop([&quot;Outcome&quot;],axis = 1)y = diabetes_data[&quot;Outcome&quot;]# 훈련 세트를 사용하여 다양한 하이퍼 파라미터로 여러 모델을 훈련하고 검증 세트에서 가장 잘 수행되는 모델과 하이퍼 파라미터를 선택합니다.# 모델 유형과 하이퍼 파라미터가 선택되면 전체 훈련 세트에서 이러한 하이퍼 파라미터를 사용하여 최종 모델을 훈련시키고 일반화 된 오류는 테스트 세트에서 최종적으로 측정됩니다.X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, random_state = 56)# StratifiedKFold 클래스는 계층화 된 샘플링을 수행하여 각 클래스의 대표 비율을 포함하는 폴드를 생성합니다.cv = StratifiedKFold(n_splits=10, shuffle = False, random_state = 76)# 로지스틱 회귀clf_logreg = LogisticRegression()# 적합 모델clf_logreg.fit(X_train, y_train)# 검증 세트에 대한 클래스 예측을합니다.y_pred_class_logreg = cross_val_predict(clf_logreg, X_train, y_train, cv = cv)# 클래스 1에 대한 예측 확률, 양성 클래스의 확률y_pred_prob_logreg = cross_val_predict(clf_logreg, X_train, y_train, cv = cv, method=&quot;predict_proba&quot;)y_pred_prob_logreg_class1 = y_pred_prob_logreg[:, 1]# SGD 분류기clf_SGD = SGDClassifier()# 적합 모델clf_SGD.fit(X_train, y_train)# 검증 세트에 대한 클래스 예측을합니다.y_pred_class_SGD = cross_val_predict(clf_SGD, X_train, y_train, cv = cv)# 클래스 1에 대한 예측 확률y_pred_prob_SGD = cross_val_predict(clf_SGD, X_train, y_train, cv = cv, method=&quot;decision_function&quot;)# 랜덤 포레스트 분류기clf_rfc = RandomForestClassifier()# 적합 모델clf_rfc.fit(X_train, y_train)# 검증 세트에 대한 클래스 예측을합니다.y_pred_class_rfc = cross_val_predict(clf_rfc, X_train, y_train, cv = cv)# 클래스 1에 대한 예측 확률y_pred_prob_rfc = cross_val_predict(clf_rfc, X_train, y_train, cv = cv, method=&quot;predict_proba&quot;)y_pred_prob_rfc_class1 = y_pred_prob_rfc[:, 1] 빠른 참고 : SkLearn의 “predict_log_proba”는 확률의 로그를 제공합니다. 확률이 매우 작아 질 수 있으므로 종종 더 편리합니다. Null 정확도(Null accuracy) 항상 가장 빈번한 클래스를 예측하여 얻을 수있는 정확도. 이것은 항상 0/1을 예측하는 멍청한 모델이 “null_accuracy”%의 시간에 맞을 것이라는 것을 의미합니다. 12345678910111213141516from sklearn.base import BaseEstimatorimport numpy as npclass BaseClassifier(BaseEstimator): def fit(self, X, y=None): pass def predict(self, X): return np.zeros((len(X), 1), dtype=bool) base_clf = BaseClassifier()cross_val_score(base_clf, X_train, y_train, cv=10, scoring=&quot;accuracy&quot;).mean()# Method 2# calculate null accuracy (for binary / multi-class classification problems)# null_accuracy = y_train.value_counts().head(1) / len(y_train) 0.6509981851179674 분류 정확도(Classification Accuracy) 분류 정확도 또는 정확도는 총 입력 샘플 수에 대한 올바른 예측 수의 비율입니다. $$Accuracy = \\frac{Number\\ of\\ correct\\ predictions}{Total\\ number\\ of\\ predictions\\ made} = \\frac{TP + TN}{TP + TN + FP + FN}$$ 정확도 측정 항목을 사용하는 경우: 각 클래스에 속하는 샘플 수가 거의 동일한 경우정확도 측정 항목을 사용하지 않는 경우: 하나의 클래스 만 대부분의 샘플을 보유 할 때.예:훈련 세트에 클래스 A의 샘플이 98 %이고 클래스 B의 샘플이 2 %라고 가정합니다. 그러면 우리 모델은 클래스 A에 속하는 모든 훈련 샘플을 간단히 예측하여 98 %의 훈련 정확도를 쉽게 얻을 수 있습니다.동일한 모델이 클래스 A의 60 % 샘플과 클래스 B의 40 % 샘플이있는 테스트 세트에서 테스트되면 테스트 정확도가 60 %로 떨어집니다. 분류 정확도는 우리에게 높은 정확도를 달성한다는 잘못된 감각을 줄 수 있습니다. 1234567# 정확도 계산acc_logreg = cross_val_score(clf_logreg, X_train, y_train, cv = cv, scoring = 'accuracy').mean()acc_SGD = cross_val_score(clf_SGD, X_train, y_train, cv = cv, scoring = 'accuracy').mean()acc_rfc = cross_val_score(clf_rfc, X_train, y_train, cv = cv, scoring = 'accuracy').mean()acc_logreg, acc_SGD, acc_rfc (0.7797035692679977, 0.611222020568663, 0.7606473079249849) 로그 손실 / 로그 손실 / 로지스틱 손실 / 교차 엔트로피 손실 로그 손실로 작업 할 때 분류기는 모든 샘플에 대해 각 클래스에 확률을 할당해야합니다. 로그 손실은 실제 레이블과 비교하고 잘못된 분류에 페널티를 적용하여 모델 확률의 불확실성을 측정합니다. 로그 손실은 둘 이상의 레이블에 대해서만 정의됩니다. 로그 손실은 예측 확률이 향상됨에 따라 점차 감소하므로 로그 손실이 0에 가까울수록 정확도가 높아지고 로그 손실이 0에서 멀어지면 정확도가 낮아집니다. 로그 손실은 (0, ∞] 범위에 있습니다. M 클래스에 속하는 N 개의 샘플이 있다고 가정하면 로그 손실은 다음과 같이 계산됩니다.$$ Log\\ Loss = \\frac{-1}{N} \\sum_{i=1}^{N} \\sum_{i=1}^{M} y_{ij} * \\log(\\hat{y_{ij}})$$ $y_{ij}$ ,샘플 i가 클래스 j에 속하는지 여부를 나타냅니다. $p_{ij}$ ,샘플 i가 클래스 j에 속하는 확률을 나타냅니다. 음수 부호 부정 $\\log(\\hat{y_{ij}})$ 항상 음수 인 출력. $\\hat{y_{ij}}$ 확률 (0-1)을 출력하고, $\\log(x)$ 0 &lt;x &lt;1 인 경우 음수입니다. 예:학습 레이블은 0과 1이지만 학습 예측은 0.4, 0.6, 0.89 등입니다. 모델의 오류 측정 값을 계산하기 위해 0.5보다 큰 값을 갖는 모든 관측 값을 1로 분류 할 수 있습니다. 우리는 오 분류를 증가시킬 위험이 높습니다. 확률이 0.4, 0.45, 0.49 인 많은 값이 1의 참값을 가질 수 있기 때문입니다.이것이 logLoss가 등장하는 곳입니다.이제 LogLoss의 공식을 자세히 살펴 보겠습니다. 값에 대한 4 가지 주요 사례가있을 수 있습니다. $y_{ij}$ 과 $p_{ij}$ 사례 1 : $y_{ij}$j =1 , $p_{ij}$ = 높음 사례 2 : $y_{ij}$ =1 , $p_{ij}$ = 낮음 사례 3 : $y_{ij}$ =0 , $p_{ij}$ = 낮음 사례 4 : $y_{ij}$ =0 , $p_{ij}$ = 높음 LogLoss는 불확실성을 어떻게 측정합니까?케이스 1과 케이스 3이 더 많이있는 경우 로그 로스 공식 내부의 합계 (및 평균)는 케이스 2와 케이스 4가 추가 된 경우에 비해 훨씬 더 커질 것입니다. 이제이 값은 좋은 예측을 나타내는 Case 1 및 Case 3만큼 큽니다. (-1)을 곱하면 값을 가능한 한 작게 만듭니다. 이것은 이제 직관적으로 의미합니다.-값이 작을수록 모델이 더 좋습니다. 즉, 로그 손실이 더 작고, 모델이 더 좋습니다. 즉, 불확실성이 더 작고, 모델이 더 좋습니다. 123456789101112131415# logloss 계산logloss_logreg = cross_val_score(clf_logreg, X_train, y_train, cv = cv, scoring = 'neg_log_loss').mean()logloss_rfc = cross_val_score(clf_rfc, X_train, y_train, cv = cv, scoring = 'neg_log_loss').mean()# SGDClassifier의 힌지 손실은 확률 추정을 지원하지 않습니다.# Scikit-learn의 CalibratedClassifierCV에서 SGDClassifier를 기본 추정기로 설정하여 확률 추정치를 생성 할 수 있습니다.from sklearn.calibration import CalibratedClassifierCVnew_clf_SGD = CalibratedClassifierCV(clf_SGD)new_clf_SGD.fit(X_train, y_train)logloss_SGD = cross_val_score(new_clf_SGD, X_train, y_train, cv = cv, scoring = 'neg_log_loss').mean()logloss_logreg, logloss_SGD, logloss_rfc (-0.48368646454082465, -0.6383384003043665, -0.4664817973667718) ROC 곡선AUC혼동 매트릭스분류 보고서정밀도-재현율 트레이드 오프결론회귀 지표평균 절대 오차평균 제곱 오차RMSE평균 제곱근 로그 오차R_ 제곱조정 된 R- 제곱NLP 메트릭보너스다중 클래스 분류다중 라벨 분류다중 출력 분류","link":"/2020/12/01/study/kaggle_how_to_choose_right_metric_for_evaluating_ml_model_vipulgandhi/"},{"title":"불균형 분류에 대한 평가 지표 둘러보기 (번역)","text":"출처: Jason Brownlee, Tour of Evaluation Metrics for Imbalanced Classification, Machine Learning Mastery, 2020.01.08https://machinelearningmastery.com/tour-of-evaluation-metrics-for-imbalanced-classification/ 접기/펼치기 불균형 분류에 대한 평가 지표 둘러보기(Tour of Evaluation Metrics for Imbalanced Classification) 분류기는 평가에 사용되는 측정 항목만큼만 우수합니다.모델을 평가하기 위해 잘못된 메트릭을 선택하면 불량 모델을 선택하거나 최악의 경우 모델의 예상 성능에 대해 오해 할 가능성이 있습니다.적절한 측정 항목을 선택하는 것은 일반적으로 응용 기계 학습에서 어렵지만 불균형 분류 문제의 경우 특히 어렵습니다. 첫째, 널리 사용되는 대부분의 표준 메트릭은 균형 잡힌 클래스 분포를 가정하고 일반적으로 모든 클래스가 아니므로 모든 예측 오류가 아닌 불균형 분류에 대해 동일하기 때문입니다.이 자습서에서는 불균형 분류에 사용할 수있는 메트릭을 발견합니다.이 자습서를 완료하면 다음을 알게됩니다. 분류를 위한 메트릭 선택의 문제 및 편향된 클래스 분포가있을 때 특히 어떻게 어려운지에 대해 설명합니다. 등급, 임계 값 및 확률이라고하는 분류기 모델을 평가하기위한 세 가지 주요 메트릭 유형이있는 방법 어디서부터 시작해야할지 모르는 경우 불균형 분류에 대한 메트릭을 선택하는 방법. 모든 예제에 대한 단계별 자습서 및 Python 소스 코드 파일을 포함하여 저의 새로운 저서 Imbalanced Classification with Python으로 프로젝트 를 시작하십시오 . 평가 지표의 과제(Challenge of Evaluation Metrics) 평가 메트릭은 예측 모델의 성능을 정량화합니다.여기에는 일반적으로 데이터 세트에서 모델을 학습시키고, 모델을 사용하여 학습 중에 사용되지 않은 홀드 아웃 데이터 세트에 대한 예측을 수행 한 다음 예측을 홀드 아웃 데이터 세트의 예상 값과 비교합니다.분류 문제의 경우 메트릭에는 예상 클래스 레이블을 예측 된 클래스 레이블과 비교하거나 문제에 대한 클래스 레이블의 예측 확률을 해석하는 작업이 포함됩니다.모델을 선택하고 데이터 준비 방법을 함께 사용하는 것은 평가 메트릭에 의해 안내되는 검색 문제입니다. 실험은 다른 모델로 수행되며 각 실험의 결과는 측정 항목으로 정량화됩니다. 평가 측정은 분류 성능을 평가하고 분류 자 ​​모델링을 안내하는 데 중요한 역할을합니다. — 불균형 데이터 분류 : 검토 , 2009. 분류 정확도 또는 분류 오류와 같은 분류 예측 모델을 평가하는 데 널리 사용되는 표준 메트릭이 있습니다.표준 메트릭은 대부분의 문제에서 잘 작동하므로 널리 채택됩니다. 그러나 모든 메트릭은 문제 또는 문제에서 중요한 것에 대해 가정합니다. 따라서 사용자 또는 프로젝트 이해 관계자가 모델 또는 예측에 대해 중요하다고 생각하는 것을 가장 잘 포착하는 평가 지표를 선택해야하므로 모델 평가 지표를 선택하기가 어렵습니다.이 문제는 클래스 분포에 왜곡이있을 때 더욱 어려워집니다. 그 이유는 소수 클래스와 다수 클래스 간의 비율이 1 : 100 또는 1 : 1000과 같이 클래스가 불균형하거나 심각하게 불균형 할 때 많은 표준 메트릭이 신뢰할 수 없거나 오해의 소지가 있기 때문입니다. 클래스 불균형의 경우, 데이터가 왜곡 될 때 왜곡되지 않은 데이터에 사용되는 상대적으로 강력한 기본 절차가 비참하게 무너질 수 있기 때문에 문제는 훨씬 더 심각합니다. — 페이지 187, 불균형 학습 : 기초, 알고리즘 및 응용 프로그램 , 2013. 예를 들어, 심각하게 불균형 한 분류 문제에 대한 분류 정확도를보고하는 것은 위험 할 정도로 오해의 소지가 있습니다. 프로젝트 이해 관계자가 결과를 사용하여 결론을 도출하거나 새로운 프로젝트를 계획하는 경우입니다. 실제로 불균형 도메인에서 공통 메트릭을 사용하면 최적이 아닌 분류 모델로 이어질 수 있으며 이러한 측정은 왜곡 된 도메인에 민감하지 않기 때문에 잘못된 결론을 내릴 수 있습니다. — 불균형 분포 하에서의 예측 모델링 조사 , 2015. 중요한 것은 불균형 분류로 작업 할 때 종종 다른 평가 지표가 필요하다는 것입니다.모든 클래스를 동등하게 중요하게 취급하는 표준 평가 메트릭과 달리 불균형 분류 문제는 일반적으로 소수 클래스의 분류 오류를 다수 클래스의 분류 오류보다 더 중요하게 평가합니다. 소수 클래스에 초점을 맞춘 성능 메트릭이 필요할 수 있으며, 이는 효과적인 모델을 훈련하는 데 필요한 관찰이 부족한 소수 클래스이기 때문에 어렵습니다. 불균형 데이터 세트의 주된 문제는 사용 가능한 데이터 샘플에서 제대로 표현되지 않은 케이스의 성능에 대한 사용자 선호도 편향과 종종 관련된다는 사실에 있습니다. — 불균형 분포 하에서의 예측 모델링 조사 , 2015. 이제 모델 평가 메트릭을 선택하는 문제에 익숙해 졌으므로 선택할 수있는 여러 메트릭의 몇 가지 예를 살펴 보겠습니다. 분류 자 평가 지표의 분류(Taxonomy of Classifier Evaluation Metrics) 분류기 모델을 평가할 때 선택할 수있는 메트릭은 수십 가지가 있으며 학계에서 제안한 메트릭의 모든 애완 동물 버전을 고려할 경우 수백 가지가 있습니다.선택할 수있는 메트릭을 처리하기 위해 Cesar Ferri 등이 제안한 분류법을 사용합니다 . “ 분류를위한 성능 측정의 실험적 비교 “라는 제목의 2008 년 논문에서 . 2013 년 책“ 불균형 학습 ” 에서도 채택되었으며 유용하다고 생각합니다.평가 지표를 세 가지 유용한 그룹으로 나눌 수 있습니다. 그들은: 임계 값 메트릭 순위 지표 확률 메트릭. 이 구분은 일반적으로 분류 자에 대해 실무자가 사용하는 상위 메트릭, 특히 불균형 분류가 분류 체계에 깔끔하게 적합하기 때문에 유용합니다. 여러 기계 학습 연구자들이 분류 맥락에서 사용되는 세 가지 평가 지표 제품군을 식별했습니다. 이들은 임계 메트릭 (예 : 정확도 및 F- 측정), 순위 지정 방법 및 메트릭 (예 : 수신기 작동 특성 (ROC) 분석 및 AUC), 확률 적 메트릭 (예 : 평균 제곱근 오차)입니다. — 페이지 189, 불균형 학습 : 기초, 알고리즘 및 응용 프로그램 , 2013. 차례로 각 그룹을 자세히 살펴 보겠습니다. 불균형 분류에 대한 임계 값 메트릭(Threshold Metrics for Imbalanced Classification) How to Choose an Evaluation Metric","link":"/2020/12/01/study/Tour_of_Evaluation_Metrics_for_Imbalanced_Classification/"},{"title":"파이썬 머신러닝 완벽가이드 3장","text":"출처: 권철민, 『파이썬 머신러닝 완벽 가이드 (개정판)』, 위키북스, 2020.02, 148-182쪽 접기/펼치기 Chapter 03. 평가 머신러닝 구성: 데이터 가공/변환, 모델 학습/예측, 평가(Evaluation) 프로세스로 구성 성능 평가 지표(Evaluation Metric): 모델이 분류인지 회귀인지에 따라 여러 종류로 나뉨 회귀: 대부분 실제값과 예측값의 오차 평균값에 기반 ex) 오차에 절댓값을 씌운 뒤 평균 오차를 구하거나 오차의 제곱값에 루트를 씌운 뒤, 평균 오차를 구하는 방법→ 기본적으로 예측 오차를 가지고 정규화 수준을 재가공하는 방법 (5장에서 다시 설명) 분류: 일반적으로는 실제 결과 데이터와 예측 결과 데이터가 얼마나 정확하고 오류가 적게 발생하는가에 기반 단, 단순히 이러한 정확도만 가지고 판단할 경우 잘못된 평가 결과로 빠질 수 있음 0과 1로 결정값이 한정되는 이진 분류의 성능 평가 지표에 관해 집중적으로 살펴볼 예정 0이냐 1이냐 혹은 긍정/부정을 판단하는 이진 분류에서는, 정확도보다는 다른 성능 평가 지표가 더 중요시되는 경우가 많음 분류의 성능 평가 지표 정확도(Accuracy) 오차행렬(Confusion Matrix) 정밀도(Precision) 재현율(Recall) F1 스코어 ROC AUC 분류의 나눔 이진 분류: 결정 클래스 값 종류 유형에 따라 긍정/부정과 같은 2개의 결괏값만을 가짐 멀티 분류: 여러 개의 결정 클래스 값을 가지는 멀티 분류 01. 정확도(Accuracy) 정확도: 실제 데이터에서 예측 데이터가 얼마나 같은지 판단하는 지표$$ 정확도(Accuracy) = \\frac{예측 결과가 동일한 데이터 수}{전체 예측 데이터 수} $$ 정확도는 직관적으로 모델 예측 성능을 나타내는 평가 지표 단, 이진 분류의 경우 데이터 구성에 따라 ML 모델 성능을 왜곡할 수 있어서 정확도 수치 하나로만 성능을 평가하지 않음 - 예시 2장의 타이타닉 예제 수행 결과를 보면 정확도의 한계를 볼 수 있음 ML 알고리즘을 적용한 후 예측 정확도 결과가 보통 80%대였지만, 탑승객이 남자인 경우보다 여자인 경우 생존 확률이 높았기 때문에 별다른 알고리즘 적용 없이 성별이 여자인 경우 무조건 생존, 남자인 경우 사망으로 예측 결과를 예측해도 비슷한 수치가 나올 수 있음→ 성별 조건 하나만으로 결정하는 수준 낮은 알고리즘도 높은 정확도를 나타내는 상황이 발생할 수 있음 - 추가 실습 사이킷런의 BaseEstimator 클래스를 상속받아 아무런 학습을 하지 않고, 성별에 따라 생존자를 예측하는 단순한 Classifier를 생성 (사이킷런은 BaseEstimator를 상속받으면 Customized 형태의 Estimator를 개발자가 생성할 수 있게 함) MyDummyClassifier 클래스: 학습을 수행하는 fit() 메서드는 아무것도 수행하지 않고, 예측을 수행하는 predict() 메서드는 단순이 Sex 피처가 1이면 0, 그렇지 않으면 1로 예측하는 매우 단순한 Classifier 12345import pandas as pdimport numpy as npfrom IPython.display import Imageimport warnings warnings.filterwarnings('ignore') 123456789101112131415from sklearn.base import BaseEstimatorclass MyDummyClassifier(BaseEstimator): # fit() 메서드는 아무것도 학습하지 않음 def fit(self, X, y=None): pass # predict() 메서드는 단순히 Sex 피처가 1이면 0, 아니면 1로 예측 def predict(self, X): pred = np.zeros( (X.shape[0],1) ) for i in range(X.shape[0]): if X['Sex'].iloc[i] == 1: pred[i] = 0 else : pred[i] = 1 return pred 1234567891011121314151617181920212223242526272829303132333435## 생성된 MyDummyClassifier를 이용해 타이타닉 생존자 예측 수행from sklearn.model_selection import train_test_splitfrom sklearn.metrics import accuracy_scorefrom sklearn.preprocessing import LabelEncoder## Null 처리 함수def fillna(df): df['Age'].fillna(df['Age'].mean(), inplace=True) df['Cabin'].fillna('N', inplace=True) df['Embarked'].fillna('N', inplace=True) df['Fare'].fillna(0, inplace=True) return df## 머신러닝에 불필요한 피처 제거def drop_features(df): df.drop(['PassengerId', 'Name', 'Ticket'], axis=1, inplace=True) return df## Label Encoding 수행def format_features(df): df['Cabin'] = df['Cabin'].str[:1] features = ['Cabin', 'Sex', 'Embarked'] for feature in features: le = LabelEncoder() le.fit(df[feature]) df[feature] = le.transform(df[feature]) return df## 앞에서 실행한 Data Preprocessing 함수 호출def transform_features(df): df = fillna(df) df = drop_features(df) df = format_features(df) return df 12345678910111213# 원본 데이터를 재로딩, 데이터 가공, 학습 데이터/테스트 데이터 분할titanic_df = pd.read_csv('../data/titanic/train.csv')y_titanic_df = titanic_df['Survived']X_titanic_df = titanic_df.drop(['Survived'], axis=1)X_titanic_df = transform_features(X_titanic_df)X_train, X_test, y_train, y_test = train_test_split(X_titanic_df, y_titanic_df, test_size=0.2, random_state=0)# 위에서 생성한 Dummy Classifier를 활용해서 학습/예측/평가 수행myclf = MyDummyClassifier()myclf.fit(X_train, y_train)mypredictions = myclf.predict(X_test)print('Dummy Classifier의 정확도는: {0:.4f}'.format(accuracy_score(y_test, mypredictions))) Dummy Classifier의 정확도는: 0.7877 단순한 알고리즘으로 예측하더라도 데이터 구성에 따라 정확도 결과는 약 78.77%로 꽤 높은 수치가 나올 수 있음→ 정확도를 평가 지표로 사용할 때는 신중할 필요가 있음 특히, 불균형한(imbalanced) 레이블 값 분포에서 ML 모델의 성능을 판단할 경우, 적합한 평가 지표가 아님 ex) 100개의 데이터가 있고 이 중 90개의 데이터 레이블이 0, 단 10개의 데이터 레이블이 1이라고 한다면 무조건 0으로 예측 결과를 반환하는 ML 모델의 경우라도 정확도가 90%가 됨 - MNIST 데이터 세트로 살펴보기 MNIST 데이터 세트를 변환해 불균형한 데이터 세트로 만든 뒤, 정확도 지표 적용 시 발생하는 문제 살펴보기 MNIST 데이터 세트는 0부터 9까지 숫자 이미지 픽셀 정보를 가지고 있으며, 숫자 Digit를 예측하는데 사용됨 사이킷런은 load_digits() API를 통해 MNIST 데이터 세트를 제공 원래 MNIST 데이터 세트는 레이블 값이 0부터 9까지 있는 멀티 레이블 분류를 위한 것이나, 이를 레이블 값이 7인 것만 True, 나머지 값은 모두 False로 변환해 이진 분류 문제로 바꾸어 실습→ 전체 데이터의 10%만 True, 나머지 90%는 False인 불균형한 데이터 세트로 변형 MNIST 데이터셋을 multi classification에서 binary classification으로 변경 불균형한 데이터 세트에 모든 데이터를 False로, 즉 0으로 예측하는 classifier를 이용해 정확도를 측정하면 약 90%에 가까운 예측 정확도를 나타냄 아무것도 하지 않고 무조건 특정한 결과로 찍어도 데이터 분포도가 균일하지 않은 경우, 높은 수치가 나타날 수 있음 - ex) step 1. 불균형한 데이터 세트와 Dummy Classifier 생성 123456789101112131415161718192021from sklearn.datasets import load_digitsfrom sklearn.model_selection import train_test_splitfrom sklearn.base import BaseEstimatorfrom sklearn.metrics import accuracy_scoreimport numpy as npimport pandas as pdclass MyFakeClassifier(BaseEstimator): def fit(self, x, y): pass # 입력값으로 들어오는 X 데이터 세트 크기만큼 모두 0값으로 만들어 변환 def predict(self, X): return np.zeros((len(X), 1), dtype=bool) # 사이킷런의 내장 데이터 세트인 load_digits()를 이용해 MNIST 데이터 로딩digits = load_digits()# digits 번호가 7번이면 True고, 이를 astype(int)로 1로 변환, 7번이 아니면 False고 0으로 변환y = (digits.target == 7).astype(int)X_train, X_test, y_train, y_test = train_test_split(digits.data, y, random_state=11) - step 2. 불균형한 데이터로 생성한 y_test 데이터 분포도를 확인하고 MyFakeClassifier를 이용해 예측과 평가 수행 12345678910# 불균형한 레이블 데이터 분포도 확인print('레이블 테스트 세트 크기:', y_test.shape)print('테스트 세트 레이블 0과 1의 분포도')print(pd.Series(y_test).value_counts())# Dummy Classifier로 학습/예측/정확도 평가fakeclf = MyFakeClassifier()fakeclf.fit(X_train, y_train)fakepred = fakeclf.predict(X_test)print('모든 예측을 0으로 하여도 정확도는:{:.3f}'.format(accuracy_score(y_test, fakepred))) 레이블 테스트 세트 크기: (450,) 테스트 세트 레이블 0과 1의 분포도 0 405 1 45 dtype: int64 모든 예측을 0으로 하여도 정확도는:0.900 단순히 predict() 결과를 np.zero()로 모두 0값으로 반환함에도 불구하고 450개의 테스트 데이터 세트에 수행한 예측 정확도는 90% 단지 모든 것을 0으로만 예측해도 MyFakeClassifier의 정확도가 90%로 유수의 ML 알고리즘과 비슷한 결과를 냄→ 정확도 평가 지표는 불균형한 레이블 데이터 세트에서 성능 세트로 사용해서는 안 됨→ 정확도를 분류 평가 지표로 사용 시, 한계를 극복하기 위해 여러 가지 분류 지표를 함께 적용 02. 오차 행렬 오차 행렬(confusion matrix, 혼동행렬) 학습된 분류 모델이 예측을 수행하면서 얼마나 헷갈리는지(confused) 보여주는 지표→ 이진 분류의 예측 오류가 얼마인지, 어떤 유형의 예측 오류가 발생하는지를 나타내는 지표 오차 행렬은 4분면 행렬에서 실제 레이블 클래스 값과 예측 레이블 클래스 값이 어떠한 유형을 가지고 매핑되는지를 나타냄 4분면의 왼쪽, 오른쪽을 예측된 클래스 값 기준으로 Negative와 Positive로 분류하고 4분면의 위, 아래를 실제 클래스 값 기준으로 Negative와 Positive로 분류하면 예측 클래스와 실제 클래스 값 유형에 따라 결정되는 TN, FP, FN, TP 형태로 오차 행렬의 4분면이 만들어짐 True/False: 예측값과 실제값이 같음/틀림, Negative/Positive: 예측 결괏값이 부정(0) / 긍정(1) TN: 예측값을 Negative 값 0으로 예측, 실제 값도 Negative 값 0 FP: 예측값을 Positive 값 1로 예측, 실제 값은 Negative 값 0 FN: 예측값을 Negative 값 0으로 예측, 실제 값은 Positive 값 1 TP: 예측값을 Positive 값 1로 예측, 실제 값도 Positive 값 1 사이킷런은 오차 행렬을 구하기 위해 confusion_matrix() API를 제공 - 정확도 예제에서 다룬 MyFakeClassifier의 예측 성능 지표를 오차 행렬로 표현해보기(예측 결과인 fakepred와 실제 결과인 y_test를 confusion_matrix()의 인자로 입력해 오차 행렬을 confusion_matrix()를 이용해 배열 형태로 출력) 123from sklearn.metrics import confusion_matrixconfusion_matrix(y_test, fakepred) array([[405, 0], [ 45, 0]], dtype=int64) 출력된 오차 행렬은 ndarray 형태 이진 분류의 TN, FP, FN, FP는 상단 도표와 동일한 위치를 가지고 array에서 가져올 수 있음→ TN은 array[0,0]으로 405, FP는 array[0,1]로 0, FN은 array[1,0]으로 45, TP는 array[1,1]로 0에 해당 앞 절의 MyFakeClassifier는 load_digits()에서 target == 7인지 아닌지에 따라 클래스 값을 Ture/False 이진 분류로 변경한 데이터 세트를 사용해서 무조건 Negative로 예측하는 Classifier였고 테스트 데이터 세트의 클래스 값 분포는 0이 405건, 1이 45건 TN: 전체 450건 데이터 중, 무조건 Negative 0으로 예측해서 True가 된 결과 405건 FP: Positive 1로 예측한 건수가 없으므로 0건 FN: Positive 1인 건수 45건을 Negative로 예측해서 False가 된 결과 45건 TP: Positive 1로 예측한 건수가 없으므로 0건 TP, TN, FP, TN 값은 Classifier 성능의 여러 면모를 판단할 수 있는 기반 정보를 제공 이 값을 조합해 Classifier 성능을 측정할 수 있는 주요 지표인 정확도(Accuracy), 정밀도(Precision), 재현율(Recall) 값을 알 수 있음 cf) 정확도는 예측값과 실제 값이 얼마나 동일한가에 관한 비율만으로 결정 → 오차 행렬에서 True에 해당하는 값인 TN과 TP에 좌우됨 정확도 = 예측 결과와 실제 값이 동일한 건수 / 전체 데이터 수 = $\\frac{TN + TP}{TN + FP + FN + TP}$ 일반적으로 불균형한 레이블 클래스를 가지는 이진 분류 모델에서는 많은 데이터 중 중점적으로 찾아야 하는 매우 적은 수의 결괏값에 Positive를 설정해 1값을 부여, 그렇지 않은 경우는 Negative로 0값을 부여하는 경우가 많음 ex) 사기 행위 예측 모델: 사기 행위 Positive 양성, 1 / 정상 행위 Negative 음성, 0 암 검진 예측 모델: 암이 양성일 경우 Positive 양성, 1 / 암이 음성일 경우 Negative 음성, 0 불균형한 이진 분류 데이터 세트에서는 Positive 데이터 건수가 매우 작기 때문에 데이터에 기반한 ML 알고리즘은 Positive보다는 Negative로 예측 정확도가 높아지는 경향 발생 10,000건의 데이터 세트에서 9,900건이 Negative고 100건이 Positive라면 Negative로 예측하는 경향이 더 강해 TN은 매우 커지고 TP는 매우 작아짐 Negative로 예측할 때, 정확도가 높기 때문에 FN(Negative로 예측할 때 틀린 데이터 수)이 매우 작고, Positive로 예측하는 경우가 작기 때문에 FP 역시 작아짐→ 정확도 지표는 비대칭한 데이터 세트에서 Positive에 관한 예측 정확도를 판단하지 못한 채 Negative에 관한 예측 정확도만으로도 분류의 정확도가 매우 높게 나타나는 수치적인 판단 오류를 일으킴 - 정리 정확도는 분류(Classification) 모델의 성능을 측정할 수 있는 한 가지 요소 03. 정밀도와 재현율 정밀도 &amp; 재현율: Positive 데이터 세트의 예측 성능에 조금 더 초점을 맞춘 평가 지표 앞서 만든 MyFakeClassifier는 Positive로 예측한 TP 값이 하나도 없기 때문에, 정밀도와 재현율 값이 모두 0 정밀도 = $\\frac{TP}{FP + TP}$ 재현율 = $\\frac{TP}{FN + TP}$ 정밀도: 예측값을 Positive로 한 대상 중에 예측과 실제 값이 Positive로 일치한 데이터의 비율 공식의 분모인 FP + TP: 예측을 Positive로 한 모든 데이터 건수 / 분자인 TP: 예측과 실제 값이 Positive로 일치한 데이터 건수 Positive 예측 성능을 더욱 정밀히 측정하기 위한 평가 지표로 ‘양성 예측도’라고도 불림 재현율: 실제 값이 Positive인 대상 중에 예측과 실제 값이 Positive로 일치한 데이터의 비율 공식의 분모인 FN + TP: 실제 값이 Positive인 모든 데이터 건수 / 분자인 TP: 예측과 실제 값이 Positive로 일치한 데이터 건수 민감도(Sensitivity) 또는 TPR(True Positive Rate)라고도 불림 정밀도와 재현율 지표 중, 이진 분류 모델의 업무 특성에 따라 특정 평가 지표가 더 중요한 지표로 간주될 수 있음 재현율이 중요 지표인 경우: 실제 Positive 양성 데이터를 Negative로 잘못 판단할 시, 업무에 큰 영향이 발생하는 경우 정밀도가 중요 지표인 경우: 실제 Negative 음성인 데이터 예측을 Positive 양성으로 잘못 판단 시, 업무상 큰 영향이 발생하는 경우 - 정리 재현율과 정밀도 모두 TP를 높이는 데 초점 재현율은 FN(실제 Positive, 예측 Negative)를 낮추는 데, 정밀도는 FP를 낮추는 데 초점→ 서로 보완적인 지표로 분류의 성능을 평가하는 데 적용되며 두 수치 모두 높은 것이 가장 좋은 성능 (둘 중 어느 한 평가 지표만 매우 높고, 다른 수치는 매우 낮은 경우는 바람직하지 않음) - ex) 타이타닉 예제 오차 행렬 및 정밀도, 재현율을 모두 구해 예측 성능 평가하기 사이킷런은 정밀도 계산을 위해 precision_score()를, 재현율 계산을 위해 recall_score()를 API로 제공 1234567891011# 평가 간편히 적용하기: coufusion, matrix, accuracy, precision, recall 등 평가를 한 번에 호출하는 get_clf_eval() 함수 만들기from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrixdef get_clf_eval(y_test, pred): confusion = confusion_matrix(y_test, pred) accuracy = accuracy_score(y_test, pred) precision = precision_score(y_test, pred) recall = recall_score(y_test, pred) print('오차행렬') print(confusion) print('정확도 : {:.4f}\\n정밀도 : {:.4f}\\n재현율 : {:.4f}'.format(accuracy, precision, recall)) 로지스틱 회귀 기반으로 타이타닉 생존자를 예측하고 confusion matrix, accuracy, precision, recall 평가 수행 1234567891011121314from sklearn.linear_model import LogisticRegression# 원본 데이터를 재로딩, 데이터 가공, 학습 데이터/테스트 데이터 분할titanic_df = pd.read_csv('../data/titanic/train.csv')y_titanic_df = titanic_df['Survived']X_titanic_df = titanic_df.drop('Survived', axis=1)X_titanic_df = transform_features(X_titanic_df)X_train, X_test, y_train, y_test = train_test_split(X_titanic_df, y_titanic_df, test_size = 0.2, random_state = 11)lr_clf = LogisticRegression()lr_clf.fit(X_train, y_train)pred = lr_clf.predict(X_test)get_clf_eval(y_test, pred) 오차행렬 [[104 14] [ 13 48]] 정확도 : 0.8492 정밀도 : 0.7742 재현율 : 0.7869 정밀도(Precision)에 비해 재현율(Recall)이 낮게 나옴 1. 정밀도/재현율 트레이드오프 분류의 결정 임계값(Threshold)을 조정해 정밀도 또는 재현율의 수치를 높일 수 있음단, 정밀도와 재현율은 상호 보완적인 평가 지표로 한쪽을 강제로 높이면 다른 하나의 수치가 떨어지기 쉬움 사이킷런의 분류 알고리즘은 예측 데이터가 특정 레이블(Label, 결정 클래스 값)에 속하는지 계산하기 위해, 먼저 개별 레이블별로 결정 확률을 구함 → 예측 확률이 큰 레이블값으로 예측 사이킷런은 개별 데이터별로 예측 확률을 반환하는 메서드인 predict_proba()를 제공 predict_proba(): 학습 완료된 사이킷런 Classifier 객체에서 호출 가능하며 테스트 피처 데이터 세트를 파라미터로 입력해주면 테스트 피처 레코드의 개별 클래스 예측 확률을 반환 (predict() 메서드와 유사하지만 반환 결과가 예측 결과 클래스 값이 아닌 예측 확률 결과) - 이진 분류에서 predict_proba()를 수행해 반환되는 ndarray는 첫 번째 칼럼이 클래스 값 0에 대한 예측 확률, 두 번째 칼럼이 클래스 값 1에 대한 예측 확률 12345678pred_proba = lr_clf.predict_proba(X_test)pred = lr_clf.predict(X_test)print('pred_proba() 결과 shape: {0}'.format(pred_proba.shape))print('pred_proba array에서 앞 3개만 샘플로 추출 \\n:', pred_proba[:3])# 예측 확률 array와 예측 결괏값 aaray를 병합(concetenate)해 예측 확률과 결괏값을 한눈에 확인pred_proba_result = np.concatenate([pred_proba, pred.reshape(-1, 1)], axis=1)print('두 개의 class 중에서 더 큰 확률을 클래스 값으로 예측 \\n', pred_proba_result[:3]) pred_proba() 결과 shape: (179, 2) pred_proba array에서 앞 3개만 샘플로 추출 : [[0.4623509 0.5376491 ] [0.87875882 0.12124118] [0.87717457 0.12282543]] 두 개의 class 중에서 더 큰 확률을 클래스 값으로 예측 [[0.4623509 0.5376491 1. ] [0.87875882 0.12124118 0. ] [0.87717457 0.12282543 0. ]] 반환 결과인 ndarray는 0과 1에 대한 확률을 나타내므로 첫 번째 칼럼 값과 두 번째 칼럼 값을 더하면 1이 됨 맨 마지막 줄의 predict() 메서드의 결과, 비교에서도 나타나듯이 두 개의 칼럼 중에서 더 큰 확률 값으로 predict() 메서드가 최종 예측 predict() 메서드는 predict_proba() 메서드에 기반해 생성된 API predict()는 predict_proba() 호출 결과로 반환된 배열에서 분류 결정 임계값보다 큰 값이 들어 있는 칼럼의 위치를 받아서 최종적으로 예측 클래스를 결정하는 API - 코드로 구현해보기 threshold 변수를 특정 값으로 설정하고 Binarizer 클래스를 객체로 생성→ 생성된 Binarizer 객체의 fit_transform() 메서드를 이용해 넘파이 ndarray를 입력→ 입력된 ndarray 값이 지정된 threshold보다 같거나 작으면 0 값으로, 크면 1 값으로 변환해 반환 123456789from sklearn.preprocessing import BinarizerX = [[1, -1, 2], [2, 0, 0], [0, 1.1, 1.2]]# X의 개별 원소들이 threshold 값보다 같거나 작으면 0을, 크면 1을 반환binarizer = Binarizer(threshold=1.1)print(binarizer.fit_transform(X)) [[0. 0. 1.] [1. 0. 0.] [0. 0. 1.]] 입력된 X 데이터 세트에서 Binarizer의 threshold 값이 1.1보다 같거나 작으면 0, 크면 1로 변환됨을 알 수 있음 - Binarizer를 이용해 사이킷런 predict()의 의사(pseudo) 코드 만들기 123456789101112from sklearn.preprocessing import Binarizer# Binarizer의 threshold 설정값. 분류 결정 임계값임.custom_threshold = 0.5# predict_proba() 반환값의 두 번째 칼럼, 즉 Positive 클래스 칼럼 하나만 추출해 Binarizer 적용pred_proba_1 = pred_proba[:, 1].reshape(-1, 1)binarizer = Binarizer(threshold=custom_threshold).fit(pred_proba_1)custom_predict = binarizer.transform(pred_proba_1)get_clf_eval(y_test, custom_predict) 오차행렬 [[104 14] [ 13 48]] 정확도 : 0.8492 정밀도 : 0.7742 재현율 : 0.7869 위 의사 코드로 계산된 평가 지표는 앞 예제의 타이타닉 데이터로 학습된 로지스틱 회귀 Classifier 객체에서 호출된 predict()로 계산된 지표 값과 정확히 일치→ predict()가 predict_proba()에 기반함을 알 수 있음 12345678910111213# 추가. 분류 결정 입계값을 0.5에서 0.4로 낮춰보기from sklearn.preprocessing import Binarizer# Binarizer의 threshold 설정값. 분류 결정 임계값임. (0.5 → 0.4)custom_threshold = 0.4# predict_proba() 반환값의 두 번째 칼럼, 즉 Positive 클래스 칼럼 하나만 추출해 Binarizer 적용pred_proba_1 = pred_proba[:, 1].reshape(-1, 1)binarizer = Binarizer(threshold=custom_threshold).fit(pred_proba_1)custom_predict = binarizer.transform(pred_proba_1)get_clf_eval(y_test, custom_predict) 오차행렬 [[98 20] [10 51]] 정확도 : 0.8324 정밀도 : 0.7183 재현율 : 0.8361 임계값을 낮추니 재현율 수치가 올라가고 정밀도가 떨어짐→ 분류 결정 임계값은 Positive 예측값을 결정하는 확률의 기준→ 확률을 0.5가 아닌 0.4부터 Positive로 예측을 너그럽게 하여 임계값이 낮아질수록 True 값이 많아짐 Positive 예측값이 많아지면 상대적으로 재현율 값이 높아짐→ 양성 예측을 많이 하다보니 실제 양성을 음성으로 예측하는 획수가 상대적으로 줄기 때문 - 임계값을 0.4부터 0.6까지 0.05씩 증가시키며 평가 지표 조사하기 123456789101112# 테스트를 수행할 모든 임계값을 리스트 객체로 저장thresholds = [0.4, 0.45, 0.5, 0.55, 0.6]def get_eval_by_threshold(y_test, pred_proba_c1, thresholds): #thresholds list 객체 내의 값을 iteration 하면서 평가 수행 for custom_threshold in thresholds: binarizer = Binarizer(threshold=custom_threshold).fit(pred_proba_c1) custom_predict = binarizer.transform(pred_proba_c1) print('\\n임계값: ', custom_threshold) get_clf_eval(y_test, custom_predict)get_eval_by_threshold(y_test, pred_proba[:,1].reshape(-1, 1), thresholds) 임계값: 0.4 오차행렬 [[98 20] [10 51]] 정확도 : 0.8324 정밀도 : 0.7183 재현율 : 0.8361 임계값: 0.45 오차행렬 [[103 15] [ 12 49]] 정확도 : 0.8492 정밀도 : 0.7656 재현율 : 0.8033 임계값: 0.5 오차행렬 [[104 14] [ 13 48]] 정확도 : 0.8492 정밀도 : 0.7742 재현율 : 0.7869 임계값: 0.55 오차행렬 [[109 9] [ 15 46]] 정확도 : 0.8659 정밀도 : 0.8364 재현율 : 0.7541 임계값: 0.6 오차행렬 [[112 6] [ 16 45]] 정확도 : 0.8771 정밀도 : 0.8824 재현율 : 0.7377 (지금까지 임계값 변화에 따른 평가 지표 값을 알아보는 코드를 작성) 사이킷런은 이와 유사한 precision_recall_curve() API를 제공 precision_recall_curve() API의 입력 파라미터와 반환 값은 아래와 같음 입력 파라미터 y_true: 실제 클래스값 배열(배열 크기 = [데이터 건수] probas_pred: Positive 칼럼의 예측 확률 배열(배열 크기 = [데이터 건수] 반환값 정밀도: 임계값별 정밀도 값을 배열로 반환 재현율: 임계값별 재현율 값을 배열로 반환 - 추가) precision_recall_curve()로 타이타닉 예측 모델의 임계값별 정밀도와 재현율 구하기 1234567891011121314151617from sklearn.metrics import precision_recall_curve# 레이블 값이 1일 때의 예측확률을 추출pred_proba_class1 = lr_clf.predict_proba(X_test)[ : , 1]# 실제값 데이터 세트와 레이블 값이 1일 때 예측확률을 precision_recall_curve의 인자로 반환precisions, recalls, thresholds = precision_recall_curve(y_test, pred_proba_class1)print('반환된 분류 결정 임계값 배열의 shape: ', thresholds.shape)# 반환된 임계값 배열 로우가 147건 이므로 샘플로 10건만 추출하되, 임계값을 15 Step으로 추출thr_index = np.arange(0, thresholds.shape[0], 15)print('샘플 추출을 위한 임계값 배열의 index 10개: ', thr_index)print('샘플용 10개의 임계값: ', np.round(thresholds[thr_index], 2))# 15 step 단위로 추출된 임계값에 따른 정밀도와 재현율 값print('샘플 임계값별 정밀도 : ', np.round(precisions[thr_index], 3))print('샘플 임계값별 재현율 : ', np.round(recalls[thr_index], 3)) 반환된 분류 결정 임계값 배열의 shape: (143,) 샘플 추출을 위한 임계값 배열의 index 10개: [ 0 15 30 45 60 75 90 105 120 135] 샘플용 10개의 임계값: [0.1 0.12 0.14 0.19 0.28 0.4 0.56 0.67 0.82 0.95] 샘플 임계값별 정밀도 : [0.389 0.44 0.466 0.539 0.647 0.729 0.836 0.949 0.958 1. ] 샘플 임계값별 재현율 : [1. 0.967 0.902 0.902 0.902 0.836 0.754 0.607 0.377 0.148] 추출된 임계값 샘플 10개에 해당하는 정밀도 값과 재현율 값을 살펴보면 임계값이 증가할수록 정밀도값은 동시에 높아지나 재현율 값은 낮아짐 precision_recall_curve() API는 정밀도와 재현율의 임계값에 따른 값 변화를 곡선 형태의 그래프로 시각화하는 데 이용할 수 있음 123456789101112131415161718192021222324252627# API 이용하여 정밀도, 재현율 곡선 시각화하기import matplotlib.pyplot as pltimport matplotlib.ticker as ticker%matplotlib inlinedef precision_recall_curve_plot(y_test, pred_proba_c1): # threshold ndarray와 이 threshold에 따른 정밀도, 재현율 ndarray 추출 precisions, recalls, thresholds = precision_recall_curve(y_test, pred_proba_c1) # x축을 threshold 값, y축을 정밀도, 재현율로 그리기 plt.figure(figsize=(8,6)) thresholds_boundary = thresholds.shape[0] plt.plot(thresholds, precisions[0: thresholds_boundary], linestyle= '--', label='precision') plt.plot(thresholds, recalls[0: thresholds_boundary], label='recall') # threshold의 값 X축의 scale을 0.1 단위로 변경 stard, end = plt.xlim() plt.xticks(np.round(np.arange(stard, end, 0.1), 2)) # x축, y축 label과 legend, 그리고 grid 설정 plt.xlabel('Threshold value') plt.ylabel('Precision and Recall value') plt.legend() plt.grid() plt.show()precision_recall_curve_plot(y_test, lr_clf.predict_proba(X_test)[:,1]) (정밀도는 점섬, 재현율은 실선으로 표현) 임계값이 낮을수록 많은 수의 양성 예측으로 재현율 값이 극도로 높아지고 정밀도 값이 극도로 낮아짐 임계값을 증가시킬수록 재현율 값이 낮아지고 정밀도 값이 높아짐 2. 정밀도와 재현율의 맹점 임계값의 변경은 정밀도와 재현율을 상호 보완할 수 있는 수준에서 해야 함 정밀도 또는 재현율 평가 지표 수치 중 하나를 극단적으로 높이는 방법이나 잘못된 방법 정밀도가 100%가 되는 방법 확실한 기준이 되는 경우만 Positive로 예측하고 나머지는 모두 Negative로 예측 정밀도 = TP / (TP + FP) 전체 환자 1,000명 중 확실한 Positive 징후만 가진 암환자가 1명이라고 하면, 한 명만 Positive로 예측하더라도 정밀도는 1/(1+0) = 100%가 됨 재현율이 100%가 되는 방법 모든 환자를 Positive로 예측 재현율 = TP / (TP + FN) 전체 환자 1,000명을 다 Positive로 예측하면 실제 양성인 사람이 30명 정도여도 TN이 수치에 포함되지 않고 FN은 0이므로 정밀도는 1/(1+0)으로 100%가 됨 04. F1 스코어 F1 스코어: 정밀도와 재현율을 결합한 지표, 정밀도와 재현율이 어느 한 쪽으로 치우치지 않을 때 상대적으로 높은 값을 가짐 코드 오류 만일 A 예측 모델의 정밀도가 0.9, 재현율이 0.1로 극단적인 차이가 나고, B 예측 모델은 정밀도가 0.5, 재현율이 0.5로 큰 차이가 없다면 A 예측 모델의 F1 스코어는 0.18이고, B 예측 모델의 F1 스코어는 0.5로 B 모델이 A 모델에 비해 매우 우수한 F1 스코어를 가지게 됨 사이킷런은 F1 스코어를 구하기 위해 f1_score()라는 API를 제공 123from sklearn.metrics import f1_scoref1 = f1_score(y_test, pred)print('F1 스코어: {:.4f}'.format(f1)) F1 스코어: 0.7805 12345678910111213141516# 추가. 타이타닉 생존자 예측에서 임계값을 변화시키며 F1 스코어를 포함한 평가 지표 구하기def get_clf_eval(y_test, pred): confusion = confusion_matrix(y_test, pred) accuracy = accuracy_score(y_test, pred) precision = precision_score(y_test, pred) recall = recall_score(y_test, pred) # F1 스코어 추가 f1 = f1_score(y_test, pred) print('오차행렬') print(confusion) # F1 score print 추가 print('\\n정확도: {:.4f}\\n정밀도: {:.4f}\\n재현율: {:.4f}\\nF1: {:.4f}'.format(accuracy, precision, recall, f1)) thresholds = [0.4, 0.45, 0.5, 0.55, 0.6]pred_proba = lr_clf.predict_proba(X_test)get_eval_by_threshold(y_test, pred_proba[:,1].reshape(-1, 1), thresholds) 임계값: 0.4 오차행렬 [[98 20] [10 51]] 정확도: 0.8324 정밀도: 0.7183 재현율: 0.8361 F1: 0.7727 임계값: 0.45 오차행렬 [[103 15] [ 12 49]] 정확도: 0.8492 정밀도: 0.7656 재현율: 0.8033 F1: 0.7840 임계값: 0.5 오차행렬 [[104 14] [ 13 48]] 정확도: 0.8492 정밀도: 0.7742 재현율: 0.7869 F1: 0.7805 임계값: 0.55 오차행렬 [[109 9] [ 15 46]] 정확도: 0.8659 정밀도: 0.8364 재현율: 0.7541 F1: 0.7931 임계값: 0.6 오차행렬 [[112 6] [ 16 45]] 정확도: 0.8771 정밀도: 0.8824 재현율: 0.7377 F1: 0.8036 05. ROC 곡선과 AUC ROC 곡선(Receiver Operation Characteristic Curve) FPR(Fales Positive Rate)이 변할 때, TPR(True Positive Rate)이 어떻게 변하는지 나타내는 곡선 FPR을 X 축으로, TPR을 Y 축으로 잡으면 FPR 변화에 따른 TPR 변화가 곡선 형태로 나타남 TPR(True Positive Rate): 재현율(Recall)이자 민감도(Sensitivity) =&gt; TPR = TP / (FN + TP) 실제값 Positive가 정확히 예측돼야 하는 수준(질병 보유자를 질병 보유했다고 양성 판정) TNR(True Negative Rate)이자 특이성(Specificity) =&gt; TNR = TN / (TN + FP) TNR인 특이성은 아래 공식으로 구할 수 있음 FPR = FP / (FP + TN) = 1 - TNR = 1 - 특이성 ROC 곡선은 FPR을 0부터 1까지 변경하면서 TPR의 변화 값을 구함 사이킷런은 ROC 곡선을 구하기 위해 roc_curve() API를 제공 입력 파라미터 y_true: 실제 클래스 값 array (array shape = [데이터 건수] y_score: predict_proba()의 반환값 array에서 Positive 칼럼의 예측 확률이 보통 사용됨 (array.shape = [n_samples] 반환 값 fpr: fpr 값을 array로 반환 tpr: tpr 값을 array로 반환 threshold: threshold 값 array 12345678910111213141516# roc_curve() API를 이용해 타이타닉 생존자 예측 모델의 FPR, TPR, 임계값 구하기from sklearn.metrics import roc_curve# 레이블 값이 1일 때 예측 확률 추출pred_proba_class1 = lr_clf.predict_proba(X_test)[:,1]fprs, tprs, thresholds = roc_curve(y_test, pred_proba_class1)# 반환된 임계값 배열 로우가 47건이므로 샘플로 10건만 추출하되 임계값을 5step으로 추출# threshold[0]은 max(예측확률) + 1로 임의 설정, 이를 제외하기 위해 np.arrange는 1부터 시작thr_index = np.arange(1, thresholds.shape[0], 5)print('샘플 추출을 위한 임계값 배열의 index 10개: ', thr_index)print('샘플용 10개의 임계값: ', np.round(thresholds[thr_index], 2))# 5 step으로 추출된 임계값에 따른 FPR, TPR 값print('샘플 임계값별 FPR: ', np.round(fprs[thr_index], 3))print('샘플 임계값별 TPR: ', np.round(tprs[thr_index], 3)) 샘플 추출을 위한 임계값 배열의 index 10개: [ 1 6 11 16 21 26 31 36 41 46 51] 샘플용 10개의 임계값: [0.97 0.65 0.63 0.56 0.45 0.4 0.35 0.15 0.13 0.11 0.11] 샘플 임계값별 FPR: [0. 0.017 0.034 0.076 0.127 0.169 0.203 0.466 0.585 0.686 0.797] 샘플 임계값별 TPR: [0.033 0.639 0.721 0.754 0.803 0.836 0.885 0.902 0.934 0.967 0.984] 결과를 살펴보면 임계깞이 1에 가까운 값에서 점점 작아지며 FPR이 점점 커짐FPR이 조금씩 커질 때 TPR은 가파르게 커짐 12345678910111213141516171819# ROC 곡선 시각화def roc_curve_plot(y_test, pred_proba_c1): #임계값에 따른 FPR, TPR 값을반환 받음 fprs, tprs, thresholds = roc_curve(y_test, pred_proba_c1) # ROC곡선을 그래프로 그림 plt.plot(fprs, tprs, label='ROC') # 가운데 대각선 직선을 그림 plt.plot([0,1], [0,1], 'k--', label='Random') # FPR X축의 Scale을 0.1 단위로 변경, X, Y축 명 설정 등 start, end = plt.xlim() plt.xticks(np.round(np.arange(start, end, 0.1), 2)) plt.xlim(0, 1) plt.ylim(0, 1) plt.xlabel('FPR(1-Sensitivity)') plt.ylabel('TPR(Recall)') plt.legend() roc_curve_plot(y_test, pred_proba[:, 1]) 일반적으로 ROC 곡선 자체는 FPR과 TPR의 변화값을 보는 데 이용 분류의 성능 지표로는 ROC 곡선 면적에 기반한 AUC 값으로 결정 AUC(Area Under Curve): 곡선 밑의 면적 값으로 1에 가까울 수록 좋은 수치AUC 수치가 커지려면 FPR이 작은 사태에서 얼마나 큰 TPR을 얻을 수 있느냐가 관건가운데 대각선 직선은 랜덤 수준(동전 던지기 수준) 이진 분류 AUC 값으로 0.5→ 보통의 분류는 0.5 이상의 AUC 값을 가짐 12345from sklearn.metrics import roc_auc_scorepred = lr_clf.predict(X_test)roc_score = roc_auc_score(y_test, pred)print('ROC AUC 값 : {:.4f}'.format(roc_score)) ROC AUC 값 : 0.8341 06. 피마 인디언 당뇨병 예측피마 인디언 당뇨병 데이터 세트 구성 살펴보기 Pregnancies: 임신횟수 Glucose: 포도당 부하 검사 수치 BloodPressure: 혈압 SkinThickness: 팔 삼두근 뒤쪽의 피하지방 측정값 Insulin: 혈청 인슐린 BMI: 체질량 지수 DiabetesPedigreeFunction : 당뇨 내력 가중치 값 Age: 나이 Outcome: 당뇨여부(0 또는 1) 12345678910111213import pandas as pdimport numpy as npimport matplotlib.pyplot as plt%matplotlib inlinefrom sklearn.model_selection import train_test_splitfrom sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_scorefrom sklearn.metrics import f1_score, confusion_matrix, precision_recall_curve, roc_curvefrom sklearn.preprocessing import StandardScaler, Binarizerfrom sklearn.linear_model import LogisticRegressionimport warningswarnings.filterwarnings('ignore') 1234# 데이터 불러오기diabetes_data = pd.read_csv('../data/pima-indians/diabetes.csv')print(diabetes_data['Outcome'].value_counts())diabetes_data.head(3) 0 500 1 268 Name: Outcome, dtype: int64 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Pregnancies Glucose BloodPressure SkinThickness Insulin BMI DiabetesPedigreeFunction Age Outcome 0 6 148 72 35 0 33.6 0.627 50 1 1 1 85 66 29 0 26.6 0.351 31 0 2 8 183 64 0 0 23.3 0.672 32 1 전체 768개의 데이터 중, Negative 값 0이 500개, Positive 값 1이 268개 12# feature 타입과 Null 개수 세어보기diabetes_data.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 768 entries, 0 to 767 Data columns (total 9 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Pregnancies 768 non-null int64 1 Glucose 768 non-null int64 2 BloodPressure 768 non-null int64 3 SkinThickness 768 non-null int64 4 Insulin 768 non-null int64 5 BMI 768 non-null float64 6 DiabetesPedigreeFunction 768 non-null float64 7 Age 768 non-null int64 8 Outcome 768 non-null int64 dtypes: float64(2), int64(7) memory usage: 54.1 KB Null 값은 없으며 피처 타입은 모두 숫자형 - 로지스틱 회귀를 이용해 예측 모델 생성하기 123456789101112# 피처 데이터 세트 X, 레이블 데이터 세트 y 추출# 맨 끝이 Outcome 칼럼으로 레이블 값, 칼럼 위치 -1을 이용해 추출X = diabetes_data.iloc[:, :-1]y = diabetes_data.iloc[:, -1]X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 156, stratify=y)# 로지스틱 회귀로 학습, 예측 및 평가 수행lr_clf = LogisticRegression()lr_clf.fit(X_train, y_train)pred = lr_clf.predict(X_test)get_clf_eval(y_test, pred) 오차행렬 [[88 12] [23 31]] 정확도: 0.7727 정밀도: 0.7209 재현율: 0.5741 F1: 0.6392 123# 임계값별로 정밀도 - 재현율 출력pred_proba = lr_clf.predict_proba(X_test)[:, 1]precision_recall_curve_plot(y_test, pred_proba) 재현율 곡선을 보면 임계값을 0.42 정도로 낮추면 정밀도와 재현율이 어느 정도 균형을 맞출 것그러나, 두 지표 모두 0.7이 되지 않는 수치 - 임계값을 인위 조작하기 전에 다시 데이터 점검해보기 12# 원본 데이터 DataFrane describe() 메서드로 피처 값의 분포도 살피기diabetes_data.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Pregnancies Glucose BloodPressure SkinThickness Insulin BMI DiabetesPedigreeFunction Age Outcome count 768.000000 768.000000 768.000000 768.000000 768.000000 768.000000 768.000000 768.000000 768.000000 mean 3.845052 120.894531 69.105469 20.536458 79.799479 31.992578 0.471876 33.240885 0.348958 std 3.369578 31.972618 19.355807 15.952218 115.244002 7.884160 0.331329 11.760232 0.476951 min 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.078000 21.000000 0.000000 25% 1.000000 99.000000 62.000000 0.000000 0.000000 27.300000 0.243750 24.000000 0.000000 50% 3.000000 117.000000 72.000000 23.000000 30.500000 32.000000 0.372500 29.000000 0.000000 75% 6.000000 140.250000 80.000000 32.000000 127.250000 36.600000 0.626250 41.000000 1.000000 max 17.000000 199.000000 122.000000 99.000000 846.000000 67.100000 2.420000 81.000000 1.000000 min() 값이 0으로 된 피처가 상당히 많음Glucose 피처는 포도당 수치로 min 값이 0으로 나올 수는 없음 123456789# min() 값이 0으로 된 피처에 대해 0 값의 건수 및 전체 데이터 건수 대비 몇 퍼센트의 비율로 존재하는지 확인해보기feature_list = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']def hist_plot(df): for col in feature_list: df[col].plot(kind='hist', bins=20).set_title('Histogram of '+col) plt.show()hist_plot(diabetes_data) SkinThickness와 Insulin의 0 값은 전체의 29.56%, 48.7%로 많은 수준 - 일괄 삭제 대신 위 피처의 0 값을 평균값으로 대체 12345678910# 위 컬럼들에 대한 0 값의 비율 확인zero_count = []zero_percent = []for col in feature_list: zero_num = diabetes_data[diabetes_data[col]==0].shape[0] zero_count.append(zero_num) zero_percent.append(np.round(zero_num/diabetes_data.shape[0]*100,2))zero = pd.DataFrame([zero_count, zero_percent], columns=feature_list, index=['count', 'percent']).Tzero .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } count percent Glucose 5.0 0.65 BloodPressure 35.0 4.56 SkinThickness 227.0 29.56 Insulin 374.0 48.70 BMI 11.0 1.43 123456# zero_features 리스트 내부 저장된 개별 피처의 0 값을 NaN 값으로 대체diabetes_data[feature_list] = diabetes_data[feature_list].replace(0, np.nan)# 위 5개 feature 에 대해 0값을 평균 값으로 대체mean_features = diabetes_data[feature_list].mean()diabetes_data[feature_list] = diabetes_data[feature_list].replace(np.nan, mean_features) 1234567891011121314X = diabetes_data.iloc[:, :-1]y = diabetes_data.iloc[:, -1]# StandardScaler 클래스를 상용하여 데이터 세트에 스케일링 적용scaler = StandardScaler()X_scaled = scaler.fit_transform(X)X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size = 0.2, random_state=156, stratify = y)# 로지스틱 회귀로 학습, 예측, 평가 수행lr_clf = LogisticRegression()lr_clf.fit(X_train, y_train)pred = lr_clf.predict(X_test)get_clf_eval(y_test, pred) 오차행렬 [[89 11] [21 33]] 정확도: 0.7922 정밀도: 0.7500 재현율: 0.6111 F1: 0.6735 데이터 변환과 스케일링으로 성능 수치가 일정 수준 개선하지만, 여전히 재현율 수치 개선이 필요함 - 분류 결정 임계값을 변화시키면서 재현율 값의 성능 수치 개선 정도를 확인하기 1234# 임계값을 0.3에서 0.5까지 0.03씩 변화시키면서 재현율과 다른 평가 지표의 값 변화를 출력thresholds = [0.3, 0.33, 0.36, 0.39, 0.42, 0.45, 0.48, 0.50]pred_proba = lr_clf.predict_proba(X_test)get_eval_by_threshold(y_test, pred_proba[:, 1].reshape(-1, 1), thresholds) 임계값: 0.3 오차행렬 [[68 32] [10 44]] 정확도: 0.7273 정밀도: 0.5789 재현율: 0.8148 F1: 0.6769 임계값: 0.33 오차행렬 [[74 26] [11 43]] 정확도: 0.7597 정밀도: 0.6232 재현율: 0.7963 F1: 0.6992 임계값: 0.36 오차행렬 [[75 25] [13 41]] 정확도: 0.7532 정밀도: 0.6212 재현율: 0.7593 F1: 0.6833 임계값: 0.39 오차행렬 [[82 18] [16 38]] 정확도: 0.7792 정밀도: 0.6786 재현율: 0.7037 F1: 0.6909 임계값: 0.42 오차행렬 [[85 15] [18 36]] 정확도: 0.7857 정밀도: 0.7059 재현율: 0.6667 F1: 0.6857 임계값: 0.45 오차행렬 [[86 14] [19 35]] 정확도: 0.7857 정밀도: 0.7143 재현율: 0.6481 F1: 0.6796 임계값: 0.48 오차행렬 [[88 12] [20 34]] 정확도: 0.7922 정밀도: 0.7391 재현율: 0.6296 F1: 0.6800 임계값: 0.5 오차행렬 [[89 11] [21 33]] 정확도: 0.7922 정밀도: 0.7500 재현율: 0.6111 F1: 0.6735 0.33: 정확도와 정밀도를 희생하고 재현율을 높이는 데 가장 좋은 임계값 0.48: 전체적인 성능 평가 지표를 유지하며 재현율을 약간 향상시키는 좋은 임계값 - 앞서 학습된 로지스틱 회귀 모델을 이용해 임계값을 0.48로 낮춘 상태에서 다시 예측하기 1234567# 임계값을 0.48로 설정한 Binarizer 생성binarizer = Binarizer(threshold=0.48)# 위에서 구한 predict_proba() 예측 확률 array에서 1에 해당하는 칼럼값을 Binarizer 변환하기pred_th_048 = binarizer.fit_transform(pred_proba[:, 1].reshape(-1, 1))get_clf_eval(y_test, pred_th_048) 오차행렬 [[88 12] [20 34]] 정확도: 0.7922 정밀도: 0.7391 재현율: 0.6296 F1: 0.6800 07. 정리 지금까지 살펴본 것 분류에 사용되는 정확도, 오차 행렬, 정밀도, 재현율, F1 스코어, ROC-AUC 이진 분류 레이블 값이 불균형하게 분포될 경우, 정확도만으로 머신러닝 모델의 예측 성능을 평가할 수 없음 오차 행렬 Negative와 Positive 값을 가지는 실제 클래스 값과 예측 클래스 값이 TN, FP, FN, TP로 매핑되는 4분면 행렬을 기반으로 예측 성능 평가 정확도, 정밀도, 재현율 수치는 위 4가지 값을 다양하게 결합하여 만들어짐 정확도, 정밀도, 재현율 수치를 통해 분류 모델 예측 성능 오류가 어떤 모양으로 발생하는지 확인 가능 정밀도(Precision)와 재현율(Recall) Positive 데이터 세트의 예측 성능에 좀 더 초점을 맞춘 지표 재현율이 더 중요한 지표인 경우: 실제 Positive 양성인 데이터 예측을 Negative로 잘못 판단할 경우 업무상 큰 영향이 발생할 때 재현율이 특별히 강조돼야 할 경우 분류의 결정 임계값(Threshold)을 조정해 정밀도 또는 재현율 수치 높이기 F1 스코어 정밀도와 재현율을 결합한 평가 지표 정밀도와 재현율이 어느 한쪽으로 치우치지 않을 때 높은 지표값을 가짐 ROC_AUC는 일반적으로 이진 분류의 성능 평가를 위해 가장 많이 사용되는 지표 AUC(Area Under Curve) 값은 ROC 곡선 밑의 면적을 구한 것(일반적으로 1에 가까울수록 좋은 수치)","link":"/2020/12/03/study/python_machine_learning_perfect_guide_ch03/"},{"title":"파이썬 머신러닝 완벽가이드 5장","text":"출처: 권철민, 『파이썬 머신러닝 완벽 가이드 (개정판)』, 위키북스, 2020.02, 290-376쪽 접기/펼치기 5장 회귀01. 회귀 소개 회귀는 현대 통계학을 이루는 큰 축 회귀 분석은 유전적 특성을 연구하던 영국의 통계학자 갈톤이 수행한 연궁서 유래했다는 것이 일반론 &lt; 회귀에 대한 예시&gt; “부모의 키가 크더라도 자식의 키가 대를 이어 무한정 커지지 않으며, 부모의 키가 작더라도 대를 이어 자식의 키가 무한정 작아지지 않는다” 즉, 회귀 분석은 이처럼 데이터 값이 평균과 같은 일정한 값으로 돌아가려는 경향을 이용한 통계학 기법이다. [출처 : 인프런] ㅡ&gt; X는 피처값(속성) ㅡ&gt; Y는 결정값 회귀는 회귀 계수의 선형/비선형 여부,독립변수의 개수, 종속변수의 개수에 따라 여러 가지 유형으로 나눌 수 있다. 회귀에서 가장 중요한 것은 바로 회귀 계수이다 이 회귀 계수가 선형이냐 아니냐에 따라 선형 회귀와 비선형 회귀로 나눌 수 있다. 그리고 독립변수의 개수가 한 개인지 여러 개인지에 따라 단일 회귀, 다중 회귀로 나뉘게 된다. 독립변수 개수 회귀계수의 결합 1개:단일 회귀 선형:선형 회귀 여러 개:다중 회귀 비선형:비선형 회귀 &lt;회귀 유형 구분&gt; 지도학습은 두 가지 유형으로 나뉘는데, 바로 분류와 회귀이다. 이 두 가지 기법의 가장 큰 차이는 분류는 예측값이 카테고리와 같은 이산형 클래스 값이고 회귀는 연속형 숫자 값이라는 것입니다. 여러 가지 회귀 중에서 선형 회귀가 가장 많이 사용된다. 선형 회귀는 실제값과 예측값의 차이(오류의 제곱 값)를 최소화하는 직선형 회귀선을 최적화하는 방식이다. 선형 회귀 모델은 규제 방법에 따라 다시 별도의유형으로 나뉠 수 있다. 규제는 일반적인 선형 회귀의 과적합 문제를 해결하기 위해서 회귀 계수에 패널티값을 적용하는 것을 말한다. - 대표적인 선형 회귀모델 일반 선형 회귀:예측값과 실제값의 RSS(Residual Sum of Squares)를 최소화할 수 있도록 회귀 계수를 최적화하며 규제(Regularization) 릿지(Ridge): 릿지 회귀는 선형 회귀에 L2 규제를 추가한 회귀 모델, 릿지 회귀는 L2 규제를 적용하는데 L2 규제는 상대적으로 큰 회귀 계수 값의 예측 영향도를 감소시키기 위해서 회귀 계수값을 더 작게 만드는 규제 모델 라쏘(Lasso):라쏘 회귀는 선형 회귀에 L1 규제를 적용한 방식, L2 규제가 회귀 계수 값의 크기를 줄이는데 반해 L1규제는 예측 영향력이 작은 피처의 회귀 계수를 0으로 만들어 회귀 예측시 피처가 선택되지 않게 함 → 이러한 특성 때문에 L1 규제는 피처 선택 가능으로도 불림 엘라스틱넷(ElasticNet):L2,L1 규제를 함께 결합한 모델, 주로 피처가 많은 데이터 세트에서 적용되며 L1규제로 피처의 개수를 줄임과 동시에 L2 규제로 계수 값의 크기를 조정 로지스틱 회귀(Logistic Regression):로지스틱 회귀는 회귀라는 이름이 붙어 있지만 사실은 분류에 사용되는 선형 모델, 로지스틱 회귀는 매우 강력한 분류 알고리즘이며 일반적으로 이진 분류뿐만 아니라 희소 영역의 분류, 예를 들어 텍스트 분류와 같은 영역에서 뛰어난 예측성능을 보임 02. 단순선형 회귀를 통한 회귀 이해단순선형회귀는 독립변수도 하나 종속변수도 하나인 선형 회귀이다. - 예시 주택가격이 주택의 크기로만 결정된다고 할때 일반적으로 주택의 크기가 크면 가격이 높아지는 경향이 있기 때문에 주택가격은 크기에 대해 선형(직선형태)의 관계로 표현할 수 있다. [출처: 인프라] - 오류합 계산 방법 절대값을 취하여 더하는 방식 오류값의 제곱을 구해서 더하는 방식(RSS) 일반적으로 미분 등의 계산을 편리하게 하기 위해서 RSS방식으로 오류합을 구한다즉, Error^2 = RSS RSS는 이제 변수가 w0,w1인 식으로 표현 할 수 있으며 RSS를 최소로 하는 w0,w1 즉 회귀 계수를 학습을 통해서 찾는 것이 머신러닝 기반 회귀의 핵심 사항이다. RSS는 회귀식의 독립변수 X, 종속변수 Y가 중심 변수가 아니라 w 변수(회귀계수)가 중심 변수임을 인지하는 것이 매우 중요(학습 데이터로 입력되는 독립변수와 종속변수는 RSS에서 모두 상수로 간주한다.) [출처 : 인프라] 03. 비용 최소화하기 - 경사 하강법(Gradient Descent)W 파라미터의 개수가 적다면 고차원 방정식으로 비용 함수가 최소가 되는 W 변숫값을 도출할 수 있겠지만 W 파라미터가 많으면 고차원 방정식을 동원하더라도 해결하기가 어렵다. 경사 하강법은 이러한 고차원 방정식에 대한 문제를 해결해 주며서 비용 함수 RSS를 최소화하는 방법을 직관적으로 제공하는 뛰어난 방식이다. [출처 : 인프라] 경사 하강법은 반복적으로 비용 함수의 반환 값, 즉 예측값과 실제값의 차이가 작아지는 방향성을 가지고 W 파라미터를 지속해서 보정해 나간다. 최초 오류 값이 100이었다면 두 번째 오류 값은 100보다 작은 90, 세 번째는 80과 같은 방식으로 지속해서 오류를 감소시키는 방향으로 W 값을 계속 업데이트해 나간다. 그리고 오류 값이 더 이상 작아지지 않으면 그 오류 값을 최소 비용으로 판단하고 그때의 W 값을 최적 파라미터로 반환한다. 경사 하강법의 핵심: “어떻게 하면 오류가 작아지는 방향으로 W 값을 보정할 수 있을까?” [출처 : 인프런] [출처 : 인프런] - 경사 하강법 수행 프로세스 - 실제값을 Y=4X+6 시뮬레이션하는 데이터 값 생성 123456789101112import numpy as npimport matplotlib.pyplot as plt%matplotlib inlinenp.random.seed(0)# y = 4X + 6 식을 근사(w1=4, w0=6). random 값은 Noise를 위해 만듬X = 2 * np.random.rand(100,1) # X는 100개의 랜덤값을 만든다.# np.random.randn는 노이즈값 이걸 사용하지 않으면 계속 1차 함수로 만들어냄 결국 퍼져보이기 위하여 사용y = 6 +4 * X+ np.random.randn(100,1) # X, y 데이터 셋 산점도로 시각화plt.scatter(X, y) &lt;matplotlib.collections.PathCollection at 0x2370c9b30d0&gt; 1X.shape, y.shape # 100개의 데이터를 다 가지고 있다는 것을 알 수 있다. ((100, 1), (100, 1)) - w0과 w1의 값을 최소화 할 수 있도록 업데이트 수행하는 함수 생성. 예측 배열 y_pred는 np.dot(X, w1.T) + w0 임 100개의 데이터 X(1,2,…,100)이 있다면 예측값은 w0 + X(1)w1 + X(2)w1 +..+ X(100)*w1이며, 이는 입력 배열 X와 w1 배열의 내적임. 새로운 w1과 w0를 update함 123456789101112131415161718# w1 과 w0 를 업데이트 할 w1_update, w0_update를 반환. def get_weight_updates(w1, w0, X, y, learning_rate=0.01): N = len(y) # 먼저 w1_update, w0_update를 각각 w1, w0의 shape와 동일한 크기를 가진 0 값으로 초기화 w1_update = np.zeros_like(w1) w0_update = np.zeros_like(w0) # 예측 배열 계산하고 예측과 실제 값의 차이 계산 y_pred = np.dot(X, w1.T) + w0 diff = y-y_pred # w0_update를 dot 행렬 연산으로 구하기 위해 모두 1값을 가진 행렬 생성 w0_factors = np.ones((N,1)) # w1과 w0을 업데이트할 w1_update와 w0_update 계산 w1_update = -(2/N)*learning_rate*(np.dot(X.T, diff)) w0_update = -(2/N)*learning_rate*(np.dot(w0_factors.T, diff)) return w1_update, w0_update 12345678910w0 = np.zeros((1,1))w1 = np.zeros((1,1))y_pred = np.dot(X, w1.T) + w0diff = y-y_predprint(diff.shape)w0_factors = np.ones((100,1))w1_update = -(2/100)*0.01*(np.dot(X.T, diff))w0_update = -(2/100)*0.01*(np.dot(w0_factors.T, diff)) print(w1_update.shape, w0_update.shape)w1, w0 (100, 1) (1, 1) (1, 1) (array([[0.]]), array([[0.]])) 반복적으로 경사 하강법을 이용하여 get_weigth_updates()를 호출하여 w1과 w0를 업데이트 하는 함수 생성 12345678910111213# 입력 인자 iters로 주어진 횟수만큼 반복적으로 w1과 w0를 업데이트 적용함. def gradient_descent_steps(X, y, iters=10000): # w0와 w1을 모두 0으로 초기화. w0 = np.zeros((1,1)) w1 = np.zeros((1,1)) # 인자로 주어진 iters 만큼 반복적으로 get_weight_updates() 호출하여 w1, w0 업데이트 수행. for ind in range(iters): w1_update, w0_update = get_weight_updates(w1, w0, X, y, learning_rate=0.01) w1 = w1 - w1_update w0 = w0 - w0_update return w1, w0 - 예측 오차 비용을 계산을 수행하는 함수 생성 및 경사 하강법 수행 123456789def get_cost(y, y_pred): N = len(y) cost = np.sum(np.square(y - y_pred))/N return costw1, w0 = gradient_descent_steps(X, y, iters=1000)print(&quot;w1:{0:.3f} w0:{1:.3f}&quot;.format(w1[0,0], w0[0,0]))y_pred = w1[0,0] * X + w0print('Gradient Descent Total Cost:{0:.4f}'.format(get_cost(y, y_pred))) w1:4.022 w0:6.162 Gradient Descent Total Cost:0.9935 12plt.scatter(X, y)plt.plot(X,y_pred) [&lt;matplotlib.lines.Line2D at 0x2370ca26040&gt;] - 미니 배치 확률적 경사 하강법을 이용한 최적 비용함수 도출 123456789101112131415161718def stochastic_gradient_descent_steps(X, y, batch_size=10, iters=1000): w0 = np.zeros((1,1)) w1 = np.zeros((1,1)) prev_cost = 100000 iter_index =0 for ind in range(iters): np.random.seed(ind) # 전체 X, y 데이터에서 랜덤하게 batch_size만큼 데이터 추출하여 sample_X, sample_y로 저장 stochastic_random_index = np.random.permutation(X.shape[0]) # 임의로 추출 sample_X = X[stochastic_random_index[0:batch_size]] sample_y = y[stochastic_random_index[0:batch_size]] # 랜덤하게 batch_size만큼 추출된 데이터 기반으로 w1_update, w0_update 계산 후 업데이트 w1_update, w0_update = get_weight_updates(w1, w0, sample_X, sample_y, learning_rate=0.01) w1 = w1 - w1_update w0 = w0 - w0_update return w1, w0 1np.random.permutation(X.shape[0]) # 랜덤 샘플링을 임의로 가져옴 array([66, 71, 54, 88, 82, 12, 36, 46, 14, 67, 10, 3, 62, 29, 97, 69, 70, 93, 31, 73, 60, 96, 28, 27, 21, 19, 33, 78, 32, 94, 1, 41, 40, 76, 37, 87, 24, 23, 50, 2, 47, 20, 77, 17, 56, 64, 68, 25, 15, 22, 16, 98, 63, 92, 86, 38, 6, 57, 95, 44, 9, 42, 81, 99, 35, 84, 59, 48, 75, 65, 85, 90, 55, 43, 58, 89, 30, 80, 34, 18, 51, 49, 52, 74, 26, 45, 39, 4, 11, 53, 91, 79, 8, 0, 5, 13, 61, 72, 7, 83]) 1234w1, w0 = stochastic_gradient_descent_steps(X, y, iters=1000)print(&quot;w1:&quot;,round(w1[0,0],3),&quot;w0:&quot;,round(w0[0,0],3))y_pred = w1[0,0] * X + w0print('Stochastic Gradient Descent Total Cost:{0:.4f}'.format(get_cost(y, y_pred))) w1: 4.028 w0: 6.156 Stochastic Gradient Descent Total Cost:0.9937 04. 사이킷런 LinearRegression 클래스1class sklearn.linear_model.LinearRegression(fit_intercept=True,normalize=False, copy_X=True, n_jobs=1) LnearRegression 클래스는 예측값과 실제 값의 RSS를 최소화해 OLS 추정 방식으로 구현한 클래스이다. LinearRegression 클래스는 fit()메서드로 X,Y 배열을 입력 받으면 회귀 계수인 W를 coef_ 속성에 저장한다. [출처 :인프런] (1) 회귀평가 지표 - 사이킷런 회귀 평가 API 사이킷런은 아쉽게도 RMSE를 제공하지 않는다. RMSE를 구하기 위해서는 MSE에 제곱근을 씌워서 계산하는 함수를 직접 만들어야 한다. 다음은 각 평가 방법에 대한 사이킷런의 API 및 cross_val_score나 GridSearchCV에서 평가 시 사용되는 scoriong 파라미터의 적용 값이다 (2) LinearRegression을 이용한 보스턴 주택 가격 예측123456789101112131415161718import numpy as npimport matplotlib.pyplot as pltimport pandas as pdimport seaborn as snsfrom scipy import statsfrom sklearn.datasets import load_boston%matplotlib inline# boston 데이타셋 로드boston = load_boston()# boston 데이타셋 DataFrame 변환 bostonDF = pd.DataFrame(boston.data , columns = boston.feature_names)# boston dataset의 target array는 주택 가격임. 이를 PRICE 컬럼으로 DataFrame에 추가함. bostonDF['PRICE'] = boston.targetprint('Boston 데이타셋 크기 :',bostonDF.shape)bostonDF.head() Boston 데이타셋 크기 : (506, 14) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT PRICE 0 0.00632 18.0 2.31 0.0 0.538 6.575 65.2 4.0900 1.0 296.0 15.3 396.90 4.98 24.0 1 0.02731 0.0 7.07 0.0 0.469 6.421 78.9 4.9671 2.0 242.0 17.8 396.90 9.14 21.6 2 0.02729 0.0 7.07 0.0 0.469 7.185 61.1 4.9671 2.0 242.0 17.8 392.83 4.03 34.7 3 0.03237 0.0 2.18 0.0 0.458 6.998 45.8 6.0622 3.0 222.0 18.7 394.63 2.94 33.4 4 0.06905 0.0 2.18 0.0 0.458 7.147 54.2 6.0622 3.0 222.0 18.7 396.90 5.33 36.2 CRIM: 지역별 범죄 발생률 ZN: 25,000평방피트를 초과하는 거주 지역의 비율 NDUS: 비상업 지역 넓이 비율 CHAS: 찰스강에 대한 더미 변수(강의 경계에 위치한 경우는 1, 아니면 0) NOX: 일산화질소 농도 RM: 거주할 수 있는 방 개수 AGE: 1940년 이전에 건축된 소유 주택의 비율 DIS: 5개 주요 고용센터까지의 가중 거리 RAD: 고속도로 접근 용이도 TAX: 10,000달러당 재산세율 PTRATIO: 지역의 교사와 학생 수 비율 B: 지역의 흑인 거주 비율 LSTAT: 하위 계층의 비율 MEDV: 본인 소유의 주택 가격(중앙값) - 각 컬럼별로 주택가격에 미치는 영향도 조사 12345678# 2개의 행과 4개의 열을 가진 subplots를 이용. axs는 4x2개의 ax를 가짐.fig, axs = plt.subplots(figsize=(16,8) , ncols=4 , nrows=2)lm_features = ['RM','ZN','INDUS','NOX','AGE','PTRATIO','LSTAT','RAD']for i , feature in enumerate(lm_features): row = int(i/4) col = i%4 # 시본의 regplot을 이용해 산점도와 선형 회귀 직선을 함께 표현 sns.regplot(x=feature , y='PRICE',data=bostonDF , ax=axs[row][col]) 결과 해석 다른 칼럼보다 RM과 LSTAT의 PRICE 영향도가 가장 두드러지게 나타남 RM(방 개수)은 양방향의 선형성이 가장 크다→ 방의 크기가 클수록 가격이 증가함 LSTAT는 음방향의 선형성이 가장 큼→ LSTAT이 적을수록 PRICE가 증가함 - LinearRegression 클래스로 보스턴 주택 가격의 회귀 모델 만들기 123456789101112131415161718from sklearn.model_selection import train_test_splitfrom sklearn.linear_model import LinearRegressionfrom sklearn.metrics import mean_squared_error , r2_scorey_target = bostonDF['PRICE']X_data = bostonDF.drop(['PRICE'],axis=1,inplace=False)X_train , X_test , y_train , y_test = train_test_split(X_data , y_target ,test_size=0.3, random_state=156)# 선형회귀 OLS로 학습/예측/평가 수행. lr = LinearRegression()lr.fit(X_train ,y_train )y_preds = lr.predict(X_test)mse = mean_squared_error(y_test, y_preds)rmse = np.sqrt(mse)print('MSE : {0:.3f} , RMSE : {1:.3F}'.format(mse , rmse))print('Variance score : {0:.3f}'.format(r2_score(y_test, y_preds))) MSE : 17.297 , RMSE : 4.159 Variance score : 0.757 123# 절편과 회귀 계수 값 보기print('절편 값:',lr.intercept_)print('회귀 계수값:', np.round(lr.coef_, 1)) 절편 값: 40.995595172164336 회귀 계수값: [ -0.1 0.1 0. 3. -19.8 3.4 0. -1.7 0.4 -0. -0.9 0. -0.6] 1234# coef_ 속성은 회귀 계수 값만 가지고 있어, 이를 피처별 회귀 계수 값으로 재 매핑# 회귀 계수를 큰 값 순으로 정렬하기 위해 Series로 생성. index가 컬럼명에 유의coeff = pd.Series(data=np.round(lr.coef_, 1), index=X_data.columns )coeff.sort_values(ascending=False) RM 3.4 CHAS 3.0 RAD 0.4 ZN 0.1 B 0.0 TAX -0.0 AGE 0.0 INDUS 0.0 CRIM -0.1 LSTAT -0.6 PTRATIO -0.9 DIS -1.7 NOX -19.8 dtype: float64 결과 해석 RM이 양의 값으로 회귀 계수가 가장 큼 NOX 피처의 회귀 계수 - 값이 너무 커보임→ 최적화를 수행하면서 피처 codfficients의 변화 살필 예정 - 5개의 폴드 세트에서 cross_val_score()로 교차 검증하기: MSE, RMSE 측정사이킷런은 cross_val_score()를 이용하는데, RMSE를 제공하지 않으므로 MSE 수치 결과를 RMSE로 변환해야 한다. cross_val_score()의 인자로 scoring=’neg_mean_squared_error’를 지칭하면 반환되는 수치 값은 음수이다. 사이킷런은 높은 지표 값일수록 좋은 모델로 평가히는데 반해, 회귀는 MSE 값이 낮을수록 좋은 회귀 모델로 평가한다. 12345678910111213141516from sklearn.model_selection import cross_val_scorey_target = bostonDF['PRICE']X_data = bostonDF.drop(['PRICE'],axis=1,inplace=False)lr = LinearRegression()# cross_val_score( )로 5 Fold 셋으로 MSE 를 구한 뒤 이를 기반으로 다시 RMSE 구함. neg_mse_scores = cross_val_score(lr, X_data, y_target, scoring=&quot;neg_mean_squared_error&quot;, cv = 5)rmse_scores = np.sqrt(-1 * neg_mse_scores)avg_rmse = np.mean(rmse_scores)# cross_val_score(scoring=&quot;neg_mean_squared_error&quot;)로 반환된 값은 모두 음수 print(' 5 folds 의 개별 Negative MSE scores: ', np.round(neg_mse_scores, 2))print(' 5 folds 의 개별 RMSE scores : ', np.round(rmse_scores, 2))print(' 5 folds 의 평균 RMSE : {0:.3f} '.format(avg_rmse)) 5 folds 의 개별 Negative MSE scores: [-12.46 -26.05 -33.07 -80.76 -33.31] 5 folds 의 개별 RMSE scores : [3.53 5.1 5.75 8.99 5.77] 5 folds 의 평균 RMSE : 5.829 결과 해석 5개 폴드 세트 교차 검증 수행 결과, 평균 RMSE는 약 5.836이 나옴 corss_val_score(scoring=”neg_mean_squared_error”)로 반환된 값은 모두 음수로 확인됨 05. 다항 회귀, 과(대)적합/과소적합(1) 다항 회귀 이해 현재까지 설명한 회귀는 y = $w_{0} + w_{1}*x_{1} + w_{2}*x_{2} + , … , + w_{n}*x_{n}$과 같이 독립변수(feature)와 종속변수(target) 관계가 일차 방정식 형태로 표현된 회귀 세상의 모든 관계를 직선으로만 표현할 수 없기 때문에 다항 회귀 개념이 필요 다항(Polynomial) 회귀: 회귀가 독립변수의 단항식이 아닌 2차, 3차 방정식과 같은 다항식으로 표현되는 것 y = $w_{0} + w_{1}*x_{1} + w_{2}*x_{2} + w_{3}*x_{1}*x_{2} + w_{4}*x_{1}^{2} + w_{5}*x_{2}^{2}$ 다항 회귀는 선형 회귀(비선형 회귀가 아님) cf) 회귀에서 선형/비선형을 나누는 기준은 회귀 계수가 선형/비선형인지에 따른 것으로 독립변수의 선형/비선형 여부와는 무관함 사이킷런은 다항 회귀를 위한 클래스를 명시적으로 제공하지 않아, 비선형 함수를 선형 모델에 적용시키는 방법으로 구현 PolynomialFeatures 클래스로 피처를 다항식 피처로 변환함 123456789101112from sklearn.preprocessing import PolynomialFeaturesimport numpy as np# 다항식으로 변환한 단항식 생성, [[0,1],[2,3]]의 2X2 행렬 생성X = np.arange(4).reshape(2,2)print('일차 단항식 계수 feature:\\n',X )# degree = 2 인 2차 다항식으로 변환하기 위해 PolynomialFeatures를 이용하여 변환poly = PolynomialFeatures(degree=2)poly.fit(X)poly_ftr = poly.transform(X)print('변환된 2차 다항식 계수 feature:\\n', poly_ftr) 일차 단항식 계수 feature: [[0 1] [2 3]] 변환된 2차 다항식 계수 feature: [[1. 0. 1. 0. 0. 1.] [1. 2. 3. 4. 6. 9.]] - 3차 다항 회귀 함수를 임의로 설정하고 회귀 계수 예측하기 1234567891011def polynomial_func(X): y = 1 + 2*X[:,0] + 3*X[:,0]**2 + 4*X[:,1]**3 print(X[:, 0]) print(X[:, 1]) return yX = np.arange(0,4).reshape(2,2)print('일차 단항식 계수 feature: \\n' ,X)y = polynomial_func(X)print('삼차 다항식 결정값: \\n', y) 일차 단항식 계수 feature: [[0 1] [2 3]] [0 2] [1 3] 삼차 다항식 결정값: [ 5 125] - 일차 단항식 계수를 삼차 다항식 계수로 변환하고, 선형 회귀에 적용 123456789# 3 차 다항식 변환 poly_ftr = PolynomialFeatures(degree=3).fit_transform(X)print('3차 다항식 계수 feature: \\n',poly_ftr)# Linear Regression에 3차 다항식 계수 feature와 3차 다항식 결정값으로 학습 후 회귀 계수 확인model = LinearRegression()model.fit(poly_ftr,y)print('Polynomial 회귀 계수\\n' , np.round(model.coef_, 2))print('Polynomial 회귀 Shape :', model.coef_.shape) 3차 다항식 계수 feature: [[ 1. 0. 1. 0. 0. 1. 0. 0. 0. 1.] [ 1. 2. 3. 4. 6. 9. 8. 12. 18. 27.]] Polynomial 회귀 계수 [0. 0.18 0.18 0.36 0.54 0.72 0.72 1.08 1.62 2.34] Polynomial 회귀 Shape : (10,) 결과 해석 일차 단항식 계수 피처는 2개였지만, 3차 다항식 Polynomial 변환 이후에는 다항식 계수 피처가 10개로 늘어남 늘어난 피처 데이터 세트에 LinearRegression을 통해 3차 다항 회귀 형태의 다항 회귀를 적용하면 회귀 계수가 10개로 늘어남 10개의 회귀 계수가 도출됐으며 원래 다항식 계수 값과는 차이가 있지만, 다항 회귀로 근사함을 알 수 있음 - 사이킷런의 Pipeline 객체를 이용해 한 번에 다항 회귀 구현하기 1234567891011121314151617from sklearn.preprocessing import PolynomialFeaturesfrom sklearn.linear_model import LinearRegressionfrom sklearn.pipeline import Pipelineimport numpy as npdef polynomial_func(X): y = 1 + 2*X[:,0] + 3*X[:,0]**2 + 4*X[:,1]**3 return y# Pipeline 객체로 Streamline 하게 Polynomial Feature변환과 Linear Regression을 연결model = Pipeline([('poly', PolynomialFeatures(degree=3)), ('linear', LinearRegression())])X = np.arange(4).reshape(2,2)y = polynomial_func(X)model = model.fit(X, y)print('Polynomial 회귀 계수\\n', np.round(model.named_steps['linear'].coef_, 2)) Polynomial 회귀 계수 [0. 0.18 0.18 0.36 0.54 0.72 0.72 1.08 1.62 2.34] (2) 다항 회귀를 이용한 과소적합 및 과적합 이해 다항 회귀는 피처의 직선적 관계가 아닌, 복잡한 다항 관계를 모델링할 수 있음 다항식 차수가 높아질수록 매우 복잡한 피처 관계까지 모델링이 가능함 단, 다항 회귀의 차수(degree)를 높일수록 학습 데이터에 너무 맞춘 학습이 이루어져서 테스트 데이터 환경에서 예측 정확도가 떨어짐→ 차수가 높아질수록 과적합 문제가 크게 발생 - 다항 회귀의 과소적합과 과적합 문제를 잘 보여주는 예시- 원본 소스코드 설명 피처 X와 target y가 잡음(Noise)이 포함된 다항식의 코사인 그래프 관계를 가지게 만듦 이에 기반해 다항 회귀의 차수를 변화시키며 그에 따른 회귀 예측 곡선과 예측 정확도를 비교하는 예제 학습 데이터: 30개의 임의 데이터 X, X의 코사인 값에서 약간의 잡음 변동 값을 더한 target 12345678910111213141516171819import numpy as npimport matplotlib.pyplot as pltfrom sklearn.pipeline import Pipelinefrom sklearn.preprocessing import PolynomialFeaturesfrom sklearn.linear_model import LinearRegressionfrom sklearn.model_selection import cross_val_score%matplotlib inline# random 값으로 구성된 X값에 대해 Cosine 변환값을 반환. def true_fun(X): return np.cos(1.5 * np.pi * X)# X는 0 부터 1까지 30개의 random 값을 순서대로 sampling 한 데이타 입니다. np.random.seed(0)n_samples = 30X = np.sort(np.random.rand(n_samples))# y 값은 cosine 기반의 true_fun() 에서 약간의 Noise 변동값을 더한 값입니다. y = true_fun(X) + np.random.randn(n_samples) * 0.1 - 예측 결과를 비교할 다항식 차수를 각각 1, 4, 15로 변경하며 예측 결과 비교하기 다항식 차수별로 학습 수행 후, cross_val_score()로 MSE 값을 구해 차수별 예측 성능 평가 0부터 1까지 균일하게 구성된 100개의 테스트용 데이터 세트로 차수별 회귀 예측 곡선 그리기 12345678910111213141516171819202122232425262728293031323334plt.figure(figsize=(14, 5))degrees = [1, 4, 15]# 다항 회귀의 차수(degree)를 1, 4, 15로 각각 변화시키면서 비교합니다. for i in range(len(degrees)): ax = plt.subplot(1, len(degrees), i + 1) plt.setp(ax, xticks=(), yticks=()) # 개별 degree별로 Polynomial 변환합니다. polynomial_features = PolynomialFeatures(degree=degrees[i], include_bias=False) linear_regression = LinearRegression() pipeline = Pipeline([(&quot;polynomial_features&quot;, polynomial_features), (&quot;linear_regression&quot;, linear_regression)]) pipeline.fit(X.reshape(-1, 1), y) # 교차 검증으로 다항 회귀를 평가합니다. scores = cross_val_score(pipeline, X.reshape(-1,1), y,scoring=&quot;neg_mean_squared_error&quot;, cv=10) coefficients = pipeline.named_steps['linear_regression'].coef_ print('\\nDegree {0} 회귀 계수는 {1} 입니다.'.format(degrees[i], np.round(coefficients),2)) print('Degree {0} MSE 는 {1:.2f} 입니다.'.format(degrees[i] , -1*np.mean(scores))) # 0 부터 1까지 테스트 데이터 세트를 100개로 나눠 예측을 수행합니다. # 테스트 데이터 세트에 회귀 예측을 수행하고 예측 곡선과 실제 곡선을 그려서 비교합니다. X_test = np.linspace(0, 1, 100) # 예측값 곡선 plt.plot(X_test, pipeline.predict(X_test[:, np.newaxis]), label=&quot;Model&quot;) # 실제 값 곡선 plt.plot(X_test, true_fun(X_test), '--', label=&quot;True function&quot;) plt.scatter(X, y, edgecolor='b', s=20, label=&quot;Samples&quot;) plt.xlabel(&quot;x&quot;); plt.ylabel(&quot;y&quot;); plt.xlim((0, 1)); plt.ylim((-2, 2)); plt.legend(loc=&quot;best&quot;) plt.title(&quot;Degree {}\\nMSE = {:.2e}(+/- {:.2e})&quot;.format(degrees[i], -scores.mean(), scores.std()))plt.show() Degree 1 회귀 계수는 [-2.] 입니다. Degree 1 MSE 는 0.41 입니다. Degree 4 회귀 계수는 [ 0. -18. 24. -7.] 입니다. Degree 4 MSE 는 0.04 입니다. Degree 15 회귀 계수는 [-2.98300000e+03 1.03900000e+05 -1.87417100e+06 2.03717220e+07 -1.44873987e+08 7.09318780e+08 -2.47066977e+09 6.24564048e+09 -1.15677067e+10 1.56895696e+10 -1.54006776e+10 1.06457788e+10 -4.91379977e+09 1.35920330e+09 -1.70381654e+08] 입니다. Degree 15 MSE 는 182815433.48 입니다. 실선: 다항 회귀 예측 곡선 점선: 실제 데이터 세트 X, Y의 코사인 곡선 학습 데이터: 0부터 1까지 30개의 임의 X값과 그에 따른 코사인 Y값에 잡음을 변동 값으로 추가해 구성 MSE(Mean Squared Error) 평가: 학습 데이터를 10개의 교차 검증 세트로 나누어 측정한 후 평균한 것 결과 해석 Degree 1 예측 곡선 단순한 직선으로 단순 선형 회귀와 동일 실제 데이터 세트인 코사인 데이터 세트를 직선으로 예측하기에는 너무 단순함 예측 곡선이 학습 데이터 패턴을 반영하지 못하는 과소적합 모델 MSE값: 약 0.407 Degree 4 예측 곡선 실제 데이터 세트와 유사한 모습 변동하는 잡음까지는 예측하지 못했지만, 학습 데이터 세트를 비교적 잘 반영해 코사인 곡선 기반으로 테스트 데이터를 잘 예측한 곡선을 가진 모델 MSE값: 0.043 (가장 뛰어난 예측 성능) Degree 15 예측 곡선 MSE값이 182815432이 될 정도로 이상한 오류 값 발생(과적합 강조를 위해 만든 예측 곡선) 데이터 세트의 변동 잡음까지 지나치게 반영한 결과, 예측 곡선이 학습 데이터 세트만 정확히 예측하고 테스트 값의 실제 곡선과는 완전히 다른 형태의 예측 곡선이 만들어짐 학습 데이터에 너무 충실하게 맞춘 심한 과적합 모델 결론 좋은 예측 모델은 학습 데이터 패턴을 잘 반영하면서도 복잡하지 않은, 균형 잡힌(Balanced) 모델을 의미 (3) 편향-분산 트레이드 오프 머신러닝이 극복해야 할 이슈 고편향(High Bias)성 Degree 1 모델처럼 매우 단순화된 모델로서 지나치게 한 방향성으로 치우친 경향을 보임 고분산(High Variance)성 Degree 15 모델처럼 학습 데이터 하나하나 특성을 반영하여 매우 복잡하고 지나치게 높은 변동성을 가짐 - 이미지 출처 일반적으로 편향과 분산은 한 쪽이 높으면, 한 쪽이 낮아지는 경향이 있음→ 편향이 높으면 분산이 낮아지고(과소적합), 분산이 높으면 편향이 낮아짐(과적합) 편향과 분산 관계에 따른 전체 오류 값(Total Error) 변화- 이미지 출처 편향이 너무 높으면 전체 오류가 높음 편향을 낮출수록 분산이 높아지고 전체 오류도 낮아짐 골디락스: 편향을 낮추고 분산을 높이며, 전체 오류가 가장 낮아지는 지점 골디락스 지점을 통과하며 분산을 지속적으로 높이면 전체 오류 값이 오히려 증가하며 예측 성능이 다시 저하됨 정리 과소적합: 높은 편향/낮은 분산에서 일어나기 쉬움 과적합: 낮은 편향/높은 분산에서 일어나기 쉬움→ 편향과 분산이 트레이드 오프를 이루며 오류 cost 값이 최대로 낮아지는 모델을 구축하는 것이 중요 06. 규제 선형 모델: 릿지, 라쏘, 엘라스틱넷(1) 규제 선형 모델의 개요 이전까지의 선형 모델 비용 함수는 RSS를 최소화하는(실제값과 예측값의 차이를 최소화하는) 것만 고려 학습 데이터에 지나치게 맞추게 되고, 회귀 계수가 쉽게 커지는 문제가 발생 변동성이 오히려 심해져 테스트 데이터 세트에서 예측 성능이 저하되게 쉬움→ 비용 함수는 학습 데이터의 잔차 오류값을 최소로 하는 RSS 최소화 방법과 과적합을 방지하기 위해 회귀 계수 값이 커지지 않도록 하는 방법이 균형을 이루어야 함 회귀 계수의 크기를 제어해 과적합을 개선하려면, 비용(Cost) 함수의 목표가 아래와 같이 Min(RSS(W)+alpha∗ | | W | | 2 2 ) 를 최소화하는 것으로 변경될 수 있음 비용 함수 목표 = M i n ( R S S ( W ) + a l p h a ∗ | | W | | 2 2 ) alpha: 학습 데이터 적합 정도와 회귀 계수 값의 크기 제어를 수행하는 튜닝 파라미터 alpha가 0(또는 매우 작은 값)이라면 비용 함수 식은 기존과 동일한 Min(RSS(W)+0)가 됨 alpha가 무한대(또는 매우 큰 값)이라면 비용 함수 식은 RSS(W)에 비해 alpha∗ | | W | | 2 2 값이 너무 커지므로 W 값을 0 또는 매우 작게 만들어야 Cost가 최소화 되는 비용 함수 목표를 달성할 수 있음 → alpha 값을 크게 하면 비용 함수는 회귀 계수 W의 값을 작게 해 과적합을 개선할 수 있으며 alpha 값을 작게 하면 회귀 계수 W의 값이 커져도 어느 정도 상쇄가 가능하므로 학습 데이터 적합을 더 개선할 수 있음 alpha를 0에서부터 지속적으로 값을 증가시키면 회귀 계수 값의 크기를 감소시킬 수 있음 규제(Regularization): 비용 함수에 alpha 값으로 패널티를 부여해 회귀 계수 값의 크기를 감소해 과적합을 개선하는 방식 규제 방식 L2 규제: alpha∗ | | W | | 2 2 와 같이 W 제곱에 패널티를 부여하는 방식 ← 릿지(Ridge) 회귀 L1 규제: alpha∗ | | W | | 1 와 같이 W의 절대값에 패널티를 부여, 영향력이 크지 않은 회귀 계수 값을 0으로 변환 ← 라쏘(Lasso) 회귀 (2) 릿지 회귀 사이킷런은 Ridge 클래스로 릿지 회귀를 구현 주요 생성 파라미터: alpha, 릿지 회귀의 alpha L2 규제 계수에 해당 - 보스턴 주택 가격을 Ridge 클래스로 예측하고, cross_val_score()로 평가하기 12345678910111213141516171819202122232425# 앞의 LinearRegression예제에서 분할한 feature 데이터 셋인 X_data과 Target 데이터 셋인 Y_target 데이터셋을 그대로 이용 from sklearn.linear_model import Ridgefrom sklearn.model_selection import cross_val_score# boston 데이타셋 로드boston = load_boston()# boston 데이타셋 DataFrame 변환 bostonDF = pd.DataFrame(boston.data , columns = boston.feature_names)# boston dataset의 target array는 주택 가격임. 이를 PRICE 컬럼으로 DataFrame에 추가함. bostonDF['PRICE'] = boston.targetprint('Boston 데이타셋 크기 :',bostonDF.shape)y_target = bostonDF['PRICE']X_data = bostonDF.drop(['PRICE'],axis=1,inplace=False)ridge = Ridge(alpha = 10)neg_mse_scores = cross_val_score(ridge, X_data, y_target, scoring=&quot;neg_mean_squared_error&quot;, cv = 5)rmse_scores = np.sqrt(-1 * neg_mse_scores)avg_rmse = np.mean(rmse_scores)print(' 5 folds 의 개별 Negative MSE scores: ', np.round(neg_mse_scores, 3))print(' 5 folds 의 개별 RMSE scores : ', np.round(rmse_scores,3))print(' 5 folds 의 평균 RMSE : {0:.3f} '.format(avg_rmse)) Boston 데이타셋 크기 : (506, 14) 5 folds 의 개별 Negative MSE scores: [-11.422 -24.294 -28.144 -74.599 -28.517] 5 folds 의 개별 RMSE scores : [3.38 4.929 5.305 8.637 5.34 ] 5 folds 의 평균 RMSE : 5.518 결과 해석 릿지의 5개 폴드 세트 평균 RMSE: 5.524 앞 예제(규제 없는 LinearRegression) 평균인 5.836보다 뛰어난 예측 성능을 보임 - 릿지의 alpha 값을 0, 0.1, 1, 10, 100으로 변화시키며 RMSE와 회귀 계수 값 변화 살펴보기 1234567891011# Ridge에 사용될 alpha 파라미터의 값들을 정의alphas = [0 , 0.1 , 1 , 10 , 100]# alphas list 값을 iteration하면서 alpha에 따른 평균 rmse 구함.for alpha in alphas : ridge = Ridge(alpha = alpha) #cross_val_score를 이용하여 5 fold의 평균 RMSE 계산 neg_mse_scores = cross_val_score(ridge, X_data, y_target, scoring=&quot;neg_mean_squared_error&quot;, cv = 5) avg_rmse = np.mean(np.sqrt(-1 * neg_mse_scores)) print('alpha {0} 일 때 5 folds 의 평균 RMSE : {1:.3f} '.format(alpha,avg_rmse)) alpha 0 일 때 5 folds 의 평균 RMSE : 5.829 alpha 0.1 일 때 5 folds 의 평균 RMSE : 5.788 alpha 1 일 때 5 folds 의 평균 RMSE : 5.653 alpha 10 일 때 5 folds 의 평균 RMSE : 5.518 alpha 100 일 때 5 folds 의 평균 RMSE : 5.330 결과 해석 alpha가 100일 때, 평균 RMSE가 5.332로 가장 좋음 - alpha 값 변화에 따른 피처의 회귀 계수 값을 가로 막대 그래프로 시각화 123456789101112131415161718192021# 각 alpha에 따른 회귀 계수 값을 시각화하기 위해 5개의 열로 된 맷플롯립 축 생성 fig , axs = plt.subplots(figsize=(18,6) , nrows=1 , ncols=5)# 각 alpha에 따른 회귀 계수 값을 데이터로 저장하기 위한 DataFrame 생성 coeff_df = pd.DataFrame()# alphas 리스트 값을 차례로 입력해 회귀 계수 값 시각화 및 데이터 저장. pos는 axis의 위치 지정for pos , alpha in enumerate(alphas) : ridge = Ridge(alpha = alpha) ridge.fit(X_data , y_target) # alpha에 따른 피처별 회귀 계수를 Series로 변환하고 이를 DataFrame의 컬럼으로 추가. coeff = pd.Series(data=ridge.coef_ , index=X_data.columns ) colname='alpha:'+str(alpha) coeff_df[colname] = coeff # 막대 그래프로 각 alpha 값에서의 회귀 계수를 시각화. 회귀 계수값이 높은 순으로 표현 coeff = coeff.sort_values(ascending=False) axs[pos].set_title(colname) axs[pos].set_xlim(-3,6) sns.barplot(x=coeff.values , y=coeff.index, ax=axs[pos])# for 문 바깥에서 맷플롯립의 show 호출 및 alpha에 따른 피처별 회귀 계수를 DataFrame으로 표시plt.show() 결과 해석 alpha 값을 계속 증가시킬수록 회귀 계수 값은 지속적으로 작아짐 특히, NOX 피처의 경우 alpha 값을 계속 증가시킴에 따라 회귀 계수가 크게 작아지고 있음 - DataFrame에 저장된 alpha 값 변화에 따른 릿지 회귀 계수 값 구하기 123ridge_alphas = [0 , 0.1 , 1 , 10 , 100]sort_column = 'alpha:'+str(ridge_alphas[0])coeff_df.sort_values(by=sort_column, ascending=False) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } alpha:0 alpha:0.1 alpha:1 alpha:10 alpha:100 RM 3.809865 3.818233 3.854000 3.702272 2.334536 CHAS 2.686734 2.670019 2.552393 1.952021 0.638335 RAD 0.306049 0.303515 0.290142 0.279596 0.315358 ZN 0.046420 0.046572 0.047443 0.049579 0.054496 INDUS 0.020559 0.015999 -0.008805 -0.042962 -0.052826 B 0.009312 0.009368 0.009673 0.010037 0.009393 AGE 0.000692 -0.000269 -0.005415 -0.010707 0.001212 TAX -0.012335 -0.012421 -0.012912 -0.013993 -0.015856 CRIM -0.108011 -0.107474 -0.104595 -0.101435 -0.102202 LSTAT -0.524758 -0.525966 -0.533343 -0.559366 -0.660764 PTRATIO -0.952747 -0.940759 -0.876074 -0.797945 -0.829218 DIS -1.475567 -1.459626 -1.372654 -1.248808 -1.153390 NOX -17.766611 -16.684645 -10.777015 -2.371619 -0.262847 결과 해석 alpha 값이 증가하며 회귀 계소가 지속적으로 작아짐 단, 릿지 회귀는 회귀 계수를 0으로 만들지 않음 (3) 라쏘 회귀 라쏘 회귀: W의 절댓값에 패널티를 부여하는 L1 규제를 선형 회귀에 적용한 것 L1 규제는 alpha∗ | | W | | 1 를 의미하며, 라쏘 회귀 비용함수 목표는 RSS(W) + a l p h a ∗ | | W | | 1 식을 최소화하는 W를 찾는 것 L2 규제가 회귀 계쑤 크기를 감소시키는 데 반해, L1 규제는 불필요한 회귀 계수를 급격히 감소시켜 0으로 만들고 제거함 L1 규제는 적절한 피처만 회귀에 포함시키는 피처 선택의 득성을 가짐 사이킷런은 Lasso 클래스로 라쏘 회귀를 구현 주요 파라미터: alpha, 라쏘 회귀의 alpha L1 규제 계수에 해당 - Lasso 클래스로 라쏘의 alpha 값을 변화시키며 RMSE와 각 피처의 회귀 계수 출력하기 1234567891011121314151617181920212223242526from sklearn.linear_model import Lasso, ElasticNet# alpha값에 따른 회귀 모델의 폴드 평균 RMSE를 출력하고 회귀 계수값들을 DataFrame으로 반환 def get_linear_reg_eval(model_name, params=None, X_data_n=None, y_target_n=None, verbose=True, return_coeff=True): coeff_df = pd.DataFrame() if verbose : print('####### ', model_name , '#######') for param in params: if model_name =='Ridge': model = Ridge(alpha=param) elif model_name =='Lasso': model = Lasso(alpha=param) elif model_name =='ElasticNet': model = ElasticNet(alpha=param, l1_ratio=0.7) neg_mse_scores = cross_val_score(model, X_data_n, y_target_n, scoring=&quot;neg_mean_squared_error&quot;, cv = 5) avg_rmse = np.mean(np.sqrt(-1 * neg_mse_scores)) print('alpha {0}일 때 5 폴드 세트의 평균 RMSE: {1:.3f} '.format(param, avg_rmse)) # cross_val_score는 evaluation metric만 반환하므로 모델을 다시 학습하여 회귀 계수 추출 model.fit(X_data_n , y_target_n) if return_coeff: # alpha에 따른 피처별 회귀 계수를 Series로 변환하고 이를 DataFrame의 컬럼으로 추가. coeff = pd.Series(data=model.coef_ , index=X_data_n.columns ) colname='alpha:'+str(param) coeff_df[colname] = coeff return coeff_df# end of get_linear_regre_eval 123# 라쏘에 사용될 alpha 파라미터의 값들을 정의하고 get_linear_reg_eval() 함수 호출lasso_alphas = [ 0.07, 0.1, 0.5, 1, 3]coeff_lasso_df =get_linear_reg_eval('Lasso', params=lasso_alphas, X_data_n=X_data, y_target_n=y_target) ####### Lasso ####### alpha 0.07일 때 5 폴드 세트의 평균 RMSE: 5.612 alpha 0.1일 때 5 폴드 세트의 평균 RMSE: 5.615 alpha 0.5일 때 5 폴드 세트의 평균 RMSE: 5.669 alpha 1일 때 5 폴드 세트의 평균 RMSE: 5.776 alpha 3일 때 5 폴드 세트의 평균 RMSE: 6.189 결과 해석 alpha가 0.07일 때, 가장 좋은 평균 RMSE를 보여줌 - alpha 값에 따른 피처별 회귀 계수 123# 반환된 coeff_lasso_df를 첫번째 컬럼순으로 내림차순 정렬하여 회귀계수 DataFrame출력sort_column = 'alpha:'+str(lasso_alphas[0])coeff_lasso_df.sort_values(by=sort_column, ascending=False) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } alpha:0.07 alpha:0.1 alpha:0.5 alpha:1 alpha:3 RM 3.789725 3.703202 2.498212 0.949811 0.000000 CHAS 1.434343 0.955190 0.000000 0.000000 0.000000 RAD 0.270936 0.274707 0.277451 0.264206 0.061864 ZN 0.049059 0.049211 0.049544 0.049165 0.037231 B 0.010248 0.010249 0.009469 0.008247 0.006510 NOX -0.000000 -0.000000 -0.000000 -0.000000 0.000000 AGE -0.011706 -0.010037 0.003604 0.020910 0.042495 TAX -0.014290 -0.014570 -0.015442 -0.015212 -0.008602 INDUS -0.042120 -0.036619 -0.005253 -0.000000 -0.000000 CRIM -0.098193 -0.097894 -0.083289 -0.063437 -0.000000 LSTAT -0.560431 -0.568769 -0.656290 -0.761115 -0.807679 PTRATIO -0.765107 -0.770654 -0.758752 -0.722966 -0.265072 DIS -1.176583 -1.160538 -0.936605 -0.668790 -0.000000 결과 해석 alpha의 크기가 증가함에 따라 일부 피처 회귀 계수는 아예 0으로 바뀜 NOX 속성은 alpha가 0.07일 때부터 회귀 계수가 0이며, alpha를 증가시키며 INDUS, CHAS와 같은 속성 회귀 계수가 0으로 바뀜 회귀 계수가 0인 피처는 회귀 식에서 제외되며 피처 선택의 효과를 얻을 수 있음 (4) 엘라스틱넷 회귀 엘라스틱넷(Elastic Net) 회귀: L2 규제와 L1 규제를 결합한 회귀 엘라스틱넷 회귀 비용함수 목표: RSS(W)+alpha2∗ | | W | | 2 2 + a l p h a 1 ∗ | | W | | 1 식을 최소화하는 W를 찾는 것 엘라스틱넷은 라쏘 회귀가 상관관계가 높은 피처들의 경우에, 중요 피처만을 선택하고 다른 피처 회귀 계수는 0으로 만드는 성향이 강함 alpha 값에 따라 회귀 계쑤 값이 급격히 변동할 수 있는데, 엘라스틱넷 회귀는 이를 완화하기 위해 L2 규제를 라쏘 회귀에 추가한 것 엘라스틱넷 회귀의 단점은 L1과 L2 규제가 결합된 규제로 인해 수행 시간이 상대적으로 오래 걸림 사이킷런은 Elastic Net 클래스로 엘라스틱넷 회귀를 구현 주요 파라미터: aplha, l1_ration Elastic Net 클래스의 aplha는 Ridge와 Lasso 클래스의 alpha 값과는 다름 엘라스틱넷 규제는 a * L1 + b * L2로 정의될 수 있으며, 이 때 a는 L1 규제의 alpha값, b는 L2 규제의 alpha 값 따라서 ElasticNet 클래스의 alpha 파라미터 값은 a + b 값 ElasticNet 클래스의 l1_ratio 파라미터 값은 a / (a + b) l1_ratio가 0이면 a가 0이므로 L2 규제와 동일하고, l1_ratio가 1이면 b가 0이므로 L1 규제와 동일 - Elastic Net 클래스로 엘라스틱넷 alpha 값을 변화시키며 RMSE와 각 피처의 회귀 계수 출력하기 l1_ratio를 0.7로 고정한 이유: 단순히 alpha 값의 변화만 살피기 위해 12345# 엘라스틱넷에 사용될 alpha 파라미터의 값들을 정의하고 get_linear_reg_eval() 함수 호출# l1_ratio는 0.7로 고정elastic_alphas = [ 0.07, 0.1, 0.5, 1, 3]coeff_elastic_df =get_linear_reg_eval('ElasticNet', params=elastic_alphas, X_data_n=X_data, y_target_n=y_target) ####### ElasticNet ####### alpha 0.07일 때 5 폴드 세트의 평균 RMSE: 5.542 alpha 0.1일 때 5 폴드 세트의 평균 RMSE: 5.526 alpha 0.5일 때 5 폴드 세트의 평균 RMSE: 5.467 alpha 1일 때 5 폴드 세트의 평균 RMSE: 5.597 alpha 3일 때 5 폴드 세트의 평균 RMSE: 6.068 123# 반환된 coeff_elastic_df를 첫번째 컬럼순으로 내림차순 정렬하여 회귀계수 DataFrame출력sort_column = 'alpha:'+str(elastic_alphas[0])coeff_elastic_df.sort_values(by=sort_column, ascending=False) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } alpha:0.07 alpha:0.1 alpha:0.5 alpha:1 alpha:3 RM 3.574162 3.414154 1.918419 0.938789 0.000000 CHAS 1.330724 0.979706 0.000000 0.000000 0.000000 RAD 0.278880 0.283443 0.300761 0.289299 0.146846 ZN 0.050107 0.050617 0.052878 0.052136 0.038268 B 0.010122 0.010067 0.009114 0.008320 0.007020 AGE -0.010116 -0.008276 0.007760 0.020348 0.043446 TAX -0.014522 -0.014814 -0.016046 -0.016218 -0.011417 INDUS -0.044855 -0.042719 -0.023252 -0.000000 -0.000000 CRIM -0.099468 -0.099213 -0.089070 -0.073577 -0.019058 NOX -0.175072 -0.000000 -0.000000 -0.000000 -0.000000 LSTAT -0.574822 -0.587702 -0.693861 -0.760457 -0.800368 PTRATIO -0.779498 -0.784725 -0.790969 -0.738672 -0.423065 DIS -1.189438 -1.173647 -0.975902 -0.725174 -0.031208 결과 해석 alpha 0.5일 때, RMSE가 5.468로 가장 좋은 예측 성능을 보임 alpha 값에 따른 피처들의 회귀 계수 값이 라쏘보다는 0 되는 값이 적음 (5) 선형 회귀 모델을 위한 데이터 변환 선형 회귀 모델과 같은 선형 모델은 일반적으로 피처와 타겟 간에 선형의 관계가 있다 가정하고, 이러한 최적의 선형함수를 찾아내 결과를 예측 선형 회귀 모델은 피처값과 타겟값의 분포가 정규 분포(즉 평균을 중심으로 종 모양으로 데이터 값이 분포된 형태) 형태를 매우 선호함 타겟값의 경우 정규 분포 형태가 아니라 특정값의 분포가 치우친 왜곡된 형태의 분포도일 경우 예측 성능에 부정적인 영향을 미칠 가능성이 높음 피처값 역시 왜곡된 분포도로 인해 예측 성능에 부정적인 영향을 미칠 수 있음 일반적으로 선형 회귀 모델을 적용하기전에 데이터에 대한 스케일링/정규화 작업을 수행함 단, 스케일링/정규화 작업을 선행한다고 해서 무조건 예측 성능이 향상되는 것은 아니며 중요한 피처들이나 타겟값의 분포도가 심하게 왜곡됐을 경우에 이러한 변환 작업을 수행함 피처 데이터 셋과 타겟 데이터 셋에 이러한 스케일링/정규화 작업을 수행하는 방법이 다름 - 사이킷런을 이용해 피처 데이터 세트에 적용하는 방법 세 가지 StandardScaler 클래스를 이용해 평균이 0, 분산이 1인 표준 정규 분포를 가진 데이터 셋으로 변환하거나 MinMaxScaler 클래스를 이용해 최소값이 0이고 최대값이 1인 값으로 정규화를 수행 스케일링/정규화를 수행한 데이터 셋에 다시 다항 특성을 적용하여 변환하는 방법이다. 보통 1번 방법을 통해 예측 성능에 향상이 없을 경우 이와 같은 방법을 적용 원래 값에 log 함수를 적용하면 보다 정규 분포에 가까운 형태로 값이 분포(= 로그 변환)된다. 실제로 선형 회귀에서는 앞서 소개한 1,2번 방법보다 로그 변환이 훨씬 많이 사용되는 변환 방법(1번 방법: 예측 성능 향상을 크게 기대하기 어려운 경우가 많음, 2번 방법: 피처 개수가 매우 많을 경우에는 다항 변환으로 생성되는 피처의 개수가 기하급수로 늘어나서 과적합의 이슈가 발생할 수 있음) 타겟값의 경우 일반적으로 로그 변환을 적용 결정값을 정규 분포나 다른 정규값으로 변환하면 변환된 값을 다시 원본 타겟값으로 원복하기 어려울 수 있음 왜곡된 분포도 형태의 타겟값을 로그 변환하여 예측 성능 향상이 된 경우가 많은 사례에서 검증되었기 때문에 타겟값의 경우는 로그 변환을 적용 - 보스턴 주택가격 피처 데이터 세트에 표준 정규 분포 변환, 최댓값/최솟값 정규화, 로그 변환을 적용한 후 RMSE로 각 경우별 예측 성능 측정하기 사용 함수: get_scaled_data() method 인자로 변환 방법을 결정하며, 표준 정규 분포 변환(Standard), 최댓값/최솟값 정규와(MinMax), 로그 변환(Log) 중에 하나를 선택 p_degree: 다항식 특성을 추가할 때, 다항식 차수가 입력됨 (2를 넘기지 않음) np.log1p(): log() 함수만 적용하면 언더 플로우가 발생하기 쉬워 1 + log() 함수를 적용 12345678910111213141516171819from sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures# method는 표준 정규 분포 변환(Standard), 최대값/최소값 정규화(MinMax), 로그변환(Log) 결정# p_degree는 다향식 특성을 추가할 때 적용. p_degree는 2이상 부여하지 않음. def get_scaled_data(method='None', p_degree=None, input_data=None): if method == 'Standard': scaled_data = StandardScaler().fit_transform(input_data) elif method == 'MinMax': scaled_data = MinMaxScaler().fit_transform(input_data) elif method == 'Log': scaled_data = np.log1p(input_data) else: scaled_data = input_data if p_degree != None: scaled_data = PolynomialFeatures(degree=p_degree, include_bias=False).fit_transform(scaled_data) return scaled_data 12345678910111213# Ridge의 alpha값을 다르게 적용하고 다양한 데이터 변환방법에 따른 RMSE 추출. alphas = [0.1, 1, 10, 100]#변환 방법은 모두 6개, 원본 그대로, 표준정규분포, 표준정규분포+다항식 특성# 최대/최소 정규화, 최대/최소 정규화+다항식 특성, 로그변환 scale_methods=[(None, None), ('Standard', None), ('Standard', 2), ('MinMax', None), ('MinMax', 2), ('Log', None)]for scale_method in scale_methods: X_data_scaled = get_scaled_data(method=scale_method[0], p_degree=scale_method[1], input_data=X_data) print(X_data_scaled.shape, X_data.shape) print('\\n## 변환 유형:{0}, Polynomial Degree:{1}'.format(scale_method[0], scale_method[1])) get_linear_reg_eval('Ridge', params=alphas, X_data_n=X_data_scaled, y_target_n=y_target, verbose=False, return_coeff=False) (506, 13) (506, 13) ## 변환 유형:None, Polynomial Degree:None alpha 0.1일 때 5 폴드 세트의 평균 RMSE: 5.788 alpha 1일 때 5 폴드 세트의 평균 RMSE: 5.653 alpha 10일 때 5 폴드 세트의 평균 RMSE: 5.518 alpha 100일 때 5 폴드 세트의 평균 RMSE: 5.330 (506, 13) (506, 13) ## 변환 유형:Standard, Polynomial Degree:None alpha 0.1일 때 5 폴드 세트의 평균 RMSE: 5.826 alpha 1일 때 5 폴드 세트의 평균 RMSE: 5.803 alpha 10일 때 5 폴드 세트의 평균 RMSE: 5.637 alpha 100일 때 5 폴드 세트의 평균 RMSE: 5.421 (506, 104) (506, 13) ## 변환 유형:Standard, Polynomial Degree:2 alpha 0.1일 때 5 폴드 세트의 평균 RMSE: 8.827 alpha 1일 때 5 폴드 세트의 평균 RMSE: 6.871 alpha 10일 때 5 폴드 세트의 평균 RMSE: 5.485 alpha 100일 때 5 폴드 세트의 평균 RMSE: 4.634 (506, 13) (506, 13) ## 변환 유형:MinMax, Polynomial Degree:None alpha 0.1일 때 5 폴드 세트의 평균 RMSE: 5.764 alpha 1일 때 5 폴드 세트의 평균 RMSE: 5.465 alpha 10일 때 5 폴드 세트의 평균 RMSE: 5.754 alpha 100일 때 5 폴드 세트의 평균 RMSE: 7.635 (506, 104) (506, 13) ## 변환 유형:MinMax, Polynomial Degree:2 alpha 0.1일 때 5 폴드 세트의 평균 RMSE: 5.298 alpha 1일 때 5 폴드 세트의 평균 RMSE: 4.323 alpha 10일 때 5 폴드 세트의 평균 RMSE: 5.185 alpha 100일 때 5 폴드 세트의 평균 RMSE: 6.538 (506, 13) (506, 13) ## 변환 유형:Log, Polynomial Degree:None alpha 0.1일 때 5 폴드 세트의 평균 RMSE: 4.770 alpha 1일 때 5 폴드 세트의 평균 RMSE: 4.676 alpha 10일 때 5 폴드 세트의 평균 RMSE: 4.836 alpha 100일 때 5 폴드 세트의 평균 RMSE: 6.241 결과 해석 표준 정규 분포와 최솟값/최댓값 정규화로 피처 데이터 세트를 변경해도 성능상의 개선은 없음 표준 정규 분포로 일차 변환 후 2차 다항식 변환 시, alpha = 100에서 4.631로 성능 개선 최솟값/최댓값 정규화로 일차 변환 후 2차 다항식 변환 시, aplha = 1에서 4.320으로 성능 개선 단, 다항식 변환은 피처 개수가 많을 경우 적용하기 힘들며, 데이터 건수가 많아지면 시간이 많이 소모되어 적용하기에 한계가 있음 반면, 로그 변환은 alpha가 0.1, 1, 10인 경우 모두 성능이 좋게 향상됨 일반적으로 선형 회귀를 적용하려는 데이터 세트에, 데이터 값 분포가 심하게 왜곡되어 있을 경우에, 로그 변환을 적용하는 편이 더 좋은 결과를 기대할 수 있음 07. 로지스틱 회귀 로지스틱 회귀: 선형 회귀 방식을 분류에 적용한 알고리즘 → ‘분류’에 사용 선형 회귀 계열이나, 선형 회귀와 다른 점은 학습을 통해 선형 함수의 회귀 최적선을 찾지 않고 시그모이드(Sigmoid) 함수 최적선을 찾고 시그모이드 함수 반환 값을 확률로 간주하여 확률에 따라 분류를 결정하는 것 시그모이드 함수 y = $\\frac{1}{1+e-x}$ (-x는 제곱) 시그모이드 함수는 x 값이 +, -로 아무리 커지거나 작아져도 y 값은 0과 1 사이 값만 반환 x 값이 커지면 1에 근사하며 x 값이 작아지면 0에 근사 x가 0일 때는 0.5 회귀 분제를 분류 문제에 적용하기 종양의 크기에 따라 악성 종양인지(Yes = 1), 아닌지(No = 0)를 회귀를 이용하여 1과 0 값으로 예측하는 것 종양 크기에 따라 악성될 확률이 높다고 하면 아래 왼쪽 그림과 같이 분포하며 선형 회귀 선을 그릴 수 있으나, 해당 회귀 라인은 0과 1을 제대로 분류하지 못함 오른쪽 그림처럼 시그모이드 함수를 이용하면 조금 더 정확하게 0과 1을 분류할 수 있음 - 로지스틱 회귀로 암 여부 판단하기: 위스콘신 유방암 데이터 세트 이용 12345678import pandas as pdimport matplotlib.pyplot as plt%matplotlib inlinefrom sklearn.datasets import load_breast_cancerfrom sklearn.linear_model import LogisticRegressioncancer = load_breast_cancer() 12345678from sklearn.preprocessing import StandardScalerfrom sklearn.model_selection import train_test_split# StandardScaler( )로 평균이 0, 분산 1로 데이터 분포도 변환scaler = StandardScaler()data_scaled = scaler.fit_transform(cancer.data)X_train , X_test, y_train , y_test = train_test_split(data_scaled, cancer.target, test_size=0.3, random_state=0) - 로지스틱 회귀로 학습 및 예측하고, 정확도와 ROC-AUC 값 구하기 12345678910from sklearn.metrics import accuracy_score, roc_auc_score# 로지스틱 회귀를 이용하여 학습 및 예측 수행. lr_clf = LogisticRegression()lr_clf.fit(X_train, y_train)lr_preds = lr_clf.predict(X_test)# accuracy와 roc_auc 측정print('accuracy: {:0.3f}'.format(accuracy_score(y_test, lr_preds)))print('roc_auc: {:0.3f}'.format(roc_auc_score(y_test , lr_preds))) accuracy: 0.977 roc_auc: 0.972 사이킷런 LogisticRegression 클래스의 주요 하이퍼 파라미터로 penalty와 C가 존재 penalty는 규제의 유형을 설정하며 ‘l2’로 설정 시 L2 규제를, ‘l1’으로 설정 시 L1 규제를 뜻함 C는 규제 강도를 조절하는 alpha 값의 역수로 C = $\\frac{1}{alpha}$ C 값이 작을수록 규제 강도가 큼을 의미 - 위스콘신 데이터 세트에서 해당 하이퍼 파라미터를 최적화하기 123456789from sklearn.model_selection import GridSearchCVparams={'penalty':['l2', 'l1'], 'C':[0.01, 0.1, 1, 1, 5, 10]}grid_clf = GridSearchCV(lr_clf, param_grid=params, scoring='accuracy', cv=3 )grid_clf.fit(data_scaled, cancer.target)print('최적 하이퍼 파라미터:{0}, 최적 평균 정확도:{1:.3f}'.format(grid_clf.best_params_, grid_clf.best_score_)) C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: Traceback (most recent call last): File &quot;C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py&quot;, line 531, in _fit_and_score estimator.fit(X_train, y_train, **fit_params) File &quot;C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py&quot;, line 1304, in fit solver = _check_solver(self.solver, self.penalty, self.dual) File &quot;C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py&quot;, line 442, in _check_solver raise ValueError(&quot;Solver %s supports only 'l2' or 'none' penalties, &quot; ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty. warnings.warn(&quot;Estimator fit failed. The score on this train-test&quot; C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: Traceback (most recent call last): File &quot;C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py&quot;, line 531, in _fit_and_score estimator.fit(X_train, y_train, **fit_params) File &quot;C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py&quot;, line 1304, in fit solver = _check_solver(self.solver, self.penalty, self.dual) File &quot;C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py&quot;, line 442, in _check_solver raise ValueError(&quot;Solver %s supports only 'l2' or 'none' penalties, &quot; ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty. warnings.warn(&quot;Estimator fit failed. The score on this train-test&quot; C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: Traceback (most recent call last): File &quot;C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py&quot;, line 531, in _fit_and_score estimator.fit(X_train, y_train, **fit_params) File &quot;C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py&quot;, line 1304, in fit solver = _check_solver(self.solver, self.penalty, self.dual) File &quot;C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py&quot;, line 442, in _check_solver raise ValueError(&quot;Solver %s supports only 'l2' or 'none' penalties, &quot; ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty. warnings.warn(&quot;Estimator fit failed. The score on this train-test&quot; C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: Traceback (most recent call last): File &quot;C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py&quot;, line 531, in _fit_and_score estimator.fit(X_train, y_train, **fit_params) File &quot;C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py&quot;, line 1304, in fit solver = _check_solver(self.solver, self.penalty, self.dual) File &quot;C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py&quot;, line 442, in _check_solver raise ValueError(&quot;Solver %s supports only 'l2' or 'none' penalties, &quot; ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty. warnings.warn(&quot;Estimator fit failed. The score on this train-test&quot; C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: Traceback (most recent call last): File &quot;C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py&quot;, line 531, in _fit_and_score estimator.fit(X_train, y_train, **fit_params) File &quot;C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py&quot;, line 1304, in fit solver = _check_solver(self.solver, self.penalty, self.dual) File &quot;C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py&quot;, line 442, in _check_solver raise ValueError(&quot;Solver %s supports only 'l2' or 'none' penalties, &quot; ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty. warnings.warn(&quot;Estimator fit failed. The score on this train-test&quot; 최적 하이퍼 파라미터:{'C': 1, 'penalty': 'l2'}, 최적 평균 정확도:0.975 C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: Traceback (most recent call last): File &quot;C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py&quot;, line 531, in _fit_and_score estimator.fit(X_train, y_train, **fit_params) File &quot;C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py&quot;, line 1304, in fit solver = _check_solver(self.solver, self.penalty, self.dual) File &quot;C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py&quot;, line 442, in _check_solver raise ValueError(&quot;Solver %s supports only 'l2' or 'none' penalties, &quot; ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty. warnings.warn(&quot;Estimator fit failed. The score on this train-test&quot; 로지스틱 회귀는 가볍고 빠르며, 이진 분류 예측 성능까지 뛰어남 이진 분류의 기본 모델로 사용하는 경우가 많음 로지스틱 회귀는 희소한 데이터 세트 분류에서도 뛰어난 성능을 보임 텍스트 분류에서도 자주 사용 08. 회귀 트리 회귀 함수를 기반으로 하지 않고, 결정 트리와 같이 트리를 기반으로 하는 회귀 방식 소개 트리 기반이 회귀: 회귀 트리를 이용하는 것 회귀를 위한 트리를 생성하고 이를 기반으로 회귀를 예측하는 것 4장 분류에서 언급한 분류 트리와 비슷하나, 리프 노트에서 예측 결정 값을 만드는 과정에 차이가 있음 분류 트리는 특정 클래스 레이블을 결정하나, 회귀 트리는 리프 노드에 속한 데이터 값의 평균값을 구해 회귀 예측값을 계산 예시(p.335-336) 피처가 단 하나인 X 피처 데이터 세트와 결정값 Y가 2차원 평면에 있다고 가정 데이터 세트의 X 피처를 결정 트리 기반으로 분할하면 X값의 균일도를 반영한 지니 계수에 따라 분할됨 루트 노드를 Split 0 기준으로 분할하고, 분할된 규칙 노드에서 다시 Split 1과 Split 2 규칙 노드로 분할할 수 있음 Split 2는 다시 재귀적으로 Split 3 규칙 노드로 트리 규칙으로 변환될 수 있음 리프 노드 생성 기준에 부합하는 트리 분할이 완료됐다면, 리프 노드에 소속된 데이터 값의 평균값을 구해 최종적으로 리프 노드에 결정 값으로 할당함 사이킷런 트리 기반 회귀와 분류의 Estimator 클래스 알고리즘 회귀 Estimator 클래스 분류 Estimator 클래스 Decision Tree DecisionTreeRegressor DecisionTreeClassifier Gradient Boosting GradientBoostingRegressor GradientBoostingClassifier XGBoost XGBRegressor XGBClassifier LightGBM LGBMRegressor LGBMClassifier - 사이킷런 랜덤 포레스트 회귀 트리인 RandomForestRegressor로 보스턴 주택 가격 예측 수행하기 12345678910111213141516171819202122from sklearn.datasets import load_bostonfrom sklearn.model_selection import cross_val_scorefrom sklearn.ensemble import RandomForestRegressorimport pandas as pdimport numpy as np# 보스턴 데이터 세트 로드boston = load_boston()bostonDF = pd.DataFrame(boston.data, columns = boston.feature_names)bostonDF['PRICE'] = boston.targety_target = bostonDF['PRICE']X_data = bostonDF.drop(['PRICE'], axis=1,inplace=False)rf = RandomForestRegressor(random_state=0, n_estimators=1000)neg_mse_scores = cross_val_score(rf, X_data, y_target, scoring=&quot;neg_mean_squared_error&quot;, cv = 5)rmse_scores = np.sqrt(-1 * neg_mse_scores)avg_rmse = np.mean(rmse_scores)print(' 5 교차 검증의 개별 Negative MSE scores: ', np.round(neg_mse_scores, 2))print(' 5 교차 검증의 개별 RMSE scores : ', np.round(rmse_scores, 2))print(' 5 교차 검증의 평균 RMSE : {0:.3f} '.format(avg_rmse)) 5 교차 검증의 개별 Negative MSE scores: [ -7.88 -13.14 -20.57 -46.23 -18.88] 5 교차 검증의 개별 RMSE scores : [2.81 3.63 4.54 6.8 4.34] 5 교차 검증의 평균 RMSE : 4.423 - 결정 트리, GBM, XGBoost, LightGBM의 Regressor을 모두 이용해 보스턴 주택 가격 예측 수행 사용 함수: get_model_cv_prediction() 입력 모델과 데이터 세트를 입력 받아, 교차 검증으로 평균 RMSE를 계산하는 함수 123456def get_model_cv_prediction(model, X_data, y_target): neg_mse_scores = cross_val_score(model, X_data, y_target, scoring=&quot;neg_mean_squared_error&quot;, cv = 5) rmse_scores = np.sqrt(-1 * neg_mse_scores) avg_rmse = np.mean(rmse_scores) print('##### ',model.__class__.__name__ , ' #####') print(' 5 교차 검증의 평균 RMSE : {0:.3f} '.format(avg_rmse)) - 다양한 유형의 회귀 트리를 생성하고, 보스턴 주택 가격 예측하기 123456789101112131415from sklearn.tree import DecisionTreeRegressorfrom sklearn.ensemble import GradientBoostingRegressorfrom xgboost import XGBRegressorfrom lightgbm import LGBMRegressordt_reg = DecisionTreeRegressor(random_state=0, max_depth=4)rf_reg = RandomForestRegressor(random_state=0, n_estimators=1000)gb_reg = GradientBoostingRegressor(random_state=0, n_estimators=1000)xgb_reg = XGBRegressor(n_estimators=1000)lgb_reg = LGBMRegressor(n_estimators=1000)# 트리 기반의 회귀 모델을 반복하면서 평가 수행 models = [dt_reg, rf_reg, gb_reg, xgb_reg, lgb_reg]for model in models: get_model_cv_prediction(model, X_data, y_target) ##### DecisionTreeRegressor ##### 5 교차 검증의 평균 RMSE : 5.978 ##### RandomForestRegressor ##### 5 교차 검증의 평균 RMSE : 4.423 ##### GradientBoostingRegressor ##### 5 교차 검증의 평균 RMSE : 4.269 ##### XGBRegressor ##### 5 교차 검증의 평균 RMSE : 4.251 ##### LGBMRegressor ##### 5 교차 검증의 평균 RMSE : 4.646 - feature_importances_를 이용해 보스턴 주택 가격 모델의 피처별 중요도 시각화하기 회귀 트리 Regressor 클래스는 선형 회귀와 다른 처리 방식으로, 회귀 계수를 제공하는 coef_ 속성이 없으나, feature_importances_를 이용해 피처별 중요도를 알 수 있음 1234567891011import seaborn as sns%matplotlib inlinerf_reg = RandomForestRegressor(n_estimators=1000)# 앞 예제에서 만들어진 X_data, y_target 데이터 셋을 적용하여 학습합니다. rf_reg.fit(X_data, y_target)feature_series = pd.Series(data=rf_reg.feature_importances_, index=X_data.columns )feature_series = feature_series.sort_values(ascending=False)sns.barplot(x= feature_series, y=feature_series.index) &lt;matplotlib.axes._subplots.AxesSubplot at 0x167a0bb8940&gt; - 회귀 트리 Regressor가 예측값을 판단하는 방법을 선형 회귀와 비교하여 시각화하기 보스턴 데이터 세트를 100개만 샘플링하고 RM과 PRICE 칼럼만 추출 2차원 평면상에서 X축에 독립변수인 RM, Y축에 종속변수인 PRICE만 가지고 더 직관적으로 예측값을 시각화하기 위한 것 12345678import matplotlib.pyplot as plt%matplotlib inlinebostonDF_sample = bostonDF[['RM','PRICE']]bostonDF_sample = bostonDF_sample.sample(n=100,random_state=0)print(bostonDF_sample.shape)plt.figure()plt.scatter(bostonDF_sample.RM , bostonDF_sample.PRICE,c=&quot;darkorange&quot;) (100, 2) &lt;matplotlib.collections.PathCollection at 0x167a237f970&gt; - LinearRegression과 DecisionTreeRegressor를 max_depth 2, 7로 학습하기 1234567891011121314151617181920212223import numpy as npfrom sklearn.linear_model import LinearRegression# 선형 회귀와 결정 트리 기반의 Regressor 생성. DecisionTreeRegressor의 max_depth는 각각 2, 7lr_reg = LinearRegression()rf_reg2 = DecisionTreeRegressor(max_depth=2)rf_reg7 = DecisionTreeRegressor(max_depth=7)# 실제 예측을 적용할 테스트용 데이터 셋을 4.5 ~ 8.5 까지 100개 데이터 셋 생성. X_test = np.arange(4.5, 8.5, 0.04).reshape(-1, 1)# 보스턴 주택가격 데이터에서 시각화를 위해 피처는 RM만, 그리고 결정 데이터인 PRICE 추출X_feature = bostonDF_sample['RM'].values.reshape(-1,1)y_target = bostonDF_sample['PRICE'].values.reshape(-1,1)# 학습과 예측 수행. lr_reg.fit(X_feature, y_target)rf_reg2.fit(X_feature, y_target)rf_reg7.fit(X_feature, y_target)pred_lr = lr_reg.predict(X_test)pred_rf2 = rf_reg2.predict(X_test)pred_rf7 = rf_reg7.predict(X_test) 1234567891011121314151617fig , (ax1, ax2, ax3) = plt.subplots(figsize=(14,4), ncols=3)# X축값을 4.5 ~ 8.5로 변환하며 입력했을 때, 선형 회귀와 결정 트리 회귀 예측 선 시각화# 선형 회귀로 학습된 모델 회귀 예측선 ax1.set_title('Linear Regression')ax1.scatter(bostonDF_sample.RM, bostonDF_sample.PRICE, c=&quot;darkorange&quot;)ax1.plot(X_test, pred_lr,label=&quot;linear&quot;, linewidth=2 )# DecisionTreeRegressor의 max_depth를 2로 했을 때 회귀 예측선 ax2.set_title('Decision Tree Regression: \\n max_depth=2')ax2.scatter(bostonDF_sample.RM, bostonDF_sample.PRICE, c=&quot;darkorange&quot;)ax2.plot(X_test, pred_rf2, label=&quot;max_depth:3&quot;, linewidth=2 )# DecisionTreeRegressor의 max_depth를 7로 했을 때 회귀 예측선 ax3.set_title('Decision Tree Regression: \\n max_depth=7')ax3.scatter(bostonDF_sample.RM, bostonDF_sample.PRICE, c=&quot;darkorange&quot;)ax3.plot(X_test, pred_rf7, label=&quot;max_depth:7&quot;, linewidth=2) 정리 선형 회귀: 예측 회귀선을 직선으로 표현 회귀 트리: 분할되는 데이터 지점에 따라 브랜치를 만들며 계단 형태로 회귀선을 만듦 DecisionTreeRegressor의 max_depth = 7인 경우, 학습 데이터 세트의 이상치(outlier) 데이터도 학습하면서 복잡한 계단 형태의 회귀선을 만들어 과적합 되기 쉬운 모델이 됨 09. 회귀 실습- 자전거 대여 수요 예측 데이터 설명 기간: 2011년 1월 - 2012년 12월 날짜/시간, 기온, 습도, 풍속 등 정보 1시간 간격으로 자전거 대여 횟수 기록 데이터의 주요 칼럼 (결정값: count) datetime: hourly date + timestamp season: 1 = 봄, 2 = 여름, 3 = 가을, 4 = 겨울 holiday: 1= 토/일요일의 주말 제외한 국경일 등의 휴일, 0 = 휴일 아닌 날 workingday: 1 = 토/일요일의 주말 및 휴일이 아닌 주중, 0 = 주말 및 휴일 weather: 1 = 맑음, 약간 구름 낀 흐림, 2 = 안개, 안개 + 흐림, 3 = 가벼운 눈, 가벼운 비 + 천둥, 4 = 심한 눈/비, 천둥/번개 temp: 온도(섭씨) atemp: 체감온도(섭씨) humidity: 상대습도 windspeed: 풍속 casual: 사전 등록되지 않은 사용자 대여 횟수 registered: 사전 등록된 사용자 대여 횟수 count: 대여 획수 (1) 데이터 클렌징 및 가공 bike_train.csv 데이터 세트로 모델을 학습한 후, 대여 횟수(count) 예측 123456789101112import numpy as npimport pandas as pdimport seaborn as snsimport matplotlib.pyplot as plt%matplotlib inlineimport warningswarnings.filterwarnings(&quot;ignore&quot;, category=RuntimeWarning)bike_df = pd.read_csv('./data/bike_train.csv')print(bike_df.shape)bike_df.head(3) (10886, 12) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } datetime season holiday workingday weather temp atemp humidity windspeed casual registered count 0 2011-01-01 00:00:00 1 0 0 1 9.84 14.395 81 0.0 3 13 16 1 2011-01-01 01:00:00 1 0 0 1 9.02 13.635 80 0.0 8 32 40 2 2011-01-01 02:00:00 1 0 0 1 9.02 13.635 80 0.0 5 27 32 12# 데이터 타입 살펴보기bike_df.info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 10886 entries, 0 to 10885 Data columns (total 12 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 datetime 10886 non-null object 1 season 10886 non-null int64 2 holiday 10886 non-null int64 3 workingday 10886 non-null int64 4 weather 10886 non-null int64 5 temp 10886 non-null float64 6 atemp 10886 non-null float64 7 humidity 10886 non-null int64 8 windspeed 10886 non-null float64 9 casual 10886 non-null int64 10 registered 10886 non-null int64 11 count 10886 non-null int64 dtypes: float64(3), int64(8), object(1) memory usage: 1020.7+ KB 데이터 타입 확인 Null 데이터 없음 datetime 칼럼만 object형, 년-월-일 시:분:초 형식 가공 필요 123456789# 문자열을 datetime 타입으로 변경. bike_df['datetime'] = bike_df.datetime.apply(pd.to_datetime)# datetime 타입에서 년, 월, 일, 시간 추출bike_df['year'] = bike_df.datetime.apply(lambda x : x.year)bike_df['month'] = bike_df.datetime.apply(lambda x : x.month)bike_df['day'] = bike_df.datetime.apply(lambda x : x.day)bike_df['hour'] = bike_df.datetime.apply(lambda x: x.hour)bike_df.head(3) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } datetime season holiday workingday weather temp atemp humidity windspeed casual registered count year month day hour 0 2011-01-01 00:00:00 1 0 0 1 9.84 14.395 81 0.0 3 13 16 2011 1 1 0 1 2011-01-01 01:00:00 1 0 0 1 9.02 13.635 80 0.0 8 32 40 2011 1 1 1 2 2011-01-01 02:00:00 1 0 0 1 9.02 13.635 80 0.0 5 27 32 2011 1 1 2 12345# datetime 삭제# casule + registered = count이므로 casule, registered 값도 삭제drop_columns = ['datetime','casual','registered']bike_df.drop(drop_columns, axis=1,inplace=True) - 다양한 회귀 모델을 데이터 세트에 적용해 예측 성능 측정하기 캐글에서 요구한 성능 평가 방법은 RMSLE(Root Mean Square Log Error)로 오류 값 로그에 대한 RMSE 단, 사이킷런은 RMSLE를 제공하지 않아 RMSLE를 수행하는 성능 형가 함수를 만들어야 함 123456789101112131415161718192021from sklearn.metrics import mean_squared_error, mean_absolute_error# log 값 변환 시 NaN등의 이슈로 log() 가 아닌 log1p() 를 이용하여 RMSLE 계산def rmsle(y, pred): log_y = np.log1p(y) log_pred = np.log1p(pred) squared_error = (log_y - log_pred) ** 2 rmsle = np.sqrt(np.mean(squared_error)) return rmsle# 사이킷런의 mean_square_error() 를 이용하여 RMSE 계산def rmse(y,pred): return np.sqrt(mean_squared_error(y,pred))# MSE, RMSE, RMSLE 를 모두 계산 def evaluate_regr(y,pred): rmsle_val = rmsle(y,pred) rmse_val = rmse(y,pred) # MAE 는 scikit learn의 mean_absolute_error() 로 계산 mae_val = mean_absolute_error(y,pred) print('RMSLE: {0:.3f}, RMSE: {1:.3F}, MAE: {2:.3F}'.format(rmsle_val, rmse_val, mae_val)) (2) 로그 변환, 피처 인코딩, 모델 학습/예측/평가 회귀 모델을 이용해 자전거 대여 횟수 예측하기 먼저, 결괏값이 정규 분포로 되어 있는지 확인해야 함 카테고리형 회귀 모델은 원-핫 인코딩으로 피처를 인코딩해야 함 - 사이킷런의 LinearRegression 객체로 회귀 예측하기 12345678910111213from sklearn.model_selection import train_test_split , GridSearchCVfrom sklearn.linear_model import LinearRegression , Ridge , Lassoy_target = bike_df['count']X_features = bike_df.drop(['count'],axis=1,inplace=False)X_train, X_test, y_train, y_test = train_test_split(X_features, y_target, test_size=0.3, random_state=0)lr_reg = LinearRegression()lr_reg.fit(X_train, y_train)pred = lr_reg.predict(X_test)evaluate_regr(y_test ,pred) RMSLE: 1.165, RMSE: 140.900, MAE: 105.924 결과 해석 실제 Target 데이터 값인 대여 횟수(Count)를 감안하면 예측 오류로서는 비교적 큰 값 - 실제값과 예측값이 어느 정도 차이 나는지 DataFrame 칼럼으로 만들어서 오류 값이 가장 큰 순으로 5개만 확인하기 123456789def get_top_error_data(y_test, pred, n_tops = 5): # DataFrame에 컬럼들로 실제 대여횟수(count)와 예측 값을 서로 비교 할 수 있도록 생성. result_df = pd.DataFrame(y_test.values, columns=['real_count']) result_df['predicted_count']= np.round(pred) result_df['diff'] = np.abs(result_df['real_count'] - result_df['predicted_count']) # 예측값과 실제값이 가장 큰 데이터 순으로 출력. print(result_df.sort_values('diff', ascending=False)[:n_tops]) get_top_error_data(y_test,pred,n_tops=5) real_count predicted_count diff 1618 890 322.0 568.0 3151 798 241.0 557.0 966 884 327.0 557.0 412 745 194.0 551.0 2817 856 310.0 546.0 결과 해석 가장 큰 상위 5 오류값은 546 - 568로 실제값을 감안하면 오륙 꽤 큼 회귀에서 큰 예측 오류가 발생할 경우, Target 값의 분포가 왜곡된 형태를 이루는지를 확인해야 함 Target 값 분포는 정규 분포 형태가 가장 좋으며, 왜곡된 경우에는 회귀 예측 성능이 저하되는 경우가 쉽게 발생함 - 판다스 DataFrame의 hist()를 이용해 자전거 대여 모델의 Target 값인 count 칼럼이 정규 분포를 이루는지 확인하기 1y_target.hist() &lt;matplotlib.axes._subplots.AxesSubplot at 0x167a2792220&gt; 결과 해석 count 칼럼 값이 정규 분포가 아닌, 0 - 200 사이에 왜곡된 것을 알 수 있음 왜곡된 값을 정규 분포 형태로 바꾸는 방법: 로그를 적용해 변환하는 것 Numpy의 log1p()이용 변경된 Target 값을 기반으로 학습하고, 예측한 값은 expm1() 함수를 이용해 원래의 scale 값으로 원상 복구 - lop1p()를 적용한 ‘count’값이 분포 확인하기 12y_log_transform = np.log1p(y_target)y_log_transform.hist() &lt;matplotlib.axes._subplots.AxesSubplot at 0x167a272a3d0&gt; 정규 분포 형태는 아니지만, 왜곡 정도가 많이 향상됨 - 위 데이터로 다시 학습하고 평가하기 12345678910111213141516# 타겟 컬럼인 count 값을 log1p 로 Log 변환y_target_log = np.log1p(y_target)# 로그 변환된 y_target_log를 반영하여 학습/테스트 데이터 셋 분할X_train, X_test, y_train, y_test = train_test_split(X_features, y_target_log, test_size=0.3, random_state=0)lr_reg = LinearRegression()lr_reg.fit(X_train, y_train)pred = lr_reg.predict(X_test)# 테스트 데이터 셋의 Target 값은 Log 변환되었으므로 다시 expm1를 이용하여 원래 scale로 변환y_test_exp = np.expm1(y_test)# 예측 값 역시 Log 변환된 타겟 기반으로 학습되어 예측되었으므로 다시 exmpl으로 scale변환pred_exp = np.expm1(pred)evaluate_regr(y_test_exp ,pred_exp) RMSLE: 1.017, RMSE: 162.594, MAE: 109.286 RMSLE 오류는 줄어들었으나, RMSE는 오히려 더 늘어남 - 각 피처의 회귀 계수 값을 시각화해 확인하기 123coef = pd.Series(lr_reg.coef_, index=X_features.columns)coef_sort = coef.sort_values(ascending=False)sns.barplot(x=coef_sort.values, y=coef_sort.index) &lt;matplotlib.axes._subplots.AxesSubplot at 0x167a23ae1c0&gt; 결과 해석 Year 피처 회귀 계수 값이 독보적으로 큼 Year는 2011, 2012 두 개의 값으로, year에 따라 자전거 대여 횟수가 크게 영향을 받는다고 할 수 없음 Category 피처지만 숫자형 값으로 되어 있고 2011, 2012가 매우 큰 숫자라 영향을 주게 됨 원-핫 인코딩을 적용해 변환하여야 함 - 여러 칼럼 원-핫 인코딩하고 선형 회귀 모델(LinearRegression, Ridge, Lasso 모두 학습해 예측 성능 확인하기 사용 함수: get_model_predict() 모델과 학습/테스트 데이터 세트를 입력하면 성능 평가 수치를 반환하는 함수 123# 'year', month', 'day', hour'등의 피처들을 One Hot EncodingX_features_ohe = pd.get_dummies(X_features, columns=['year', 'month','day', 'hour', 'holiday', 'workingday','season','weather']) 12345678910111213141516171819202122# 원-핫 인코딩이 적용된 feature 데이터 세트 기반으로 학습/예측 데이터 분할. X_train, X_test, y_train, y_test = train_test_split(X_features_ohe, y_target_log, test_size=0.3, random_state=0)# 모델과 학습/테스트 데이터 셋을 입력하면 성능 평가 수치를 반환def get_model_predict(model, X_train, X_test, y_train, y_test, is_expm1=False): model.fit(X_train, y_train) pred = model.predict(X_test) if is_expm1 : y_test = np.expm1(y_test) pred = np.expm1(pred) print('###',model.__class__.__name__,'###') evaluate_regr(y_test, pred)# end of function get_model_predict # model 별로 평가 수행lr_reg = LinearRegression()ridge_reg = Ridge(alpha=10)lasso_reg = Lasso(alpha=0.01)for model in [lr_reg, ridge_reg, lasso_reg]: get_model_predict(model,X_train, X_test, y_train, y_test,is_expm1=True) ### LinearRegression ### RMSLE: 0.590, RMSE: 97.688, MAE: 63.382 ### Ridge ### RMSLE: 0.590, RMSE: 98.529, MAE: 63.893 ### Lasso ### RMSLE: 0.635, RMSE: 113.219, MAE: 72.803 결과 해석 원-핫 인코딩 적용 후, 선형 회귀 예측 성능이 많이 향상됨 - 원-핫 인코딩으로 피처가 늘어났으므로, 회귀 계수 상위 25개 피처를 추출해 시각화하기 123coef = pd.Series(lr_reg.coef_ , index=X_features_ohe.columns)coef_sort = coef.sort_values(ascending=False)[:20]sns.barplot(x=coef_sort.values , y=coef_sort.index) &lt;matplotlib.axes._subplots.AxesSubplot at 0x167a25a88b0&gt; 결과 해석 선형 회귀 모델 시 month_9, month_8, month_7 등의 월 관련 피처와 workingday 관련 피처, hour 관련 피처의 회귀 계수가 높은 것을 알 수 있음 월, 주말/주중, 시간대 등 상식선에서 자전거 타는 데 필요한 피처의 회귀 계수가 높아짐→ 선형 회귀 수행 시에는 피처를 어떻게 인코딩하는가가 성능에 큰 영향을 미칠 수 있음 - 회귀 트리로 회귀 예측 수행하기 12345678910111213from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressorfrom xgboost import XGBRegressorfrom lightgbm import LGBMRegressor# 랜덤 포레스트, GBM, XGBoost, LightGBM model 별로 평가 수행rf_reg = RandomForestRegressor(n_estimators=500)gbm_reg = GradientBoostingRegressor(n_estimators=500)xgb_reg = XGBRegressor(n_estimators=500)lgbm_reg = LGBMRegressor(n_estimators=500)for model in [rf_reg, gbm_reg, xgb_reg, lgbm_reg]: # XGBoost의 경우 DataFrame이 입력 될 경우 버전에 따라 오류 발생 가능. ndarray로 변환. get_model_predict(model,X_train.values, X_test.values, y_train.values, y_test.values,is_expm1=True) ### RandomForestRegressor ### RMSLE: 0.356, RMSE: 50.371, MAE: 31.261 ### GradientBoostingRegressor ### RMSLE: 0.330, RMSE: 53.324, MAE: 32.736 ### XGBRegressor ### RMSLE: 0.342, RMSE: 51.732, MAE: 31.251 ### LGBMRegressor ### RMSLE: 0.319, RMSE: 47.215, MAE: 29.029 결과 해석 앞의 선형 회귀 모델보다 회귀 예측 성능이 개선됨 단, 회귀 트리가 선형 트리보다 나은 성능을 가진다는 의미가 아님 데이터 세트 유형에 따라 결과는 얼마든지 달라질 수 있음 10. 회귀 실습- 캐글 주택 가격: 고급 회귀 기법 데이터 설명 변수: 79개 미국 아이오와주의 에임스(Ames) 지방 주택 가격 정보- 피처별 설명 확인하기 성능 평가 RMSLE(Root Mean Squared Log Error) 기반 가격이 비싼 주택일수록 예측 결과 오류가 전체 오류에 미치는 비중이 높으므로, 이를 상쇄하기 위해 오류 값을 로그 변환한 RMSLE를 이용 (1) 데이터 사전 처리(Preprocessing)1234567891011import warningswarnings.filterwarnings('ignore')import pandas as pdimport numpy as npimport seaborn as snsimport matplotlib.pyplot as plt%matplotlib inlinehouse_df_org = pd.read_csv('./data/house_price.csv')house_df = house_df_org.copy()house_df.head(3) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Id MSSubClass MSZoning LotFrontage LotArea Street Alley LotShape LandContour Utilities ... PoolArea PoolQC Fence MiscFeature MiscVal MoSold YrSold SaleType SaleCondition SalePrice 0 1 60 RL 65.0 8450 Pave NaN Reg Lvl AllPub ... 0 NaN NaN NaN 0 2 2008 WD Normal 208500 1 2 20 RL 80.0 9600 Pave NaN Reg Lvl AllPub ... 0 NaN NaN NaN 0 5 2007 WD Normal 181500 2 3 60 RL 68.0 11250 Pave NaN IR1 Lvl AllPub ... 0 NaN NaN NaN 0 9 2008 WD Normal 223500 3 rows × 81 columns 123456# 데이터 세트 전체 크기와 칼럼 타입, Null이 있는 칼럼과 건수를 내림차순으로 출력print('데이터 세트의 Shape:', house_df.shape)print('\\n전체 feature 들의 type \\n',house_df.dtypes.value_counts())isnull_series = house_df.isnull().sum()print('\\nNull 컬럼과 그 건수:\\n ', isnull_series[isnull_series &gt; 0].sort_values(ascending=False)) 데이터 세트의 Shape: (1460, 81) 전체 feature 들의 type object 43 int64 35 float64 3 dtype: int64 Null 컬럼과 그 건수: PoolQC 1453 MiscFeature 1406 Alley 1369 Fence 1179 FireplaceQu 690 LotFrontage 259 GarageYrBlt 81 GarageType 81 GarageFinish 81 GarageQual 81 GarageCond 81 BsmtFinType2 38 BsmtExposure 38 BsmtFinType1 37 BsmtCond 37 BsmtQual 37 MasVnrArea 8 MasVnrType 8 Electrical 1 dtype: int64 데이터 타입 확인 테이터 세트는 1460개의 레코드와 81개의 피처로 구성 피처 타입은 숫자형과 문자형 모두 존재 Target을 제외한 80개 피처 중, 43개가 문자형이고 37개가 숫자형 1480개 데이터 중, PoolQC, MiseFeature, Alley, Fence는 1000개가 넘는 Null 값을 가짐 Null 값이 너무 많은 피처는 drop - 회귀 모델 적용 전, 타깃 값 분포가 정규 분포인지 확인하기 아래 그래프에서 볼 수 있듯, 데이터 값 분포가 왼쪽으로 치우친 형태로 정규 분포에서 벗어나 있음 12plt.title('Original Sale Price Histogram')sns.distplot(house_df['SalePrice']) &lt;matplotlib.axes._subplots.AxesSubplot at 0x167a3cf4340&gt; - 로그 변환(Log Transformation)을 적용하여, 정규 분포가 아닌 결괏값을 정규 분포 형태로 변환하기 Numpy의 log1p()로 로그 변환한 결괏값 기반으로 학습 예측 시에는 결괏값을 expm1()로 환원 123plt.title('Log Transformed Sale Price Histogram')log_SalePrice = np.log1p(house_df['SalePrice'])sns.distplot(log_SalePrice, color = 'g') &lt;matplotlib.axes._subplots.AxesSubplot at 0x167a3df6160&gt; SalePrice를 로그 변환해 정규 분포 형태로 결괏값이 분포함을 확인할 수 있음 - 다음 작업 SalePrice를 로그 변환하고 DataFrame에 반영 Null 값이 많은 피처인 PoolQC, MiseFeature, Alley, Fence, FireplaceQu 삭제 단순 식별자인 Id 삭제 LotFrontage Null 값은 259개로 비교적 많으나, 평균값으로 대체 나머지 피처 Null 값은 많지 않으므로 숫자형의 경우 평균값으로 대체 123456789101112# SalePrice 로그 변환original_SalePrice = house_df['SalePrice']house_df['SalePrice'] = np.log1p(house_df['SalePrice'])# Null 이 너무 많은 컬럼들과 불필요한 컬럼 삭제house_df.drop(['Id','PoolQC' , 'MiscFeature', 'Alley', 'Fence','FireplaceQu'], axis=1 , inplace=True)# Drop 하지 않는 숫자형 Null컬럼들은 평균값으로 대체house_df.fillna(house_df.mean(),inplace=True)# Null 값이 있는 피처명과 타입을 추출null_column_count = house_df.isnull().sum()[house_df.isnull().sum() &gt; 0]print('## Null 피처의 Type :\\n', house_df.dtypes[null_column_count.index]) ## Null 피처의 Type : MasVnrType object BsmtQual object BsmtCond object BsmtExposure object BsmtFinType1 object BsmtFinType2 object Electrical object GarageType object GarageFinish object GarageQual object GarageCond object dtype: object - 문자형 피처는 원-핫 인코딩으로 변환하기 사용 함수: get_dummies() 자동으로 문자열 피처를 원-핫 인코딩으로 변환하면서 Null 값을 ‘None’ 칼럼으로 대체해주어 Null 값을 대체하는 별도의 로직이 필요 없음 원-핫 인코딩을 적용하면 칼럼이 증가하기 때문에, 변환 후 늘어난 칼럼 값까지 확인하기 123456print('get_dummies() 수행 전 데이터 Shape:', house_df.shape)house_df_ohe = pd.get_dummies(house_df)print('get_dummies() 수행 후 데이터 Shape:', house_df_ohe.shape)null_column_count = house_df_ohe.isnull().sum()[house_df_ohe.isnull().sum() &gt; 0]print('## Null 피처의 Type :\\n', house_df_ohe.dtypes[null_column_count.index]) get_dummies() 수행 전 데이터 Shape: (1460, 75) get_dummies() 수행 후 데이터 Shape: (1460, 271) ## Null 피처의 Type : Series([], dtype: object) 결과 해석 원-핫 인코딩 후 피처가 75개에서 272개로 증가 Null 값을 가진 피처는 없음 (2) 선형 회귀 모델 학습/예측/평가RMSE 평가 함수 생성 12345678910111213def get_rmse(model): pred = model.predict(X_test) mse = mean_squared_error(y_test , pred) rmse = np.sqrt(mse) print('{0} 로그 변환된 RMSE: {1}'.format(model.__class__.__name__,np.round(rmse, 3))) return rmsedef get_rmses(models): rmses = [ ] for model in models: rmse = get_rmse(model) rmses.append(rmse) return rmses LinearRegression, Ridge, Lasso 학습, 예측, 평가 1234567891011121314151617181920from sklearn.model_selection import train_test_splitfrom sklearn.metrics import mean_squared_errory_target = house_df_ohe['SalePrice']X_features = house_df_ohe.drop('SalePrice',axis=1, inplace=False)X_train, X_test, y_train, y_test = train_test_split(X_features, y_target, test_size=0.2, random_state=156)# LinearRegression, Ridge, Lasso 학습, 예측, 평가lr_reg = LinearRegression()lr_reg.fit(X_train, y_train)ridge_reg = Ridge()ridge_reg.fit(X_train, y_train)lasso_reg = Lasso()lasso_reg.fit(X_train, y_train)models = [lr_reg, ridge_reg, lasso_reg]get_rmses(models) LinearRegression 로그 변환된 RMSE: 0.132 Ridge 로그 변환된 RMSE: 0.128 Lasso 로그 변환된 RMSE: 0.176 [0.1318957657915436, 0.12750846334053045, 0.17628250556471395] 회귀 계수값과 컬럼명 시각화를 위해 상위 10개, 하위 10개(-값으로 가장 큰 10개) 회귀 계수값과 컬럼명을 가지는 Series생성 함수. 12345678def get_top_bottom_coef(model): # coef_ 속성을 기반으로 Series 객체를 생성. index는 컬럼명. coef = pd.Series(model.coef_, index=X_features.columns) # + 상위 10개 , - 하위 10개 coefficient 추출하여 반환. coef_high = coef.sort_values(ascending=False).head(10) coef_low = coef.sort_values(ascending=False).tail(10) return coef_high, coef_low 인자로 입력되는 여러개의 회귀 모델들에 대한 회귀계수값과 컬럼명 시각화 1234567891011121314151617181920def visualize_coefficient(models): # 3개 회귀 모델의 시각화를 위해 3개의 컬럼을 가지는 subplot 생성 fig, axs = plt.subplots(figsize=(24,10),nrows=1, ncols=3) fig.tight_layout() # 입력인자로 받은 list객체인 models에서 차례로 model을 추출하여 회귀 계수 시각화. for i_num, model in enumerate(models): # 상위 10개, 하위 10개 회귀 계수를 구하고, 이를 판다스 concat으로 결합. coef_high, coef_low = get_top_bottom_coef(model) coef_concat = pd.concat( [coef_high , coef_low] ) # 순차적으로 ax subplot에 barchar로 표현. 한 화면에 표현하기 위해 tick label 위치와 font 크기 조정. axs[i_num].set_title(model.__class__.__name__+' Coeffiecents', size=25) axs[i_num].tick_params(axis=&quot;y&quot;,direction=&quot;in&quot;, pad=-120) for label in (axs[i_num].get_xticklabels() + axs[i_num].get_yticklabels()): label.set_fontsize(22) sns.barplot(x=coef_concat.values, y=coef_concat.index , ax=axs[i_num])# 앞 예제에서 학습한 lr_reg, ridge_reg, lasso_reg 모델의 회귀 계수 시각화. models = [lr_reg, ridge_reg, lasso_reg]visualize_coefficient(models) 5 폴드 교차검증으로 모델별로 RMSE와 평균 RMSE출력 1234567891011121314from sklearn.model_selection import cross_val_scoredef get_avg_rmse_cv(models): for model in models: # 분할하지 않고 전체 데이터로 cross_val_score( ) 수행. 모델별 CV RMSE값과 평균 RMSE 출력 rmse_list = np.sqrt(-cross_val_score(model, X_features, y_target, scoring=&quot;neg_mean_squared_error&quot;, cv = 5)) rmse_avg = np.mean(rmse_list) print('\\n{0} CV RMSE 값 리스트: {1}'.format( model.__class__.__name__, np.round(rmse_list, 3))) print('{0} CV 평균 RMSE 값: {1}'.format( model.__class__.__name__, np.round(rmse_avg, 3)))# 앞 예제에서 학습한 lr_reg, ridge_reg, lasso_reg 모델의 CV RMSE값 출력 models = [lr_reg, ridge_reg, lasso_reg]get_avg_rmse_cv(models) LinearRegression CV RMSE 값 리스트: [0.135 0.165 0.168 0.111 0.198] LinearRegression CV 평균 RMSE 값: 0.155 Ridge CV RMSE 값 리스트: [0.117 0.154 0.142 0.117 0.189] Ridge CV 평균 RMSE 값: 0.144 Lasso CV RMSE 값 리스트: [0.161 0.204 0.177 0.181 0.265] Lasso CV 평균 RMSE 값: 0.198 각 모델들의 alpha값을 변경하면서 하이퍼 파라미터 튜닝 후 다시 재 학습/예측/평가 123456789101112131415from sklearn.model_selection import GridSearchCVdef get_best_params(model, params): grid_model = GridSearchCV(model, param_grid=params, scoring='neg_mean_squared_error', cv=5) grid_model.fit(X_features, y_target) rmse = np.sqrt(-1* grid_model.best_score_) print('{0} 5 CV 시 최적 평균 RMSE 값: {1}, 최적 alpha:{2}'.format(model.__class__.__name__, np.round(rmse, 4), grid_model.best_params_)) return grid_model.best_estimator_ridge_params = { 'alpha':[0.05, 0.1, 1, 5, 8, 10, 12, 15, 20] }lasso_params = { 'alpha':[0.001, 0.005, 0.008, 0.05, 0.03, 0.1, 0.5, 1,5, 10] }best_rige = get_best_params(ridge_reg, ridge_params)best_lasso = get_best_params(lasso_reg, lasso_params) Ridge 5 CV 시 최적 평균 RMSE 값: 0.1418, 최적 alpha:{'alpha': 12} Lasso 5 CV 시 최적 평균 RMSE 값: 0.142, 최적 alpha:{'alpha': 0.001} 123456789101112131415# 앞의 최적화 alpha값으로 학습데이터로 학습, 테스트 데이터로 예측 및 평가 수행. lr_reg = LinearRegression()lr_reg.fit(X_train, y_train)ridge_reg = Ridge(alpha=12)ridge_reg.fit(X_train, y_train)lasso_reg = Lasso(alpha=0.001)lasso_reg.fit(X_train, y_train)# 모든 모델의 RMSE 출력models = [lr_reg, ridge_reg, lasso_reg]get_rmses(models)# 모든 모델의 회귀 계수 시각화 models = [lr_reg, ridge_reg, lasso_reg]visualize_coefficient(models) LinearRegression 로그 변환된 RMSE: 0.132 Ridge 로그 변환된 RMSE: 0.124 Lasso 로그 변환된 RMSE: 0.12 숫자 피처들에 대한 데이터 분포 왜곡도 확인 후 높은 왜곡도를 가지는 피처 추출 1234567891011from scipy.stats import skew# object가 아닌 숫자형 피쳐의 컬럼 index 객체 추출.features_index = house_df.dtypes[house_df.dtypes != 'object'].index# house_df에 컬럼 index를 [ ]로 입력하면 해당하는 컬럼 데이터 셋 반환. apply lambda로 skew( )호출 skew_features = house_df[features_index].apply(lambda x : skew(x))# skew 정도가 1 이상인 컬럼들만 추출. skew_features_top = skew_features[skew_features &gt; 1]print(skew_features_top.sort_values(ascending=False)) MiscVal 24.451640 PoolArea 14.813135 LotArea 12.195142 3SsnPorch 10.293752 LowQualFinSF 9.002080 KitchenAbvGr 4.483784 BsmtFinSF2 4.250888 ScreenPorch 4.117977 BsmtHalfBath 4.099186 EnclosedPorch 3.086696 MasVnrArea 2.673661 LotFrontage 2.382499 OpenPorchSF 2.361912 BsmtFinSF1 1.683771 WoodDeckSF 1.539792 TotalBsmtSF 1.522688 MSSubClass 1.406210 1stFlrSF 1.375342 GrLivArea 1.365156 dtype: float64 왜곡도가 1인 피처들은 로그 변환 적용하고 다시 하이퍼 파라미터 튜닝 후 재 학습/예측/평가 1house_df[skew_features_top.index] = np.log1p(house_df[skew_features_top.index]) 1234567891011# Skew가 높은 피처들을 로그 변환 했으므로 다시 원-핫 인코딩 적용 및 피처/타겟 데이터 셋 생성,house_df_ohe = pd.get_dummies(house_df)y_target = house_df_ohe['SalePrice']X_features = house_df_ohe.drop('SalePrice',axis=1, inplace=False)X_train, X_test, y_train, y_test = train_test_split(X_features, y_target, test_size=0.2, random_state=156)# 피처들을 로그 변환 후 다시 최적 하이퍼 파라미터와 RMSE 출력ridge_params = { 'alpha':[0.05, 0.1, 1, 5, 8, 10, 12, 15, 20] }lasso_params = { 'alpha':[0.001, 0.005, 0.008, 0.05, 0.03, 0.1, 0.5, 1,5, 10] }best_ridge = get_best_params(ridge_reg, ridge_params)best_lasso = get_best_params(lasso_reg, lasso_params) Ridge 5 CV 시 최적 평균 RMSE 값: 0.1275, 최적 alpha:{'alpha': 10} Lasso 5 CV 시 최적 평균 RMSE 값: 0.1252, 최적 alpha:{'alpha': 0.001} 123456789101112131415# 앞의 최적화 alpha값으로 학습데이터로 학습, 테스트 데이터로 예측 및 평가 수행. lr_reg = LinearRegression()lr_reg.fit(X_train, y_train)ridge_reg = Ridge(alpha=10)ridge_reg.fit(X_train, y_train)lasso_reg = Lasso(alpha=0.001)lasso_reg.fit(X_train, y_train)# 모든 모델의 RMSE 출력models = [lr_reg, ridge_reg, lasso_reg]get_rmses(models)# 모든 모델의 회귀 계수 시각화 models = [lr_reg, ridge_reg, lasso_reg]visualize_coefficient(models) LinearRegression 로그 변환된 RMSE: 0.128 Ridge 로그 변환된 RMSE: 0.122 Lasso 로그 변환된 RMSE: 0.119 이상치 데이터 검출을 위해 주요 피처인 GrLivArea값에 대한 산포도 확인 1234plt.scatter(x = house_df_org['GrLivArea'], y = house_df_org['SalePrice'])plt.ylabel('SalePrice', fontsize=15)plt.xlabel('GrLivArea', fontsize=15)plt.show() 이상치 데이터 삭제 후 재 학습/예측/평가 12345678910# GrLivArea와 SalePrice 모두 로그 변환되었으므로 이를 반영한 조건 생성. cond1 = house_df_ohe['GrLivArea'] &gt; np.log1p(4000)cond2 = house_df_ohe['SalePrice'] &lt; np.log1p(500000)outlier_index = house_df_ohe[cond1 &amp; cond2].indexprint('아웃라이어 레코드 index :', outlier_index.values)print('아웃라이어 삭제 전 house_df_ohe shape:', house_df_ohe.shape)# DataFrame의 index를 이용하여 아웃라이어 레코드 삭제. house_df_ohe.drop(outlier_index, axis=0, inplace=True)print('아웃라이어 삭제 후 house_df_ohe shape:', house_df_ohe.shape) 아웃라이어 레코드 index : [ 523 1298] 아웃라이어 삭제 전 house_df_ohe shape: (1460, 271) 아웃라이어 삭제 후 house_df_ohe shape: (1458, 271) 12345678y_target = house_df_ohe['SalePrice']X_features = house_df_ohe.drop('SalePrice',axis=1, inplace=False)X_train, X_test, y_train, y_test = train_test_split(X_features, y_target, test_size=0.2, random_state=156)ridge_params = { 'alpha':[0.05, 0.1, 1, 5, 8, 10, 12, 15, 20] }lasso_params = { 'alpha':[0.001, 0.005, 0.008, 0.05, 0.03, 0.1, 0.5, 1,5, 10] }best_ridge = get_best_params(ridge_reg, ridge_params)best_lasso = get_best_params(lasso_reg, lasso_params) Ridge 5 CV 시 최적 평균 RMSE 값: 0.1125, 최적 alpha:{'alpha': 8} Lasso 5 CV 시 최적 평균 RMSE 값: 0.1122, 최적 alpha:{'alpha': 0.001} 123456789101112131415# 앞의 최적화 alpha값으로 학습데이터로 학습, 테스트 데이터로 예측 및 평가 수행. lr_reg = LinearRegression()lr_reg.fit(X_train, y_train)ridge_reg = Ridge(alpha=8)ridge_reg.fit(X_train, y_train)lasso_reg = Lasso(alpha=0.001)lasso_reg.fit(X_train, y_train)# 모든 모델의 RMSE 출력models = [lr_reg, ridge_reg, lasso_reg]get_rmses(models)# 모든 모델의 회귀 계수 시각화 models = [lr_reg, ridge_reg, lasso_reg]visualize_coefficient(models) LinearRegression 로그 변환된 RMSE: 0.129 Ridge 로그 변환된 RMSE: 0.103 Lasso 로그 변환된 RMSE: 0.1 회귀 트리 학습/예측/평가XGBoost와 LightGBM 학습/예측/평가 123456from xgboost import XGBRegressorxgb_params = {'n_estimators':[1000]}xgb_reg = XGBRegressor(n_estimators=1000, learning_rate=0.05, colsample_bytree=0.5, subsample=0.8)best_xgb = get_best_params(xgb_reg, xgb_params) XGBRegressor 5 CV 시 최적 평균 RMSE 값: 0.1178, 최적 alpha:{'n_estimators': 1000} 123456from lightgbm import LGBMRegressorlgbm_params = {'n_estimators':[1000]}lgbm_reg = LGBMRegressor(n_estimators=1000, learning_rate=0.05, num_leaves=4, subsample=0.6, colsample_bytree=0.4, reg_lambda=10, n_jobs=-1)best_lgbm = get_best_params(lgbm_reg, lgbm_params) LGBMRegressor 5 CV 시 최적 평균 RMSE 값: 0.1163, 최적 alpha:{'n_estimators': 1000} 트리 회귀 모델의 피처 중요도 시각화 123456789101112131415161718192021222324# 모델의 중요도 상위 20개의 피처명과 그때의 중요도값을 Series로 반환.def get_top_features(model): ftr_importances_values = model.feature_importances_ ftr_importances = pd.Series(ftr_importances_values, index=X_features.columns ) ftr_top20 = ftr_importances.sort_values(ascending=False)[:20] return ftr_top20def visualize_ftr_importances(models): # 2개 회귀 모델의 시각화를 위해 2개의 컬럼을 가지는 subplot 생성 fig, axs = plt.subplots(figsize=(24,10),nrows=1, ncols=2) fig.tight_layout() # 입력인자로 받은 list객체인 models에서 차례로 model을 추출하여 피처 중요도 시각화. for i_num, model in enumerate(models): # 중요도 상위 20개의 피처명과 그때의 중요도값 추출 ftr_top20 = get_top_features(model) axs[i_num].set_title(model.__class__.__name__+' Feature Importances', size=25) #font 크기 조정. for label in (axs[i_num].get_xticklabels() + axs[i_num].get_yticklabels()): label.set_fontsize(22) sns.barplot(x=ftr_top20.values, y=ftr_top20.index , ax=axs[i_num])# 앞 예제에서 get_best_params( )가 반환한 GridSearchCV로 최적화된 모델의 피처 중요도 시각화 models = [best_xgb, best_lgbm]visualize_ftr_importances(models) 회귀 모델들의 예측 결과 혼합을 통한 최종 예측1234567891011121314151617181920212223def get_rmse_pred(preds): for key in preds.keys(): pred_value = preds[key] mse = mean_squared_error(y_test , pred_value) rmse = np.sqrt(mse) print('{0} 모델의 RMSE: {1}'.format(key, rmse))# 개별 모델의 학습ridge_reg = Ridge(alpha=8)ridge_reg.fit(X_train, y_train)lasso_reg = Lasso(alpha=0.001)lasso_reg.fit(X_train, y_train)# 개별 모델 예측ridge_pred = ridge_reg.predict(X_test)lasso_pred = lasso_reg.predict(X_test)# 개별 모델 예측값 혼합으로 최종 예측값 도출pred = 0.4 * ridge_pred + 0.6 * lasso_predpreds = {'최종 혼합': pred, 'Ridge': ridge_pred, 'Lasso': lasso_pred}#최종 혼합 모델, 개별모델의 RMSE 값 출력get_rmse_pred(preds) 최종 혼합 모델의 RMSE: 0.10007930884470519 Ridge 모델의 RMSE: 0.10345177546603272 Lasso 모델의 RMSE: 0.10024170460890039 12345678910111213141516xgb_reg = XGBRegressor(n_estimators=1000, learning_rate=0.05, colsample_bytree=0.5, subsample=0.8)lgbm_reg = LGBMRegressor(n_estimators=1000, learning_rate=0.05, num_leaves=4, subsample=0.6, colsample_bytree=0.4, reg_lambda=10, n_jobs=-1)xgb_reg.fit(X_train, y_train)lgbm_reg.fit(X_train, y_train)xgb_pred = xgb_reg.predict(X_test)lgbm_pred = lgbm_reg.predict(X_test)pred = 0.5 * xgb_pred + 0.5 * lgbm_predpreds = {'최종 혼합': pred, 'XGBM': xgb_pred, 'LGBM': lgbm_pred} get_rmse_pred(preds) 최종 혼합 모델의 RMSE: 0.1017007808403327 XGBM 모델의 RMSE: 0.10738299364833828 LGBM 모델의 RMSE: 0.10382510019327311 스태킹 모델을 통한 회귀 예측12345678910111213141516171819202122232425262728293031from sklearn.model_selection import KFoldfrom sklearn.metrics import mean_absolute_error# 개별 기반 모델에서 최종 메타 모델이 사용할 학습 및 테스트용 데이터를 생성하기 위한 함수. def get_stacking_base_datasets(model, X_train_n, y_train_n, X_test_n, n_folds ): # 지정된 n_folds값으로 KFold 생성. kf = KFold(n_splits=n_folds, shuffle=False, random_state=0) #추후에 메타 모델이 사용할 학습 데이터 반환을 위한 넘파이 배열 초기화 train_fold_pred = np.zeros((X_train_n.shape[0] ,1 )) test_pred = np.zeros((X_test_n.shape[0],n_folds)) print(model.__class__.__name__ , ' model 시작 ') for folder_counter , (train_index, valid_index) in enumerate(kf.split(X_train_n)): #입력된 학습 데이터에서 기반 모델이 학습/예측할 폴드 데이터 셋 추출 print('\\t 폴드 세트: ',folder_counter,' 시작 ') X_tr = X_train_n[train_index] y_tr = y_train_n[train_index] X_te = X_train_n[valid_index] #폴드 세트 내부에서 다시 만들어진 학습 데이터로 기반 모델의 학습 수행. model.fit(X_tr , y_tr) #폴드 세트 내부에서 다시 만들어진 검증 데이터로 기반 모델 예측 후 데이터 저장. train_fold_pred[valid_index, :] = model.predict(X_te).reshape(-1,1) #입력된 원본 테스트 데이터를 폴드 세트내 학습된 기반 모델에서 예측 후 데이터 저장. test_pred[:, folder_counter] = model.predict(X_test_n) # 폴드 세트 내에서 원본 테스트 데이터를 예측한 데이터를 평균하여 테스트 데이터로 생성 test_pred_mean = np.mean(test_pred, axis=1).reshape(-1,1) #train_fold_pred는 최종 메타 모델이 사용하는 학습 데이터, test_pred_mean은 테스트 데이터 return train_fold_pred , test_pred_mean 기반 모델은 리지, 라소, XGBoost, LightGBM 으로 만들고 최종 메타 모델은 라소로 생성하여 학습/예측/평가 12345678910# get_stacking_base_datasets( )은 넘파이 ndarray를 인자로 사용하므로 DataFrame을 넘파이로 변환. X_train_n = X_train.valuesX_test_n = X_test.valuesy_train_n = y_train.values# 각 개별 기반(Base)모델이 생성한 학습용/테스트용 데이터 반환. ridge_train, ridge_test = get_stacking_base_datasets(ridge_reg, X_train_n, y_train_n, X_test_n, 5)lasso_train, lasso_test = get_stacking_base_datasets(lasso_reg, X_train_n, y_train_n, X_test_n, 5)xgb_train, xgb_test = get_stacking_base_datasets(xgb_reg, X_train_n, y_train_n, X_test_n, 5) lgbm_train, lgbm_test = get_stacking_base_datasets(lgbm_reg, X_train_n, y_train_n, X_test_n, 5) Ridge model 시작 폴드 세트: 0 시작 폴드 세트: 1 시작 폴드 세트: 2 시작 폴드 세트: 3 시작 폴드 세트: 4 시작 Lasso model 시작 폴드 세트: 0 시작 폴드 세트: 1 시작 폴드 세트: 2 시작 폴드 세트: 3 시작 폴드 세트: 4 시작 XGBRegressor model 시작 폴드 세트: 0 시작 폴드 세트: 1 시작 폴드 세트: 2 시작 폴드 세트: 3 시작 폴드 세트: 4 시작 LGBMRegressor model 시작 폴드 세트: 0 시작 폴드 세트: 1 시작 폴드 세트: 2 시작 폴드 세트: 3 시작 폴드 세트: 4 시작 123456789101112131415# 개별 모델이 반환한 학습 및 테스트용 데이터 세트를 Stacking 형태로 결합. Stack_final_X_train = np.concatenate((ridge_train, lasso_train, xgb_train, lgbm_train), axis=1)Stack_final_X_test = np.concatenate((ridge_test, lasso_test, xgb_test, lgbm_test), axis=1)# 최종 메타 모델은 라쏘 모델을 적용. meta_model_lasso = Lasso(alpha=0.0005)#기반 모델의 예측값을 기반으로 새롭게 만들어진 학습 및 테스트용 데이터로 예측하고 RMSE 측정.meta_model_lasso.fit(Stack_final_X_train, y_train)final = meta_model_lasso.predict(Stack_final_X_test)mse = mean_squared_error(y_test , final)rmse = np.sqrt(mse)print('스태킹 회귀 모델의 최종 RMSE 값은:', rmse) 스태킹 회귀 모델의 최종 RMSE 값은: 0.0979915406689774 정리 선형 회귀와 비용 함수 RSS 경사 하강법 다항회귀와 과소적합/과대적합 규제 -L2규제를 적용한 릿지, L1규제를 적용한 라쏘, L1과 L2규제가 결합된 엘라스틱넷 회귀 분류를 위한 로지스틱 회귀 CART 기반의 회귀 트리 왜곡도 개선을 위한 데이터 변환과 원-핫 인코딩 실습 예제를 통한 데이터 정제와 변환 그리고 선형회귀/회귀트리/혼합모델/스태킹 모델 학습/예측/평가비교","link":"/2020/12/03/study/python_machine_learning_perfect_guide_ch05/"},{"title":"파이썬 머신러닝 완벽가이드 2장","text":"출처: 권철민, 『파이썬 머신러닝 완벽 가이드 (개정판)』, 위키북스, 2020.02, 87-147쪽 접기/펼치기 사이킷런으로 시작하는 머신러닝정리사이킷런 많은 머신러닝 알고리즘을 제공 쉽고 직관적인 API 프레임워크 편리하고 다양한 모듈 지원 머신러닝 애플리케이션 전처리 작업: 데이터의 가공 및 변환 과정 데이터 시트 분리 작업: 데이터를 학습 데이터와 테스트 데이터로 분리 모델 학습: 학습 데이터를 기반으로 머신러닝 알고리즘을 적용 예측: 학습된 모델을 기반으로 테스트 데이터에 대한 예측을 수행 평가: 예측된 결과값을 실제 결과값과 비교해 머신러닝 모델에 대한 평가를 수행 데이터 전처리 작업 오류 데이터의 보정이나 결손값(Null) 처리 등의 다양한 데이터 클렌징 작업 레이블 인코딩이나 원-핫 인코딩 같은 인코딩 작업 데이터의 스케일링/정규화 작업 등으로 머신러닝 알고리즘이 최적으로 수행될 수 있데 데이터를 사전 처리하는 것 머신러닝 모델 학습 데이터 세트로 학습한 뒤 반드시 별도의 테스트 데이터 세트로 평가되어야 한다. 테스트 데이터의 건수 부족이나 고정된 테스트 데이터 세트를 이용한 반복적인 모델의 학습과 평가는 해당 테스트 데이터 세트에만 치우치는 빈약한 머신러닝 모델을 만들 가능성이 높다. 이를 해결하기 위해 학습 데이터 세트를 학습 데이터와 검증 데이터로 구성된 여러 개의 폴드 세트로 분리해 교차검증을 수행한다. 사이킷런은 교차 검증을 지원하기 위해 KFord, StratifiedKFold, cross_val_score 등의 다양한 클래스와 함수를 제공한다. 또한 머신러닝 모델의 최적의 하이퍼 파라미터를 교차 검증을 통해 추출하기 위해 GridSearchCV를 제공한다. 사이킷런 소개와 특징사이킷런(scikit-learn)은 파이썬 머신러닝 라이브러리 중 가장 많이 사용되는 라이브러리이다. 사이킷런은 파이썬 기반의 머신러닝을 위한 가장 쉽고 효율적인 개발 라이브러리를 제공한다. 사이킷런의 특징 파이썬 기반의 다른 머신러닝 패키지도 사이킷런 스타일의 API를 지향할 정도로 쉽고 가장 파이썬스러운 API를 제공한다. # API: 운영체제가 제공하는 함수의 집합체 머신러닝을 위한 매우 다양한 알고리즘과 개발을 위한 편리한 프레임워크와 API를 제공한다. colab에서 anaconda install 설치 방법Conda + Google ColabA guide to installing Conda when using Google Colabhttps://towardsdatascience.com/conda-google-colab-75f7c867a522 먼저 Google Colab에서 기본적으로 사용되는 Python을 확인해야합니다. 다음 명령을 실행하면 기본 Python 실행 파일의 절대 경로가 반환됩니다. 1!which python # should return /usr/local/bin/python /usr/local/bin/python 이제 기본 Python의 버전 번호를 확인하십시오. 1!python --version Python 3.6.9 마지막으로 PYTHONPATH변수가 설정 되었는지 확인합니다 .이 명령을 작성하는 시점에는이 명령 만 반환됩니다 /env/python(이상하게도 이것은 Google Colab 파일 시스템 내에 존재하지 않는 것으로 보이는 디렉토리입니다). 1!echo $PYTHONPATH /env/python Miniconda 설치Google Colab 셀에서 실행되는 경우 아래 코드는 해당 Miniconda 버전에 대한 설치 프로그램 스크립트를 다운로드하여 /usr/local. /usr/local기본 위치가 아닌에 직접 설치 ~/miniconda3하면 Conda 및 모든 필수 종속성을 Google Colab 내에서 자동으로 사용할 수 있습니다. 123456%%bashMINICONDA_INSTALLER_SCRIPT=Miniconda3-4.5.4-Linux-x86_64.shMINICONDA_PREFIX=/usr/localwget https://repo.continuum.io/miniconda/$MINICONDA_INSTALLER_SCRIPTchmod +x $MINICONDA_INSTALLER_SCRIPT./$MINICONDA_INSTALLER_SCRIPT -b -f -p $MINICONDA_PREFIX PREFIX=/usr/local installing: python-3.6.5-hc3d631a_2 ... installing: ca-certificates-2018.03.07-0 ... installing: conda-env-2.6.0-h36134e3_1 ... installing: libgcc-ng-7.2.0-hdf63c60_3 ... installing: libstdcxx-ng-7.2.0-hdf63c60_3 ... installing: libffi-3.2.1-hd88cf55_4 ... installing: ncurses-6.1-hf484d3e_0 ... installing: openssl-1.0.2o-h20670df_0 ... installing: tk-8.6.7-hc745277_3 ... installing: xz-5.2.4-h14c3975_4 ... installing: yaml-0.1.7-had09818_2 ... installing: zlib-1.2.11-ha838bed_2 ... installing: libedit-3.1.20170329-h6b74fdf_2 ... installing: readline-7.0-ha6073c6_4 ... installing: sqlite-3.23.1-he433501_0 ... installing: asn1crypto-0.24.0-py36_0 ... installing: certifi-2018.4.16-py36_0 ... installing: chardet-3.0.4-py36h0f667ec_1 ... installing: idna-2.6-py36h82fb2a8_1 ... installing: pycosat-0.6.3-py36h0a5515d_0 ... installing: pycparser-2.18-py36hf9f622e_1 ... installing: pysocks-1.6.8-py36_0 ... installing: ruamel_yaml-0.15.37-py36h14c3975_2 ... installing: six-1.11.0-py36h372c433_1 ... installing: cffi-1.11.5-py36h9745a5d_0 ... installing: setuptools-39.2.0-py36_0 ... installing: cryptography-2.2.2-py36h14c3975_0 ... installing: wheel-0.31.1-py36_0 ... installing: pip-10.0.1-py36_0 ... installing: pyopenssl-18.0.0-py36_0 ... installing: urllib3-1.22-py36hbe7ace6_0 ... installing: requests-2.18.4-py36he2e5f8d_1 ... installing: conda-4.5.4-py36_0 ... installation finished. WARNING: You currently have a PYTHONPATH environment variable set. This may cause unexpected behavior when running the Python interpreter in Miniconda3. For best results, please verify that your PYTHONPATH only points to directories of packages that are compatible with the Python interpreter in Miniconda3: /usr/local --2020-11-30 00:23:29-- https://repo.continuum.io/miniconda/Miniconda3-4.5.4-Linux-x86_64.sh Resolving repo.continuum.io (repo.continuum.io)... 104.18.201.79, 104.18.200.79, 2606:4700::6812:c94f, ... Connecting to repo.continuum.io (repo.continuum.io)|104.18.201.79|:443... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://repo.anaconda.com/miniconda/Miniconda3-4.5.4-Linux-x86_64.sh [following] --2020-11-30 00:23:29-- https://repo.anaconda.com/miniconda/Miniconda3-4.5.4-Linux-x86_64.sh Resolving repo.anaconda.com (repo.anaconda.com)... 104.16.131.3, 104.16.130.3, 2606:4700::6810:8303, ... Connecting to repo.anaconda.com (repo.anaconda.com)|104.16.131.3|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 58468498 (56M) [application/x-sh] Saving to: ‘Miniconda3-4.5.4-Linux-x86_64.sh’ 0K .......... .......... .......... .......... .......... 0% 62.9M 1s 50K .......... .......... .......... .......... .......... 0% 3.74M 8s 100K .......... .......... .......... .......... .......... 0% 3.90M 10s 150K .......... .......... .......... .......... .......... 0% 3.98M 11s 200K .......... .......... .......... .......... .......... 0% 82.5M 9s 250K .......... .......... .......... .......... .......... 0% 138M 7s 300K .......... .......... .......... .......... .......... 0% 4.21M 8s 350K .......... .......... .......... .......... .......... 0% 35.7M 7s 400K .......... .......... .......... .......... .......... 0% 137M 7s 450K .......... .......... .......... .......... .......... 0% 187M 6s 500K .......... .......... .......... .......... .......... 0% 31.4M 6s 550K .......... .......... .......... .......... .......... 1% 159M 5s 600K .......... .......... .......... .......... .......... 1% 182M 5s 650K .......... .......... .......... .......... .......... 1% 175M 4s 700K .......... .......... .......... .......... .......... 1% 167M 4s 750K .......... .......... .......... .......... .......... 1% 6.16M 4s 800K .......... .......... .......... .......... .......... 1% 154M 4s 850K .......... .......... .......... .......... .......... 1% 149M 4s 900K .......... .......... .......... .......... .......... 1% 110M 4s 950K .......... .......... .......... .......... .......... 1% 47.8M 4s 1000K .......... .......... .......... .......... .......... 1% 121M 4s 1050K .......... .......... .......... .......... .......... 1% 187M 3s 1100K .......... .......... .......... .......... .......... 2% 53.5M 3s 1150K .......... .......... .......... .......... .......... 2% 78.1M 3s 1200K .......... .......... .......... .......... .......... 2% 74.5M 3s 1250K .......... .......... .......... .......... .......... 2% 66.5M 3s 1300K .......... .......... .......... .......... .......... 2% 136M 3s 1350K .......... .......... .......... .......... .......... 2% 68.3M 3s 1400K .......... .......... .......... .......... .......... 2% 86.2M 3s 1450K .......... .......... .......... .......... .......... 2% 62.2M 3s 1500K .......... .......... .......... .......... .......... 2% 10.7M 3s 1550K .......... .......... .......... .......... .......... 2% 134M 3s 1600K .......... .......... .......... .......... .......... 2% 163M 3s 1650K .......... .......... .......... .......... .......... 2% 82.2M 3s 1700K .......... .......... .......... .......... .......... 3% 123M 2s 1750K .......... .......... .......... .......... .......... 3% 94.5M 2s 1800K .......... .......... .......... .......... .......... 3% 199M 2s 1850K .......... .......... .......... .......... .......... 3% 88.1M 2s 1900K .......... .......... .......... .......... .......... 3% 151M 2s 1950K .......... .......... .......... .......... .......... 3% 84.9M 2s 2000K .......... .......... .......... .......... .......... 3% 84.4M 2s 2050K .......... .......... .......... .......... .......... 3% 143M 2s 2100K .......... .......... .......... .......... .......... 3% 135M 2s 2150K .......... .......... .......... .......... .......... 3% 157M 2s 2200K .......... .......... .......... .......... .......... 3% 154M 2s 2250K .......... .......... .......... .......... .......... 4% 149M 2s 2300K .......... .......... .......... .......... .......... 4% 186M 2s 2350K .......... .......... .......... .......... .......... 4% 38.1M 2s 2400K .......... .......... .......... .......... .......... 4% 159M 2s 2450K .......... .......... .......... .......... .......... 4% 143M 2s 2500K .......... .......... .......... .......... .......... 4% 145M 2s 2550K .......... .......... .......... .......... .......... 4% 30.8M 2s 2600K .......... .......... .......... .......... .......... 4% 151M 2s 2650K .......... .......... .......... .......... .......... 4% 166M 2s 2700K .......... .......... .......... .......... .......... 4% 148M 2s 2750K .......... .......... .......... .......... .......... 4% 142M 2s 2800K .......... .......... .......... .......... .......... 4% 192M 2s 2850K .......... .......... .......... .......... .......... 5% 187M 2s 2900K .......... .......... .......... .......... .......... 5% 159M 2s 2950K .......... .......... .......... .......... .......... 5% 160M 2s 3000K .......... .......... .......... .......... .......... 5% 178M 2s 3050K .......... .......... .......... .......... .......... 5% 30.6M 2s 3100K .......... .......... .......... .......... .......... 5% 124M 2s 3150K .......... .......... .......... .......... .......... 5% 122M 2s 3200K .......... .......... .......... .......... .......... 5% 185M 2s 3250K .......... .......... .......... .......... .......... 5% 156M 2s 3300K .......... .......... .......... .......... .......... 5% 136M 1s 3350K .......... .......... .......... .......... .......... 5% 122M 1s 3400K .......... .......... .......... .......... .......... 6% 141M 1s 3450K .......... .......... .......... .......... .......... 6% 139M 1s 3500K .......... .......... .......... .......... .......... 6% 144M 1s 3550K .......... .......... .......... .......... .......... 6% 109M 1s 3600K .......... .......... .......... .......... .......... 6% 134M 1s 3650K .......... .......... .......... .......... .......... 6% 143M 1s 3700K .......... .......... .......... .......... .......... 6% 123M 1s 3750K .......... .......... .......... .......... .......... 6% 122M 1s 3800K .......... .......... .......... .......... .......... 6% 143M 1s 3850K .......... .......... .......... .......... .......... 6% 133M 1s 3900K .......... .......... .......... .......... .......... 6% 158M 1s 3950K .......... .......... .......... .......... .......... 7% 121M 1s 4000K .......... .......... .......... .......... .......... 7% 131M 1s 4050K .......... .......... .......... .......... .......... 7% 128M 1s 4100K .......... .......... .......... .......... .......... 7% 128M 1s 4150K .......... .......... .......... .......... .......... 7% 130M 1s 4200K .......... .......... .......... .......... .......... 7% 131M 1s 4250K .......... .......... .......... .......... .......... 7% 126M 1s 4300K .......... .......... .......... .......... .......... 7% 154M 1s 4350K .......... .......... .......... .......... .......... 7% 118M 1s 4400K .......... .......... .......... .......... .......... 7% 86.9M 1s 4450K .......... .......... .......... .......... .......... 7% 151M 1s 4500K .......... .......... .......... .......... .......... 7% 137M 1s 4550K .......... .......... .......... .......... .......... 8% 166M 1s 4600K .......... .......... .......... .......... .......... 8% 33.4M 1s 4650K .......... .......... .......... .......... .......... 8% 185M 1s 4700K .......... .......... .......... .......... .......... 8% 192M 1s 4750K .......... .......... .......... .......... .......... 8% 126M 1s 4800K .......... .......... .......... .......... .......... 8% 121M 1s 4850K .......... .......... .......... .......... .......... 8% 145M 1s 4900K .......... .......... .......... .......... .......... 8% 186M 1s 4950K .......... .......... .......... .......... .......... 8% 165M 1s 5000K .......... .......... .......... .......... .......... 8% 182M 1s 5050K .......... .......... .......... .......... .......... 8% 190M 1s 5100K .......... .......... .......... .......... .......... 9% 186M 1s 5150K .......... .......... .......... .......... .......... 9% 102M 1s 5200K .......... .......... .......... .......... .......... 9% 142M 1s 5250K .......... .......... .......... .......... .......... 9% 120M 1s 5300K .......... .......... .......... .......... .......... 9% 134M 1s 5350K .......... .......... .......... .......... .......... 9% 115M 1s 5400K .......... .......... .......... .......... .......... 9% 123M 1s 5450K .......... .......... .......... .......... .......... 9% 118M 1s 5500K .......... .......... .......... .......... .......... 9% 130M 1s 5550K .......... .......... .......... .......... .......... 9% 108M 1s 5600K .......... .......... .......... .......... .......... 9% 133M 1s 5650K .......... .......... .......... .......... .......... 9% 126M 1s 5700K .......... .......... .......... .......... .......... 10% 202M 1s 5750K .......... .......... .......... .......... .......... 10% 109M 1s 5800K .......... .......... .......... .......... .......... 10% 163M 1s 5850K .......... .......... .......... .......... .......... 10% 183M 1s 5900K .......... .......... .......... .......... .......... 10% 164M 1s 5950K .......... .......... .......... .......... .......... 10% 101M 1s 6000K .......... .......... .......... .......... .......... 10% 154M 1s 6050K .......... .......... .......... .......... .......... 10% 176M 1s 6100K .......... .......... .......... .......... .......... 10% 156M 1s 6150K .......... .......... .......... .......... .......... 10% 166M 1s 6200K .......... .......... .......... .......... .......... 10% 150M 1s 6250K .......... .......... .......... .......... .......... 11% 177M 1s 6300K .......... .......... .......... .......... .......... 11% 192M 1s 6350K .......... .......... .......... .......... .......... 11% 152M 1s 6400K .......... .......... .......... .......... .......... 11% 176M 1s 6450K .......... .......... .......... .......... .......... 11% 184M 1s 6500K .......... .......... .......... .......... .......... 11% 188M 1s 6550K .......... .......... .......... .......... .......... 11% 163M 1s 6600K .......... .......... .......... .......... .......... 11% 178M 1s 6650K .......... .......... .......... .......... .......... 11% 31.7M 1s 6700K .......... .......... .......... .......... .......... 11% 194M 1s 6750K .......... .......... .......... .......... .......... 11% 147M 1s 6800K .......... .......... .......... .......... .......... 11% 193M 1s 6850K .......... .......... .......... .......... .......... 12% 146M 1s 6900K .......... .......... .......... .......... .......... 12% 177M 1s 6950K .......... .......... .......... .......... .......... 12% 172M 1s 7000K .......... .......... .......... .......... .......... 12% 188M 1s 7050K .......... .......... .......... .......... .......... 12% 190M 1s 7100K .......... .......... .......... .......... .......... 12% 153M 1s 7150K .......... .......... .......... .......... .......... 12% 156M 1s 7200K .......... .......... .......... .......... .......... 12% 161M 1s 7250K .......... .......... .......... .......... .......... 12% 118M 1s 7300K .......... .......... .......... .......... .......... 12% 188M 1s 7350K .......... .......... .......... .......... .......... 12% 167M 1s 7400K .......... .......... .......... .......... .......... 13% 171M 1s 7450K .......... .......... .......... .......... .......... 13% 189M 1s 7500K .......... .......... .......... .......... .......... 13% 184M 1s 7550K .......... .......... .......... .......... .......... 13% 103M 1s 7600K .......... .......... .......... .......... .......... 13% 105M 1s 7650K .......... .......... .......... .......... .......... 13% 103M 1s 7700K .......... .......... .......... .......... .......... 13% 127M 1s 7750K .......... .......... .......... .......... .......... 13% 148M 1s 7800K .......... .......... .......... .......... .......... 13% 106M 1s 7850K .......... .......... .......... .......... .......... 13% 119M 1s 7900K .......... .......... .......... .......... .......... 13% 134M 1s 7950K .......... .......... .......... .......... .......... 14% 158M 1s 8000K .......... .......... .......... .......... .......... 14% 197M 1s 8050K .......... .......... .......... .......... .......... 14% 189M 1s 8100K .......... .......... .......... .......... .......... 14% 180M 1s 8150K .......... .......... .......... .......... .......... 14% 170M 1s 8200K .......... .......... .......... .......... .......... 14% 192M 1s 8250K .......... .......... .......... .......... .......... 14% 194M 1s 8300K .......... .......... .......... .......... .......... 14% 183M 1s 8350K .......... .......... .......... .......... .......... 14% 160M 1s 8400K .......... .......... .......... .......... .......... 14% 191M 1s 8450K .......... .......... .......... .......... .......... 14% 194M 1s 8500K .......... .......... .......... .......... .......... 14% 184M 1s 8550K .......... .......... .......... .......... .......... 15% 174M 1s 8600K .......... .......... .......... .......... .......... 15% 192M 1s 8650K .......... .......... .......... .......... .......... 15% 158M 1s 8700K .......... .......... .......... .......... .......... 15% 23.0M 1s 8750K .......... .......... .......... .......... .......... 15% 164M 1s 8800K .......... .......... .......... .......... .......... 15% 193M 1s 8850K .......... .......... .......... .......... .......... 15% 189M 1s 8900K .......... .......... .......... .......... .......... 15% 197M 1s 8950K .......... .......... .......... .......... .......... 15% 122M 1s 9000K .......... .......... .......... .......... .......... 15% 171M 1s 9050K .......... .......... .......... .......... .......... 15% 173M 1s 9100K .......... .......... .......... .......... .......... 16% 199M 1s 9150K .......... .......... .......... .......... .......... 16% 159M 1s 9200K .......... .......... .......... .......... .......... 16% 183M 1s 9250K .......... .......... .......... .......... .......... 16% 161M 1s 9300K .......... .......... .......... .......... .......... 16% 117M 1s 9350K .......... .......... .......... .......... .......... 16% 132M 1s 9400K .......... .......... .......... .......... .......... 16% 166M 1s 9450K .......... .......... .......... .......... .......... 16% 128M 1s 9500K .......... .......... .......... .......... .......... 16% 171M 1s 9550K .......... .......... .......... .......... .......... 16% 147M 1s 9600K .......... .......... .......... .......... .......... 16% 124M 1s 9650K .......... .......... .......... .......... .......... 16% 171M 1s 9700K .......... .......... .......... .......... .......... 17% 197M 1s 9750K .......... .......... .......... .......... .......... 17% 168M 1s 9800K .......... .......... .......... .......... .......... 17% 127M 1s 9850K .......... .......... .......... .......... .......... 17% 167M 1s 9900K .......... .......... .......... .......... .......... 17% 181M 1s 9950K .......... .......... .......... .......... .......... 17% 153M 1s 10000K .......... .......... .......... .......... .......... 17% 174M 1s 10050K .......... .......... .......... .......... .......... 17% 179M 1s 10100K .......... .......... .......... .......... .......... 17% 183M 1s 10150K .......... .......... .......... .......... .......... 17% 154M 1s 10200K .......... .......... .......... .......... .......... 17% 185M 1s 10250K .......... .......... .......... .......... .......... 18% 180M 1s 10300K .......... .......... .......... .......... .......... 18% 182M 1s 10350K .......... .......... .......... .......... .......... 18% 147M 1s 10400K .......... .......... .......... .......... .......... 18% 187M 1s 10450K .......... .......... .......... .......... .......... 18% 140M 1s 10500K .......... .......... .......... .......... .......... 18% 149M 1s 10550K .......... .......... .......... .......... .......... 18% 168M 1s 10600K .......... .......... .......... .......... .......... 18% 161M 1s 10650K .......... .......... .......... .......... .......... 18% 142M 1s 10700K .......... .......... .......... .......... .......... 18% 165M 1s 10750K .......... .......... .......... .......... .......... 18% 26.5M 1s 10800K .......... .......... .......... .......... .......... 19% 134M 1s 10850K .......... .......... .......... .......... .......... 19% 181M 1s 10900K .......... .......... .......... .......... .......... 19% 183M 1s 10950K .......... .......... .......... .......... .......... 19% 134M 1s 11000K .......... .......... .......... .......... .......... 19% 186M 1s 11050K .......... .......... .......... .......... .......... 19% 133M 1s 11100K .......... .......... .......... .......... .......... 19% 143M 1s 11150K .......... .......... .......... .......... .......... 19% 153M 1s 11200K .......... .......... .......... .......... .......... 19% 149M 1s 11250K .......... .......... .......... .......... .......... 19% 167M 1s 11300K .......... .......... .......... .......... .......... 19% 183M 1s 11350K .......... .......... .......... .......... .......... 19% 134M 1s 11400K .......... .......... .......... .......... .......... 20% 105M 1s 11450K .......... .......... .......... .......... .......... 20% 170M 1s 11500K .......... .......... .......... .......... .......... 20% 177M 1s 11550K .......... .......... .......... .......... .......... 20% 124M 1s 11600K .......... .......... .......... .......... .......... 20% 127M 1s 11650K .......... .......... .......... .......... .......... 20% 170M 1s 11700K .......... .......... .......... .......... .......... 20% 192M 1s 11750K .......... .......... .......... .......... .......... 20% 169M 1s 11800K .......... .......... .......... .......... .......... 20% 189M 1s 11850K .......... .......... .......... .......... .......... 20% 177M 1s 11900K .......... .......... .......... .......... .......... 20% 191M 1s 11950K .......... .......... .......... .......... .......... 21% 150M 1s 12000K .......... .......... .......... .......... .......... 21% 180M 1s 12050K .......... .......... .......... .......... .......... 21% 187M 1s 12100K .......... .......... .......... .......... .......... 21% 191M 1s 12150K .......... .......... .......... .......... .......... 21% 170M 1s 12200K .......... .......... .......... .......... .......... 21% 180M 1s 12250K .......... .......... .......... .......... .......... 21% 109M 1s 12300K .......... .......... .......... .......... .......... 21% 181M 1s 12350K .......... .......... .......... .......... .......... 21% 91.5M 1s 12400K .......... .......... .......... .......... .......... 21% 144M 1s 12450K .......... .......... .......... .......... .......... 21% 129M 1s 12500K .......... .......... .......... .......... .......... 21% 197M 1s 12550K .......... .......... .......... .......... .......... 22% 154M 1s 12600K .......... .......... .......... .......... .......... 22% 184M 1s 12650K .......... .......... .......... .......... .......... 22% 183M 1s 12700K .......... .......... .......... .......... .......... 22% 146M 1s 12750K .......... .......... .......... .......... .......... 22% 138M 1s 12800K .......... .......... .......... .......... .......... 22% 32.5M 1s 12850K .......... .......... .......... .......... .......... 22% 128M 1s 12900K .......... .......... .......... .......... .......... 22% 148M 1s 12950K .......... .......... .......... .......... .......... 22% 171M 1s 13000K .......... .......... .......... .......... .......... 22% 193M 1s 13050K .......... .......... .......... .......... .......... 22% 147M 1s 13100K .......... .......... .......... .......... .......... 23% 167M 1s 13150K .......... .......... .......... .......... .......... 23% 139M 1s 13200K .......... .......... .......... .......... .......... 23% 149M 1s 13250K .......... .......... .......... .......... .......... 23% 154M 1s 13300K .......... .......... .......... .......... .......... 23% 190M 1s 13350K .......... .......... .......... .......... .......... 23% 148M 1s 13400K .......... .......... .......... .......... .......... 23% 141M 1s 13450K .......... .......... .......... .......... .......... 23% 190M 1s 13500K .......... .......... .......... .......... .......... 23% 186M 1s 13550K .......... .......... .......... .......... .......... 23% 119M 1s 13600K .......... .......... .......... .......... .......... 23% 185M 1s 13650K .......... .......... .......... .......... .......... 23% 189M 1s 13700K .......... .......... .......... .......... .......... 24% 182M 1s 13750K .......... .......... .......... .......... .......... 24% 160M 1s 13800K .......... .......... .......... .......... .......... 24% 190M 1s 13850K .......... .......... .......... .......... .......... 24% 185M 1s 13900K .......... .......... .......... .......... .......... 24% 186M 1s 13950K .......... .......... .......... .......... .......... 24% 150M 1s 14000K .......... .......... .......... .......... .......... 24% 190M 1s 14050K .......... .......... .......... .......... .......... 24% 186M 1s 14100K .......... .......... .......... .......... .......... 24% 130M 1s 14150K .......... .......... .......... .......... .......... 24% 113M 1s 14200K .......... .......... .......... .......... .......... 24% 117M 1s 14250K .......... .......... .......... .......... .......... 25% 180M 1s 14300K .......... .......... .......... .......... .......... 25% 189M 1s 14350K .......... .......... .......... .......... .......... 25% 123M 1s 14400K .......... .......... .......... .......... .......... 25% 171M 1s 14450K .......... .......... .......... .......... .......... 25% 188M 1s 14500K .......... .......... .......... .......... .......... 25% 187M 1s 14550K .......... .......... .......... .......... .......... 25% 31.8M 1s 14600K .......... .......... .......... .......... .......... 25% 187M 1s 14650K .......... .......... .......... .......... .......... 25% 164M 1s 14700K .......... .......... .......... .......... .......... 25% 137M 0s 14750K .......... .......... .......... .......... .......... 25% 134M 0s 14800K .......... .......... .......... .......... .......... 26% 191M 0s 14850K .......... .......... .......... .......... .......... 26% 184M 0s 14900K .......... .......... .......... .......... .......... 26% 194M 0s 14950K .......... .......... .......... .......... .......... 26% 128M 0s 15000K .......... .......... .......... .......... .......... 26% 151M 0s 15050K .......... .......... .......... .......... .......... 26% 173M 0s 15100K .......... .......... .......... .......... .......... 26% 160M 0s 15150K .......... .......... .......... .......... .......... 26% 160M 0s 15200K .......... .......... .......... .......... .......... 26% 167M 0s 15250K .......... .......... .......... .......... .......... 26% 158M 0s 15300K .......... .......... .......... .......... .......... 26% 188M 0s 15350K .......... .......... .......... .......... .......... 26% 166M 0s 15400K .......... .......... .......... .......... .......... 27% 129M 0s 15450K .......... .......... .......... .......... .......... 27% 179M 0s 15500K .......... .......... .......... .......... .......... 27% 181M 0s 15550K .......... .......... .......... .......... .......... 27% 159M 0s 15600K .......... .......... .......... .......... .......... 27% 188M 0s 15650K .......... .......... .......... .......... .......... 27% 186M 0s 15700K .......... .......... .......... .......... .......... 27% 178M 0s 15750K .......... .......... .......... .......... .......... 27% 170M 0s 15800K .......... .......... .......... .......... .......... 27% 182M 0s 15850K .......... .......... .......... .......... .......... 27% 186M 0s 15900K .......... .......... .......... .......... .......... 27% 176M 0s 15950K .......... .......... .......... .......... .......... 28% 126M 0s 16000K .......... .......... .......... .......... .......... 28% 107M 0s 16050K .......... .......... .......... .......... .......... 28% 176M 0s 16100K .......... .......... .......... .......... .......... 28% 188M 0s 16150K .......... .......... .......... .......... .......... 28% 164M 0s 16200K .......... .......... .......... .......... .......... 28% 178M 0s 16250K .......... .......... .......... .......... .......... 28% 188M 0s 16300K .......... .......... .......... .......... .......... 28% 188M 0s 16350K .......... .......... .......... .......... .......... 28% 144M 0s 16400K .......... .......... .......... .......... .......... 28% 186M 0s 16450K .......... .......... .......... .......... .......... 28% 191M 0s 16500K .......... .......... .......... .......... .......... 28% 183M 0s 16550K .......... .......... .......... .......... .......... 29% 161M 0s 16600K .......... .......... .......... .......... .......... 29% 28.0M 0s 16650K .......... .......... .......... .......... .......... 29% 180M 0s 16700K .......... .......... .......... .......... .......... 29% 108M 0s 16750K .......... .......... .......... .......... .......... 29% 95.9M 0s 16800K .......... .......... .......... .......... .......... 29% 139M 0s 16850K .......... .......... .......... .......... .......... 29% 180M 0s 16900K .......... .......... .......... .......... .......... 29% 155M 0s 16950K .......... .......... .......... .......... .......... 29% 170M 0s 17000K .......... .......... .......... .......... .......... 29% 195M 0s 17050K .......... .......... .......... .......... .......... 29% 144M 0s 17100K .......... .......... .......... .......... .......... 30% 158M 0s 17150K .......... .......... .......... .......... .......... 30% 158M 0s 17200K .......... .......... .......... .......... .......... 30% 156M 0s 17250K .......... .......... .......... .......... .......... 30% 143M 0s 17300K .......... .......... .......... .......... .......... 30% 192M 0s 17350K .......... .......... .......... .......... .......... 30% 165M 0s 17400K .......... .......... .......... .......... .......... 30% 152M 0s 17450K .......... .......... .......... .......... .......... 30% 196M 0s 17500K .......... .......... .......... .......... .......... 30% 161M 0s 17550K .......... .......... .......... .......... .......... 30% 153M 0s 17600K .......... .......... .......... .......... .......... 30% 171M 0s 17650K .......... .......... .......... .......... .......... 30% 192M 0s 17700K .......... .......... .......... .......... .......... 31% 188M 0s 17750K .......... .......... .......... .......... .......... 31% 159M 0s 17800K .......... .......... .......... .......... .......... 31% 116M 0s 17850K .......... .......... .......... .......... .......... 31% 135M 0s 17900K .......... .......... .......... .......... .......... 31% 174M 0s 17950K .......... .......... .......... .......... .......... 31% 160M 0s 18000K .......... .......... .......... .......... .......... 31% 187M 0s 18050K .......... .......... .......... .......... .......... 31% 189M 0s 18100K .......... .......... .......... .......... .......... 31% 172M 0s 18150K .......... .......... .......... .......... .......... 31% 166M 0s 18200K .......... .......... .......... .......... .......... 31% 143M 0s 18250K .......... .......... .......... .......... .......... 32% 177M 0s 18300K .......... .......... .......... .......... .......... 32% 189M 0s 18350K .......... .......... .......... .......... .......... 32% 152M 0s 18400K .......... .......... .......... .......... .......... 32% 181M 0s 18450K .......... .......... .......... .......... .......... 32% 190M 0s 18500K .......... .......... .......... .......... .......... 32% 176M 0s 18550K .......... .......... .......... .......... .......... 32% 166M 0s 18600K .......... .......... .......... .......... .......... 32% 179M 0s 18650K .......... .......... .......... .......... .......... 32% 25.8M 0s 18700K .......... .......... .......... .......... .......... 32% 166M 0s 18750K .......... .......... .......... .......... .......... 32% 68.1M 0s 18800K .......... .......... .......... .......... .......... 33% 184M 0s 18850K .......... .......... .......... .......... .......... 33% 175M 0s 18900K .......... .......... .......... .......... .......... 33% 185M 0s 18950K .......... .......... .......... .......... .......... 33% 108M 0s 19000K .......... .......... .......... .......... .......... 33% 101M 0s 19050K .......... .......... .......... .......... .......... 33% 178M 0s 19100K .......... .......... .......... .......... .......... 33% 157M 0s 19150K .......... .......... .......... .......... .......... 33% 153M 0s 19200K .......... .......... .......... .......... .......... 33% 190M 0s 19250K .......... .......... .......... .......... .......... 33% 145M 0s 19300K .......... .......... .......... .......... .......... 33% 190M 0s 19350K .......... .......... .......... .......... .......... 33% 165M 0s 19400K .......... .......... .......... .......... .......... 34% 183M 0s 19450K .......... .......... .......... .......... .......... 34% 188M 0s 19500K .......... .......... .......... .......... .......... 34% 190M 0s 19550K .......... .......... .......... .......... .......... 34% 96.0M 0s 19600K .......... .......... .......... .......... .......... 34% 142M 0s 19650K .......... .......... .......... .......... .......... 34% 184M 0s 19700K .......... .......... .......... .......... .......... 34% 187M 0s 19750K .......... .......... .......... .......... .......... 34% 158M 0s 19800K .......... .......... .......... .......... .......... 34% 197M 0s 19850K .......... .......... .......... .......... .......... 34% 193M 0s 19900K .......... .......... .......... .......... .......... 34% 176M 0s 19950K .......... .......... .......... .......... .......... 35% 107M 0s 20000K .......... .......... .......... .......... .......... 35% 144M 0s 20050K .......... .......... .......... .......... .......... 35% 173M 0s 20100K .......... .......... .......... .......... .......... 35% 193M 0s 20150K .......... .......... .......... .......... .......... 35% 169M 0s 20200K .......... .......... .......... .......... .......... 35% 192M 0s 20250K .......... .......... .......... .......... .......... 35% 179M 0s 20300K .......... .......... .......... .......... .......... 35% 189M 0s 20350K .......... .......... .......... .......... .......... 35% 157M 0s 20400K .......... .......... .......... .......... .......... 35% 183M 0s 20450K .......... .......... .......... .......... .......... 35% 186M 0s 20500K .......... .......... .......... .......... .......... 35% 188M 0s 20550K .......... .......... .......... .......... .......... 36% 163M 0s 20600K .......... .......... .......... .......... .......... 36% 178M 0s 20650K .......... .......... .......... .......... .......... 36% 188M 0s 20700K .......... .......... .......... .......... .......... 36% 32.0M 0s 20750K .......... .......... .......... .......... .......... 36% 139M 0s 20800K .......... .......... .......... .......... .......... 36% 196M 0s 20850K .......... .......... .......... .......... .......... 36% 185M 0s 20900K .......... .......... .......... .......... .......... 36% 183M 0s 20950K .......... .......... .......... .......... .......... 36% 174M 0s 21000K .......... .......... .......... .......... .......... 36% 196M 0s 21050K .......... .......... .......... .......... .......... 36% 158M 0s 21100K .......... .......... .......... .......... .......... 37% 179M 0s 21150K .......... .......... .......... .......... .......... 37% 90.5M 0s 21200K .......... .......... .......... .......... .......... 37% 101M 0s 21250K .......... .......... .......... .......... .......... 37% 119M 0s 21300K .......... .......... .......... .......... .......... 37% 175M 0s 21350K .......... .......... .......... .......... .......... 37% 157M 0s 21400K .......... .......... .......... .......... .......... 37% 137M 0s 21450K .......... .......... .......... .......... .......... 37% 124M 0s 21500K .......... .......... .......... .......... .......... 37% 168M 0s 21550K .......... .......... .......... .......... .......... 37% 156M 0s 21600K .......... .......... .......... .......... .......... 37% 184M 0s 21650K .......... .......... .......... .......... .......... 38% 179M 0s 21700K .......... .......... .......... .......... .......... 38% 184M 0s 21750K .......... .......... .......... .......... .......... 38% 168M 0s 21800K .......... .......... .......... .......... .......... 38% 121M 0s 21850K .......... .......... .......... .......... .......... 38% 186M 0s 21900K .......... .......... .......... .......... .......... 38% 187M 0s 21950K .......... .......... .......... .......... .......... 38% 153M 0s 22000K .......... .......... .......... .......... .......... 38% 179M 0s 22050K .......... .......... .......... .......... .......... 38% 188M 0s 22100K .......... .......... .......... .......... .......... 38% 184M 0s 22150K .......... .......... .......... .......... .......... 38% 168M 0s 22200K .......... .......... .......... .......... .......... 38% 180M 0s 22250K .......... .......... .......... .......... .......... 39% 189M 0s 22300K .......... .......... .......... .......... .......... 39% 189M 0s 22350K .......... .......... .......... .......... .......... 39% 150M 0s 22400K .......... .......... .......... .......... .......... 39% 185M 0s 22450K .......... .......... .......... .......... .......... 39% 193M 0s 22500K .......... .......... .......... .......... .......... 39% 115M 0s 22550K .......... .......... .......... .......... .......... 39% 155M 0s 22600K .......... .......... .......... .......... .......... 39% 188M 0s 22650K .......... .......... .......... .......... .......... 39% 179M 0s 22700K .......... .......... .......... .......... .......... 39% 180M 0s 22750K .......... .......... .......... .......... .......... 39% 32.5M 0s 22800K .......... .......... .......... .......... .......... 40% 193M 0s 22850K .......... .......... .......... .......... .......... 40% 183M 0s 22900K .......... .......... .......... .......... .......... 40% 190M 0s 22950K .......... .......... .......... .......... .......... 40% 172M 0s 23000K .......... .......... .......... .......... .......... 40% 194M 0s 23050K .......... .......... .......... .......... .......... 40% 141M 0s 23100K .......... .......... .......... .......... .......... 40% 136M 0s 23150K .......... .......... .......... .......... .......... 40% 159M 0s 23200K .......... .......... .......... .......... .......... 40% 178M 0s 23250K .......... .......... .......... .......... .......... 40% 195M 0s 23300K .......... .......... .......... .......... .......... 40% 191M 0s 23350K .......... .......... .......... .......... .......... 40% 163M 0s 23400K .......... .......... .......... .......... .......... 41% 184M 0s 23450K .......... .......... .......... .......... .......... 41% 192M 0s 23500K .......... .......... .......... .......... .......... 41% 140M 0s 23550K .......... .......... .......... .......... .......... 41% 93.9M 0s 23600K .......... .......... .......... .......... .......... 41% 184M 0s 23650K .......... .......... .......... .......... .......... 41% 97.9M 0s 23700K .......... .......... .......... .......... .......... 41% 118M 0s 23750K .......... .......... .......... .......... .......... 41% 99.4M 0s 23800K .......... .......... .......... .......... .......... 41% 116M 0s 23850K .......... .......... .......... .......... .......... 41% 135M 0s 23900K .......... .......... .......... .......... .......... 41% 171M 0s 23950K .......... .......... .......... .......... .......... 42% 154M 0s 24000K .......... .......... .......... .......... .......... 42% 186M 0s 24050K .......... .......... .......... .......... .......... 42% 183M 0s 24100K .......... .......... .......... .......... .......... 42% 191M 0s 24150K .......... .......... .......... .......... .......... 42% 164M 0s 24200K .......... .......... .......... .......... .......... 42% 181M 0s 24250K .......... .......... .......... .......... .......... 42% 180M 0s 24300K .......... .......... .......... .......... .......... 42% 142M 0s 24350K .......... .......... .......... .......... .......... 42% 134M 0s 24400K .......... .......... .......... .......... .......... 42% 177M 0s 24450K .......... .......... .......... .......... .......... 42% 189M 0s 24500K .......... .......... .......... .......... .......... 42% 189M 0s 24550K .......... .......... .......... .......... .......... 43% 118M 0s 24600K .......... .......... .......... .......... .......... 43% 120M 0s 24650K .......... .......... .......... .......... .......... 43% 173M 0s 24700K .......... .......... .......... .......... .......... 43% 191M 0s 24750K .......... .......... .......... .......... .......... 43% 111M 0s 24800K .......... .......... .......... .......... .......... 43% 34.5M 0s 24850K .......... .......... .......... .......... .......... 43% 176M 0s 24900K .......... .......... .......... .......... .......... 43% 190M 0s 24950K .......... .......... .......... .......... .......... 43% 177M 0s 25000K .......... .......... .......... .......... .......... 43% 189M 0s 25050K .......... .......... .......... .......... .......... 43% 109M 0s 25100K .......... .......... .......... .......... .......... 44% 169M 0s 25150K .......... .......... .......... .......... .......... 44% 164M 0s 25200K .......... .......... .......... .......... .......... 44% 178M 0s 25250K .......... .......... .......... .......... .......... 44% 191M 0s 25300K .......... .......... .......... .......... .......... 44% 123M 0s 25350K .......... .......... .......... .......... .......... 44% 141M 0s 25400K .......... .......... .......... .......... .......... 44% 137M 0s 25450K .......... .......... .......... .......... .......... 44% 164M 0s 25500K .......... .......... .......... .......... .......... 44% 153M 0s 25550K .......... .......... .......... .......... .......... 44% 80.0M 0s 25600K .......... .......... .......... .......... .......... 44% 150M 0s 25650K .......... .......... .......... .......... .......... 45% 172M 0s 25700K .......... .......... .......... .......... .......... 45% 186M 0s 25750K .......... .......... .......... .......... .......... 45% 170M 0s 25800K .......... .......... .......... .......... .......... 45% 182M 0s 25850K .......... .......... .......... .......... .......... 45% 185M 0s 25900K .......... .......... .......... .......... .......... 45% 189M 0s 25950K .......... .......... .......... .......... .......... 45% 149M 0s 26000K .......... .......... .......... .......... .......... 45% 189M 0s 26050K .......... .......... .......... .......... .......... 45% 192M 0s 26100K .......... .......... .......... .......... .......... 45% 186M 0s 26150K .......... .......... .......... .......... .......... 45% 113M 0s 26200K .......... .......... .......... .......... .......... 45% 123M 0s 26250K .......... .......... .......... .......... .......... 46% 186M 0s 26300K .......... .......... .......... .......... .......... 46% 180M 0s 26350K .......... .......... .......... .......... .......... 46% 117M 0s 26400K .......... .......... .......... .......... .......... 46% 140M 0s 26450K .......... .......... .......... .......... .......... 46% 165M 0s 26500K .......... .......... .......... .......... .......... 46% 161M 0s 26550K .......... .......... .......... .......... .......... 46% 145M 0s 26600K .......... .......... .......... .......... .......... 46% 177M 0s 26650K .......... .......... .......... .......... .......... 46% 185M 0s 26700K .......... .......... .......... .......... .......... 46% 183M 0s 26750K .......... .......... .......... .......... .......... 46% 152M 0s 26800K .......... .......... .......... .......... .......... 47% 183M 0s 26850K .......... .......... .......... .......... .......... 47% 33.0M 0s 26900K .......... .......... .......... .......... .......... 47% 161M 0s 26950K .......... .......... .......... .......... .......... 47% 173M 0s 27000K .......... .......... .......... .......... .......... 47% 198M 0s 27050K .......... .......... .......... .......... .......... 47% 190M 0s 27100K .......... .......... .......... .......... .......... 47% 186M 0s 27150K .......... .......... .......... .......... .......... 47% 155M 0s 27200K .......... .......... .......... .......... .......... 47% 183M 0s 27250K .......... .......... .......... .......... .......... 47% 190M 0s 27300K .......... .......... .......... .......... .......... 47% 137M 0s 27350K .......... .......... .......... .......... .......... 47% 103M 0s 27400K .......... .......... .......... .......... .......... 48% 134M 0s 27450K .......... .......... .......... .......... .......... 48% 132M 0s 27500K .......... .......... .......... .......... .......... 48% 186M 0s 27550K .......... .......... .......... .......... .......... 48% 105M 0s 27600K .......... .......... .......... .......... .......... 48% 121M 0s 27650K .......... .......... .......... .......... .......... 48% 185M 0s 27700K .......... .......... .......... .......... .......... 48% 176M 0s 27750K .......... .......... .......... .......... .......... 48% 167M 0s 27800K .......... .......... .......... .......... .......... 48% 186M 0s 27850K .......... .......... .......... .......... .......... 48% 179M 0s 27900K .......... .......... .......... .......... .......... 48% 183M 0s 27950K .......... .......... .......... .......... .......... 49% 131M 0s 28000K .......... .......... .......... .......... .......... 49% 110M 0s 28050K .......... .......... .......... .......... .......... 49% 184M 0s 28100K .......... .......... .......... .......... .......... 49% 191M 0s 28150K .......... .......... .......... .......... .......... 49% 162M 0s 28200K .......... .......... .......... .......... .......... 49% 178M 0s 28250K .......... .......... .......... .......... .......... 49% 192M 0s 28300K .......... .......... .......... .......... .......... 49% 126M 0s 28350K .......... .......... .......... .......... .......... 49% 87.5M 0s 28400K .......... .......... .......... .......... .......... 49% 168M 0s 28450K .......... .......... .......... .......... .......... 49% 187M 0s 28500K .......... .......... .......... .......... .......... 50% 177M 0s 28550K .......... .......... .......... .......... .......... 50% 172M 0s 28600K .......... .......... .......... .......... .......... 50% 189M 0s 28650K .......... .......... .......... .......... .......... 50% 180M 0s 28700K .......... .......... .......... .......... .......... 50% 185M 0s 28750K .......... .......... .......... .......... .......... 50% 159M 0s 28800K .......... .......... .......... .......... .......... 50% 183M 0s 28850K .......... .......... .......... .......... .......... 50% 180M 0s 28900K .......... .......... .......... .......... .......... 50% 31.9M 0s 28950K .......... .......... .......... .......... .......... 50% 160M 0s 29000K .......... .......... .......... .......... .......... 50% 190M 0s 29050K .......... .......... .......... .......... .......... 50% 193M 0s 29100K .......... .......... .......... .......... .......... 51% 145M 0s 29150K .......... .......... .......... .......... .......... 51% 93.2M 0s 29200K .......... .......... .......... .......... .......... 51% 147M 0s 29250K .......... .......... .......... .......... .......... 51% 158M 0s 29300K .......... .......... .......... .......... .......... 51% 187M 0s 29350K .......... .......... .......... .......... .......... 51% 170M 0s 29400K .......... .......... .......... .......... .......... 51% 134M 0s 29450K .......... .......... .......... .......... .......... 51% 176M 0s 29500K .......... .......... .......... .......... .......... 51% 192M 0s 29550K .......... .......... .......... .......... .......... 51% 157M 0s 29600K .......... .......... .......... .......... .......... 51% 180M 0s 29650K .......... .......... .......... .......... .......... 52% 189M 0s 29700K .......... .......... .......... .......... .......... 52% 190M 0s 29750K .......... .......... .......... .......... .......... 52% 167M 0s 29800K .......... .......... .......... .......... .......... 52% 149M 0s 29850K .......... .......... .......... .......... .......... 52% 133M 0s 29900K .......... .......... .......... .......... .......... 52% 141M 0s 29950K .......... .......... .......... .......... .......... 52% 151M 0s 30000K .......... .......... .......... .......... .......... 52% 192M 0s 30050K .......... .......... .......... .......... .......... 52% 185M 0s 30100K .......... .......... .......... .......... .......... 52% 183M 0s 30150K .......... .......... .......... .......... .......... 52% 181M 0s 30200K .......... .......... .......... .......... .......... 52% 109M 0s 30250K .......... .......... .......... .......... .......... 53% 137M 0s 30300K .......... .......... .......... .......... .......... 53% 188M 0s 30350K .......... .......... .......... .......... .......... 53% 152M 0s 30400K .......... .......... .......... .......... .......... 53% 192M 0s 30450K .......... .......... .......... .......... .......... 53% 176M 0s 30500K .......... .......... .......... .......... .......... 53% 194M 0s 30550K .......... .......... .......... .......... .......... 53% 134M 0s 30600K .......... .......... .......... .......... .......... 53% 176M 0s 30650K .......... .......... .......... .......... .......... 53% 189M 0s 30700K .......... .......... .......... .......... .......... 53% 185M 0s 30750K .......... .......... .......... .......... .......... 53% 156M 0s 30800K .......... .......... .......... .......... .......... 54% 128M 0s 30850K .......... .......... .......... .......... .......... 54% 184M 0s 30900K .......... .......... .......... .......... .......... 54% 183M 0s 30950K .......... .......... .......... .......... .......... 54% 31.4M 0s 31000K .......... .......... .......... .......... .......... 54% 112M 0s 31050K .......... .......... .......... .......... .......... 54% 104M 0s 31100K .......... .......... .......... .......... .......... 54% 158M 0s 31150K .......... .......... .......... .......... .......... 54% 155M 0s 31200K .......... .......... .......... .......... .......... 54% 181M 0s 31250K .......... .......... .......... .......... .......... 54% 137M 0s 31300K .......... .......... .......... .......... .......... 54% 181M 0s 31350K .......... .......... .......... .......... .......... 54% 164M 0s 31400K .......... .......... .......... .......... .......... 55% 195M 0s 31450K .......... .......... .......... .......... .......... 55% 192M 0s 31500K .......... .......... .......... .......... .......... 55% 188M 0s 31550K .......... .......... .......... .......... .......... 55% 152M 0s 31600K .......... .......... .......... .......... .......... 55% 193M 0s 31650K .......... .......... .......... .......... .......... 55% 188M 0s 31700K .......... .......... .......... .......... .......... 55% 112M 0s 31750K .......... .......... .......... .......... .......... 55% 124M 0s 31800K .......... .......... .......... .......... .......... 55% 178M 0s 31850K .......... .......... .......... .......... .......... 55% 184M 0s 31900K .......... .......... .......... .......... .......... 55% 197M 0s 31950K .......... .......... .......... .......... .......... 56% 164M 0s 32000K .......... .......... .......... .......... .......... 56% 188M 0s 32050K .......... .......... .......... .......... .......... 56% 135M 0s 32100K .......... .......... .......... .......... .......... 56% 126M 0s 32150K .......... .......... .......... .......... .......... 56% 168M 0s 32200K .......... .......... .......... .......... .......... 56% 180M 0s 32250K .......... .......... .......... .......... .......... 56% 197M 0s 32300K .......... .......... .......... .......... .......... 56% 198M 0s 32350K .......... .......... .......... .......... .......... 56% 154M 0s 32400K .......... .......... .......... .......... .......... 56% 189M 0s 32450K .......... .......... .......... .......... .......... 56% 193M 0s 32500K .......... .......... .......... .......... .......... 57% 189M 0s 32550K .......... .......... .......... .......... .......... 57% 163M 0s 32600K .......... .......... .......... .......... .......... 57% 189M 0s 32650K .......... .......... .......... .......... .......... 57% 172M 0s 32700K .......... .......... .......... .......... .......... 57% 139M 0s 32750K .......... .......... .......... .......... .......... 57% 110M 0s 32800K .......... .......... .......... .......... .......... 57% 188M 0s 32850K .......... .......... .......... .......... .......... 57% 188M 0s 32900K .......... .......... .......... .......... .......... 57% 177M 0s 32950K .......... .......... .......... .......... .......... 57% 166M 0s 33000K .......... .......... .......... .......... .......... 57% 33.4M 0s 33050K .......... .......... .......... .......... .......... 57% 193M 0s 33100K .......... .......... .......... .......... .......... 58% 186M 0s 33150K .......... .......... .......... .......... .......... 58% 161M 0s 33200K .......... .......... .......... .......... .......... 58% 185M 0s 33250K .......... .......... .......... .......... .......... 58% 191M 0s 33300K .......... .......... .......... .......... .......... 58% 196M 0s 33350K .......... .......... .......... .......... .......... 58% 168M 0s 33400K .......... .......... .......... .......... .......... 58% 94.2M 0s 33450K .......... .......... .......... .......... .......... 58% 182M 0s 33500K .......... .......... .......... .......... .......... 58% 179M 0s 33550K .......... .......... .......... .......... .......... 58% 154M 0s 33600K .......... .......... .......... .......... .......... 58% 186M 0s 33650K .......... .......... .......... .......... .......... 59% 187M 0s 33700K .......... .......... .......... .......... .......... 59% 112M 0s 33750K .......... .......... .......... .......... .......... 59% 94.1M 0s 33800K .......... .......... .......... .......... .......... 59% 105M 0s 33850K .......... .......... .......... .......... .......... 59% 180M 0s 33900K .......... .......... .......... .......... .......... 59% 130M 0s 33950K .......... .......... .......... .......... .......... 59% 122M 0s 34000K .......... .......... .......... .......... .......... 59% 190M 0s 34050K .......... .......... .......... .......... .......... 59% 186M 0s 34100K .......... .......... .......... .......... .......... 59% 180M 0s 34150K .......... .......... .......... .......... .......... 59% 169M 0s 34200K .......... .......... .......... .......... .......... 59% 193M 0s 34250K .......... .......... .......... .......... .......... 60% 180M 0s 34300K .......... .......... .......... .......... .......... 60% 181M 0s 34350K .......... .......... .......... .......... .......... 60% 156M 0s 34400K .......... .......... .......... .......... .......... 60% 186M 0s 34450K .......... .......... .......... .......... .......... 60% 187M 0s 34500K .......... .......... .......... .......... .......... 60% 180M 0s 34550K .......... .......... .......... .......... .......... 60% 100M 0s 34600K .......... .......... .......... .......... .......... 60% 114M 0s 34650K .......... .......... .......... .......... .......... 60% 181M 0s 34700K .......... .......... .......... .......... .......... 60% 191M 0s 34750K .......... .......... .......... .......... .......... 60% 150M 0s 34800K .......... .......... .......... .......... .......... 61% 179M 0s 34850K .......... .......... .......... .......... .......... 61% 187M 0s 34900K .......... .......... .......... .......... .......... 61% 181M 0s 34950K .......... .......... .......... .......... .......... 61% 163M 0s 35000K .......... .......... .......... .......... .......... 61% 184M 0s 35050K .......... .......... .......... .......... .......... 61% 31.9M 0s 35100K .......... .......... .......... .......... .......... 61% 177M 0s 35150K .......... .......... .......... .......... .......... 61% 164M 0s 35200K .......... .......... .......... .......... .......... 61% 129M 0s 35250K .......... .......... .......... .......... .......... 61% 129M 0s 35300K .......... .......... .......... .......... .......... 61% 188M 0s 35350K .......... .......... .......... .......... .......... 61% 168M 0s 35400K .......... .......... .......... .......... .......... 62% 185M 0s 35450K .......... .......... .......... .......... .......... 62% 192M 0s 35500K .......... .......... .......... .......... .......... 62% 142M 0s 35550K .......... .......... .......... .......... .......... 62% 111M 0s 35600K .......... .......... .......... .......... .......... 62% 187M 0s 35650K .......... .......... .......... .......... .......... 62% 138M 0s 35700K .......... .......... .......... .......... .......... 62% 181M 0s 35750K .......... .......... .......... .......... .......... 62% 114M 0s 35800K .......... .......... .......... .......... .......... 62% 159M 0s 35850K .......... .......... .......... .......... .......... 62% 186M 0s 35900K .......... .......... .......... .......... .......... 62% 178M 0s 35950K .......... .......... .......... .......... .......... 63% 161M 0s 36000K .......... .......... .......... .......... .......... 63% 178M 0s 36050K .......... .......... .......... .......... .......... 63% 152M 0s 36100K .......... .......... .......... .......... .......... 63% 190M 0s 36150K .......... .......... .......... .......... .......... 63% 148M 0s 36200K .......... .......... .......... .......... .......... 63% 191M 0s 36250K .......... .......... .......... .......... .......... 63% 178M 0s 36300K .......... .......... .......... .......... .......... 63% 190M 0s 36350K .......... .......... .......... .......... .......... 63% 157M 0s 36400K .......... .......... .......... .......... .......... 63% 104M 0s 36450K .......... .......... .......... .......... .......... 63% 114M 0s 36500K .......... .......... .......... .......... .......... 64% 176M 0s 36550K .......... .......... .......... .......... .......... 64% 158M 0s 36600K .......... .......... .......... .......... .......... 64% 190M 0s 36650K .......... .......... .......... .......... .......... 64% 188M 0s 36700K .......... .......... .......... .......... .......... 64% 184M 0s 36750K .......... .......... .......... .......... .......... 64% 155M 0s 36800K .......... .......... .......... .......... .......... 64% 186M 0s 36850K .......... .......... .......... .......... .......... 64% 187M 0s 36900K .......... .......... .......... .......... .......... 64% 113M 0s 36950K .......... .......... .......... .......... .......... 64% 169M 0s 37000K .......... .......... .......... .......... .......... 64% 184M 0s 37050K .......... .......... .......... .......... .......... 64% 181M 0s 37100K .......... .......... .......... .......... .......... 65% 33.2M 0s 37150K .......... .......... .......... .......... .......... 65% 147M 0s 37200K .......... .......... .......... .......... .......... 65% 183M 0s 37250K .......... .......... .......... .......... .......... 65% 184M 0s 37300K .......... .......... .......... .......... .......... 65% 187M 0s 37350K .......... .......... .......... .......... .......... 65% 130M 0s 37400K .......... .......... .......... .......... .......... 65% 131M 0s 37450K .......... .......... .......... .......... .......... 65% 182M 0s 37500K .......... .......... .......... .......... .......... 65% 141M 0s 37550K .......... .......... .......... .......... .......... 65% 158M 0s 37600K .......... .......... .......... .......... .......... 65% 158M 0s 37650K .......... .......... .......... .......... .......... 66% 191M 0s 37700K .......... .......... .......... .......... .......... 66% 167M 0s 37750K .......... .......... .......... .......... .......... 66% 170M 0s 37800K .......... .......... .......... .......... .......... 66% 183M 0s 37850K .......... .......... .......... .......... .......... 66% 176M 0s 37900K .......... .......... .......... .......... .......... 66% 189M 0s 37950K .......... .......... .......... .......... .......... 66% 121M 0s 38000K .......... .......... .......... .......... .......... 66% 129M 0s 38050K .......... .......... .......... .......... .......... 66% 192M 0s 38100K .......... .......... .......... .......... .......... 66% 188M 0s 38150K .......... .......... .......... .......... .......... 66% 166M 0s 38200K .......... .......... .......... .......... .......... 66% 180M 0s 38250K .......... .......... .......... .......... .......... 67% 185M 0s 38300K .......... .......... .......... .......... .......... 67% 128M 0s 38350K .......... .......... .......... .......... .......... 67% 96.4M 0s 38400K .......... .......... .......... .......... .......... 67% 138M 0s 38450K .......... .......... .......... .......... .......... 67% 177M 0s 38500K .......... .......... .......... .......... .......... 67% 185M 0s 38550K .......... .......... .......... .......... .......... 67% 170M 0s 38600K .......... .......... .......... .......... .......... 67% 188M 0s 38650K .......... .......... .......... .......... .......... 67% 177M 0s 38700K .......... .......... .......... .......... .......... 67% 188M 0s 38750K .......... .......... .......... .......... .......... 67% 108M 0s 38800K .......... .......... .......... .......... .......... 68% 139M 0s 38850K .......... .......... .......... .......... .......... 68% 178M 0s 38900K .......... .......... .......... .......... .......... 68% 187M 0s 38950K .......... .......... .......... .......... .......... 68% 169M 0s 39000K .......... .......... .......... .......... .......... 68% 180M 0s 39050K .......... .......... .......... .......... .......... 68% 182M 0s 39100K .......... .......... .......... .......... .......... 68% 187M 0s 39150K .......... .......... .......... .......... .......... 68% 31.7M 0s 39200K .......... .......... .......... .......... .......... 68% 170M 0s 39250K .......... .......... .......... .......... .......... 68% 137M 0s 39300K .......... .......... .......... .......... .......... 68% 149M 0s 39350K .......... .......... .......... .......... .......... 69% 149M 0s 39400K .......... .......... .......... .......... .......... 69% 159M 0s 39450K .......... .......... .......... .......... .......... 69% 179M 0s 39500K .......... .......... .......... .......... .......... 69% 185M 0s 39550K .......... .......... .......... .......... .......... 69% 152M 0s 39600K .......... .......... .......... .......... .......... 69% 179M 0s 39650K .......... .......... .......... .......... .......... 69% 181M 0s 39700K .......... .......... .......... .......... .......... 69% 193M 0s 39750K .......... .......... .......... .......... .......... 69% 165M 0s 39800K .......... .......... .......... .......... .......... 69% 126M 0s 39850K .......... .......... .......... .......... .......... 69% 105M 0s 39900K .......... .......... .......... .......... .......... 69% 156M 0s 39950K .......... .......... .......... .......... .......... 70% 158M 0s 40000K .......... .......... .......... .......... .......... 70% 204M 0s 40050K .......... .......... .......... .......... .......... 70% 152M 0s 40100K .......... .......... .......... .......... .......... 70% 173M 0s 40150K .......... .......... .......... .......... .......... 70% 116M 0s 40200K .......... .......... .......... .......... .......... 70% 172M 0s 40250K .......... .......... .......... .......... .......... 70% 154M 0s 40300K .......... .......... .......... .......... .......... 70% 89.7M 0s 40350K .......... .......... .......... .......... .......... 70% 116M 0s 40400K .......... .......... .......... .......... .......... 70% 187M 0s 40450K .......... .......... .......... .......... .......... 70% 187M 0s 40500K .......... .......... .......... .......... .......... 71% 190M 0s 40550K .......... .......... .......... .......... .......... 71% 107M 0s 40600K .......... .......... .......... .......... .......... 71% 165M 0s 40650K .......... .......... .......... .......... .......... 71% 189M 0s 40700K .......... .......... .......... .......... .......... 71% 189M 0s 40750K .......... .......... .......... .......... .......... 71% 155M 0s 40800K .......... .......... .......... .......... .......... 71% 184M 0s 40850K .......... .......... .......... .......... .......... 71% 175M 0s 40900K .......... .......... .......... .......... .......... 71% 187M 0s 40950K .......... .......... .......... .......... .......... 71% 167M 0s 41000K .......... .......... .......... .......... .......... 71% 189M 0s 41050K .......... .......... .......... .......... .......... 71% 147M 0s 41100K .......... .......... .......... .......... .......... 72% 183M 0s 41150K .......... .......... .......... .......... .......... 72% 106M 0s 41200K .......... .......... .......... .......... .......... 72% 32.3M 0s 41250K .......... .......... .......... .......... .......... 72% 189M 0s 41300K .......... .......... .......... .......... .......... 72% 190M 0s 41350K .......... .......... .......... .......... .......... 72% 165M 0s 41400K .......... .......... .......... .......... .......... 72% 119M 0s 41450K .......... .......... .......... .......... .......... 72% 32.9M 0s 41500K .......... .......... .......... .......... .......... 72% 195M 0s 41550K .......... .......... .......... .......... .......... 72% 155M 0s 41600K .......... .......... .......... .......... .......... 72% 179M 0s 41650K .......... .......... .......... .......... .......... 73% 186M 0s 41700K .......... .......... .......... .......... .......... 73% 193M 0s 41750K .......... .......... .......... .......... .......... 73% 105M 0s 41800K .......... .......... .......... .......... .......... 73% 106M 0s 41850K .......... .......... .......... .......... .......... 73% 112M 0s 41900K .......... .......... .......... .......... .......... 73% 171M 0s 41950K .......... .......... .......... .......... .......... 73% 145M 0s 42000K .......... .......... .......... .......... .......... 73% 167M 0s 42050K .......... .......... .......... .......... .......... 73% 176M 0s 42100K .......... .......... .......... .......... .......... 73% 189M 0s 42150K .......... .......... .......... .......... .......... 73% 116M 0s 42200K .......... .......... .......... .......... .......... 73% 114M 0s 42250K .......... .......... .......... .......... .......... 74% 189M 0s 42300K .......... .......... .......... .......... .......... 74% 184M 0s 42350K .......... .......... .......... .......... .......... 74% 150M 0s 42400K .......... .......... .......... .......... .......... 74% 193M 0s 42450K .......... .......... .......... .......... .......... 74% 209M 0s 42500K .......... .......... .......... .......... .......... 74% 185M 0s 42550K .......... .......... .......... .......... .......... 74% 159M 0s 42600K .......... .......... .......... .......... .......... 74% 192M 0s 42650K .......... .......... .......... .......... .......... 74% 143M 0s 42700K .......... .......... .......... .......... .......... 74% 137M 0s 42750K .......... .......... .......... .......... .......... 74% 123M 0s 42800K .......... .......... .......... .......... .......... 75% 157M 0s 42850K .......... .......... .......... .......... .......... 75% 180M 0s 42900K .......... .......... .......... .......... .......... 75% 189M 0s 42950K .......... .......... .......... .......... .......... 75% 176M 0s 43000K .......... .......... .......... .......... .......... 75% 191M 0s 43050K .......... .......... .......... .......... .......... 75% 124M 0s 43100K .......... .......... .......... .......... .......... 75% 183M 0s 43150K .......... .......... .......... .......... .......... 75% 140M 0s 43200K .......... .......... .......... .......... .......... 75% 141M 0s 43250K .......... .......... .......... .......... .......... 75% 32.0M 0s 43300K .......... .......... .......... .......... .......... 75% 196M 0s 43350K .......... .......... .......... .......... .......... 76% 168M 0s 43400K .......... .......... .......... .......... .......... 76% 194M 0s 43450K .......... .......... .......... .......... .......... 76% 190M 0s 43500K .......... .......... .......... .......... .......... 76% 188M 0s 43550K .......... .......... .......... .......... .......... 76% 131M 0s 43600K .......... .......... .......... .......... .......... 76% 181M 0s 43650K .......... .......... .......... .......... .......... 76% 183M 0s 43700K .......... .......... .......... .......... .......... 76% 178M 0s 43750K .......... .......... .......... .......... .......... 76% 167M 0s 43800K .......... .......... .......... .......... .......... 76% 186M 0s 43850K .......... .......... .......... .......... .......... 76% 183M 0s 43900K .......... .......... .......... .......... .......... 76% 183M 0s 43950K .......... .......... .......... .......... .......... 77% 159M 0s 44000K .......... .......... .......... .......... .......... 77% 185M 0s 44050K .......... .......... .......... .......... .......... 77% 129M 0s 44100K .......... .......... .......... .......... .......... 77% 142M 0s 44150K .......... .......... .......... .......... .......... 77% 165M 0s 44200K .......... .......... .......... .......... .......... 77% 182M 0s 44250K .......... .......... .......... .......... .......... 77% 189M 0s 44300K .......... .......... .......... .......... .......... 77% 181M 0s 44350K .......... .......... .......... .......... .......... 77% 138M 0s 44400K .......... .......... .......... .......... .......... 77% 97.1M 0s 44450K .......... .......... .......... .......... .......... 77% 139M 0s 44500K .......... .......... .......... .......... .......... 78% 152M 0s 44550K .......... .......... .......... .......... .......... 78% 116M 0s 44600K .......... .......... .......... .......... .......... 78% 121M 0s 44650K .......... .......... .......... .......... .......... 78% 174M 0s 44700K .......... .......... .......... .......... .......... 78% 193M 0s 44750K .......... .......... .......... .......... .......... 78% 160M 0s 44800K .......... .......... .......... .......... .......... 78% 181M 0s 44850K .......... .......... .......... .......... .......... 78% 188M 0s 44900K .......... .......... .......... .......... .......... 78% 126M 0s 44950K .......... .......... .......... .......... .......... 78% 160M 0s 45000K .......... .......... .......... .......... .......... 78% 153M 0s 45050K .......... .......... .......... .......... .......... 78% 148M 0s 45100K .......... .......... .......... .......... .......... 79% 188M 0s 45150K .......... .......... .......... .......... .......... 79% 149M 0s 45200K .......... .......... .......... .......... .......... 79% 190M 0s 45250K .......... .......... .......... .......... .......... 79% 187M 0s 45300K .......... .......... .......... .......... .......... 79% 33.5M 0s 45350K .......... .......... .......... .......... .......... 79% 162M 0s 45400K .......... .......... .......... .......... .......... 79% 145M 0s 45450K .......... .......... .......... .......... .......... 79% 142M 0s 45500K .......... .......... .......... .......... .......... 79% 187M 0s 45550K .......... .......... .......... .......... .......... 79% 157M 0s 45600K .......... .......... .......... .......... .......... 79% 181M 0s 45650K .......... .......... .......... .......... .......... 80% 195M 0s 45700K .......... .......... .......... .......... .......... 80% 196M 0s 45750K .......... .......... .......... .......... .......... 80% 163M 0s 45800K .......... .......... .......... .......... .......... 80% 181M 0s 45850K .......... .......... .......... .......... .......... 80% 189M 0s 45900K .......... .......... .......... .......... .......... 80% 127M 0s 45950K .......... .......... .......... .......... .......... 80% 112M 0s 46000K .......... .......... .......... .......... .......... 80% 187M 0s 46050K .......... .......... .......... .......... .......... 80% 186M 0s 46100K .......... .......... .......... .......... .......... 80% 178M 0s 46150K .......... .......... .......... .......... .......... 80% 167M 0s 46200K .......... .......... .......... .......... .......... 81% 146M 0s 46250K .......... .......... .......... .......... .......... 81% 129M 0s 46300K .......... .......... .......... .......... .......... 81% 167M 0s 46350K .......... .......... .......... .......... .......... 81% 123M 0s 46400K .......... .......... .......... .......... .......... 81% 149M 0s 46450K .......... .......... .......... .......... .......... 81% 157M 0s 46500K .......... .......... .......... .......... .......... 81% 182M 0s 46550K .......... .......... .......... .......... .......... 81% 169M 0s 46600K .......... .......... .......... .......... .......... 81% 180M 0s 46650K .......... .......... .......... .......... .......... 81% 195M 0s 46700K .......... .......... .......... .......... .......... 81% 183M 0s 46750K .......... .......... .......... .......... .......... 81% 110M 0s 46800K .......... .......... .......... .......... .......... 82% 138M 0s 46850K .......... .......... .......... .......... .......... 82% 159M 0s 46900K .......... .......... .......... .......... .......... 82% 153M 0s 46950K .......... .......... .......... .......... .......... 82% 169M 0s 47000K .......... .......... .......... .......... .......... 82% 185M 0s 47050K .......... .......... .......... .......... .......... 82% 188M 0s 47100K .......... .......... .......... .......... .......... 82% 179M 0s 47150K .......... .......... .......... .......... .......... 82% 155M 0s 47200K .......... .......... .......... .......... .......... 82% 185M 0s 47250K .......... .......... .......... .......... .......... 82% 181M 0s 47300K .......... .......... .......... .......... .......... 82% 191M 0s 47350K .......... .......... .......... .......... .......... 83% 31.7M 0s 47400K .......... .......... .......... .......... .......... 83% 193M 0s 47450K .......... .......... .......... .......... .......... 83% 193M 0s 47500K .......... .......... .......... .......... .......... 83% 200M 0s 47550K .......... .......... .......... .......... .......... 83% 160M 0s 47600K .......... .......... .......... .......... .......... 83% 184M 0s 47650K .......... .......... .......... .......... .......... 83% 202M 0s 47700K .......... .......... .......... .......... .......... 83% 190M 0s 47750K .......... .......... .......... .......... .......... 83% 178M 0s 47800K .......... .......... .......... .......... .......... 83% 139M 0s 47850K .......... .......... .......... .......... .......... 83% 80.2M 0s 47900K .......... .......... .......... .......... .......... 83% 180M 0s 47950K .......... .......... .......... .......... .......... 84% 163M 0s 48000K .......... .......... .......... .......... .......... 84% 191M 0s 48050K .......... .......... .......... .......... .......... 84% 185M 0s 48100K .......... .......... .......... .......... .......... 84% 122M 0s 48150K .......... .......... .......... .......... .......... 84% 142M 0s 48200K .......... .......... .......... .......... .......... 84% 180M 0s 48250K .......... .......... .......... .......... .......... 84% 130M 0s 48300K .......... .......... .......... .......... .......... 84% 161M 0s 48350K .......... .......... .......... .......... .......... 84% 151M 0s 48400K .......... .......... .......... .......... .......... 84% 191M 0s 48450K .......... .......... .......... .......... .......... 84% 192M 0s 48500K .......... .......... .......... .......... .......... 85% 188M 0s 48550K .......... .......... .......... .......... .......... 85% 156M 0s 48600K .......... .......... .......... .......... .......... 85% 131M 0s 48650K .......... .......... .......... .......... .......... 85% 138M 0s 48700K .......... .......... .......... .......... .......... 85% 169M 0s 48750K .......... .......... .......... .......... .......... 85% 110M 0s 48800K .......... .......... .......... .......... .......... 85% 185M 0s 48850K .......... .......... .......... .......... .......... 85% 175M 0s 48900K .......... .......... .......... .......... .......... 85% 188M 0s 48950K .......... .......... .......... .......... .......... 85% 168M 0s 49000K .......... .......... .......... .......... .......... 85% 189M 0s 49050K .......... .......... .......... .......... .......... 85% 180M 0s 49100K .......... .......... .......... .......... .......... 86% 177M 0s 49150K .......... .......... .......... .......... .......... 86% 153M 0s 49200K .......... .......... .......... .......... .......... 86% 181M 0s 49250K .......... .......... .......... .......... .......... 86% 187M 0s 49300K .......... .......... .......... .......... .......... 86% 192M 0s 49350K .......... .......... .......... .......... .......... 86% 165M 0s 49400K .......... .......... .......... .......... .......... 86% 30.5M 0s 49450K .......... .......... .......... .......... .......... 86% 192M 0s 49500K .......... .......... .......... .......... .......... 86% 183M 0s 49550K .......... .......... .......... .......... .......... 86% 160M 0s 49600K .......... .......... .......... .......... .......... 86% 199M 0s 49650K .......... .......... .......... .......... .......... 87% 142M 0s 49700K .......... .......... .......... .......... .......... 87% 119M 0s 49750K .......... .......... .......... .......... .......... 87% 142M 0s 49800K .......... .......... .......... .......... .......... 87% 191M 0s 49850K .......... .......... .......... .......... .......... 87% 177M 0s 49900K .......... .......... .......... .......... .......... 87% 193M 0s 49950K .......... .......... .......... .......... .......... 87% 133M 0s 50000K .......... .......... .......... .......... .......... 87% 121M 0s 50050K .......... .......... .......... .......... .......... 87% 191M 0s 50100K .......... .......... .......... .......... .......... 87% 185M 0s 50150K .......... .......... .......... .......... .......... 87% 114M 0s 50200K .......... .......... .......... .......... .......... 88% 157M 0s 50250K .......... .......... .......... .......... .......... 88% 185M 0s 50300K .......... .......... .......... .......... .......... 88% 178M 0s 50350K .......... .......... .......... .......... .......... 88% 155M 0s 50400K .......... .......... .......... .......... .......... 88% 192M 0s 50450K .......... .......... .......... .......... .......... 88% 155M 0s 50500K .......... .......... .......... .......... .......... 88% 128M 0s 50550K .......... .......... .......... .......... .......... 88% 179M 0s 50600K .......... .......... .......... .......... .......... 88% 188M 0s 50650K .......... .......... .......... .......... .......... 88% 170M 0s 50700K .......... .......... .......... .......... .......... 88% 49.4M 0s 50750K .......... .......... .......... .......... .......... 88% 160M 0s 50800K .......... .......... .......... .......... .......... 89% 194M 0s 50850K .......... .......... .......... .......... .......... 89% 189M 0s 50900K .......... .......... .......... .......... .......... 89% 176M 0s 50950K .......... .......... .......... .......... .......... 89% 162M 0s 51000K .......... .......... .......... .......... .......... 89% 193M 0s 51050K .......... .......... .......... .......... .......... 89% 186M 0s 51100K .......... .......... .......... .......... .......... 89% 186M 0s 51150K .......... .......... .......... .......... .......... 89% 156M 0s 51200K .......... .......... .......... .......... .......... 89% 193M 0s 51250K .......... .......... .......... .......... .......... 89% 176M 0s 51300K .......... .......... .......... .......... .......... 89% 183M 0s 51350K .......... .......... .......... .......... .......... 90% 177M 0s 51400K .......... .......... .......... .......... .......... 90% 195M 0s 51450K .......... .......... .......... .......... .......... 90% 151M 0s 51500K .......... .......... .......... .......... .......... 90% 197M 0s 51550K .......... .......... .......... .......... .......... 90% 159M 0s 51600K .......... .......... .......... .......... .......... 90% 31.8M 0s 51650K .......... .......... .......... .......... .......... 90% 191M 0s 51700K .......... .......... .......... .......... .......... 90% 201M 0s 51750K .......... .......... .......... .......... .......... 90% 138M 0s 51800K .......... .......... .......... .......... .......... 90% 165M 0s 51850K .......... .......... .......... .......... .......... 90% 163M 0s 51900K .......... .......... .......... .......... .......... 90% 158M 0s 51950K .......... .......... .......... .......... .......... 91% 152M 0s 52000K .......... .......... .......... .......... .......... 91% 179M 0s 52050K .......... .......... .......... .......... .......... 91% 169M 0s 52100K .......... .......... .......... .......... .......... 91% 175M 0s 52150K .......... .......... .......... .......... .......... 91% 176M 0s 52200K .......... .......... .......... .......... .......... 91% 195M 0s 52250K .......... .......... .......... .......... .......... 91% 185M 0s 52300K .......... .......... .......... .......... .......... 91% 189M 0s 52350K .......... .......... .......... .......... .......... 91% 134M 0s 52400K .......... .......... .......... .......... .......... 91% 184M 0s 52450K .......... .......... .......... .......... .......... 91% 178M 0s 52500K .......... .......... .......... .......... .......... 92% 172M 0s 52550K .......... .......... .......... .......... .......... 92% 170M 0s 52600K .......... .......... .......... .......... .......... 92% 183M 0s 52650K .......... .......... .......... .......... .......... 92% 189M 0s 52700K .......... .......... .......... .......... .......... 92% 195M 0s 52750K .......... .......... .......... .......... .......... 92% 161M 0s 52800K .......... .......... .......... .......... .......... 92% 179M 0s 52850K .......... .......... .......... .......... .......... 92% 180M 0s 52900K .......... .......... .......... .......... .......... 92% 195M 0s 52950K .......... .......... .......... .......... .......... 92% 96.2M 0s 53000K .......... .......... .......... .......... .......... 92% 168M 0s 53050K .......... .......... .......... .......... .......... 92% 186M 0s 53100K .......... .......... .......... .......... .......... 93% 178M 0s 53150K .......... .......... .......... .......... .......... 93% 145M 0s 53200K .......... .......... .......... .......... .......... 93% 180M 0s 53250K .......... .......... .......... .......... .......... 93% 175M 0s 53300K .......... .......... .......... .......... .......... 93% 169M 0s 53350K .......... .......... .......... .......... .......... 93% 161M 0s 53400K .......... .......... .......... .......... .......... 93% 145M 0s 53450K .......... .......... .......... .......... .......... 93% 138M 0s 53500K .......... .......... .......... .......... .......... 93% 179M 0s 53550K .......... .......... .......... .......... .......... 93% 152M 0s 53600K .......... .......... .......... .......... .......... 93% 112M 0s 53650K .......... .......... .......... .......... .......... 94% 29.2M 0s 53700K .......... .......... .......... .......... .......... 94% 149M 0s 53750K .......... .......... .......... .......... .......... 94% 149M 0s 53800K .......... .......... .......... .......... .......... 94% 171M 0s 53850K .......... .......... .......... .......... .......... 94% 164M 0s 53900K .......... .......... .......... .......... .......... 94% 164M 0s 53950K .......... .......... .......... .......... .......... 94% 145M 0s 54000K .......... .......... .......... .......... .......... 94% 168M 0s 54050K .......... .......... .......... .......... .......... 94% 174M 0s 54100K .......... .......... .......... .......... .......... 94% 189M 0s 54150K .......... .......... .......... .......... .......... 94% 171M 0s 54200K .......... .......... .......... .......... .......... 95% 171M 0s 54250K .......... .......... .......... .......... .......... 95% 157M 0s 54300K .......... .......... .......... .......... .......... 95% 185M 0s 54350K .......... .......... .......... .......... .......... 95% 135M 0s 54400K .......... .......... .......... .......... .......... 95% 185M 0s 54450K .......... .......... .......... .......... .......... 95% 187M 0s 54500K .......... .......... .......... .......... .......... 95% 178M 0s 54550K .......... .......... .......... .......... .......... 95% 160M 0s 54600K .......... .......... .......... .......... .......... 95% 191M 0s 54650K .......... .......... .......... .......... .......... 95% 186M 0s 54700K .......... .......... .......... .......... .......... 95% 191M 0s 54750K .......... .......... .......... .......... .......... 95% 142M 0s 54800K .......... .......... .......... .......... .......... 96% 186M 0s 54850K .......... .......... .......... .......... .......... 96% 159M 0s 54900K .......... .......... .......... .......... .......... 96% 164M 0s 54950K .......... .......... .......... .......... .......... 96% 165M 0s 55000K .......... .......... .......... .......... .......... 96% 181M 0s 55050K .......... .......... .......... .......... .......... 96% 188M 0s 55100K .......... .......... .......... .......... .......... 96% 167M 0s 55150K .......... .......... .......... .......... .......... 96% 150M 0s 55200K .......... .......... .......... .......... .......... 96% 173M 0s 55250K .......... .......... .......... .......... .......... 96% 178M 0s 55300K .......... .......... .......... .......... .......... 96% 180M 0s 55350K .......... .......... .......... .......... .......... 97% 145M 0s 55400K .......... .......... .......... .......... .......... 97% 159M 0s 55450K .......... .......... .......... .......... .......... 97% 153M 0s 55500K .......... .......... .......... .......... .......... 97% 155M 0s 55550K .......... .......... .......... .......... .......... 97% 138M 0s 55600K .......... .......... .......... .......... .......... 97% 182M 0s 55650K .......... .......... .......... .......... .......... 97% 189M 0s 55700K .......... .......... .......... .......... .......... 97% 32.7M 0s 55750K .......... .......... .......... .......... .......... 97% 169M 0s 55800K .......... .......... .......... .......... .......... 97% 193M 0s 55850K .......... .......... .......... .......... .......... 97% 188M 0s 55900K .......... .......... .......... .......... .......... 97% 182M 0s 55950K .......... .......... .......... .......... .......... 98% 160M 0s 56000K .......... .......... .......... .......... .......... 98% 192M 0s 56050K .......... .......... .......... .......... .......... 98% 194M 0s 56100K .......... .......... .......... .......... .......... 98% 183M 0s 56150K .......... .......... .......... .......... .......... 98% 174M 0s 56200K .......... .......... .......... .......... .......... 98% 187M 0s 56250K .......... .......... .......... .......... .......... 98% 185M 0s 56300K .......... .......... .......... .......... .......... 98% 189M 0s 56350K .......... .......... .......... .......... .......... 98% 160M 0s 56400K .......... .......... .......... .......... .......... 98% 189M 0s 56450K .......... .......... .......... .......... .......... 98% 176M 0s 56500K .......... .......... .......... .......... .......... 99% 192M 0s 56550K .......... .......... .......... .......... .......... 99% 174M 0s 56600K .......... .......... .......... .......... .......... 99% 184M 0s 56650K .......... .......... .......... .......... .......... 99% 184M 0s 56700K .......... .......... .......... .......... .......... 99% 187M 0s 56750K .......... .......... .......... .......... .......... 99% 154M 0s 56800K .......... .......... .......... .......... .......... 99% 183M 0s 56850K .......... .......... .......... .......... .......... 99% 185M 0s 56900K .......... .......... .......... .......... .......... 99% 192M 0s 56950K .......... .......... .......... .......... .......... 99% 167M 0s 57000K .......... .......... .......... .......... .......... 99% 179M 0s 57050K .......... .......... .......... .......... ........ 100% 170M=0.5s 2020-11-30 00:23:30 (122 MB/s) - ‘Miniconda3-4.5.4-Linux-x86_64.sh’ saved [58468498/58468498] Python 3.6.5 :: Anaconda, Inc. 사이킷런 버전은 다음과 같이 확인할 수 있다. version앞뒤의 “__”는 언더스코어가 연이어 두개 있는 것이다. 123import sklearnprint(sklearn.__version__) 0.22.2.post1 첫 번째 머신러닝 만들어 보기 - 붓꽃 품종 예측하기첫 번째로 만들어 볼 머신러링 모델은 붓꽃 데이터 세트로 붓꽃의 품종을 분류하는 것이다. 붓꽃 데이터 세트는 꽃잎의 길이와 너비, 꽃받침의 길이와 너비 피처를 기반으로 꽃의 품종을 예측하기 위한 것이다. # 피쳐(Feature)는 기계 학습과 패턴 인식의 용어이다. 관찰 대상에게서 발견된 개별적이고 측정가능한 경험적(heuristic) 속성을 말한다.분류는 대표적인 지도학습 방법의 하나이다. 지도학습은 학습을 위한 다양한 피처와 분류 결정값인 레이블 데이터로 모델을 학습한 뒤, 별도의 테스트 데이터 세트에서 미지의 레이블을 예측하는 것. 즉, 명확한 정답이 주어진 데이터를 먼저 학습한 뒤 미지의 정답을 예측하는 방식. 이 때, 학습을 위해 주어진 데이터 세트를 학습 데이터 세트, 머신러닝 모델의 예측 성능을 평가하기 위해 별도로 주어진 데이터 세트를 테스트 데이터 세트로 지칭한다. 새로운 주피터 노트북을 생성하고 사이킷런에서 사용할 모듈을 임포트 한다. 사이킷런 패키지 내의 모듈명은 sklearn으로 시작하는 명명규칙이 있다. 항목은 아래와 같다. sklearn.datasets: 자체적으로 제공하는 데이터 세트를 생성하는 모듈의 모임 sklearn.tree: 트리 기반 ML 알고리즘을 구현한 클래스의 모임 * ML:Machine Learning 예측 데이터로 데이터를 분리하거나 최적의 하이퍼 파라미터로 평가하기 위한 다양한 모듈의 모임이다. 하이퍼 파라미터를 통해 머신러닝 알고리즘의 성능을 튜닝할 수 있다. 붓꽃 데이터 세트를 생성하는 데는 load_iris()를 이용하며, ML 알고리즘은 의사 결정 트리 알고리즘으로, 이를 구현한 DecisionTreeClassifier를 적용한다. 데이터 세트를 학습 데이터와 테스트 데이터로 분리하는 데는 train_test_split()함수를 사용 한다. 123from sklearn.datasets import load_iris from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import train_test_split load_iris() 함수를 이용해 붓꽃 데이터 세트를 로딩한 후, 피처들과 데이터 값이 어떻게 구성돼 있는지 확인하기 위해 DataFrame으로 변환한다. 1234567891011121314151617import pandas as pd # 붓꽃 데이터 세트를 로딩합니다.iris = load_iris() # iris.data는 Iris 데이터 세트에서 피처(feature)만으로 된 데이터를 numpy로 가지고 있습니다. iris_data = iris.data # iris.target은 붓꽃 데이터 세트에서 레이블(결정 값) 데이터를 numpy로 가지고 있습니다. *numpy: 벡터 및 행렬 연산에 있어서 매우 편리한 기능을 제공iris_label = iris.target print('iris target값:', iris_label) print('iris target명:', iris.target_names) # 붓꽃 데이터 세트를 자세히 보기 위해 DataFrame으로 변환합니다. iris_df = pd.DataFrame(data=iris_data, columns=iris.feature_names) iris_df['label'] = iris.target iris_df.head(3) iris target값: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2] iris target명: ['setosa' 'versicolor' 'virginica'] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) label 0 5.1 3.5 1.4 0.2 0 1 4.9 3.0 1.4 0.2 0 2 4.7 3.2 1.3 0.2 0 학습용 데이터와 테스트용 데이터는 반드시 분리해야 한다. 학습 데이터로 학습된 모델이 얼마나 뛰어난 성능을 가지는지 평가하려면 테스트 데이터 세트가 필요하기 때문이다. 이를 위해서 사이킷런은 train_test_split( ) API를 제공한다. train_test_split( )을 이용하면 학습 데이터와 테스트 데이터를 test_size 파라미터 입력 값의 비율로 쉽게 분할한다. 123x_train, x_test, y_train, y_test = train_test_split(iris_data, iris_label, test_size=0.2, random_state=11) train_test_split()의 파라미터 iris_data는 피처 데이터 세트 iris_label은 레이블 데이터 세트 test_size=0.2는 전체 데이터 세트 중 테스트 데이터 세트 비율(테스트 20%, 학습 데이터 80%) random_state는 호출할 때마다 같은 학습/테스트 용 데이터 세트를 생성하기 위해 주어지는 난수 발생 값. 지정하지 않으면 수행 할때마다 다른 학습/데이터 용 데이터 생성됨 위 예제에서 train_test_split()은 학습용 피처 데이터 세트를 x_train으로, 테스트용 피처 테이터 세트를 x_test로, 학습용 레이블 데이터 세트는 y_train으로, 테스트용 레이블 데이터 세트를 y_test로 반환한다. 학습 데이터를 확보 했으니 이 데이터를 기반으로 머신러닝 분류 알고리즘의 하나인 의사 결정을 트리를 이용해 학습과 예측을 수행해 보자. 먼저 사이킷런의 의사 결정 트리 클래스인 DecisionTreeClassifier를 객체로 생성한다. 생성된 DecisionTreeClassifier 객체의 fit() 메서드에 학습용 피처 데이터 속성과 결정값 데이터 세트를 입력해 호출하면 학습을 수행한다. 12345# DecisionTreeClassifier 객체 생성dt_clf = DecisionTreeClassifier(random_state=11)# 학습 수행dt_clf.fit(x_train, y_train) DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini', max_depth=None, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort='deprecated', random_state=11, splitter='best') 의사 결정 트리 기반의 DecisionTreeClassifier 객체는 학습 데이터를 기반으로 학습 완료한다. 예측은 반드시 학습 데이터가 아닌 다른 데이터를 이용해야 하며, 일반적으로 테스트 데이터 세트를 이용한다. DecisionTreeClassifier 객체의 predict() 메서드 테스트용 피처 데이터 세트를 입력해 호출하면 학습된 모델 기반에서 테스트 데이터 세트에 대한 예측값을 반환하게 된다. 12# 학습이 완료된 DecisionTreeClassifier 객체에서 테스트 데이터 세트로 예측 수행.pred = dt_clf.predict(x_test) 일반적으로 머신러닝 모델의 성능 평가 방법은 여러 가지가 있으나, 여기서는 정확도를 측정해 보겠다. 정확도는 예측 결과가 실제 레이블 값과 얼마나 정확하게 맞는지 평가하는 지표이다. 예측한 붓꽃 품종과 실제 테스트 데이터 세트의 붓꽃 품종이 얼마나 일치하는지 확인해 보겠다. 사이킷런은 정확도 측정을 위해 accuracy_score() 함수를 제공한다. accuaracy_score()의 첫 번째 파라미터로 실제 레이블 데이터 세트, 두 번째 파라미터로 예측 레이블 데이터 세트를 입력하면 된다. 12from sklearn.metrics import accuracy_scoreprint('예측 정확도: {0:.4f}'.format(accuracy_score(y_test, pred))) 예측 정확도: 0.9333 학습한 의사 결정 트리의 알고리즘 예측 정확도가 약0.9333(93.33%)으로 측정됐다. 앞의 붓꽃 데이터 세트로 분류를 예측한 프로세스를 정리하면 다음과 같다. 데이터 세트 분리: 데이터를 학습 데이터와 테스트 데이터로 분리한다. 모델 학습: 학습 데이터를 기반으로 ML 알고리즘을 적용해 모델을 학습시킨다. 예측 수행: 학습된 ML 모델을 이용해 테스트 데이터의 분류를 예측한다. 평가: 이렇게 예측된 결과값과 테스트 데이터의 실제 결과값을 비교해 ML 모델 성능을 평가한다. 사이킷런의 기반 프레임워크 익히기Estimator 이해 및 fit(), predict() 메서드사이킷런은 API 일관성과 개발 편의성을 제공하기 위한 노력이 엿보이는 패키지이다. 사이킷런은 ML 모델 학습을 위해서 fit( )을, 학습된 모델의 예측을 위해 predict( ) 메서드를 제공한다. 지도학습의 주요 두 축인 분류(Classification)와 회귀(Regression)의 다양한 알고리즘을 구현한 모든 사이킷런 클래스는 fit( )과 predict( ) 만을 이용해 간단하게 학습과 예측 결과를 반환한다. 사이킷런에서는 분류 알고리즘을 구현한 클래스를 Classifier로, 그리고 회귀 알고리즘을 구현한 클래스를 Regressor로 지칭한다. 사이킷런은 매우 많은 유형의 Classifier와 Regressor 클래스를 제공한다. 이들 Classifier와 Regressor를 합쳐서 Estimator 클래스라고 부른다. 즉, 지도학습의 모든 알고리즘을 구현한 클래스를 통칭해서 Estimator라고 부른다. cross_val_score( )(간편하게 교차검증 결과 평가 지표(정확도, 정밀도등)를 반환)와 같은 evaluation 함수, GridSearchSV와 같은 하이퍼 파라미터 튜닝을 지원하는 클래스의 경우 이 Estimator를 인자로 받는다. 인자로 받은 Estimator에 대해서 cross_val_score( ), GridSearchCV.fit( ) 함수 내에서 이 Estimator의 fit( )과 predict( )를 호출해서 평가를 하거나 하이퍼 파라미터 튜닝을 수행하는 것이다. 사이킷런에서 비지도 학습인 차원 축소, 클러스터링, 피처 추출등을 구현한 클래스의 대부분은 fit()과 transform()을 적용한다. 비지도학습과 피처 추출에서 fit()은 지도학습에서의 학습과 같은 의미가 아닌 입력 데이터의 형태에 맞춰 데이터를 변환하기 위한 사전 작업이라고 말할수 있다. 사이킷런의 주요 모듈다음은 사이킷런의 주요 모듈을 요약한 것이다. 자주 쓰이는 핵심 모듈 위주로 정리한 것이다. 일반적으로 머신러닝 모델을 구축하는 주요 프로세스는 피처의 가공, 변경, 추출을 수행하는 피처 처리(feature processing), ML 알고리즘 학습/예측 수행, 그리고 모델 평가의 단계를 반복적으로 수행하는 것이다. 사이킷런 패키지는 머신러닝 모델을 구축하는 주요 프로세스를 지원하기 위해 매우 편리하고 다양하며 유연한 모듈을 지원한다. 사이킷런에 내장된 데이터 세트는 일반적으로 딕셔너리 형태로 돼 있다. # 딕셔너리란 사전형 데이터를 의미하며, key와 value를 1대1로 대응시킨 형태를 말한다. 키는 보통 data, target, target_name, feature_names, DESCR로 구성돼 있다. 개별 키가 가리키는 의미는 다음과 같다. data는 피처의 데이터 세트를 가리킨다. target은 분류 시 레이블 값, 회귀일 때는 숫자 결과값 데이터 세트이다. target_names는 개별 레이블의 이름을 나타낸다. feature_names는 피처의 이름을 나타낸다. DESCR은 데이터 세트에 대한 설명과 각 피처의 설명을 나타낸다. data, target은 넘파이 배열 타입, target_names, feature_names는 넘파이 배열 또는 파이썬 리스트 타입, DESCR은 스트링 타입이다. 피처의 데이터 값을 반환받기 위해서는 내장 데이터 세트 API를 호출한 뒤에 그 key값을 지정하면 된다. 1234from sklearn.datasets import load_irisiris_data = load_iris()print(type(iris_data)) &lt;class 'sklearn.utils.Bunch'&gt; load_iris() API의 반환 결과는 sklearn, utils, bunch 클래스이다. 데이터 세트에 내장돼 있는 대부분의 데이터 세트는 이와 같이 딕셔너리 형태의 값을 반환한다. 딕셔너리 형태이므로 load_iries() 데이터 세트의 Key 값을 확인해 보자. 12keys = iris_data.keys()print('붓꽃 데이터 세트의 키들:', keys) 붓꽃 데이터 세트의 키들: dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names', 'filename']) 데이터 키는 피처들의 데이터 값을 가리킨다. 데이터 세트가 딕셔너리 형태이기 때문에 피처 데이터 값을 추출하기 위해서는 데이터 세트.data를 이용하면 된다.load_iris()가 반환하는 객체의 키인 feature_names, target_names, data, taget이 가르키는 값을 다음 예제 코드에 출력했다. 12345678910111213141516print('\\n feature_names 의 type:',type(iris_data.feature_names)) print(' feature_names 의 shape:',len(iris_data.feature_names)) print(iris_data.feature_names) print('\\n target_names 의 type:',type(iris_data.target_names)) print(' feature_names 의 shape:',len(iris_data.target_names)) print(iris_data.target_names) print('\\n data 의 type:',type(iris_data.data)) print(' data 의 shape:',iris_data.data.shape) print(iris_data['data']) print('\\n target 의 type:',type(iris_data.target)) print(' target 의 shape:',iris_data.target.shape) print(iris_data.target) feature_names 의 type: &lt;class 'list'&gt; feature_names 의 shape: 4 ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)'] target_names 의 type: &lt;class 'numpy.ndarray'&gt; feature_names 의 shape: 3 ['setosa' 'versicolor' 'virginica'] data 의 type: &lt;class 'numpy.ndarray'&gt; data 의 shape: (150, 4) [[5.1 3.5 1.4 0.2] [4.9 3. 1.4 0.2] [4.7 3.2 1.3 0.2] [4.6 3.1 1.5 0.2] [5. 3.6 1.4 0.2] [5.4 3.9 1.7 0.4] [4.6 3.4 1.4 0.3] [5. 3.4 1.5 0.2] [4.4 2.9 1.4 0.2] [4.9 3.1 1.5 0.1] [5.4 3.7 1.5 0.2] [4.8 3.4 1.6 0.2] [4.8 3. 1.4 0.1] [4.3 3. 1.1 0.1] [5.8 4. 1.2 0.2] [5.7 4.4 1.5 0.4] [5.4 3.9 1.3 0.4] [5.1 3.5 1.4 0.3] [5.7 3.8 1.7 0.3] [5.1 3.8 1.5 0.3] [5.4 3.4 1.7 0.2] [5.1 3.7 1.5 0.4] [4.6 3.6 1. 0.2] [5.1 3.3 1.7 0.5] [4.8 3.4 1.9 0.2] [5. 3. 1.6 0.2] [5. 3.4 1.6 0.4] [5.2 3.5 1.5 0.2] [5.2 3.4 1.4 0.2] [4.7 3.2 1.6 0.2] [4.8 3.1 1.6 0.2] [5.4 3.4 1.5 0.4] [5.2 4.1 1.5 0.1] [5.5 4.2 1.4 0.2] [4.9 3.1 1.5 0.2] [5. 3.2 1.2 0.2] [5.5 3.5 1.3 0.2] [4.9 3.6 1.4 0.1] [4.4 3. 1.3 0.2] [5.1 3.4 1.5 0.2] [5. 3.5 1.3 0.3] [4.5 2.3 1.3 0.3] [4.4 3.2 1.3 0.2] [5. 3.5 1.6 0.6] [5.1 3.8 1.9 0.4] [4.8 3. 1.4 0.3] [5.1 3.8 1.6 0.2] [4.6 3.2 1.4 0.2] [5.3 3.7 1.5 0.2] [5. 3.3 1.4 0.2] [7. 3.2 4.7 1.4] [6.4 3.2 4.5 1.5] [6.9 3.1 4.9 1.5] [5.5 2.3 4. 1.3] [6.5 2.8 4.6 1.5] [5.7 2.8 4.5 1.3] [6.3 3.3 4.7 1.6] [4.9 2.4 3.3 1. ] [6.6 2.9 4.6 1.3] [5.2 2.7 3.9 1.4] [5. 2. 3.5 1. ] [5.9 3. 4.2 1.5] [6. 2.2 4. 1. ] [6.1 2.9 4.7 1.4] [5.6 2.9 3.6 1.3] [6.7 3.1 4.4 1.4] [5.6 3. 4.5 1.5] [5.8 2.7 4.1 1. ] [6.2 2.2 4.5 1.5] [5.6 2.5 3.9 1.1] [5.9 3.2 4.8 1.8] [6.1 2.8 4. 1.3] [6.3 2.5 4.9 1.5] [6.1 2.8 4.7 1.2] [6.4 2.9 4.3 1.3] [6.6 3. 4.4 1.4] [6.8 2.8 4.8 1.4] [6.7 3. 5. 1.7] [6. 2.9 4.5 1.5] [5.7 2.6 3.5 1. ] [5.5 2.4 3.8 1.1] [5.5 2.4 3.7 1. ] [5.8 2.7 3.9 1.2] [6. 2.7 5.1 1.6] [5.4 3. 4.5 1.5] [6. 3.4 4.5 1.6] [6.7 3.1 4.7 1.5] [6.3 2.3 4.4 1.3] [5.6 3. 4.1 1.3] [5.5 2.5 4. 1.3] [5.5 2.6 4.4 1.2] [6.1 3. 4.6 1.4] [5.8 2.6 4. 1.2] [5. 2.3 3.3 1. ] [5.6 2.7 4.2 1.3] [5.7 3. 4.2 1.2] [5.7 2.9 4.2 1.3] [6.2 2.9 4.3 1.3] [5.1 2.5 3. 1.1] [5.7 2.8 4.1 1.3] [6.3 3.3 6. 2.5] [5.8 2.7 5.1 1.9] [7.1 3. 5.9 2.1] [6.3 2.9 5.6 1.8] [6.5 3. 5.8 2.2] [7.6 3. 6.6 2.1] [4.9 2.5 4.5 1.7] [7.3 2.9 6.3 1.8] [6.7 2.5 5.8 1.8] [7.2 3.6 6.1 2.5] [6.5 3.2 5.1 2. ] [6.4 2.7 5.3 1.9] [6.8 3. 5.5 2.1] [5.7 2.5 5. 2. ] [5.8 2.8 5.1 2.4] [6.4 3.2 5.3 2.3] [6.5 3. 5.5 1.8] [7.7 3.8 6.7 2.2] [7.7 2.6 6.9 2.3] [6. 2.2 5. 1.5] [6.9 3.2 5.7 2.3] [5.6 2.8 4.9 2. ] [7.7 2.8 6.7 2. ] [6.3 2.7 4.9 1.8] [6.7 3.3 5.7 2.1] [7.2 3.2 6. 1.8] [6.2 2.8 4.8 1.8] [6.1 3. 4.9 1.8] [6.4 2.8 5.6 2.1] [7.2 3. 5.8 1.6] [7.4 2.8 6.1 1.9] [7.9 3.8 6.4 2. ] [6.4 2.8 5.6 2.2] [6.3 2.8 5.1 1.5] [6.1 2.6 5.6 1.4] [7.7 3. 6.1 2.3] [6.3 3.4 5.6 2.4] [6.4 3.1 5.5 1.8] [6. 3. 4.8 1.8] [6.9 3.1 5.4 2.1] [6.7 3.1 5.6 2.4] [6.9 3.1 5.1 2.3] [5.8 2.7 5.1 1.9] [6.8 3.2 5.9 2.3] [6.7 3.3 5.7 2.5] [6.7 3. 5.2 2.3] [6.3 2.5 5. 1.9] [6.5 3. 5.2 2. ] [6.2 3.4 5.4 2.3] [5.9 3. 5.1 1.8]] target 의 type: &lt;class 'numpy.ndarray'&gt; target 의 shape: (150,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2] Moder Selection 모듈 소개학습/테스트 데이터 세트 분리 - train_test_split()먼저 테스트 데이터 세트를 이용하지 않고 학습 데이터 세트로만 학습하고 예측하면 무엇이 문제인지 살펴보자. 다음 예제는 학습과 예측을 동일한 데이터 세트로 수행한 결과이다. 12345678910111213from sklearn.datasets import load_irisfrom sklearn.tree import DecisionTreeClassifierfrom sklearn.metrics import accuracy_scoreiris=load_iris()dt_clf=DecisionTreeClassifier()train_data=iris.datatrain_label=iris.targetdt_clf.fit(train_data,train_label)# 학습 데이터로 예측 하기pred=dt_clf.predict(train_data)print('예측 정확도:',accuracy_score(train_label,pred)) 예측 정확도: 1.0 정확도가 100%가 나왔다. 이러한 결과가 나온 이유는 이미 학습한 데이터를 기반으로 예측을 하였기 때문이다. 따라서 예측을 수행하는 데이터 세트는 학습용 데이터 세트가 아닌 다른 테스트 데이터 세트여야 한다. 사이킷런에서는 train_test_split()를 사용해서 데이터 세트를 쉽게 분리할 수 있다. train_test_split(): 첫 번째 파라미터: 피처 데이터 세트, 두 번째 파라미터: 레이블 데이터 세트 그리고 선택적으로 다음 파라미터를 입력 받는다. test_size: 전체 데이터에서 테스트 데이터 세트 크기를 얼마로 샘플링할 것인가를 결정한다.(디폴트가 0.25, 25%이다) train_size: 전체 데이터에서 학습용 데이터 세트 크기를 얼마로 샘플링할 것인가를 결정한다. shuffle: 데이터를 분리하기 전에 데이터를 미리 섞을지를 결정한다. (디폴트는 True입니다) 데이터를 분산시켜서 좀 더 효율적인 학습 및 테스트 데이터 세트를 만들어 준다. random_state: 호출할 때마다 동일한 학습/테스트용 데이터 세트를 생성하기 위해 주어지는 난수 값이다. 이 파라미터를 지정하지 않으면 수행할 때마다 다른 학습/테스트 용으로 데이터를 생성하게 된다. 붓꽃 데이터를 학습 데이터를 70%, 데스트 데이터를 30%로 분리하는 예제를 해보겠다. 12345678910from sklearn.tree import DecisionTreeClassifierfrom sklearn.metrics import accuracy_scorefrom sklearn.datasets import load_irisfrom sklearn.model_selection import train_test_splitdf_clf=DecisionTreeClassifier()iris_data=load_iris()x_train, x_test, y_train, y_test = train_test_split(iris_data.data, iris_data.target, test_size=0.3, random_state=121) 학습 데이터를 기반으로 DecisionTreeClassifier를 학습하고 이 모델을 예측 정확도를 측정해 보자. 123df_clf.fit(x_train,y_train)pred=df_clf.predict(x_test)print('예측 정확도:{0:.4f}'.format(accuracy_score(y_test,pred))) 예측 정확도:0.9556 테스트 데이터로 예측을 수행한 결과 정확도가 약95.56%이다. 붓꽃 데이터는 150개의 데이터로 데이터 양이 크지 않아 전체의 30% 정도인 테스트 데이터는 45개 정도밖에 되지 않으므로 이를 통해 알고리즘의 예측성을 판단하기에는 그리 적절하지 않다. 학습을 위한 데이터의 양을 일정 수준 이상으로 보장하는 것도 중요하지만, 학습된 모델에 대해 다양한 데이터를 기반으로 예측 성능을 평가해보는 것도 매우 중요하다. 교차 검증알고리즘을 학습시키는 학습 데이터와 이에 대한 예측 성능을 평가하기 위한 별도의 테스트용 데이터가 필요하다. 하지만 이 방법 역시 과적합에 취약하다는 약점을 가질 수 있다. 과적합은 모델이 학습 데이터에만 과도하게 최적화 되어서 실제 예측을 다른 데이터로 시도했을 경우 예측 성능이 과도하게 떨어지는 현상을 말한다.이러한 문제를 개선하기 위해서 사용하는 교차검증에 대해서 배워보도록 하겠다. 교차 검증은 쉽게 말하자면 예를들어 시험을 보기 위해서 예비 모의고사를 여러 번 풀어보는 것이라고 생각하면 된다. 교차 검증은 데이터의 편중을 막기 위해서 별도의 여러개의 세트로 구성된 학습 데이터 세트와 검증 데이터 세트에서 학습과 평가를 수행하는 것이다. 대부분의 ML 모델에서 성능 평가는 교차 검증을 기반으로 1차 평가를 한 뒤에 최종적으로는 테스트 데이터세트에 적용해 평가하는 방식으로 진행된다. ML에서 사용되는 데이터 세트를 세분화 하면 학습, 검증, 테스트 3가지로 나눌 수 있다. 검증 데이터 세트는 최종 평가가 이루어지기 전에 학습된 모델을 다양하게 평가하는게 사용이 된다. K 폴드 교차 검증 K 폴드 교차 검증은 먼저 K개의 데이터 폴드 세트를 만들은 후 K번만큼 각 폴드 세트에 학습과 평가를 반복적으로 수행하는 방법이다. 5폴드 교차 검증을 수행한다. 5개의 폴드된 데이터 세트를 학습과 검증을 위한 데이터 세트로 변경하면서 5번 평가를 수행한 뒤, 5개의 평가를 평균한 결과를 가지고 예측성능 평가를 한다. 총 5개의 폴드 세트에 5번의 학습과 검증 평가가 반복적으로 수행된다. 학습 데이터와 검증 데이터를 점진적으로 변경하면서 마지막 5번째까지 학습과 검증을 수행하게 되는 것이다. 5개의 예측 평가를 구하게 되면 최종적으로 평균을 취해서 결과가 반영이 된다. 사이킷런에서는 K 폴드 교차 검증 프로세스를 위해서 KFold와 StrarifiedKFold 클래스를 제공한다. 먼저 KFold 클래스를 이용해서 붓꽃 데이터 세트를 교차 검증및 예측 정확도를 알아보도록 하겠다. 1234567891011121314from sklearn.tree import DecisionTreeClassifierfrom sklearn.metrics import accuracy_scorefrom sklearn.model_selection import KFoldimport numpy as npiris=load_iris()features=iris.datalabel=iris.targetdt_clf=DecisionTreeClassifier(random_state=156)# 5개의 세트르 분리하는 KFold 객체와 폴드 세트별 정확도를 담을 리스트 객체 생성하기Kfold=KFold(n_splits=5)cv_accuracy=[]print('붓꽃 데이터 크기:',features.shape[0]) 붓꽃 데이터 크기: 150 KFold로 KFord 객체를 생성했으니 이제 생성된 KFold 개체의 split()을 호출해 전체 붓꽃 데이터를 5개의 폴드 데이터 세트로 분리한다. 전체 데이터 세트는 150개이다. 따라서 학습용 데이터 세트는 120개, 검증 테스트 데이터 세트는 30개로 분할하게 된다. 다음으로는 5개의 폴드 세트를 생성하는 KFold 객체의 split()을 호출하여 교차 검증 수행 마다 학습과 검증을 반복해 예측 정확도를 측정하도록 해보겠다. 1234567891011121314151617181920212223n_iter=0# KFold객체의 split( ) 호출하면 폴드 별 학습용, 검증용 테스트의 로우 인덱스를 array로 반환 for train_index, test_index in Kfold.split(features): # kfold.split( )으로 반환된 인덱스를 이용하여 학습용, 검증용 테스트 데이터 추출 x_train, x_test = features[train_index], features[test_index] y_train, y_test = label[train_index], label[test_index] #학습 및 예측 dt_clf.fit(x_train, y_train) pred = dt_clf.predict(x_test) n_iter += 1 # 반복 시 마다 정확도 측정 accuracy = np.round(accuracy_score(y_test,pred),4) train_size=x_train.shape[0] test_size=x_test.shape[0] print('\\n#{0} 교차 검증 정확도 :{1}, 학습 데이터 크기: {2}, 검증 데이터 크기 {3}'.format(n_iter,accuracy, train_size, test_size)) print('#{0} 검증 세트 인덱스:{1}'.format(n_iter, test_index)) cv_accuracy.append(accuracy) # 개별 iteration별 정확도를 합하여 평균 정확도 계산 print('\\n## 평균 검증 정확도:',np.mean(cv_accuracy)) #1 교차 검증 정확도 :1.0, 학습 데이터 크기: 120, 검증 데이터 크기 30 #1 검증 세트 인덱스:[ 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29] ## 평균 검증 정확도: 1.0 #2 교차 검증 정확도 :0.9667, 학습 데이터 크기: 120, 검증 데이터 크기 30 #2 검증 세트 인덱스:[30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59] ## 평균 검증 정확도: 0.98335 #3 교차 검증 정확도 :0.8667, 학습 데이터 크기: 120, 검증 데이터 크기 30 #3 검증 세트 인덱스:[60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89] ## 평균 검증 정확도: 0.9444666666666667 #4 교차 검증 정확도 :0.9333, 학습 데이터 크기: 120, 검증 데이터 크기 30 #4 검증 세트 인덱스:[ 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119] ## 평균 검증 정확도: 0.941675 #5 교차 검증 정확도 :0.7333, 학습 데이터 크기: 120, 검증 데이터 크기 30 #5 검증 세트 인덱스:[120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149] ## 평균 검증 정확도: 0.9 5번 교차 검증 결과 평균 검증 정확도는 0.9이다. 그리고 교차 검증 시마다 검증 세트의 인덱스가 달라짐을 알 수 있다. Stratified K 폴드 Startified K폴드는 불균형한 분포도를 가진 레이블 데이터 집합을 위한 K폴드 방식이다. 불균형한 분포도를 가진 레이블 데이터 집합은 특정 레이블 값이 특이하게 많거나 매우 적어서 값의 분포가 한쪽으로 치우치는 것을 말한다. Startifide K폴드는 K폴드가 레이블 데이터 집합이 원본 데이터 집합의 레이블 분포를 학습 및 테스트 세트에 제대로 분배하지 못하는 경우의 문제를 해결해 준다. 123456import pandas as pdiris=load_iris()iris_df=pd.DataFrame(data=iris_data,columns=iris.feature_names)iris_df['label']=iris.targetiris_df['label'].value_counts() 2 50 1 50 0 50 Name: label, dtype: int64 iris 원본 데이터는 50:50:50의 비율로 구성되어 있는 것을 확인 하였다. 이슈가 발생하는 현상으 도출하기 위해 3개의 폴드 세트를 KFold로 생성하고, 각 교차 검증 시마다 생성되는 학습/검증 레이블 데이터 값의 분포도를 확인해 보자. 12345678910kfold = KFold(n_splits=3)# kfold.split(X)는 폴드 세트를 3번 반복할 때마다 달라지는 학습/테스트 용 데이터 로우 인덱스 번호 반환. n_iter = 0for train_index, test_index in kfold.split(iris_df): n_iter+=1 label_train = iris_df['label'].iloc[train_index] label_test = iris_df['label'].iloc[test_index] print('\\n##교차 검증: {0}'.format(n_iter)) print('학습 레이블 데이터 분포: \\n', label_train.value_counts()) print('검증 레이블 데이터 분포: \\n', label_test.value_counts()) ##교차 검증: 1 학습 레이블 데이터 분포: 2 50 1 50 Name: label, dtype: int64 검증 레이블 데이터 분포: 0 50 Name: label, dtype: int64 ##교차 검증: 2 학습 레이블 데이터 분포: 2 50 0 50 Name: label, dtype: int64 검증 레이블 데이터 분포: 1 50 Name: label, dtype: int64 ##교차 검증: 3 학습 레이블 데이터 분포: 1 50 0 50 Name: label, dtype: int64 검증 레이블 데이터 분포: 2 50 Name: label, dtype: int64 StatifiedKFold는 KFold로 분할 된 레이블 데이터 세트가 전체 레이블 값의 분포도를 반영하지 못하는 문제를 해결해 준다. StatifiedKFold는 레이블 데이터 분포도에 따라 학습/검증 데이터를 나누기 때문에 split() 메서드에 인자로 피처 데이터 세트뿐만 아니라 레이블 데이터 세트도 반드시 필요하다. 포드 세트는 3개로 설정 하자. 123456789101112from sklearn.model_selection import StratifiedKFoldskf = StratifiedKFold(n_splits=3)n_iter = 0for train_index, test_index in skf.split(iris_df, iris_df['label']): n_iter += 1 label_train = iris_df['label'].iloc[train_index] label_test = iris_df['label'].iloc[test_index] print('\\n## 교차검증: {0}'.format(n_iter)) print('학습 레이블 데이터 분포: \\n', label_train.value_counts()) print('검증 레이블 데이터 분포: \\n', label_test.value_counts()) ## 교차검증: 1 학습 레이블 데이터 분포: 2 34 1 33 0 33 Name: label, dtype: int64 검증 레이블 데이터 분포: 1 17 0 17 2 16 Name: label, dtype: int64 ## 교차검증: 2 학습 레이블 데이터 분포: 1 34 2 33 0 33 Name: label, dtype: int64 검증 레이블 데이터 분포: 2 17 0 17 1 16 Name: label, dtype: int64 ## 교차검증: 3 학습 레이블 데이터 분포: 0 34 2 33 1 33 Name: label, dtype: int64 검증 레이블 데이터 분포: 2 17 1 17 0 16 Name: label, dtype: int64 출력 결과를 보면 학습 레이블과 검증 레이블 데이터 값의 분포도가 동일하게 할당됐음을 알 수 있다. 첫 번째 교차 검증에서 학습 레이블은 0, 1, 2 값이 각각 33개로, 레이블별로 동일하게 할당됐고 검증 레이블 역시 0, 1, 2 값이 각각 17개로 레이블 별로 동일하게 할당된걸 알 수 있다. 다음은 StratifiedKFold를 이용해 데이터를 분리한 것이다. 123456789101112131415161718192021222324252627dt_clf = DecisionTreeClassifier(random_state=156) skfold = StratifiedKFold(n_splits=3) n_iter=0 cv_accuracy=[] # StratifiedKFold의 split( ) 호출시 반드시 레이블 데이터 셋도 추가 입력 필요 for train_index, test_index in skfold.split(features, label): # split( )으로 반환된 인덱스를 이용하여 학습용, 검증용 테스트 데이터 추출 X_train, X_test = features[train_index], features[test_index] y_train, y_test = label[train_index], label[test_index] #학습 및 예측 dt_clf.fit(X_train , y_train) pred = dt_clf.predict(X_test) # 반복 시 마다 정확도 측정 n_iter += 1 accuracy = np.round(accuracy_score(y_test,pred), 4) train_size = X_train.shape[0] test_size = X_test.shape[0] print('\\n#{0} 교차 검증 정확도 :{1}, 학습 데이터 크기: {2}, 검증 데이터 크기: {3}' .format(n_iter, accuracy, train_size, test_size)) print('#{0} 검증 세트 인덱스:{1}'.format(n_iter,test_index)) cv_accuracy.append(accuracy) # 교차 검증별 정확도 및 평균 정확도 계산 print('\\n## 교차 검증별 정확도:', np.round(cv_accuracy, 4)) print('## 평균 검증 정확도:', np.mean(cv_accuracy)) #1 교차 검증 정확도 :0.98, 학습 데이터 크기: 100, 검증 데이터 크기: 50 #1 검증 세트 인덱스:[ 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115] ## 교차 검증별 정확도: [0.98] ## 평균 검증 정확도: 0.98 #2 교차 검증 정확도 :0.94, 학습 데이터 크기: 100, 검증 데이터 크기: 50 #2 검증 세트 인덱스:[ 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132] ## 교차 검증별 정확도: [0.98 0.94] ## 평균 검증 정확도: 0.96 #3 교차 검증 정확도 :0.98, 학습 데이터 크기: 100, 검증 데이터 크기: 50 #3 검증 세트 인덱스:[ 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149] ## 교차 검증별 정확도: [0.98 0.94 0.98] ## 평균 검증 정확도: 0.9666666666666667 3개의 Stratified K 폴드로 교차 검증한 결과 평균 검증 정확도가 약 96.66%로 측정되었다. Stratified K 폴드의 경우 원본 데이터의 레이블 분포도 특성을 반영한 학습 및 검증 데이터 세트를 만들 수 있으므로 왜곡된 레이블 데이터 세트에서는 반드시 Stratified K 폴드를 이용해 교차 검증해야 한다. 사실, 일반적으로 분류(Classification)에서의 교차 검증은 K 폴드가 아니라 Stratified K 폴드로 분할돼야 한다. 회귀(Regression)에서는 Stratified K 폴드가 지원되지 않는다. 이유는 간단하다. 회귀의 결정값은 이산값 형태의 레이블이 아니라 연속된 숫자값이기 때문에 결정값별로 분포를 정하는 의미가 없기 때문이다. 교차 검증을 보다 간편하게 - cross_val_score( ) 사이킷런은 교차 검증을 좀 더 편리하게 수행할 수 있게 해주는 API를 제공한다. 대표적인 것이 cross_val_score( )이다. KFold로 데이터를 학습하고 예측하는 코드를 보면 먼저 ① 폴드 세트를 설정하고 ② for 루프에서 반복으로 학습 및 테스트 데이터의 인덱스를 추출한 뒤 ③ 반복적으로 학습과 예측을 수행하고 예측 성능을 반환한다. cross_val_score( )는 일련의 과정을 한꺼번에 수행해주는 API 이다. 다음은 cross_val_score( ) API의 선언 형태이다. cross_val_score(estimator, X, y=None, scoring=None, cv=None, n_jobs=1, verbose=0, fit_params=None, pre_dispatch='2*n_jobs') 이 중 estimator, X, y, scoring, cv가 주요 파라미터이다 esmitator : 사이킷런의 분류 알고리즘 클래스인 Classifier 또는 회귀 알고리즘 클래스인 Regressor를 의미 X : 피처 데이터 세트 y : 레이블 데이터 세트 scoring : 예측 성능 평가 지표를 기술 cv : 교차 검증 폴드 수 cross_val_score( ) 수행 후 반환 값은 scoring 파라미터로 지정된 성능 지표 측정값을 배열 형태로 반환한다. cross_val_score( )는 classifier가 입력되면 Stratified K 폴드 방식으로 레이블값의 분포에 따라 학습/테스트 세트를 분할한다.(회귀인 경우에는 Stratified K 폴드 방식으로 분할할 수 없으므로 K 폴드 방식으로 분할한다.) 다음 코드에서 cross_val_score()의 자세한 사용법을 살펴보자. 교차 검증 폴드 수는 3, 성능평가 지표는 정확도인 accuracy로 하자. 123456789101112131415from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import cross_val_score , cross_validate from sklearn.datasets import load_iris iris_data = load_iris() dt_clf = DecisionTreeClassifier(random_state=156) data = iris_data.data label = iris_data.target # 성능 지표는 정확도(accuracy) , 교차 검증 세트 는 3개 scores = cross_val_score(dt_clf , data , label , scoring='accuracy',cv=3) print('교차 검증별 정확도:',np.round(scores, 4)) print('평균 검증 정확도:', np.round(np.mean(scores), 4)) 교차 검증별 정확도: [0.98 0.94 0.98] 평균 검증 정확도: 0.9667 cross_val_score( )는 cv로 지정된 횟수만큼 scoring 파라미터로 지정된 평가 지표로 평가 결과값을 배열로 반환한다. 그리고 일반적으로 이를 평균하여 평가 수치로 사용한다. cross_val_score( ) API는 내부에서 Estimator를 학습(fit), 예측(predict), 평가(evaluation)시켜주므로 간단하게 교차 검증을 수행할 수 있다. cross_val_score( )와 앞 예제의 StratifiedKFold의 수행 결과를 비교해 보면 각 교차 검증별 정확도와 평균 검증 정확도가 모두 동일함을 알 수 있다. 이는 cross_val_score( )가 내부적으로 StratifiedKFold를 이용하기 때문이다. GridSearchCV - 교차 검증과 최적 하이퍼 파라미터 튜닝을 한 번에 사이킷런은 GridSearchCV API를 이용해 Classifier나 Regressor와 같은 알고리즘에 사용되는 하이퍼 파라미터를 순차적으로 입력하면서 편리하게 최적의 파라미터를 도출할 수 있는 방안을 제공한다. (Grid는 격자라는 뜻으로, 촘촘하게 파라미터를 입력하면서 테스트를 하는 방식이다.) 예를 들어 결정 트리 알고리즘의 여러 하이퍼 파라미터를 순차적으로 변경하면서 최고 성능을 가지는 파라미터 조합을 찾고자 한다면 다음과 같이 파라미터의 집합을 만들고 이를 순차적으로 적용하면서 최적화를 수행할 수 있다. parameters = {'max_depth':[1, 2, 3], 'min_samples_split':[2,3]} 총 6회에 걸쳐 파라미터를 순차적으로 바꿔 실행하면서 최적의 파라미터와 수행 결과를 도출할 수 있다. GridSearchCV는 사용자가 튜닝하고자 하는 여러 종류의 하이퍼 파라미터를 다양하게 테스트하면서 최적의 파라미터를 편리하게 찾게 해주지만 동시에 순차적으로 파라미터를 테스트하므로 수행시간이 상대적으로 오래 걸리는 것에 유념해야 한다. 위의 경우 순차적으로 6회에 걸쳐 하이퍼 파라미터를 변경하면서 교차 검증 데이터 세트에 수행 성능을 측정한다. CV가 3회라면 개별 파라미터 조합마다 3개의 폴딩 세트를 3회에 걸쳐 학습/평가해 평균값으로 성능을 측정한다. 6개의 파라미터 조합이라면 총 CV 3회 * 6개 파라미터 조합 = 18회의 학습/평가가 이뤄진다. GridSerchCV클래스의 생성자로 들어가는 주요 파라미터는 다음과 같다. estimator : classifier, regressor, pipeline이 사용될 수 있다. param_grid : key + 리스트 값을 가지는 딕셔너리가 주어진다. estimator의 튜닝을 위해 파라미터명과 사용될 여러 파라미터 값을 지정한다. scoring : 예측 성능을 측정할 평가 방법을 지정한다. 보통은 사이킷런의 성능 평가 지표를 지정하는 문자열(예 : 정확도의 경우 ‘accuracy’)로 지정하나 별도의 성능 평가 지표 함수도 지정할 수 있다. cv : 교차 검증을 위해 분할되는 학습 / 테스트 세트의 갯수를 지정한다. refit : 디폴트가 True이며 True로 생성 시 가장 최적의 하이퍼 파라미터를 찾은 뒤 입력된 esitmator 객체를 해당 하이퍼 파라미터로 재학습 시킨다. 다음은 결정 트리 알고리즘의 여러 가지 최적화 파라미터를 순차적으로 적용해 붓꽃 데이터를 예측 분석하는 데 GridSearchCV를 이용한다. 테스트할 하이퍼 파라미터 세트는 딕셔너리 형태로 하이퍼 파라미터의 명칭은 문자열 Key 값으로, 하이퍼 파라미터의 값은 리스트 형으로 설정한다. 1234567891011from sklearn.datasets import load_iris from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import GridSearchCV # 데이터를 로딩하고 학습데이타와 테스트 데이터 분리 iris = load_iris() X_train, X_test, y_train, y_test = train_test_split(iris_data.data, iris_data.target, test_size=0.2, random_state=121) dtree = DecisionTreeClassifier() ### parameter 들을 dictionary 형태로 설정 parameters = {'max_depth':[1, 2, 3], 'min_samples_split':[2,3]} 학습 데이터 세트를 GridSearchCV 객체의 fit(학습 데이터 세트) 메서드에 인자로 입력한다. GridSearchCV 객체의 fit(학습 데이터 세트) 메서드를 수행하면 학습 데이터를 cv에 기술된 폴딩 세트로 분할해 param_grid에 기술된 하이퍼 파라미터를 순차적으로 변경하면서 학습/평가를 수행하고 그 결과를 cv_result 속성에 기록한다. cv_result 는 gridsearchcv의 결과 세트로서 딕셔너리 형태로 key값과 리스트 형태의 value값을 가진다. cv_result를 Pandas의 DataFrame으로 변환하면 내용을 좀 더 쉽게 볼 수 있다. 12345678910111213import pandas as pd # param_grid의 하이퍼 파라미터들을 3개의 train, test set fold 로 나누어서 테스트 수행 설정. ### refit=True 가 default 임. True이면 가장 좋은 파라미터 설정으로 재 학습 시킴. grid_dtree = GridSearchCV(dtree, param_grid=parameters, cv=3, refit=True, return_train_score=True) # 붓꽃 Train 데이터로 param_grid의 하이퍼 파라미터들을 순차적으로 학습/평가 . grid_dtree.fit(X_train, y_train) # GridSearchCV 결과를 추출해 DataFrame으로 변경scores_df = pd.DataFrame(grid_dtree.cv_results_) scores_df[['params', 'mean_test_score', 'rank_test_score', 'split0_test_score', 'split1_test_score', 'split2_test_score']] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } params mean_test_score rank_test_score split0_test_score split1_test_score split2_test_score 0 {'max_depth': 1, 'min_samples_split': 2} 0.700000 5 0.700 0.7 0.70 1 {'max_depth': 1, 'min_samples_split': 3} 0.700000 5 0.700 0.7 0.70 2 {'max_depth': 2, 'min_samples_split': 2} 0.958333 3 0.925 1.0 0.95 3 {'max_depth': 2, 'min_samples_split': 3} 0.958333 3 0.925 1.0 0.95 4 {'max_depth': 3, 'min_samples_split': 2} 0.975000 1 0.975 1.0 0.95 5 {'max_depth': 3, 'min_samples_split': 3} 0.975000 1 0.975 1.0 0.95 위의 결과에서 총 6개의 결과를 볼 수 있으며, 이는 하이퍼 파라미터 max_depth와 min_samples_split을 순차적으로 총 6번 변경하면서 학습 및 평가를 수행했음을 나타낸다. 맨 마지막에서 두 번째 행을 보면 평가한 결과 예측 성능이 1위라는 의미이다. split0_test_score, split1_test_score, split2_test_score는 CV가 3인 경우, 즉 3개의 폴딩 세트에서 각각 테스트한 성능 수치이다. mean_test_score는 이 세 개 성능 수치를 평균화한 것이다. 주요 칼럼별 의미는 다음과 같이 정리할 수 있다. params 칼럼에는 수행할 때마다 적용된 개별 하이퍼 파라미터 값을 나타낸다. rank_test_score는 하이퍼 파라미터별로 성능이 좋은 score 순위를 나타낸다. mean_test_score는 개별 하이퍼 파라미터별로 CV의 폴딩 테스트 세트에 대해 총 수행한 평가 평균값이다. GridSearchCV 객체의 fit( )을 수행하면 최고 성능을 나타낸 하이퍼 파라미터 값과 그때의 평가 결과 값이 각각 best_params, best_score_속성에 기록된다. 12print('GridSearchCV 최적 파라미터:', grid_dtree.best_params_) print('GridSearchCV 최고 정확도: {0:.4f}'.format(grid_dtree.best_score_)) GridSearchCV 최적 파라미터: {'max_depth': 3, 'min_samples_split': 2} GridSearchCV 최고 정확도: 0.9750 GridSearchCV 객체의 생성 파라미터로 refit = True가 디폴트이다. refit = True이면 GridSearchCV가 최적 성능을 나타내는 하이퍼 파라미터로 Estimator를 학습해 best_estimator_로 저장한다. 123456# GridSearchCV의 refit으로 이미 학습이 된 estimator 반환 estimator = grid_dtree.best_estimator_ # GridSearchCV의 best_estimator_는 이미 최적 하이퍼 파라미터로 학습이 됨 pred = estimator.predict(X_test) print('테스트 데이터 세트 정확도: {0:.4f}'.format(accuracy_score(y_test,pred))) 테스트 데이터 세트 정확도: 0.9667 일반적으로 학습 데이터를 GridSearchCV를 이용해 최적 하이퍼 파라미터 튜닝을 수행한 뒤에 별도의 테스트 세트에서 이를 평가하는 것이 일반적인 머신러닝 모델 적용 방법이다. 데이터 전처리데이터 전처리(Data Preprocessing)은 ML 알고리즘만큼 중요하다. ML 알고리즘은 데이터에 기반하고 있기 때문에 어떤 데이터를 입력으로 가지느냐에 따라 결과도 크게 달라질 수 있다.(Garbage In, Garbage Out). 사이킷런의 ML 알고리즘을 적용하기 전에 데이터에 대해 미리 처리해야 할 기본 사항이 있다. 결손값, 즉 NaN, Null 값은 허용되지 않는다. 따라서 이러한 Null 값은 고정된 다른 값으로 변환해야 한다. 피처 값 중 Null 값이 얼마 되지 않는다면 피처의 평균값 등으로 간단히 대체할 수 있다. 하지만 Null 값이 대부분이라면 오히려 해당 피처는 드롭하는 것이 더 좋다. 사이킷런의 머신러닝 알고리즘은 문자열 값을 입력 값으로 허용하지 않는다. 그래서 모든 문자열 값은 인코딩돼서 숫자형으로 변환해야 한다. 문자열 피처는 일반적으로 카테고리형 피처와 텍스트형 피처를 의미한다. 데이터 인코딩 머신러닝을 위한 대표적인 인코딩 방식은 레이블 인코딩(Label encoding)과 원-핫 인코딩(One Hot encoding)이 있다. 레이블 인코딩은 카테고리 피처를 코드형 숫자 값으로 변환한다. 주의해야 할 점은 ‘01’, ‘02’와 같은 코드 값 역시 문자열이므로 1, 2와 같은 숫자형 값으로 변환돼야 한다. 레이블 인코딩 사이킷런의 레이블 인코딩(Label encoding)은 LabelEncoder 클래스를 구현한다. LabelEncoder를 객체로 생성한 후 fit( )과 transform( )을 호출해 레이블 인코딩을 수행한다. 123456789from sklearn.preprocessing import LabelEncoder items=['TV','냉장고','전자렌지','컴퓨터','선풍기','선풍기','믹서','믹서'] # LabelEncoder를 객체로 생성한 후 , fit( ) 과 transform( ) 으로 label 인코딩 수행. encoder = LabelEncoder() encoder.fit(items) labels = encoder.transform(items) print('인코딩 변환값:',labels) 인코딩 변환값: [0 1 4 5 3 3 2 2] 문자열 값이 어떤 숫자 값으로 인코딩됐는지 직관적으로 알고 싶을 경우에는 LabelEncoder 객체의 classes_ 속성값으로 확인한다. 1print('인코딩 클래스:',encoder.classes_) 인코딩 클래스: ['TV' '냉장고' '믹서' '선풍기' '전자렌지' '컴퓨터'] classes_ 속성은 0번부터 순서대로 변환된 인코딩 값에 대한 원본값을 가지고 있다. inverse_transform( )을 통해 인코딩된 값을 다시 디코딩할 수 있다. 1print('디코딩 원본 값:',encoder.inverse_transform([4, 5, 2, 0, 1, 1, 3, 3])) 디코딩 원본 값: ['전자렌지' '컴퓨터' '믹서' 'TV' '냉장고' '냉장고' '선풍기' '선풍기'] 레이블 인코딩은 간단하게 문자열 값을 숫자형 카테고리 값으로 변환한다. 하지만 레이블 인코딩이 일괄적인 숫자 값으로 변환이 되면서 몇몇 ML 알고리즘에는 이를 적용할 경우 예측 성능이 떨어지는 경우가 발생할 수 있다. 이는 숫자 값의 경우 크고 작음에 대한 특성이 작용하기 때문이다. 즉, 냉장고가 1, 믹서가 2로 변환되면, 1보다 2가 더 큰 값이므로 특정 ML 알고리즘에서 가중치가 더 부여되거나 더 중요하게 인식할 가능성이 발생합니다. 하지만 냉장고와 믹서의 숫자 변환 값은 단순 코드이지 숫자 값에 따른 순서나 중요도로 인식돼서는 않된다. 이러한 특성 때문에 레이블 인코딩은 선형 회귀와 같은 ML 알고리즘에는 적용하지 않아야 한다. 트리 계열의 ML 알고리즘은 숫자의 이러한 특성을 반영하지 않으므로 레이블 인코딩도 별 문제가 없다. # 원-핫 인코딩(One-Hot Encoding)은 레이블 인코딩의 이러한 문제점을 해결하기 위한 인코딩 방식이다. 원-핫 인코딩(one-hot incoding) 원-핫 인코딩은 피처 값의 유형에 따라 새로운 피처를 추가해 고유 값에 대항하는 칼럼에만 1을 표시하고 나머지 칼럼에는 0을 표시하는 방식이다. 즉, 행 형태로 돼 있는 피처의 고유 값을 열 형태로 차원을 변환한 뒤, 고유 값에 해당하는 칼럼에만 1을 표시하고 나머지 칼럼에는 0을 표시한다. 원-핫 인코딩은 사이킷런에서 OneHotEncoder 클래스로 쉽게 변환이 가능하다. 단, LabelEncoder와 다르게 약간 주의할 점이 있다. 첫 번쨰는 OneHotEncoder로 변환하기 전에 모든 문자열 값이 숫자형 값으로 변환돼야 한다는 것이며, 두 번째는 입력 값으로 2차원 데이터가 필요하다는 점이다. 1234567891011121314151617181920from sklearn.preprocessing import OneHotEncoder import numpy as np items=['TV','냉장고','전자렌지','컴퓨터','선풍기','선풍기','믹서','믹서'] # 먼저 숫자값으로 변환을 위해 LabelEncoder로 변환합니다. encoder = LabelEncoder() encoder.fit(items) labels = encoder.transform(items) # 2차원 데이터로 변환합니다. labels = labels.reshape(-1,1) # 원-핫 인코딩을 적용합니다. oh_encoder = OneHotEncoder() oh_encoder.fit(labels) oh_labels = oh_encoder.transform(labels) print('원-핫 인코딩 데이터') print(oh_labels.toarray()) print('원-핫 인코딩 데이터 차원') print(oh_labels.shape) 원-핫 인코딩 데이터 [[1. 0. 0. 0. 0. 0.] [0. 1. 0. 0. 0. 0.] [0. 0. 0. 0. 1. 0.] [0. 0. 0. 0. 0. 1.] [0. 0. 0. 1. 0. 0.] [0. 0. 0. 1. 0. 0.] [0. 0. 1. 0. 0. 0.] [0. 0. 1. 0. 0. 0.]] 원-핫 인코딩 데이터 차원 (8, 6) 8개의 레코드와 1개의 칼럼을 가진 원본 데이터가 8개의 레코드와 6개의 칼럼을 가진 데이터로 변환 됐다. TV가 0, 냉장고 1, 믹서 2, 선풍기 3, 전자레인지 4, 컴퓨터 5로 인코딩됐으므로 첫 번째 칼럼이 TV, …, 마지막 여섯 번째 칼럼이 컴퓨터를 나타낸다. 따라서 원본 데이터의 첫 번째 레코드가 TV이므로 변환된 데이터의 첫 번째 레코드의 첫 번째 칼럼이 1이고, 나머지 칼럼은 모두 0이 된다. 이어 두 번째 레코드가 냉장고 이므로 변환된 데이터의 두 번쩨 레코드의 냉장고에 해당하는 두 번째 칼럼이 1이고, 나머지 칼럼은 모두 0이 된다. 원핫인코딩: https://wikidocs.net/22647 판다스에는 원-핫 인코딩을 더 쉽게 지원하는 API가 있다. get_dummies( )를 이용하면 된다. 사이킷런의 OneHotEncoder와 다르게 문자열 카테고리 값을 숫자 형으로 변환할 필요 없이 바로 변환할 수 있다. 12345import pandas as pd df = pd.DataFrame({'item':['TV','냉장고','전자렌지','컴퓨터','선풍기','선풍기','믹서','믹서'] }) pd.get_dummies(df) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } item_TV item_냉장고 item_믹서 item_선풍기 item_전자렌지 item_컴퓨터 0 1 0 0 0 0 0 1 0 1 0 0 0 0 2 0 0 0 0 1 0 3 0 0 0 0 0 1 4 0 0 0 1 0 0 5 0 0 0 1 0 0 6 0 0 1 0 0 0 7 0 0 1 0 0 0 get_dummies()를 이용하면 숫자형 값으로 변환 없이도 바로 변환이 가능하다. 피처 스케일링과 정규화 서로 다른 변수의 값 범위를 일정한 수준으로 맞추는 작업을 피처 스케일링이라 한다. 대표적으로 표준화와 정규화가 있다. 표준화는 데이터의 피처 각각이 평균이 0이고 분산이1인 가우시안 정규 분포를 가진 값으로 변환하는 것을 의미한다. 일반적으로 정규화는 서로 다른 피처의 크기를 통일하기 위해 크기를 변환해주는 개념이다. 예를 들어 피처 A는 거리를 나타내는 변수로서 값이 0 ~ 100KM로 주어지고 피처 B는 금액을 나타내는 속성으로 0 ~ 1,000,000,000원으로 주어진다면 이 변수를 모두 동잏한 크기 단위로 비교하기 위해 값을 모두 최소 0 ~ 최대 1의 값으로 변환하는 것이다. 즉, 개별 데이터의 크기를 모두 똑같은 단위로 변경하는 것이다. 그런데 사이킷런에서 제공하는 Normalizer 모듈은 일반 정규화는 약간의 차이가 있다. 사이킷런에서의 Normalizer 모듈은 선형대수에서의 정규화 개념이 적용됐으며, 개별 벡터의 크기를 맞추기 위해 변환하는 것을 의미한다. 즉, 개별 벡터를 모든 피처 벡터의 크기로 나눠준다. 3개의 피처 x, y, z가 있으면 새로운 데이터는 다음과 같다. StandardScaler StandardScaler는 표준화를 지원하기 위한 클래스이다. 즉, 개별 피처를 평균이 0이고, 분산이 1인 값으로 변환해준다. 이렇게 가우시안 정규 분포를 가질 수 있도록 데이터를 변환한다. 특히 사이킷런에서 구현한 RBF 커널을 이용하는 SVM(Support Vector Machine), 선형 회귀(Linear Regression), 로지스틱 회귀(Logistic Regression)은 데이터가 가우시안 분포를 가진다는 가정하에 구현됐기에 사전에 표준화를 적용하는 건 성능 향상에 중요한 요소가 될 수 있다. # 가우시안분포 = 정규분포 1234567891011from sklearn.datasets import load_irisimport pandas as pd# 붓꽃 데이터 세트를 로딩하고 DataFrame으로 변환합니다.iris = load_iris()iris_data = iris.datairis_df = pd.DataFrame(data = iris_data, columns = iris.feature_names)print('feature 들의 평균 값')print(iris_df.mean())print(&quot;/nfeature 들의 분산 값&quot;)print(iris_df.var()) feature 들의 평균 값 sepal length (cm) 5.843333 sepal width (cm) 3.057333 petal length (cm) 3.758000 petal width (cm) 1.199333 dtype: float64 /nfeature 들의 분산 값 sepal length (cm) 0.685694 sepal width (cm) 0.189979 petal length (cm) 3.116278 petal width (cm) 0.581006 dtype: float64 다음으로 StandardScaler을 사용해서 표준화를 진행해보자. 1234567891011121314from sklearn.preprocessing import StandardScaler# StandardScaler객체 생성scaler=StandardScaler()# StandardScaler로 데이터 세트 변화, fit()과 transform()호출scaler.fit(iris_df)iris_scaled=scaler.transform(iris_df)# transform() 시 스케일 변화된 데이터 세트가 NumPy ndarray로 변환돼 이를 DataFrame으로 변환iris_df_scaled=pd.DataFrame(data=iris_scaled, columns=iris.feature_names)print('feature 평균')print(iris_df_scaled.mean())print('\\nfeature 분산')print(iris_df_scaled.var()) feature 평균 sepal length (cm) -1.690315e-15 sepal width (cm) -1.842970e-15 petal length (cm) -1.698641e-15 petal width (cm) -1.409243e-15 dtype: float64 feature 분산 sepal length (cm) 1.006711 sepal width (cm) 1.006711 petal length (cm) 1.006711 petal width (cm) 1.006711 dtype: float64 모든 칼럼 값의 평균이 0에 아주 가까운 값으로 그리고 분산은 1에 아주 가까운 값으로 변환됐음을 확인할수 있다. MinMaxScaler MinMaxScaler는 데이터값을 0과 1사이의 범위 값으로 변환한다.(음수 값이 있으면 -1에서 1값으로 변환한다.) 데이터의 분포가 가우시안 분포가 아닐 경우에 Min, Max Scale을 적용해 볼 수 있다. 1234567891011121314from sklearn.preprocessing import MinMaxScaler# MinMaxScaler객체 생성scaler=MinMaxScaler()# MinMaxScaler로 데이터 세트 변환, fit()과 transform()호출scaler.fit(iris_df)iris_scaled=scaler.transform(iris_df)# transform() 시 스케일 변환된 데이터 세트가 NumPy ndarry로 변환돼 이를 DataFrame으로 변환iris_df_scaled = pd.DataFrame(data=iris_scaled, columns=iris.feature_names)print('feature 최솟값')print(iris_df_scaled.min())print('/\\fearure 최댓값')print(iris_df_scaled.max()) feature 최솟값 sepal length (cm) 0.0 sepal width (cm) 0.0 petal length (cm) 0.0 petal width (cm) 0.0 dtype: float64 / earure 최댓값 sepal length (cm) 1.0 sepal width (cm) 1.0 petal length (cm) 1.0 petal width (cm) 1.0 dtype: float64 모든 피처에 0에서 1 사이의 값으로 변환되는 스케일링이 적용됐음을 알 수 있다. 학습 데이터와 테스트 데이터의 스케일링 변환시 유의점 StandadScaler나 MinMaxScaler 같은 Scaler 객체를 이용해 데이터의 스케일링 변환시 fit(), transform(), fit_transform() 메소드를 이용한다. 그리고 한번에 적용하는 기능을 수행해야 한다. fit(): 데이터 변환을 위한 기준 정보설정을 적용 trancsform(): 설정된 정보를 이용해 데이터를 변환 fit_transform(): fit(), transform()을 한번에 적용 데이터 전처리 시 fit( ), transform( ) 함수를 사용하는데 이 때 학습용 데이터에는 fit과 transform을 사용하되 테스트 데이터에는 fit을 사용하지 않도록 해야한다. 여기서 fit( )은 데이터 변환을 위한 기준 정보를 설정하며 transform( )은 설정된 정보를 이용해 데이터를 반환한다. 그런데 테스트 데이터에서 다시 fit( )을 사용해버리면 학습 데이터와는 또 다른 새로운 스케일링 기준 정보를 만들게 되어 올바른 예측 결과를 도출하지 못할 수도 있다. 학습 데이터와 테스트 데이터의 fit(), transform(), fit_transform()을 이용해 스케일링 변환 시 유의 할점은 아래와 같다. 가능하다면 전체 데이터의 스케일링 변환을 적용한 뒤 학습과 테스트 데이터로 분리 1이 여의치 않다면 테스트 데이터 변환시에는 fit()이나 fit_transform() 을 사용하지 않고 학습데이터로 이미 fit( )된 Scaler 객체를 통해 transfrom()으로 변한 사이킷런으로 수행하는 타이타닉 생존자 예측캐글에서 제공하는 타이타닉 탑승자 데이터를 기반으로 생존자 예측을 사이킷런으로 수행행 보겠다. 타이타닉 탑승자 데이터에 대해 개략적으로 살펴보자. Passengerid : 탑승자 번호 survived : 생존 여부 0 : 사망 / 1 : 생존 pclass : 티켓의 선실 등급 sex : 성별 name :이름 Age : 나이 sibsp : 같이 탑승한 형제자매 또는 배우자 인원수 parch : 같이 탑승한 부모님 또는 어린이 인원수 ticket : 티켓 번호 fare : 요금 cabin : 선실 번호 embarked : 중간 정착 항구 C = Cherbourg, Q = Queenstown, S = Southampton 탑승자 파일을 판다스의 read_csv()를 이용해 DataFrame으로 로딩하자. 예제를 실행 할때는 맷플롯립과 시본을 이용해 시각화 하자 123456# Mount Google Drivefrom google.colab import drive # import drive from google colabROOT = &quot;/content/drive&quot; # default location for the driveprint(ROOT) # print content of ROOT (Optional)drive.mount(ROOT) # we mount the google drive at /content/drive /content/drive Mounted at /content/drive 12345from os.path import joinMY_GOOGLE_DRIVE_PATH = 'My Drive/Colab Notebooks/pymldg-rev/2장' # 주소만 수정하면됨PROJECT_PATH = join(ROOT, MY_GOOGLE_DRIVE_PATH)print(PROJECT_PATH) /content/drive/My Drive/Colab Notebooks/pymldg-rev/2장 1%cd &quot;{PROJECT_PATH}&quot; /content/drive/My Drive/Colab Notebooks/pymldg-rev/2장 12345678910# 모듈 불러오기import numpy as npimport pandas as pdimport matplotlib.pyplot as pltimport seaborn as sns%matplotlib inline# Excel 데이터 데이터프레임으로 변환titanic_df = pd.read_csv(&quot;titanic_train.csv&quot;)titanic_df.head(3) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked 0 1 0 3 Braund, Mr. Owen Harris male 22.0 1 0 A/5 21171 7.2500 NaN S 1 2 1 1 Cumings, Mrs. John Bradley (Florence Briggs Th... female 38.0 1 0 PC 17599 71.2833 C85 C 2 3 1 3 Heikkinen, Miss. Laina female 26.0 0 0 STON/O2. 3101282 7.9250 NaN S 로딩된 데이터 칼럼 타입을 확인해 보겠습니다. DataFrame의 info() 메서드를 통해 쉽게 확인이 가능 합다 12print('/n ### 학습 데이터 정보 ### /n')print(titanic_df.info()) # 데이터를 불러오고 가장 먼저 해야하는 작업 /n ### 학습 데이터 정보 ### /n &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 891 entries, 0 to 890 Data columns (total 12 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 PassengerId 891 non-null int64 1 Survived 891 non-null int64 2 Pclass 891 non-null int64 3 Name 891 non-null object 4 Sex 891 non-null object 5 Age 714 non-null float64 6 SibSp 891 non-null int64 7 Parch 891 non-null int64 8 Ticket 891 non-null object 9 Fare 891 non-null float64 10 Cabin 204 non-null object 11 Embarked 889 non-null object dtypes: float64(2), int64(5), object(5) memory usage: 83.7+ KB None 판다스는 넘파이 기반으로 만들어졌고 넘파이의 String타입이 길이 제한이 있어 이에 대한 구분을 위해 object 타입으로 명기한다. age, cabin, embarked 칼럼은 각각 714개, 204개, 889개의 not Null 값을 가지고 있고, 각각 177개, 608개, 2개의 Null을 가지고 있다. 사이킷런 머신러닝 알고리즘은 Null 값을 허용하지 않으므로 Null 값을 어떻게 처리할지 결정해야 한다. DataFrame의 fillna() 함수를 사용해 간단하게 Null 값을 평균 또는 고정 값으로 변경한다. 1234titanic_df['Age'].fillna(titanic_df['Age'].mean(),inplace=True)titanic_df['Cabin'].fillna('N',inplace = True)titanic_df['Embarked'].fillna('N',inplace = True)print('데이터 셋 Null 값 개수',titanic_df.isnull().sum().sum()) 데이터 셋 Null 값 개수 0 현재 남아있는 문자열 피처는 Sex, Cabin, Embarked 이다. 먼저 피처들의 값 분류를 살펴보자. 123print(' Sex 값 분포 :\\n',titanic_df['Sex'].value_counts()) # value_counts: 각 값의 개수 세기print('\\n Cabin 값 분포 :\\n',titanic_df['Cabin'].value_counts())print('\\n Embarked 값 분포 :\\n',titanic_df['Embarked'].value_counts()) Sex 값 분포 : male 577 female 314 Name: Sex, dtype: int64 Cabin 값 분포 : N 687 C23 C25 C27 4 B96 B98 4 G6 4 F2 3 ... C32 1 C148 1 D6 1 E46 1 C103 1 Name: Cabin, Length: 148, dtype: int64 Embarked 값 분포 : S 644 C 168 Q 77 N 2 Name: Embarked, dtype: int64 Sex, Embarked 값은 문제 없어 보인다. cabin의 경우 N이 가장 많은 것도 특이하지만 , 속성값이 제대로 정리되지 않은 것 같다. cabin의 경우 선실 번호 중 선실 등급을 나타내는 알파벳이 중요해 보인다. 일등식의 경우 삼등실에 투숙한 사람보다 살아날 확률이 높다. Cabin 속성을의 앞 문자만 추출하자. 12titanic_df['Cabin']=titanic_df['Cabin'].str[:1]titanic_df['Cabin'].head(3) 0 N 1 C 2 N Name: Cabin, dtype: object 머신러닝 알고리즘을 적용해 예측을 수행하지 전에 데이터를 먼저 탐색해 보겠다.어떤 유형의 승객이 생존확률이 높았는지 보자 성별이 생존 확률에 어떤 영향을 미쳤는지, 성별에 따른 생존자 수를 비교해 보자. 부자와 가난한 사람의 생존확률 , 일등실, 이등실, 삼등실에 따라 생종확률을 살펴보자. 나이에 따른 생존확률 1titanic_df.groupby(['Sex','Survived'])['Survived'].count() # male: 남성, female: 여성 Sex Survived female 0 81 1 233 male 0 468 1 109 Name: Survived, dtype: int64 Survied 칼럼은 레이블로서 결정 클래스 값이다. Survied 0은 사망, 1은 생존을 의미한다. 남자가 여자에 비해 사망자가 더 많았다. 시각화는 시본 패키지를 이용해 할 것이다. 시본은 기본적으로 맷플롯립에 기반하지만, 좀더 새련된 비주얼과 쉬운 API, 판다스의 연동 등으로 데이터 분석을 위한 시각화로 애용 되는 패키지다. Seaborn 패키지를 이용해 시각화를 진행 barplot() 을이용 X축에 ‘Sex’칼럼 Y축에 ‘Survived’ 칼럼 1sns.barplot(x='Sex',y='Survived',data=titanic_df) # 여성의 생존율이 높은걸 확인 &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fe6a0d67be0&gt; 부자와 가난한 사람 간의 생존 확률은 어떻게 다를지 확인하자. 객실 등급별 생존 확률을 살펴보자 앞에 barplot() 함수에 x 좌표에 ‘Pclass’를 , 그리고 hue파라미터를 추가해 hue=’Sex’와 같이 입력하면 된다. 1sns.barplot(x='Pclass',y='Survived',hue = 'Sex',data=titanic_df) # 높은 객실의 승객 생존율이 높은걸 확인 &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fe6a0d18278&gt; 여성의 경우 일, 이등실에 따른 생존 확률의 차이가 크지 않으나 삼등실의 경우 차이가 크다. 남성의 경우 일등실의 생존 확률이 월등히 높다는 것을 볼수 있다. 이번에는 나이에 따른 생존 확률을 알아보자 0~5: baby 6~12: child 13~18:teenager 19~25: student 26~35: yuong adult 36~60: adult 61이상: elderly -1이하 오류값: unknown 12345678910111213141516171819202122232425# 입력 age에 따라 구분값을 반환하는 함수 설정. DataFrame의 apply lambda식에 사용. def get_category(age): cat = '' if age &lt;= -1: cat = 'Unknown' elif age &lt;= 5: cat = 'Baby' elif age &lt;= 12: cat = 'Child' elif age &lt;= 18: cat = 'Teenager' elif age &lt;= 25: cat = 'Student' elif age &lt;= 35: cat = 'Young Adult' elif age &lt;= 60: cat = 'Adult' else : cat = 'Elderly' return cat#막대 그래프의 크기 figure를 설정plt.figure(figsize=(10,6))#x축의 값을 순차적으로 표시하기 위한 설정group_names = ['Unknown', 'Baby', 'Child', 'Teenager', 'Student', 'Young Adult', 'Adult', 'Elderly']#lambda 식에 위에서 생성한 get_category() 함수를 반환값으로 지정.#get_category(X)는 입력값으로 'Age' 칼럼 값을 받아서 해당하는 cat 반환titanic_df['Age_cat'] =titanic_df['Age'].apply(lambda x : get_category(x))sns.barplot(x='Age_cat', y='Survived',hue ='Sex', data=titanic_df,order=group_names)titanic_df.drop('Age_cat',axis=1,inplace=True) # axis: 열의 값을 설정 아기의 경우 생존이 비교적 높고, 아이의 경우 생존률이 낮은 것을 확인했다. 분석결과를 기초로 Sex, Age, PClass 등이 생존을 좌우하는 것을 확인할 수 있었다. 이제 남아있는 문자열 카테고리 피처를 숫자형 카테고리 피처로 변환한다. 인코딩은 사이킷런의 LabelEncoder 클래스를 이용해 레이블 인코딩을 적용한다. LabelEncoder 객체는 카테고리 값의 유형 수에 따라 0 ~ (카테고리 유형 수-1)까지의 숫자 값으로 변환한다. 123456789101112from sklearn import preprocessingdef encode_features(dataDF): features = ['Cabin','Sex','Embarked'] for feature in features: le = preprocessing.LabelEncoder() le = le.fit(dataDF[feature]) dataDF[feature] = le.transform(dataDF[feature]) return dataDFtitanic_df = encode_features(titanic_df)titanic_df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked 0 1 0 3 Braund, Mr. Owen Harris 1 22.0 1 0 A/5 21171 7.2500 7 3 1 2 1 1 Cumings, Mrs. John Bradley (Florence Briggs Th... 0 38.0 1 0 PC 17599 71.2833 2 0 2 3 1 3 Heikkinen, Miss. Laina 0 26.0 0 0 STON/O2. 3101282 7.9250 7 3 3 4 1 1 Futrelle, Mrs. Jacques Heath (Lily May Peel) 0 35.0 1 0 113803 53.1000 2 3 4 5 0 3 Allen, Mr. William Henry 1 35.0 0 0 373450 8.0500 7 3 Sex, Cabin, Embarked 속성이 숫자형으로 바뀐 것을 알 수 있다. 지금까지 피처를 가공한 내역을 정리하고 이를 함수로 만들어 쉽게 재사용할 수 있도록 만들겠다. 데이터의 전처리를 전체적으로 호출하는 함수는 transform_features() 이며 Null 처리, 포매팅, 인코딩을 수행하는 내부 함수로 구성했다. 1234567891011121314151617181920212223242526272829# Null 처리 함수def fillna(df): df['Age'].fillna(df['Age'].mean(),inplace = True) df['Cabin'].fillna('N',inplace=True) df['Embarked'].fillna('N', inplace = True) df['Fare'].fillna(0,inplace = True) return df#머신러닝 알고리즘에 불필요한 속성 제거def drop_features(df): df.drop(['PassengerId', 'Name', 'Ticket'],axis=1,inplace=True) return df#레이블 인코딩 수행def format_features(df): df['Cabin'] = df['Cabin'].str[:1] features = ['Cabin','Sex','Embarked'] for feature in features: le = LabelEncoder() le = le.fit(df[feature]) df[feature] = le.transform(df[feature]) return df# 앞에서 설정한 데이터 전처리 함수 호출def transform_features(df): df = fillna(df) df = drop_features(df) df= format_features(df) return df 원본데이터를 가공하기 위해 원본 csv 파일을 다시 로딩 Survived 속성만 별도 분리해 클래스 결정값 데이터 세트로 만들기 Survived 속성을 드롭해 피처 데이터 세트 만들기 생성된 데이터 세트에 transform_features()를 적용해 데이터 가공 123456#원본 데이터 재로딩하고, 피처 데이터 세트와 레이블 데이터 세트 추출.titanic_df = pd.read_csv(&quot;titanic_train.csv&quot;)y_titanic_df=titanic_df['Survived']X_titanic_df=titanic_df.drop('Survived',axis=1)X_titanic_df = transform_features(X_titanic_df) train_test_split() API를 이용해 별도의 테스트 데이터 세트 추출, 세트 크기는 전체의 20% 설정 123from sklearn.model_selection import train_test_splitX_train, X_test, y_train, y_test=train_test_split(X_titanic_df, y_titanic_df, \\ test_size=0.2, random_state=11) ML 알고리즘 결정 트리, 랜덤 포레스트, 로지스틱 회귀를 이용해 타이타닉 생존자를 예측해 보자 결정 트리를 위해 DecisionTreeClassifie 클래스 제공 랜덤 포레스트를 위해 RandomForestClassifier 클래스 제공 로지스틱 회귀를 위해 LogisticRegression 클래스 제공 위의 사이킷런 클래스를 이용해 train_test_split()으로 분리한 데이터와 테스트 데이터를 기반으로 머신러닝 모델을 학습(fit)하고 예측(predict) 한다. 예측 성능 평가는 정확도로 하기위해 accuracy_score() API를 사용한다. 123456789101112131415161718192021222324from sklearn.tree import DecisionTreeClassifierfrom sklearn.ensemble import RandomForestClassifierfrom sklearn.linear_model import LogisticRegressionfrom sklearn.metrics import accuracy_score#결정트리, Random Forest, 로지스틱 회귀를 위한 사이킷런 Classifier 클래스 생성dt_clf = DecisionTreeClassifier(random_state=11)rf_clf = RandomForestClassifier(random_state=11)lr_clf = LogisticRegression()#DecisionTreeClassifier 학습/예측/평가dt_clf.fit(X_train, y_train)dt_pred = dt_clf.predict(X_test)print('DecisionTreeClassifier 정확도: {0:.4f}'.format(accuracy_score(y_test, dt_pred)))#RandomForestClassifier 학습/예측/평가rf_clf.fit(X_train, y_train)rf_pred = rf_clf.predict(X_test)print('RandomForestClassifier 정확도:{0:.4f}'.format(accuracy_score(y_test, rf_pred)))# LogisticRegression 학습/예측/평가lr_clf.fit(X_train , y_train)lr_pred = lr_clf.predict(X_test)print('LogisticRegression 정확도: {0:.4f}'.format(accuracy_score(y_test, lr_pred))) DecisionTreeClassifier 정확도: 0.7877 RandomForestClassifier 정확도:0.8547 LogisticRegression 정확도: 0.8492 /usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG) 최적화 작업을 수행하지 않았고, 데이터 양도 충분하지 않기 때문에 어떤 알고리즘이 가장 성능이 좋은지 평가할 수는 없다. 교차 검증을 위해 KFord 클래스(폴드 개수는 5개로 설정), cross_val_score, GridSearchCV클래스 모두 사용 한다. 12345678910111213141516171819202122232425from sklearn.model_selection import KFolddef exec_kfold(clf, folds=5): # 폴드 세트를 5개인 KFold객체를 생성, 폴드 수만큼 예측결과 저장을 위한 리스트 객체 생성. kfold = KFold(n_splits=folds) scores = [] # KFold 교차 검증 수행. for iter_count , (train_index, test_index) in enumerate(kfold.split(X_titanic_df)): # X_titanic_df 데이터에서 교차 검증별로 학습과 검증 데이터를 가리키는 index 생성 X_train, X_test = X_titanic_df.values[train_index], X_titanic_df.values[test_index] y_train, y_test = y_titanic_df.values[train_index], y_titanic_df.values[test_index] # Classifier 학습, 예측, 정확도 계산 clf.fit(X_train, y_train) predictions = clf.predict(X_test) accuracy = accuracy_score(y_test, predictions) scores.append(accuracy) print(&quot;교차 검증 {0} 정확도: {1:.4f}&quot;.format(iter_count, accuracy)) # 5개 fold에서의 평균 정확도 계산. mean_score = np.mean(scores) print(&quot;평균 정확도: {0:.4f}&quot;.format(mean_score)) # exec_kfold 호출exec_kfold(dt_clf , folds=5) 교차 검증 0 정확도: 0.7542 교차 검증 1 정확도: 0.7809 교차 검증 2 정확도: 0.7865 교차 검증 3 정확도: 0.7697 교차 검증 4 정확도: 0.8202 평균 정확도: 0.7823 평균 정확도는 약 78.23% 이다. 교차 검증을 cross_val_score() API를 이용해 수행한다. 1234567from sklearn.model_selection import cross_val_scorescores = cross_val_score(dt_clf, X_titanic_df , y_titanic_df , cv=5)for iter_count,accuracy in enumerate(scores): print(&quot;교차 검증 {0} 정확도: {1:.4f}&quot;.format(iter_count, accuracy))print(&quot;평균 정확도: {0:.4f}&quot;.format(np.mean(scores))) 교차 검증 0 정확도: 0.7430 교차 검증 1 정확도: 0.7753 교차 검증 2 정확도: 0.7921 교차 검증 3 정확도: 0.7865 교차 검증 4 정확도: 0.8427 평균 정확도: 0.7879 방금 전의 K폴드의 평균 정확도가 약간 다른 이유는 cross_val_score가 stratifiedKFord를 이용해 폴드 세트를 분할하기 때문이다. GridSearchCV를 이용해 DecisionTreeClassifier의 최적 하이퍼 파라미터를 찾고 예측 성능을 측정해 본다. 12345678910111213141516from sklearn.model_selection import GridSearchCVparameters = {'max_depth':[2,3,5,10], 'min_samples_split':[2,3,5], 'min_samples_leaf':[1,5,8]}grid_dclf = GridSearchCV(dt_clf , param_grid=parameters , scoring='accuracy' , cv=5)grid_dclf.fit(X_train , y_train)print('GridSearchCV 최적 하이퍼 파라미터 :',grid_dclf.best_params_)print('GridSearchCV 최고 정확도: {0:.4f}'.format(grid_dclf.best_score_))best_dclf = grid_dclf.best_estimator_# GridSearchCV의 최적 하이퍼 파라미터로 학습된 Estimator로 예측 및 평가 수행. dpredictions = best_dclf.predict(X_test)accuracy = accuracy_score(y_test , dpredictions)print('테스트 세트에서의 DecisionTreeClassifier 정확도 : {0:.4f}'.format(accuracy)) GridSearchCV 최적 하이퍼 파라미터 : {'max_depth': 3, 'min_samples_leaf': 5, 'min_samples_split': 2} GridSearchCV 최고 정확도: 0.7992 테스트 세트에서의 DecisionTreeClassifier 정확도 : 0.8715 하이퍼 파라미터인 max_depth=3, min_samples_leaf=1, min_samples_split=2로 DecisionTreeClassifier를 학습시킨 뒤 예측 정확도가 약 87.15%로 향상됐다. 일반적으로 하이퍼 파라미터를 듀닝하더라고 이 정도 수준으로 증가하기는 어렵다. 테스트용 데이터 세트가 작기 때문에 수치상으로 예측 성능이 많이 증가 한것처럼 보인다.","link":"/2020/12/01/study/python_machine_learning_perfect_guide_ch02/"},{"title":"파이썬 머신러닝 완벽가이드 4장","text":"출처: 권철민, 『파이썬 머신러닝 완벽 가이드 (개정판)』, 위키북스, 2020.02, 183-289쪽 접기/펼치기 ##1. 분류(Classification)의 개요 지도학습은 레이블(Label) -&gt; 기대가 되는 값, 예측되는 값즉, 명시적인 정답이 있는 데이터가 주어진 상태에서 학습하는 머신러닝 방식. 즉, 기존 데이터가 어떤 레이블에 속하는지 패턴을 알고리즘으로 인지한 뒤에 새롭게 관측된 데이터에 대한 레이블을 판별하는 것이다. 분류는 다양한 머신러닝 알고리즘으로 구현할 수 있습니다. 나이브 베이즈 : 베이즈 통계와 생성 모델에 기반 로지스틱 회귀 : 독립변수와 종속변수의 선형 관계성에 기반 결정 트리 : 데이터 균일도에 따른 규칙 기반 서포트 벡터 머신 : 개별 클래스 간의 최대 분류 마진을 효과적으로 찾아줌 신경망 : 심층 연결 기반 앙상블 : 서로 같거나 다른 머신러닝 알고리즘을 결합 이번 장에서는 이 다양한 알고리즘 중에서 앙상블 방법을 집중적으로 다루게 됩니다.앙상블은 분류에서 가장 각광을 받는 방법 중 하나이다. 앙상블은 일반적으로 배깅과 부스팅 방식으로 나뉘게 됩니다. 배깅 랜덤 포레스트 : 뛰어난 예측 성능, 상대적으로 빠른 수행 시간, 유연성 등 -&gt; 분석가가 애용하는 알고리즘 부스팅 그래디언트 부스팅 : 뛰어난 예측성능을 가지고 있지만 수행시간이 너무 오래 걸리는 단점으로 최적화 모델 튜닝이 어려웠다. but. XGBoost(eXtra Gradient Boost)와 LightGBM 등 기존 그래디언트 부스팅의 예측 성능을 한 단계 발전과 수행 시간 단축으로 정형 데이터 분류 영역에서 가장 활용도가 높은 알고리즘으로 자리 잡음. 2. 결정 트리1. 결정트리 개요 데이터에 있는 규칙을 학습을 통해 트리 기반의 분류 규칙을 만든다. 일반적으로 if/else 기반으로 나타내는데 스무고개 게임처럼 if/else를 반복하며 분류 장점 균일도를 기반으로 하기 때무에 쉽고 직관적이다. 룰이 매우 명확하여 규칙노드와 리프 노드가 만들어지는 기준을 파악할 수 있다. 정보의 균일도만 신경쓰면 되기 때문에 사적 가공이 많이 필요하지 않다. 단점 과적합으로 알고리즘 성능이 떨어질 수 있다.피처가 많고 균일도가 다양하게 존재할수록 트리의 깊이가 커지고 복잡해질 수 밖에 없다.(학습 데이터를 기반으로 정확도를 올리기 위해 계속 조건을 추가하기 때문에 깊이가 깊어지고 복잡한 모델이되어 새로운 상황에 대한 예측력이 떨어진다.) 결정 트리 구조 루트노드 : 트리 구조가 시작되는 곳 규칙노드 : 규칙조건이 되는 것 리프노드 : 결정된 클래스 값(더 이상 자식 노드가 없는 것) 서브트리 : 새로운 규칙 조건마다 생성 규칙의 기준은 순수도를 가장 높여줄 수 있는 쪽을 선택해 진행한다. – 최대한 균일한 데이터 세트를 구성할 수 있도록 분할하는 것이 필요 그림에서 균일한 데이터 세트의 순서는 1 = 2 &gt; 3 이다. 항아리에 10개의 구슬이 들어 있고 그 중 절반가량이 빨간색이고 나머지 절반가량이 파란색인 경우 그 구슬들의 집합은 빨간색과 파란색이 섞여 있어 불순한 것으로 간주한다 (항아리 2). 반면에 항아리에 빨간색 또는 파란색 구슬만 있는 경우 그 구슬 집합은 완벽하게 순수한 것으로 간주한다. 결정노드는 정보 균일도가 높은 데이터를 먼저 선택하도록 규칙을 만든다. 즉 데이터를 나눌 수 있는 조건을 찾아 자식 노드를 만들며 내려가게 된다. 이때 정보의 균일도를 측정하는 대표적인 방법은 엔트로피를 이용한 정보 이득 지수와 지니계수가 있다. 엔트로피, 정보이득지수, 지니 계수 엔트로피 : 주어진 데이터 집합의 혼잡도 (&lt;-&gt;균일도) (값이 작을수록 데이터가 균일) 정보이득지수 : 1- 엔트로피 지수 (정보 이득이 높은 속성을 기준으로 분할) 지니계수: 0이 가장 평등하고 1로 갈수록 불평등하다. (지니 계수가 낮을수록 데이터 균일도가 높다. 지니계수가 낮은 속성을 기준으로 분할) 2. 결정트리 파라미터 min_samples_split(분리될 노드에 최소 자료 수) 노드를 분할하기 위한 최소한의 샘플 데이터 수 과적합 제어 용도 디폴트 2 (작게 설정할수록 과적합 가능성 증가) min_samples_leaf(잎사귀 노드에 최소 자료 수) 말단 노드가 되기 위한 최소한의 샘플 데이터 수 과적합 제어 용도 비대칭적 데이터(하나의 피처가 과도하게 많은 경우) 에는 특정 클래스의 데이터가 극도로 작을 수 있으므로 작게 설정 max_features 최적의 분할을 위해 고려할 최대 피처 개수 디폴트 = none (데이터 세트의 모든 피처를 사용해 분할) max_features = sqrt/ auto : √(전체 피처 개수) log = log2(전체 피처 개수) 선정 Int형으로 지정하면 대상 피처의 개수, float형으로 지정하면 전체 피처 중 대상 피처의 퍼센트 max _depth 트리의 최대 깊이 디폴트 = None (완벽하게 클래스 결정값이 될 때까지 깊이를 계속 키우며 분할 하거나 노드가 가지는 데이터의 개수가 min_sample_split보다 작아질때까지 계속 깊이를증가 시킨다.) 깊이가 깊어지면 과적합하므로 주의 ###3. 결정 트리 모델의 시각화 결정트리는 Graphviz 패키지 이용하여 시각화 할 수 있다. 12345678910111213141516from sklearn.tree import DecisionTreeClassifierfrom sklearn.datasets import load_irisfrom sklearn.model_selection import train_test_splitimport warningswarnings.filterwarnings('ignore')# DecisionTree Classifier 생성dt_clf = DecisionTreeClassifier(random_state=156)# 붓꽃 데이터를 로딩하고, 학습과 테스트 데이터 셋으로 분리iris_data = load_iris()X_train , X_test , y_train , y_test = train_test_split(iris_data.data, iris_data.target, test_size=0.2, random_state=11)# DecisionTreeClassifer 학습. dt_clf.fit(X_train , y_train) DecisionTreeClassifier(random_state=156) test_size = test데이터의 비율 random_state = 난수 값 고정 123456789101112from sklearn.tree import export_graphviz# export_graphviz()의 호출 결과로 out_file로 지정된 tree.dot 파일을 생성함. export_graphviz(dt_clf, out_file=&quot;tree.dot&quot;, class_names=iris_data.target_names , \\feature_names = iris_data.feature_names, impurity=True, filled=True)import graphviz# 위에서 생성된 tree.dot 파일을 Graphviz 읽어서 Jupyter Notebook상에서 시각화 with open(&quot;tree.dot&quot;) as f: dot_graph = f.read()graphviz.Source(dot_graph) Impurity가 True일 경우 각 노드의 불순물을 표시한다. filled는 True일 경우 분류를 위한 다수 클래스, 회귀 값의 극한 또는 다중 출력의 노드 순도를 나타내기 위해 노드를 색칠한다. 123456789from sklearn.tree import export_graphvizexport_graphviz(dt_clf, out_file='tree1.dot', class_names=iris_data.target_names, feature_names=iris_data.feature_names, impurity=False, filled=True)import graphvizwith open('tree1.dot') as f: dot_graph = f.read()graphviz.Source(dot_graph) 두 그림을 비교해보면 impurity=False일때 gini 계수가 사라진 것을 알 수 있다. ###4. Graphviz로 시각화된 결정 트리 지표 설명 이 그림은 직관적으로 리프 노드와 브랜치 노드를 볼 수 있어서 가져왔다. 자식노드가 없는 리프 노드에서 최종적으로 어떤 클래스인지 결정된다. 그 노드에 도달하기까지의 조건을 만족한다면 거기서 이 꽃이 어떤 종류의 붓꽃인지 예측하는 것이다. ####1)리프노드가 되는 조건? 최종 데이터가 오직 “하나의” 클래스 값으로 구성 하이퍼 파라미터 조건 충족(뒤에서 자세히 설명)브랜치 노드 안에는 맨 위와 같이 5개의 지표가 존재하고, 리프 노드에는 주황색 노드와 같이 4개의 지표가 존재한다. 맨 위의 노드 구성을 예시로 설명해보겠다. petal length(꽃잎 길이) ≤ 2.45 자식노드를 만들기 위한 규칙조건 (없으면 리프노드라는 증거!) 꽃잎 길이가 2.45 이하인 데이터와 초과인 데이터로 분류하겠다는 의미 gini = 0.666 지니계수 아래의 value 분포도를 통해 계산 높을수록 데이터 불균일 samples = 120 아직 아무런 조건으로도 나뉘어있지 않은 상황 세 품종 데이터 전체 갯수가 120개 value = [38, 41, 41] 품종 순서대로 Setosa 38개, Versicolor 41개, Virginica41개라는 의미 리스트 안의 값을 모두 더하면 samples의 개수와 같음 class = versicolor 하위 노드를 가질경우 value에서 가장 많은 값의 품종선택 여기서는 versicolor = virginica 41으로 같으므로 인덱스 작은것 선택 노드 색깔이 의미하는 것 붓꽃 데이터의 레이블 값을 의미, 색깔이 짙어질수록 지니계수가 낮아 데이터가 균일하고 해당 레이블에 속하는 샘플이 많다는 의미이다. 0 : Setosa(주황) 1 : Versicolor(초록) 2 : Virginica(보라) 위의 주황색 노드에서 전체 41개의 샘플이 모두 Setosa이므로 매우 균일한 상태라고 볼 수 있다. ####2) 하이퍼 파라미터 변경에 따른 트리 변화너무 복잡한 트리가 되면 과적합이 발생하여 오히려 예측성능이 낮아질 수 있다. 이를 제어하는 파라미터를 알아보자. max_depth min_samples_split min_samples_leaf max_depth : 너무 깊어지지 않도록! 적절히 설정하는 것이 중요할 것이다. 너무 간단해도, 너무 복잡해도 성능이 좋지 않을 것이기 때문이다. min_samples_split : 현재 sample 갯수를 보고 자식을 만들지 말지 결정! 기본 설정값은 2이다. 현재 sample이 2개이고, 두 개가 다른 품종이라면 자식노드를 만들어 분할해야한다. 하지만 이 파라미터를 4로 변경하면 샘플이 다른 품종이 섞인 3개여도 분할을 멈추고 리프노드가 된다. 따라서 자연스레 트리 깊이도 줄어든다. min_samples_leaf : sample갯수가 이 값 이하가 되도록 부모 규칙 변경! 리프노드가 될 수 있는 조건은 샘플수의 디폴트가 1이다. 즉, 샘플이 하나 남아야 리프노드로 인정되는 것이다. 그래야 한 품종만 남는다. 하지만 그러면 트리의 리프노드는 너무 많아지고 더 복잡해진다. 따라서 이 파라미터로 리프노드의 엄격했던 기준을 완화시켜주도록 한다. 자식 샘플 갯수가 4여도 리프노드로 만들어줄게! 그러니까 규칙을 좀만 널널하게 해줘~ 이런 식이다. ####3) 어떤 속성이 좋은 모델을 만들까? 사이킷런에는 규칙을 정하는 데 있어 피처(속성)의 중요한 역할 지표를 DecisionTreeclassifierr 객체의 featureimportances 속성으로 제공한다 반환되는 ndarray값은 피처 순서대로 중요도가 할당되어있다. 막대그래프로 시각화하면 더욱 직관적으로 확인 가능하다. 12345678910111213import seaborn as snsimport numpy as np%matplotlib inline# feature importance 추출print(&quot;Feature importances:/n{0}&quot;. format(np.round(dt_clf.feature_importances_, 3)))# feature별 importance 매핑for name, value in zip(iris_data.feature_names, dt_clf.feature_importances_): print('{0}:{1:3f}'.format(name, value))# feature importance를 column 별로 시각화하기sns.barplot(x=dt_clf.feature_importances_, y=iris_data.feature_names) Feature importances:/n[0.025 0. 0.555 0.42 ] sepal length (cm):0.025005 sepal width (cm):0.000000 petal length (cm):0.554903 petal width (cm):0.420092 &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f25ee2f30f0&gt; ####4) 결정 트리 과적합(Overfitting) 청일점, 홍일점처럼 일부 이상치 데이터까지 분류하기 위해서 분할이 자주 일어나면 결정 기준 경계도 많아지게 된다. 이렇게 복잡한 모델은 학습 데이터셋의 특성과 약간만 다른형태의 데이터 셋이 들어오면 제대로 예측할 수 없다. 123456789101112from sklearn.datasets import make_classificationimport matplotlib.pyplot as plt%matplotlib inlineplt.title(&quot;3 Class values with 2 Features Sample data creation&quot;)# 2차원 시각화를 위해서 feature는 2개, 결정값 클래스는 3가지 유형의 classification 샘플 데이터 생성. X_features, y_labels = make_classification(n_features=2, n_redundant=0, n_informative=2, n_classes=3, n_clusters_per_class=1,random_state=0)# plot 형태로 2개의 feature로 2차원 좌표 시각화, 각 클래스값은 다른 색깔로 표시됨. plt.scatter(X_features[:, 0], X_features[:, 1], marker='o', c=y_labels, s=25, cmap='rainbow', edgecolor='k') &lt;matplotlib.collections.PathCollection at 0x7f25edd95f60&gt; 과적합 예시 1234567891011121314151617181920212223242526import numpy as np# Classifier의 Decision Boundary를 시각화 하는 함수def visualize_boundary(model, X, y): fig,ax = plt.subplots() # 학습 데이타 scatter plot으로 나타내기 ax.scatter(X[:, 0], X[:, 1], c=y, s=25, cmap='rainbow', edgecolor='k', clim=(y.min(), y.max()), zorder=3) ax.axis('tight') ax.axis('off') xlim_start , xlim_end = ax.get_xlim() ylim_start , ylim_end = ax.get_ylim() # 호출 파라미터로 들어온 training 데이타로 model 학습 . model.fit(X, y) # meshgrid 형태인 모든 좌표값으로 예측 수행. xx, yy = np.meshgrid(np.linspace(xlim_start,xlim_end, num=200),np.linspace(ylim_start,ylim_end, num=200)) Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape) # contourf() 를 이용하여 class boundary 를 visualization 수행. n_classes = len(np.unique(y)) contours = ax.contourf(xx, yy, Z, alpha=0.3, levels=np.arange(n_classes + 1) - 0.5, cmap='rainbow', clim=(y.min(), y.max()), zorder=1) 12345from sklearn.tree import DecisionTreeClassifier# 특정한 트리 생성 제약없는 결정 트리의 Decsion Boundary 시각화.dt_clf = DecisionTreeClassifier().fit(X_features, y_labels)visualize_boundary(dt_clf, X_features, y_labels) 123# min_samples_leaf=6 으로 트리 생성 조건을 제약한 Decision Boundary 시각화dt_clf = DecisionTreeClassifier( min_samples_leaf=6).fit(X_features, y_labels)visualize_boundary(dt_clf, X_features, y_labels) 더 일반화된 모델 ####5) 결정 트리 실습 - 사용자 행동 인식 데이터 세트 Ch04-02. 결정트리 실습 - 사용자 행동 인식 데이터 세트 주제 : 결정 트리를 이용해 사용자 행동 인식 데이터 셋에 대한 예측 분류 수행 데이터 셋 : UCI 머신러닝 리포지토리에서 제공, 해당 데이터는 30명에게 스마트폰 센서를 장착한 뒤 사람의 동작과 관련된 여러가지 피처를 수집한 데이터임. 12345678910from google.colab import drive # 패키지 불러오기 from os.path import join ROOT = &quot;/content/drive&quot; # 드라이브 기본 경로print(ROOT) # print content of ROOT (Optional)drive.mount(ROOT) # 드라이브 기본 경로 MountMY_GOOGLE_DRIVE_PATH = 'My Drive/human_activity/' # 프로젝트 경로PROJECT_PATH = join(ROOT, MY_GOOGLE_DRIVE_PATH) # 프로젝트 경로print(PROJECT_PATH) /content/drive Mounted at /content/drive /content/drive/My Drive/human_activity/ 1%cd &quot;{PROJECT_PATH}&quot; /content/drive/My Drive/human_activity 123import pandas as pdimport matplotlib.pyplot as plt%matplotlib inline 123456# features.txt 파일에는 피처 이름 index와 피처명이 공백으로 분리되어 있음. 이를 DataFrame으로 로드.feature_name_df = pd.read_csv('/content/drive/My Drive/human_activity/features.txt', sep='\\s+', header=None, names=['column_index', 'column_name'])# 피처명 index를 제거하고, 피처명만 리스트 객체로 생성한 뒤 샘플로 10개만 추출feature_name = feature_name_df.iloc[:, 1].values.tolist()print('전체 피처명에서 10개만 추출:', feature_name[:10]) 전체 피처명에서 10개만 추출: ['tBodyAcc-mean()-X', 'tBodyAcc-mean()-Y', 'tBodyAcc-mean()-Z', 'tBodyAcc-std()-X', 'tBodyAcc-std()-Y', 'tBodyAcc-std()-Z', 'tBodyAcc-mad()-X', 'tBodyAcc-mad()-Y', 'tBodyAcc-mad()-Z', 'tBodyAcc-max()-X'] 1234# 중복된 피처명 확인feature_dup_df=feature_name_df.groupby('column_name').count()print(feature_dup_df[feature_dup_df['column_index']&gt;1].count())feature_dup_df[feature_dup_df['column_index']&gt;1].head() column_index 42 dtype: int64 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } column_index column_name fBodyAcc-bandsEnergy()-1,16 3 fBodyAcc-bandsEnergy()-1,24 3 fBodyAcc-bandsEnergy()-1,8 3 fBodyAcc-bandsEnergy()-17,24 3 fBodyAcc-bandsEnergy()-17,32 3 12345678# 중복된 피처명에 대해서는 원본 피처명에 _1 또는 _2를 추가로 부여해 새로운 피처명을 가지는 DataFrame반환def get_new_feature_name_df(old_feature_name_df): feature_dup_df=pd.DataFrame(data=old_feature_name_df.groupby('column_name').cumcount(), columns=['dup_cnt']) feature_dup_df=feature_dup_df.reset_index() new_feature_name_df=pd.merge(old_feature_name_df.reset_index(),feature_dup_df,how='outer') new_feature_name_df['column_name']=new_feature_name_df[['column_name','dup_cnt']].apply(lambda x:x[0]+'_'+str(x[1]) if x[1]&gt;0 else x[0], axis=1) new_feature_name_df=new_feature_name_df.drop(['index'],axis=1) return new_feature_name_df train/test data load 1234567891011121314151617181920212223242526import pandas as pddef get_human_dataset(): # 각 데이터 파일은 공백으로 분리되어 있으므로 read_csv에서 공백 문자를 sep으로 할당. feature_name_df=pd.read_csv('/content/drive/My Drive/human_activity/features.txt', sep='\\s+', header=None, names=['column_index','column_name']) # 중복된 피처명을 수정하는 get_new_feature_name_df()를 이용, 신규 피처명 DataFrame 생성 new_feature_name_df = get_new_feature_name_df(feature_name_df) # DataFrame에 피처명을 칼럼으로 부여하기 위해 리스트 객체로 다시 변환 feature_name=new_feature_name_df.iloc[:, 1].values.tolist() # train 피처 데이터셋과 test 피처 데이터를 DataFrame으로 로딩. 칼럼명은 feature_name 적용 X_train=pd.read_csv('/content/drive/My Drive/human_activity/train/X_train.txt',sep='\\s+',names=feature_name) X_test=pd.read_csv('/content/drive/My Drive/human_activity/test/X_test.txt',sep='\\s+',names=feature_name) # train label과 test label 데이터를 DataFrame으로 로딩하고 칼럼명은 action으로 부여 y_train=pd.read_csv('/content/drive/My Drive/human_activity/train/y_train.txt',sep='\\s+',header=None,names=['action']) y_test=pd.read_csv('/content/drive/My Drive/human_activity/test/y_test.txt',sep='\\s+',header=None,names=['action']) # 로드된 학습/테스트용 DataFrame을 모두 반환 return X_train, X_test, y_train, y_testX_train, X_test, y_train, y_test = get_human_dataset() 12print('## 학습 피처 데이터셋 info()')print(X_train.info()) ## 학습 피처 데이터셋 info() &lt;class 'pandas.core.frame.DataFrame'&gt; RangeIndex: 7352 entries, 0 to 7351 Columns: 561 entries, tBodyAcc-mean()-X to angle(Z,gravityMean) dtypes: float64(561) memory usage: 31.5 MB None 1print(y_train['action'].value_counts()) 6 1407 5 1374 4 1286 1 1226 2 1073 3 986 Name: action, dtype: int64 레이블 값은 1, 2, 3, 4, 5, 6의 6개 값이고 이는 움직임 위치와 관련된 속성이다. 분포도는 특정 값으로 왜곡되지 않고 비교적 고르게 분포되어있다. DecisionTreeClassifier 1234567891011from sklearn.tree import DecisionTreeClassifierfrom sklearn.metrics import accuracy_scoredt_clf = DecisionTreeClassifier(random_state=156)dt_clf.fit(X_train, y_train)pred = dt_clf.predict(X_test)accuracy = accuracy_score(y_test, pred)print('결정 트리 예측 정확도 : {0:.4f}'.format(accuracy))# DecisionTreeClassifier의 하이퍼 파라미터 추출print('DecisionTreeClassifie의 기본 하이퍼 파라미터:\\n', dt_clf.get_params()) 결정 트리 예측 정확도 : 0.8548 DecisionTreeClassifie의 기본 하이퍼 파라미터: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'presort': 'deprecated', 'random_state': 156, 'splitter': 'best'} Tree Depth에 따른 예측 성능 변화 123456789from sklearn.model_selection import GridSearchCVparams = { 'max_depth' : [6, 8, 10, 12, 16, 20, 24]}# 5개의 cv세트로 7개의 max_depth 테스트grid_cv = GridSearchCV(dt_clf, param_grid=params, scoring='accuracy', cv=5, verbose=1)grid_cv.fit(X_train, y_train)print('GridSearchCV 최고 평균 정확도 수치 : {0:.4f}'.format(grid_cv.best_score_))print('GridSearchCV 최적 하이퍼 파라미터: ',grid_cv.best_params_) Fitting 5 folds for each of 7 candidates, totalling 35 fits [Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers. [Parallel(n_jobs=1)]: Done 35 out of 35 | elapsed: 1.8min finished GridSearchCV 최고 평균 정확도 수치 : 0.8513 GridSearchCV 최적 하이퍼 파라미터: {'max_depth': 16} max_depth가 16일때 5개의 폴드 세트의 최고 평균 정확도가 약 85.13%로 도출되었다. 그렇다면 max_depth값에 따라 어떻게 예측 성능이 변화했는지 GridSearchCV객체의 cvresult 속성을 통해서 살펴보도록 할 것이다. 12345# GridSearchCV객체의 cv_result_ 속성을 DataFrame으로 생성.cv_results_df = pd.DataFrame(grid_cv.cv_results_)# max_depth 파라미터 값과 그때의 테스트 셋의 정확도 수치 추출cv_results_df[['param_max_depth', 'mean_test_score']] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } param_max_depth mean_test_score 0 6 0.850791 1 8 0.851069 2 10 0.851209 3 12 0.844135 4 16 0.851344 5 20 0.850800 6 24 0.849440 12345678max_depths = [ 6, 8 ,10, 12, 16 ,20, 24]# max_depth 값을 변화 시키면서 그때마다 학습과 테스트 셋에서의 예측 성능 측정for depth in max_depths: dt_clf = DecisionTreeClassifier(max_depth=depth, random_state=156) dt_clf.fit(X_train , y_train) pred = dt_clf.predict(X_test) accuracy = accuracy_score(y_test , pred) print('max_depth = {0} 정확도: {1:.4f}'.format(depth , accuracy)) max_depth = 6 정확도: 0.8558 max_depth = 8 정확도: 0.8707 max_depth = 10 정확도: 0.8673 max_depth = 12 정확도: 0.8646 max_depth = 16 정확도: 0.8575 max_depth = 20 정확도: 0.8548 max_depth = 24 정확도: 0.8548 12345678910# max_depth와 min_samples_split을 같이 변경하면서 성능 튜닝params = { 'max_depth' : [8, 12, 16, 20], 'min_samples_split' : [16, 24] }grid_cv = GridSearchCV(dt_clf, param_grid=params, scoring='accuracy', cv=5, verbose=1)grid_cv.fit(X_train, y_train)print('GridSearchCV 최고 평균 정확도 수치 : {0:.4f}'.format(grid_cv.best_score_))print('GridSearchCV 최적 하이퍼 파라미터: ',grid_cv.best_params_) Fitting 5 folds for each of 8 candidates, totalling 40 fits [Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers. [Parallel(n_jobs=1)]: Done 40 out of 40 | elapsed: 2.2min finished GridSearchCV 최고 평균 정확도 수치 : 0.8549 GridSearchCV 최적 하이퍼 파라미터: {'max_depth': 8, 'min_samples_split': 16} 1234best_df_clf = grid_cv.best_estimator_pred1 = best_df_clf.predict(X_test)accuracy = accuracy_score(y_test , pred1)print('결정 트리 예측 정확도:{0:.4f}'.format(accuracy)) 결정 트리 예측 정확도:0.8717 1234567891011import seaborn as snsftr_importances_values = best_df_clf.feature_importances_# Top 중요도로 정렬을 쉽게 하고, 시본(Seaborn)의 막대그래프로 쉽게 표현하기 위해 Series변환ftr_importances = pd.Series(ftr_importances_values, index=X_train.columns )# 중요도값 순으로 Series를 정렬ftr_top20 = ftr_importances.sort_values(ascending=False)[:20]plt.figure(figsize=(8,6))plt.title('Feature importances Top 20')sns.barplot(x=ftr_top20 , y = ftr_top20.index)plt.show() ##3. 앙상블 학습 ####1) 앙생블 학습 개요 Ensemble Learning : 여러 개의 분류기(Classifier)를 생성하고 그 예측을 결합함으로써 보다 정확한 최종 예측을 도축하는 기법 마치 집단 지성으로 어려운 문제를 쉽게 해결하는 것처럼 목표: 다양한 분류기의 예측 결과를 결합 -&gt; 단일 분류기보다 신뢰성이 높은 예측값 얻는 것 대부분의 정형 데이터 분류 시 뛰어난 성능 보임 (이미지, 영상, 음성 등의 비정형 데이터의 분류는 딥러닝이 더 뛰어난 성능) 대표적인 앙상블 알고리즘: Random Forest, Gradient Boosting 기존의 Gradient Boosting을 뛰어넘는 새로운 알고리즘 개발 XGBoost LightGBM (XGBoost보다 훨씬 빠른 수행 속도) Stacking (여러 가지 모델의 결과를 기반으로 메타 모델을 수립) Ensemble Learning 유형: 전통적으로 Voting, Bagging, Boosting 3가지 + Stacking을 포함한 다양한 앙상블 방법 Bagging, Boosting은 결정 트리 알고리즘 기반, Voting과 Stacking은 서로 다른 알고리즘을 기반(Ensemble의 한 개념) Voting과 Bagging : 여러 개의 분류기가 투표를 통해 최종 예측 결과를 결정하는 방식 Voting은 서로 다른 알고리즘을 가진 분류기 결합, Bagging은 각각의 분류기가 모두 같은 유형의 알고리즘 기반이지만, 데이터 샘플링을 서로 다르게 가져가면서 학습을 수행해 Voting을 수행하는 것이다. (대표적인 Bagging: Random Forest) Voting : 다른 ML 알고리즘이 같은 데이터 세트에 대해 학습하고 예측한 결과를 가지고 보팅을 통해 최종 예측 결과 선정 Bagging : 단일 ML 알고리즘이 Bootstrapping 방식으로 샘플링된 데이터 세트에 대해서 학습을 통해 개별적인 예측을 수행한 결과를 보팅을 통해 최종 예측 결과 선정 Bootstrapping 분할 방식: 개별 분류기에게 데이터를 샘플링해서 추출하는 방식, 여러 개의 데이터 세트를 중첩되게 분리하는 것 (Voting 방식과 다름) 교차 검증이 데이터 세트 간에 중첩 허용하지 않는 것과 다르게, Bagging은 중첩 허용 5 size만큼 Bootstrap 실행 (중복 허용, 복원 추출 개념) 각 분류기 k개만큼 데이터를 샘플링 -&gt; 개별적인 예측을 보팅(결과 값 평균)을 통해 최종 예측 결과 선정 OOB error(Out-of-Bag error): 학습데이터에서 미 추출된 데이터에 대해 각각의 분류기가 예측하고 Error율 계산해서 평균 냄 -&gt; 학습데이터 내에서 미 추출된 데이터를 검증 데이터로 써서 검증데이터에 대한 성능지표를 계산할 수 있는 장점 Bagging과 Tree의 차이점 Tree: 쉽고 직관적인 분류 기준을 가지고 있지만, Low Bias(정답과 예측값의 거리), High Variance(모델별 예측값 간의 거리) =&gt; overfitting 발생 Bagging: 위와 같은 문제를 해결하기 위해 모델이 예측한 값의 평균을 사용하여 bias를 유지하고 Variance를 감소, 학습데이터의 noise에 강건해짐, 모형해석의 어려움(단점) 결정 트리 알고리즘의 장점은 그대로 취하고 단점은 보완하면서 bias-variance trade-off의 효과를 극대화할 수 있음 Boosting : 여러 개의 분류기가 순차적으로 학습을 수행하되, 앞에서 학습한 분류기가 예측이 틀린 데이터에 대해서 올바르게 예측할 수 있도록 다음 분류기에게 가중치(weight)를 부여하면서 학습과 예측 진행 Stacking : 여러 가지 다른 모델의 예측 결과값을 다시 학습 데이터로 만들어서 다른 모델(메타 모델)로 재학습시켜 결과를 예측 ####2) 보팅 유형 – 하드 보팅(Hard Voting)과 소프트 보팅(Soft Voting) 하드 보팅 : 예측한 결과값들 중 다수의 분류기가 결정한 예측값을 최종 보팅 결과값으로 선정 (다수결 원칙과 비슷) 소프트 보팅 : 분류기들의 레이블 값 결정 확률을 모두 더하고 이를 평균해서 이들 중 확률이 가장 높은 레이블 값을 최종 보팅 결과값으로 선정 (일반적인 보팅 방법) 일반적으로 하드 보팅보다는 소프트 보팅이 예측 성능이 좋아서 더 많이 사용됨. ####3) 보팅 분류기(Voting Classifier) 사이킷런은 보팅 방식의 앙상블을 구현한 보팅 분류기 클래스를 제공하고 있다. 암 데이터로 위스콘신 데이터 세트를 생성 12345678910111213import pandas as pdfrom sklearn.ensemble import VotingClassifierfrom sklearn.linear_model import LogisticRegressionfrom sklearn.neighbors import KNeighborsClassifierfrom sklearn.datasets import load_breast_cancerfrom sklearn.model_selection import train_test_splitfrom sklearn.metrics import accuracy_scorecancer = load_breast_cancer()data_df = pd.DataFrame(cancer.data, columns=cancer.feature_names)data_df.head(3) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } mean radius mean texture mean perimeter mean area mean smoothness mean compactness mean concavity mean concave points mean symmetry mean fractal dimension radius error texture error perimeter error area error smoothness error compactness error concavity error concave points error symmetry error fractal dimension error worst radius worst texture worst perimeter worst area worst smoothness worst compactness worst concavity worst concave points worst symmetry worst fractal dimension 0 17.99 10.38 122.8 1001.0 0.11840 0.27760 0.3001 0.14710 0.2419 0.07871 1.0950 0.9053 8.589 153.40 0.006399 0.04904 0.05373 0.01587 0.03003 0.006193 25.38 17.33 184.6 2019.0 0.1622 0.6656 0.7119 0.2654 0.4601 0.11890 1 20.57 17.77 132.9 1326.0 0.08474 0.07864 0.0869 0.07017 0.1812 0.05667 0.5435 0.7339 3.398 74.08 0.005225 0.01308 0.01860 0.01340 0.01389 0.003532 24.99 23.41 158.8 1956.0 0.1238 0.1866 0.2416 0.1860 0.2750 0.08902 2 19.69 21.25 130.0 1203.0 0.10960 0.15990 0.1974 0.12790 0.2069 0.05999 0.7456 0.7869 4.585 94.03 0.006150 0.04006 0.03832 0.02058 0.02250 0.004571 23.57 25.53 152.5 1709.0 0.1444 0.4245 0.4504 0.2430 0.3613 0.08758 로지스틱 회귀와 KNN을 기반하여 소프트 보팅 방식으로 새롭게 보팅 분류기를 만들어 보았다. 12345678910111213141516171819202122# 개별 모델은 로지스틱 회귀와 KNN 임. lr_clf = LogisticRegression()knn_clf = KNeighborsClassifier(n_neighbors=8)# 개별 모델을 소프트 보팅 기반의 앙상블 모델로 구현한 분류기 vo_clf = VotingClassifier( estimators=[('LR',lr_clf),('KNN',knn_clf)] , voting='soft' )X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, test_size=0.2 , random_state= 156)# VotingClassifier 학습/예측/평가. vo_clf.fit(X_train , y_train)pred = vo_clf.predict(X_test)print('Voting 분류기 정확도: {0:.4f}'.format(accuracy_score(y_test , pred)))# 개별 모델의 학습/예측/평가.classifiers = [lr_clf, knn_clf]for classifier in classifiers: classifier.fit(X_train , y_train) pred = classifier.predict(X_test) class_name= classifier.__class__.__name__ print('{0} 정확도: {1:.4f}'.format(class_name, accuracy_score(y_test , pred))) Voting 분류기 정확도: 0.9474 LogisticRegression 정확도: 0.9386 KNeighborsClassifier 정확도: 0.9386 ##4. 랜덤 포레스트 ###1. 랜덤 포레스트의 개요 및 실습 Bagging의 대표적인 알고리즘 Random Forest; 비교적 빠른 수행 속도 + 높은 예측 성능 여러 개의 결정 트리 분류기가 전체 데이터에서 bagging 방식으로 각자의 데이터를 샘플링 -&gt; 개별적으로 학습 수행 후 최종적으로 모든 분류기가 voting을 통해 예측 결정 (X+Y)의 분산이 X와 Y 각각의 분산을 더한 것보다 더 크기 때문에 Bagging Model 자체의 분산이 커질 수 있음 공분산이 0이면, 두 변수는 서로 독립적인 관계 따라서 Random Forest는 기본 Bagging과 다르게 데이터뿐만 아니라, 변수도 random하게 뽑아서 다양한 모델 만듦 (각 분류기간의 공분산을 줄이는 게 목표) 모델의 분산을 줄여 일반적으로 Bagging보다 성능이 좋음 뽑을 변수의 수는 hyper parameter(일반적으로 √p 사용, p는 변수 개수) 개별적인 분류기의 기반 알고리즘은 결정트리이지만, 개별 트리가 학습하는 데이터세트는 전체 데이터에서 일부가 중첩되게 샘플링된 데이터 세트 (Bootstrapping 방식) Bagging: bootstrap aggregating의 줄임말 랜덤 포레스트의 Subset 데이터는 Bootstrapping 방식으로 데이터가 임의로 만들어짐 Subset 데이터의 건수는 전체 데이터의 건수와 동일하지만, 개별 데이터가 중첩되어 만들어짐 데이터가 중첩된 개별 데이터 세트에 결정 트리 분류기를 각각 적용 ⇒ 랜덤 포레스트! 정리배깅: 같은 알고리즘으로 여러 개의 분류기를 만들어서 보팅으로 최종 결정하는 알고리즘. 배깅의 대표적인 알고리즘은 랜덤포레스트 랜덤포레스트의 장점 : 1. 앙상블 알고리즘 중 비교적 빠른 수행 속도를 가지고 있음 다양한 영역에서 높은 예측 성능 결정 트리의 쉽고 직관적인 장점 그대로 가지고 있음 ###2. 랜덤 포레스트 하이퍼 파라미터 및 튜닝하이퍼 파라미터란, 일반적으로 머신러닝에서 어떠한 임의의 모델을 학습시킬때, 사람이 직접 튜닝 (설정) 해주어야하는 변수를 말한다. RandomForest의 단점: 하이퍼 파라미터가 너무 많다 시간이 많이 소모된다 예측 성능이 크게 향상되는 경우가 많지 않다 트리 기반 자체의 하이퍼 파라미터가 원래 많고, 배깅, 부스팅, 학습, 정규화를 위한 하이퍼 파라미터까지 추가되므로 많을 수 밖에 없다. 코드: n_estimators: 결정 트리의 개수 지정/ max_features는 결정트리에 max_features 파라미터와 같음(최적의 분할을 위해 고려할 최대 피처갯수). 기본이 sqrt(전체 피처갯수) / max_depth(트리의 최대 깊이 규정), min_samples_leaf(말단 노드가 되기 위한 최소한의 샘플 데이터 수): 과적합 개선 ##5. GBM ###1. GBM의 개요 및 실습부스팅 알고리즘: 순차적으로 학습-예측하면서 잘못 예측한 데이터에 가중치 부여를 하고, 오류를 개선해 나가면서 학습하는 방식 에이다 부스트, GBM 과의 차이: GBM은 가중치 업데이트를 경사하강법을 이용한다. 경사하강법 보통 GBM이 랜덤 포레스트 보다는 예측 성능이 뛰어나다. BUT, 수행시간 문제는 GBM이 극복해야할 중요한 과제. ###2. GBM 하이퍼 파라미터 및 튜닝n_estimators: 결정트리갯수/ max_depth:. Max_features: 위와 같이 loss: 경사 하강법에서 사용할 비용 함수 지정 learning_rate: weak learner가 순차적으로 오류값을 보정해 나가는데 적용하는 계수. 범위는 0과 1사이 , 기본값은 0.1, 너무 작은 값: 예측성능은 높아지지만 속도 느림. 너무 큰 값: 예측성능이 떨어지지만 속도는 빠름/ subsample: 학 습에 사용하는 데이터 샘플링 비율 (ex. 0.5 면 학습데이터 50%) 장점: 과적합에도 강한 뛰어난 예측 성능을 가진 알고리즘 단점: 수행시간이 오래걸림 ##6. XGBoost (eXtra Gradient Boost) ###1. XGBoost 개요트리 기반의 앙상블 학습 ###2. 파이썬 래퍼 XGBoost 하이퍼 파라미터 파라미터의 유형 일반 파라미터 : 일반적으로 실행 시 스레드의 개수나 냐ㅣ둣 모드 등의 선택을위한 파라미터로서 디폴트 파라미터 값을 바꾸는 경우는 거의 없다. 부스터 파라미터 : 트리 최적화, 부스팅, regularization 등의 관련 파라미터들을 지칭. 학습 태스크 파라미터 : 학습 수행 시의 객체 함수, 평가를 위한 지표등을 설정하는 파라미터. 주요 일반 파라미터 booster: gbtree(나무 기반 모델) or gblinear (회귀 기반 모델)선택, 디폴트는 gbtree silent : 출력 메시지를 나타내고 싶지 않을 경우 1로 설정 , 디폴트 = 0 nthread : cpu의 실행 스레드 개수를 조정, 디폴트는 cpu의 전체 스레드를 다 사용하는 것. 주요 부스터 파라미터 regularization(정규화) : 과적합 모델을 일반화 해주는 기법 L1 Regularization (lasso): 기존 함수에 계수의 절댓값에 가중치를 곱한 것을 더해주는 것 L2 Regularization (ridge) : 기존함수에 계수의 제곱의 가중치를 곱한 것을 더해주는 것 학습 태스크 파라미터 log loss : 로그 우도에 -1을 곱한 값우도, 가능도(liklelihood) : 어떤 값이 관측 되었을때, 이것이 어던 확률 분포에서 온건지에대한 확률 ROC: 분류모델의 성능을 보여주는 그래프 AUC: ROC 곡선의 아래 면적 (1일때 가장 좋음) ###3. 과적합 제어 과적합을 제어하는 방법 eta 값을 낮춘다. (0.01 ~ 0.1) → eta 값을 낮추면 num_round(n_estimator)를 반대로 높여주어야 한다. max_depth 값을 낮춘다. min_child_weight 값을 높인다. gamma 값을 높인다. subsample과 colsample_bytree를 낮춘다. XGBoost는 자체적으로 교차 검증, 성능평가, 피처 중요도 등의 시각화 기능을 가지고 있다. 또한 다른 여러가지 성능을 향상시키는 기능을 가지고 있다. Ex) Early Stopping 조기 중단 기능 GBM의 경우 n_estimators에 지정된 횟수만큼 학습을 끝까지 수행하지만, XGB의 경우 오류가 더 이상 개선되지 않으면 수행을 중지 Ex) n_estimators 를 200으로 설정하고, 조기 중단 파라미터 값을 50으로 설정하면, 1부터 200회까지 부스팅을 반복하다가 50회를 반복하는 동안 학습오류가 감소하지 않으면 더 이상 부스팅을 진행하지 않고 종료합니다. ##7. LightGBM ###1. LightGBM XGBoost는 GBM보다는 빠르지만 여전히 학습시간이 오래 걸리고, 대용량 데이터로 학습 성능을 기대하려면 높은 병렬도로 학습을 진행해야 한다. LightGBM의 장점 XGBoost보다 학습에 걸리는 시간이 훨씬 적다. 메모리 사용량도 상대적으로 적다. 기능상의 다양상도 XGBoost보다 약간 더 많다. ex. 카테고리형 피처의 자동 변환(원ㅡ핫 인코딩 사용하지않는)과 이에 따른 최적 분할이다. XGBoost와 마찬가지로 대용량 데이터에 대한 뛰어난 성능 및 병렬컴퓨팅 기능을 제공하고 최근에는 추가로GPU까지 지원한다. XGBoost의 장점은 계승하고 단점은 보완하는 방식으로 개발되었다. LightGBM의 단점 적은(10,000건 이하)의 데이터 셋에 적용할 경우 과적합 발생 쉽다. → 오버피팅에 더 강하지만 균형을 맞추기 위한 시간이 필요하다. LightGBM : 리프 중심 트리 분할(Leaf Wise) 방식 사용 → 트리의 균형을 맞추지 않고 최대 손실값(max delta loss)를 가지는 리프 노드를 지속적으로 분할하며 트리 깊이 확장하면서 트리의 깊이가 깊어지고 비대칭적 규칙 트리 생성한다. → 학습을 반복할 수록 균형트리분할방식보다 예측 오류 손실을 최소화할 수 있다. 패키지 설명 패키지명 : ‘lightgbm’ 초기에 lightgbm은 독자적인 모듈로 설계되었으나 편의를 위해 scikit-learn wrapper로 호환이 가능하게 추가로 설계되었다. 패키지 내에 파이썬 래퍼, 사이킷런 래퍼 모두 내장하고있다. 사이킷런 래퍼 LightGBM클래스는 분류를 위한 LGBMClassifier클래스와 회귀를 위한 LGBMRegressor클래스이다. fit( ), predict( ) 기반의 학습 및 예측과 사이킷런이 제공하는 다양한 기능 활용이 가능하다. → 사이킷런에 익숙하다면 별도의 파이썬 래퍼 클래스를 사용하지 않아도 된다. Light GBM은 leaf-wise 방식을 취하고 있기 때문에 수렴이 굉장히 빠르지만, 파라미터 조정에 실패할 경우 과적합을 초래할 수 있다. ###2.LightGBM 설치아나콘다로 쉽게 설치 가능하다. 단, 윈도우에 설치할 경우에는 Visual Studio Build tool 2015 이상이 먼저 설치돼 있어야한다. 그 후에 OS 터미널에서 conda명령어를 수행한다. (윈도우10에서는 아나콘다 프롬프트→관리자 권한으로 실행→ conda명령어 수행) 1conda install -c conda-forge lightgbm 도중에 나오는 Proceed([y]/n)에서 y를 입력하고 엔터를 누른다. ###3. LightGBM 하이퍼 파라미터 XGBoost와 많은 부분이 유사하다. 주의할 점은 위의 트리분할 방식차이에 따라 이러한 트리 특성에 맞는 하이퍼 파라미터 설정이 필요하다는 점이다.(예를들어 max_depth가 매우 크게 가진다는 것) 주요파라미터 Learning Task 파라미터 objective : 최솟값을 가져야 할 손실함수 정의. 회귀, 다중클래스분류, 이진 분류인지에따라 지정 튜닝방안 num_leaves(트리의 최대 리프 개수)를 중심으로 min_data_in_leaf와 max_depth를 함께 조정하면서 복잡도를 줄이는 것이 기본 튜닝 방안 과적합을 방지하기 위해 num_leaves는 2^(max_depth)보다 작아야 한다. 예를 들어 max_depth가 7이기 때문에, 2^(max_depth)=128이 되는데, 이 때 num_leaves를 이보다 작은 70~80 정도로 설정하는 것이 낫다. learning_rate는 DOWN, n_estimators는 UP (물론 너무 키우면 과적합) learning_rate는 후반부에 건드리는 것이 좋은데, 초반부터 너무 작은 학습률을 지정하면 효율이 크게 떨어질 수 있기 때문이다. reg_lambda, reg_alpha와 같은 regularization적용 학습데이터에 사용할 피처개수나 데이터 샘플링 레코드 개수 줄이기 위해 colsample_bytree, subsample 적용 ###4. 파이썬 래퍼 LightGBM vs 사이킷런 래퍼 XGBoost vs 사이킷런 래퍼 LightGBM하이퍼 파라미터 비교 개요 XGBoost가 사이킷런 규칙에 따라 자신의 하이퍼 파라미터 변경함 LightGBM은 XGBoost와 기능이 많이 유사해서 사이킷런 래퍼의 파라미터를 XGBoost에 맞춰서 변경함 ##8. 스태킹 앙상블 스태킹(Stacking): 개별적인 여러 알고리즘을 서로 결합해 예측 결과를 도출 ⇒ 배깅(Bagging) 및 부스팅(Boosting)과 공통점 지님 ⇒ 하지만 큰 차이점은 개별 알고리즘으로 예측한 데이터를 기반으로 다시 예측을 수행한다는 것! ⇒ 즉, 개별 알고리즘의 예측 결과 데이터 세트를 최종적인 메타 데이터 세트로 만들어 별도의 ML 알고리즘으로 최종 학습을 수행하고, 다시 테스트 데이터를 기반으로 다시 최종 예측 메타 모델: 개별 모델의 예측된 데이터 세트를 다시 기반으로 학습하고 예측하는 방식 스태킹 모델은 두 종류의 모델이 필요 개별적인 기반 모델 이 개별 기반 모델의 예측 데이터를 학습 데이터로 만들어서 학습하는 최종 메타 모델 스태킹 모델의 핵심은 여러 개별 모델의 예측 데이터를 각각 스태킹 형태로 결합해 최종 메타 모델의 학습용 피처 데이터 세트와 테스트용 피처 데이터 세트를 만드는 것 주로 캐글과 같은 대회에서 높은 순위를 차지하기 위해 조금이라도 성능 수치를 높여야 할 경우 자주 사용됨! 2~3개의 개별 모델만을 결합해서 쉽게 예측 성능을 향상시킬 수 없음 스태킹을 적용한다고 해서 반드시 성능 향상 된다는 보장 없음 여러 개의 모델에 대한 예측값을 합한 후, 즉 스태킹 형태로 쌓은 뒤 이에 대한 예측을 다시 수행하는 것 스텝별로 어떻게 데이터가 만들어지고 실행되는지 알아보자 M개의 로우, N개의 칼럼을 가진 데이터 세트에 스태킹 앙상블 적용한다고 가정 학습에 사용할 ML 알고리즘 모델은 모두 3개 [스태킹 앙상블 순서] 모델별로 각각 학습 시킨 뒤 예측을 수행하면 각각 M개의 로우를 가진 1개의 레이블 값을 도출할 것임 모델별로 도출된 예측 레이블 값을 다시 합해서(스태킹) 새로운 데이터 세트를 만듦 스태킹 된 데이터 세트에 대해 최종 모델을 적용해 최종 예측 ###1.CV 세트 기반의 스태킹과적합을 개선하기 위해 최종 메타 모델을 위한 데이터 세트를 만들 때 교차 검증 기반으로 예측된 결과 데이터 세트를 이용 앞 예제에서 최종 학습할 때 레이블 데이터 세트로 학습 데이터가 아닌 테스트용 레이블 데이터 세트를 기반으로 학습했기에 과적합 문제 발생할 수 있음 ⇒ 개선을 위해 개별 모델들이 각각 교차 검증으로 메타 모델을 위한 학습용 스태킹 데이터 생성과 테스트용 스태킹 데이터를 생성한 뒤 이를 기반으로 메타 모델이 학습과 예측 수행 2단계의 스탭으로 구분 스텝 1: 각 모델별로 원본 학습/테스트 데이터(X_train, X_test)를 예측한 결과 값을 기반으로 메타 모델을 위한 학습용/테스트용 데이터(meta_train/meta_test)를 생성 스텝 2: 스텝1에서 개별 모델들이 생성한 학습용 데이터(meta_train)를 모두 스태킹 형태로 합쳐서 메타 모델이 학습할 최종 학습용 데이터 세트(final_train)를 생성 (최종 테스트용 데이터 세트도 마찬가지(meta_test를 모두 스태킹 → final_test)) 메타 모델은 최종 학습용 데이터 세트(final_train)와 원본 학습 데이터의 레이블 데이터(y_train)를 기반으로 학습한 뒤, 최종 테스트용 데이터 세트(final_test)를 예측하고, 원본 테스트 데이터의 레이블 데이터(y_test)를 기반으로 평가 *각 스텝별로 수행하는 로직을 살펴보자. 스텝 1* ⇒ 3개의 폴드만큼 반복을 수행하면서 스태킹 데이터를 생성하는 첫 번째 반복을 설명한 것 핵심은 개별 모델에서 메타 모델인 2차 모델에서 사용될 학습 데이터와 테스트용 데이터를 교차 검증을 통해서 생성하는 것 스텝 1은 개별 모델 레벨(#1)에서 수행하는 것이며, 이러한 로직을 여러 개의 개별 모델(#1 ~ #4)에서 동일하게 수행 먼저 학습용 데이터를 N개의 폴드(Fold)로 나눔(여기선 N=3) 3개의 폴드세트이므로 3번의 유사한 반복 작업을 수행, 마지막 3번째 반복에서 개별 모델의 예측 값으로 학습 데이터와 테스트 데이터를 생성 주요 프로세서 ⇒ 스태킹 데이터를 생성하는 두 번째 반복을 설명한 것 ⇒ 스태킹 데이터를 생성하는 세 번째 반복을 설명한 것 세 번째 반복을 완료하면 첫 번재, 두 번째, 세 번째 반복을 수행하면서 만들어진 폴드별 예측 데이터를 합하여 메타 모델에서 사용될 학습 데이터(meta_train)를 만들게 됨 첫 번재, 두 번째, 세 번째 반복을 수행하면서 학습 폴드 데이터로 학습된 개별 모델들이 원본 테스트 세트로 예측한 결괏값을 최종 평균하여 메타 모델에서 사용될 테스트 데이터(meta_test)를 만들게 됨. 스텝 2 각 모델들이 스텝 1로 생성한 학습/테스트 데이터(meta_train, meta_test)를 모두 합쳐서 최종적으로 메타 모델이 사용할 학습/테스트 데이터(final_train, final_test)를 생성하기만 하면 됨 메타 모델이 사용할 최종 학습 데이터(final_train)와 원본 학습 데이터의 레이블 데이터(y_train)를 합쳐서 메타 모델을 학습 최종 테스트 데이터(final_test)로 예측을 수행한 뒤, 최종 예측 결과를 원본 테스트 데이터의 레이블 데이터(y_test)와 비교해 평가","link":"/2020/12/03/study/python_machine_learning_perfect_guide_ch04/"},{"title":"파이썬 머신러닝 완벽가이드 6장","text":"출처: 권철민, 『파이썬 머신러닝 완벽 가이드 (개정판)』, 위키북스, 2020.02, 377-408쪽 접기/펼치기 Chap6 차원축소01. 차원축소(Dimension Reduction) 개요 차원축소는 매우 많은 피처로 구성된 다차원 데이터 세트의 차원을 축소해 새로운 차원의 데이터 세트를 생성하는 것이다. 피처가 많을 경우 개별 피처간에 상관관계가 높을 가능성이 크다. 선형모델에서는 입력 변수 간의 상관관계가 높을 경우 이로 인한 다중 공선성 문제로 모델의 예측 성능이 저하된다. 따라서 다차원의 피처를 차원 축소해 피처수를 줄이면 직관적으로 데이터를 해석할 수 있다. 차원 축소는 피처 선택(feature selection)과 피처 추출(feature extraction)로 나눌 수 있다. 피처 선택(특성 선택) 특정 피처에 종속성이 강한 불필요한 피처는 아예 제거하고, 데이터의 특징을 잘 나타내는 주요 피처만 선택하는 것이다. 피처 추출(특성 추출) 기존 피처를 저차원의 중요 피처로 압축해서 추출하는 것이다. 새롭게 추출된 중요 특성은 기존의 피처가 압축된 것이므로 기존의 피처와는 완전히 다른 값이 된다. 피처 추출은 기존 피처를 단순 압축이 아닌, 기존 피처가 인지하기 어려웠던 잠재적인 요소(Latent Factor)를 추출하는 것을 의미한다. ex) 고등학생의 모의고사 성적, 내신성적, 수능성적, 봉사활동 등의 여러 피처를 학업 성취도, 커뮤니케이션 능력, 문제 해결력과 같은 함축적인 요약 특성으로 추출 차원 축소는 좀 더 데이터를 잘 설명할 수 있는 잠재적인 요소를 추출하는데 의미가 있다. 대표적인 차원 축소 알고리즘에는 PCA, SVD, NMF 가 있다.(이미지나 텍스트 차원 축소에 자주 활용) 02. PCA(Principal Component Analysis, 주성분 분석) PCA는 여러 변수 간에 존재하는 상관관계를 이용해 이를 대표하는 주성분(Principal Component)을 추출해 차원을 축소하는 기법이다. PCA로 차원을 축소할 때, 기존 데이터의 정보 유실을 최소화하기 위해 가장 높은 분산을 가지는 데이터의 축을 찾아 이 축으로 차원을 축소하게 되고, 이것이 PCA의 주성분이 된다. PCA는 제일 먼저 가장 큰 데이터 변동성(Variance)을 기반으로 첫 번째 벡터축을 생성하고, 두 번째 축은 이 벡터 축에 직각이 되는 벡터(직교 벡터)를 축으로 한다. 세 번째 축은 다시 두 번째 축과 각각이 되는 벡터를 설정하는 방식으로 축을 생성한다. 이렇게 생성된 벡터 축에 원본 데이터를 투영하면 벡터 축의 개수만큼의 차원으로 원본 데이터가 차원 축소된다. PCA는 원본 데이터의 피처 개수에 비해 매우 작은 주성분으로 원본 데이터의 총 변동성을 대부분 설명할 수 있는 분석법이다. 선형대수 관점에서 해석해 보면, 입력 데이터의 공분산 행렬(Covariance Matrix)을 고유값 분해하고, 이렇게 구한 고유벡터에 입력 데이터를 선형 변환하는 것이다. 이 고유벡터가 PCA의 주성분벡터로서 입력 데이터의 분산이 큰 방향을 나타낸다. 고유값(eigenvalue)은 이 고유벡터의 크기를 나타내며, 동시에 입력 데이터의 분산을 나타낸다. 선형 변환은 특정 벡터에 행렬A를 곱해 새로운 벡터로 변환하는 것을 의미한다.(행렬을 공간으로 가정) 보통 분산은 한 개의 특정한 변수의 데이터 변동을 의미하나, 공분산은 두 변수 간의 변동을 의미한다. Cov(X, Y) &gt; 0은 X가 증가할 때 Y도 증가한다는 의미이다. 공분산 행렬은 여러 변수와 관련된 공분산을 포함하는 정방형 행령이다. X Y Z X 3.0 -0.71 -0.24 Y -0.71 4.5 0.28 Z -0.24 0.28 0.91 위 표를 보자. 공분산 행렬에서 대각선 원소는 각 변수(X, Y, Z)의 분산을 의미하며, 대각선 이외의 원소는 가능한 모든 변수 쌍 간의 공분산을 의미하낟. 즉, X, Y, Z의 분산은 각각 3.0, 4.5, 0.91이고, X와 Y의 공분산은 -0.71, X와Z의 공분산은 -0.24, Y와Z의 공분산은 0.28이다. 고유벡터는 행렬 A를 곱하더라도 방향이 변하지 않고 그 크기만 변하는 벡터를 지칭한다.(Ax = ax, A는 행렬 x는 고유벡터 a는 스칼라값) 정방행렬은 최대 차원 수 만큼의 고유벡터를 가질 수 있다.(2*2행렬은 2개, 3*3행렬은 3개의 고유벡터를 가질 수 있음) 이렇게 고유벡터는 행렬이 작용하는 힘의 방향과 관계가 있어서 행렬을 분해하는 데 사용된다. 공분산 행렬은 정방행렬(Diagonal Matrix)이며 대칭행렬(Symmetric Matrix)이다. 정방행렬 : 열과 행이 같은 행렬 대칭행렬 : 정방행렬 중에서 대각 원소를 중심으로 원소 값이 대칭되는 행렬 공분산 행렬은 개별 분산값을 대각 원소로 하는 대칭행렬이다. 대칭행렬은 고유값 분해에 있어서, 항상 고유벡터를 직교행렬(orthogonal matrix)로, 고유값을 정방 행렬로 대각화할 수 있다. 입력 데이터의 공분산 행렬을 C라고 하면 공분산 행렬의 특성으로 인해 다음과 같이 분해할 수 있다. 이 때 P는 n*n의 직교행렬이며, 시그마는 n*n 정방행렬, P^T는 행렬 P의 전치행렬이다. 위 식은 고유벡터 행렬과 고유값 행렬로 다음과 같이 대응된다. 공분산C는 고유벡터 직교 행렬 * 고유값 정방 행렬 * 고유벡터 직교 행렬의 전치 행렬로 분해된다. ei는 i번째 고유벡터를, 람다i는 i번째 고유벡터의 크기를 의미한다. e1는 가장 분산이 큰 방향을 가진 고유벡터이며, e2는 e1에 수직이면서 다음으로 가장 분산이 큰 방향을 가진 고유벡터이다. 입력 데이터의 공분산 행렬이 고유벡터와 고유값으로 분해될 수 있으모, 이렇게 분해된 고유벡터를 이용해 입력 데이터를 선형 변화하는 방식이 PCA라는 것이다. PCA의 스텝 입력 데이터 세트의 공분산 행렬을 생성한다. 공분산 행렬의 고유벡터와 고유값을 계산한다. 고유값이 가장 큰 순으로 K개(PCA 변환 차수만큼)만큼 고유벡터를 추출한다. 고유값이 가장 큰 순으로 추출된 고유벡터를 이용해 새롭게 입력데이터를 변환한다. PCA는 많은 속성으로 구성된 원본 데이터를 그 핵심을 구성하는 데이터로 압축한 것이다. 붓꽃(iris)데이터 세트는 sepal length, sepal width, petal length ,petal width 4개의 속성으로 되어 있는데, 2개의 PCA의 차원으로 압축해 원래 데이터 세트와 압축된 데이터 세트가 어떻게 달라졌는지 확인해 보자. 먼저 사이킷런의 붓꽃 데이터를 load_iris() API를 이용해 로딩한 뒤 이 데이터를 더 편하게 시각화하기 위해 DataFrame으로 변환해보자. 12345678910111213from sklearn.datasets import load_irisimport pandas as pdimport matplotlib.pyplot as plt%matplotlib inline# 사이킷런 내장 데이터 셋 API 호출iris = load_iris()# 넘파이 데이터 셋을 Pandas DataFrame으로 변환columns = ['sepal_length','sepal_width','petal_length','petal_width']irisDF = pd.DataFrame(iris.data , columns=columns)irisDF['target']=iris.targetirisDF.head(3) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sepal_length sepal_width petal_length petal_width target 0 5.1 3.5 1.4 0.2 0 1 4.9 3.0 1.4 0.2 0 2 4.7 3.2 1.3 0.2 0 각 품종에 따라 원본 붓꽃 데이터 세트가 어떻게 분포돼 있는지 2차원으로 시각화해 보자. 2차원으로 표현하므로 두 개의 속성인 sepal length와 sepal width를 X축, Y축으로 해 품종 데이터 분포를 나탄낸다. 12345678910111213#setosa는 세모, versicolor는 네모, virginica는 동그라미로 표현markers=['^', 's', 'o']#setosa의 target 값은 0, versicolor는 1, virginica는 2. 각 target 별로 다른 shape으로 scatter plot for i, marker in enumerate(markers): x_axis_data = irisDF[irisDF['target']==i]['sepal_length'] y_axis_data = irisDF[irisDF['target']==i]['sepal_width'] plt.scatter(x_axis_data, y_axis_data, marker=marker,label=iris.target_names[i])plt.legend()plt.xlabel('sepal length')plt.ylabel('sepal width')plt.show() Setosa 품종의 경우 sepal width가 3.0보다 크고, sepal length가 6.0 이하인 곳에 일정하게 분포되어 있다. Versiclor와 virginica의 경우 sepal width와 sepal length 조건만으로는 분류가 어려운 복잡한 조건임을 알 수 있다. 이제 2개의 PCA 속성으로 붓꽃 데이터의 품종 분포를 2차원으로 시각화해 보자. 먼저 붓꽃 데이터 세트에 바로 PCA를 적용하기 전에 개별 속성을 함께 스케일링해야 한다. PCA는 여러 속성의 값을 연산해야 하므로 속성의 스케일에 영향을 받는다. 따라서 여러 속성을 PCA로 압축하기 전에 각 속성값을 동일한 스케일로 변환하는 것이 필요하다. 사이킷런의 StandardScaler를 이용해 평균이 0, 분산이 1인 표준 정규 분포로 iris 데이터 세트의 속성값들을 변환한다. 1234from sklearn.preprocessing import StandardScaler# Target 값을 제외한 모든 속성 값을 StandardScaler를 이용하여 표준 정규 분포를 가지는 값들로 변환iris_scaled = StandardScaler().fit_transform(irisDF.iloc[:, :-1]) 이제 스케일링이 적용된 데이터 세트에 PCA를 적용해 4차원의 붓꽃 대이터를 2차원 PCA 데이터로 변환해 보자. 사이킷런은 PCA 변환을 위해 PCA 클래스를 제공한다. PCA 클래스는 생성 파라미터로 n_components를 입력받는다. n_components는 PCA로 변환할 차원의 수를 의미하므로 여기서는 2로 설정한다. 이후에 fit과 transform을 호출해 PCA로 변환을 수행한다. 12345678from sklearn.decomposition import PCApca = PCA(n_components=2)#fit( )과 transform( ) 을 호출하여 PCA 변환 데이터 반환pca.fit(iris_scaled)iris_pca = pca.transform(iris_scaled)print(iris_pca.shape) (150, 2) PCA 객체의 transform() 메서드를 호출해 원본 데이터 세트를 (150,2)의 데이터 세트로 iris_pca 객체 변수로 반환했다. iris_pca는 변환된 PCA 데이터 세트를 150*2 넘파이 행렬로 가지고 있다. 이를 DataFrame으로 변환한 뒤 데이터값을 확인해 보자. 12345# PCA 환된 데이터의 컬럼명을 각각 pca_component_1, pca_component_2로 명명pca_columns=['pca_component_1','pca_component_2']irisDF_pca = pd.DataFrame(iris_pca, columns=pca_columns)irisDF_pca['target']=iris.targetirisDF_pca.head(3) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } pca_component_1 pca_component_2 target 0 -2.264703 0.480027 0 1 -2.080961 -0.674134 0 2 -2.364229 -0.341908 0 이제 2개의 속성으로 PCA 변환된 데이터 세트를 2차원상에서 시각화해보자. pca_component_1 속성을 X축으로, pca_component_2 속성을 Y축으로 해서 붓꽃 품종이 어떻게 분포되는지 확인해보자. 123456789101112markers=['^', 's', 'o']#pca_component_1 을 x축, pc_component_2를 y축으로 scatter plot 수행. for i, marker in enumerate(markers): x_axis_data = irisDF_pca[irisDF_pca['target']==i]['pca_component_1'] y_axis_data = irisDF_pca[irisDF_pca['target']==i]['pca_component_2'] plt.scatter(x_axis_data, y_axis_data, marker=marker,label=iris.target_names[i])plt.legend()plt.xlabel('pca_component_1')plt.ylabel('pca_component_2')plt.show() PCA로 변환한 후에도 pca_component_1 축을 기반으로 Setosa 품종은 명확하게 구분이 가능하다. Versicolor와 Virginica는 pca_component_1 축을 기반으로 서로 겹치는 부분이 일부 존재하지만, 비교적 잘 구분됐다. 이는 PCA 첫 번째 새로운 축인 pca_component_1이 원본 데이터의 변동성을 잘 반영했기 때문이다. PCA Component별로 원본 데이터의 변동성을 얼마나 반영하는 알아보자. PCA 변환을 수행한 PCA 객체의 explained_variance_ratio_ 속성은 전체 변동성에서 개별 PCA 컴포넌트 별로 변동성 비율을 제공하고 있다. 1print(pca.explained_variance_ratio_) [0.72962445 0.22850762] 첫 번째 PCA 변환 요소인 pca_component_1이 전체 변동성의 약 72.9%를 차지하며, 두 번째인 pca_component_2가 약 22,8%를 차지한다. 따라서 PCA 2개 요소로만 변환해도 원본 데이터의 변동성을 95% 설명할 수 있다. 이번에는 원본 붓꽃 데이터 세트와 PCA로 변환된 데이터 세트에 각각 분류를 적용한 후 결과를 비교하겠다. Estimator는 RandomForestClassifier를 이용하고 cross_val_score()로 3개의 교차 검증 세트로 정확도 결과를 비교한다. 먼저 원본 붓꽃 데이터에 랜덤 포레스트를 적용한 결과는 다음과 같다. 12345678from sklearn.ensemble import RandomForestClassifierfrom sklearn.model_selection import cross_val_scoreimport numpy as nprcf = RandomForestClassifier(random_state=156)scores = cross_val_score(rcf, iris.data, iris.target,scoring='accuracy',cv=3)print('원본 데이터 교차 검증 개별 정확도:',scores)print('원본 데이터 평균 정확도:', np.mean(scores)) 원본 데이터 교차 검증 개별 정확도: [0.98 0.94 0.96] 원본 데이터 평균 정확도: 0.96 이번에는 기존 4차원 데이터를 2차원으로 PCA 변환한 데이터 세트에 랜덤 포레스트를 적용해 보겠다. 1234pca_X = irisDF_pca[['pca_component_1', 'pca_component_2']]scores_pca = cross_val_score(rcf, pca_X, iris.target, scoring='accuracy', cv=3 )print('PCA 변환 데이터 교차 검증 개별 정확도:',scores_pca)print('PCA 변환 데이터 평균 정확도:', np.mean(scores_pca)) PCA 변환 데이터 교차 검증 개별 정확도: [0.88 0.88 0.88] PCA 변환 데이터 평균 정확도: 0.88 원본 데이터 세트 대비 예측 정확도는 PCA 변환 차원 개수에 따라 예측 성능이 떨어질 수 밖에 없다. 위 붓꽃 데이터의 경우는 4개의 속성이 2개의 변환 속성으로 감소하면서 예측 성능의 정확도가 원본 데이터 대비 10% 하락했다. 10%의 정확도 하락은 비교적 큰 성능 수치의 감소지만, 4개의 속성이 2개로, 속성 개수가 50% 감소한 것을 고려한다면 PCA 변환 후에도 원본 데이터의 특성을 상당부분 유지하고 있음을 알 수 있다. 다음으로 좀 더 많은 피처를 가진 데이터 세트를 적은 PCA 컴포넌트 기반으로 변화한 뒤, 예측 영향도가 어떻게 되는지 변환된 PCA 데이터 세트에 기반해서 비교해 보겠다. 사용할 데이터 세트는 UCI Machine Learning Repository에 있는 신용카드 고객 데이터 세트이다. 다운받는 법 https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients 에 접속한다. Data Folder 를 클릭 후 default of credit card clients.xls 를 클릭하여 데이터를 다운받는다. 파일명을 credit_card.xls로 바꿔준다. 123456# Mount Google Drivefrom google.colab import drive # import drive from google colabROOT = &quot;/content/drive&quot; # default location for the driveprint(ROOT) # print content of ROOT (Optional)drive.mount(ROOT) # we mount the google drive at /content/drive /content/drive Mounted at /content/drive 123456789# import join used to join ROOT path and MY_GOOGLE_DRIVE_PATHfrom os.path import join # path to your project on Google Drive# MY_GOOGLE_DRIVE_PATH = 'My Drive/Class_Python/MachineLearning/data'MY_GOOGLE_DRIVE_PATH = 'My Drive'PROJECT_PATH = join(ROOT, MY_GOOGLE_DRIVE_PATH)print(PROJECT_PATH) /content/drive/My Drive 1%cd &quot;{PROJECT_PATH}&quot; /content/drive/My Drive 1234567# header로 의미 없는 첫 행 제거, iloc로 기존 id 제거import pandas as pddf = pd.read_excel('credit_card.xls', sheet_name='Data', header = 1)df = df.iloc[:, 1:]print(df.shape)df.head(3) (30000, 24) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } LIMIT_BAL SEX EDUCATION MARRIAGE AGE PAY_0 PAY_2 PAY_3 PAY_4 PAY_5 PAY_6 BILL_AMT1 BILL_AMT2 BILL_AMT3 BILL_AMT4 BILL_AMT5 BILL_AMT6 PAY_AMT1 PAY_AMT2 PAY_AMT3 PAY_AMT4 PAY_AMT5 PAY_AMT6 default payment next month 0 20000 2 2 1 24 2 2 -1 -1 -2 -2 3913 3102 689 0 0 0 0 689 0 0 0 0 1 1 120000 2 2 2 26 -1 2 0 0 0 2 2682 1725 2682 3272 3455 3261 0 1000 1000 1000 0 2000 1 2 90000 2 2 2 34 0 0 0 0 0 0 29239 14027 13559 14331 14948 15549 1518 1500 1000 1000 1000 5000 0 신용카드 데이터 세트는 30,000개의 레코드와 24개의 속성을 가지고 있다. 이 중에서 ‘default payment next month’ 속성이 Target 값으로 ‘다음달 연체 여부’를 의미하며 ‘연체’일 경우 1, ‘정상납부’가 0이다. 원본 데이터 세트에 PAY_0 다음에 PAY_2 칼럼이 있으므로 PAY_0 칼럼을 PAY_1으로 칼럼명을 변환하고 ‘default payment next month’칼럼도 칼럼명이 너무 길어서 ‘default’로 칼럼명을 변경한다. 이후 Target 속성인 ‘default’ 칼럼을 y_target 변수로 별도로 저장하고 피처 데이터는 default 칼럼을 제외한 별도의 DataFrame으로 만들겠다. 123df.rename(columns={'PAY_0':'PAY_1','default payment next month':'default'}, inplace=True)y_target = df['default']X_features = df.drop('default', axis=1) 해당 데이터 세트는 23개의 속성 데이터 세트가 있으나 각 속성끼리 상관도가 매우 높다. DataFrame의 corr()를 이용해 각 속성 간의 상관도를 구한 뒤 이를 시본의 heatmap으로 시각화하겠다. 1234567import seaborn as snsimport matplotlib.pyplot as plt%matplotlib inlinecorr = X_features.corr()plt.figure(figsize=(14,14))sns.heatmap(corr, annot=True, fmt='.1g') &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f6dd94a6f98&gt; BILL_AMT1 ~ BILL_AMT6 6개의 속성끼리의 상관도가 대부분 0.9 이상으로 매우 높음을 알 수 있다. 이보다 낮지만 PAY_1 ~ PAY_6까지의 속성 역시 상관도가 높다. 이렇게 높은 상관도를 가진 속성들은 소수의 PCA만으로도 자연스럽게 이 속성들의 변동성을 수용할 수 있다. 이 BILL_AMT1 ~ BILL_AMT6까지 6개 속성을 2개의 컴포넌트로 변환한 뒤 개별 컴포넌트의 변동성을 explained_variance_ratio_ 속성으로 알아보자. 1234567891011121314from sklearn.decomposition import PCAfrom sklearn.preprocessing import StandardScaler#BILL_AMT1 ~ BILL_AMT6 까지 6개의 속성명 생성cols_bill = ['BILL_AMT'+str(i) for i in range(1,7)]print('대상 속성명:',cols_bill)# 2개의 PCA 속성을 가진 PCA 객체 생성하고, explained_variance_ratio_ 계산 위해 fit( ) 호출scaler = StandardScaler()df_cols_scaled = scaler.fit_transform(X_features[cols_bill])X_features.loc[:, cols_bill] = df_cols_scaledpca = PCA(n_components=2)pca.fit(df_cols_scaled)print('PCA Component별 변동성:', pca.explained_variance_ratio_) 대상 속성명: ['BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6'] PCA Component별 변동성: [0.90555253 0.0509867 ] 단 2개의 PCA 컴포넌트만으로도 6개의 속성의 변동성을 약 95% 이상 설명할 수 있으며 특히 첫 번째 PCA 축으로 90%의 변동성을 수용할 정도로 이 6개 속성의 상관도가 매우 높다. 이번에는 원본 데이터 세트와 6개의 컴포넌트로 PCA 변환한 데이터 세트의 분류 예측 결과를 상호 비교해 보겠다. 먼저 원본 데이터 세트에 랜덤 포레스트를 이용해 타깃 값이 디폴트 값을 3개의 교차 검증 세트로 분류 예측했다. 123456789import numpy as npfrom sklearn.ensemble import RandomForestClassifierfrom sklearn.model_selection import cross_val_scorercf = RandomForestClassifier(n_estimators=300, random_state=156)scores = cross_val_score(rcf, X_features, y_target, scoring='accuracy', cv=3 )print('CV=3 인 경우의 개별 Fold세트별 정확도:',scores)print('평균 정확도:{0:.4f}'.format(np.mean(scores))) CV=3 인 경우의 개별 Fold세트별 정확도: [0.8081 0.8197 0.8232] 평균 정확도:0.8170 3개의 교차 검증 세트에서 평균 예측 정확도는 약 81.71%를 나타냈다. 이번에는 6개의 컴포넌트로 PCA 변환한 데이터 세트에 대해서 동일하게 분류 예측을 적용해 보겠다. 123456789101112131415from sklearn.decomposition import PCAfrom sklearn.preprocessing import StandardScaler# 원본 데이터셋에 먼저 StandardScaler적용scaler = StandardScaler()df_scaled = scaler.fit_transform(X_features)# 6개의 Component를 가진 PCA 변환을 수행하고 cross_val_score( )로 분류 예측 수행. pca = PCA(n_components=6)df_pca = pca.fit_transform(df_scaled)scores_pca = cross_val_score(rcf, df_pca, y_target, scoring='accuracy', cv=3)print('CV=3 인 경우의 PCA 변환된 개별 Fold세트별 정확도:',scores_pca)print('PCA 변환 데이터 셋 평균 정확도:{0:.4f}'.format(np.mean(scores_pca))) CV=3 인 경우의 PCA 변환된 개별 Fold세트별 정확도: [0.7921 0.7963 0.8024] PCA 변환 데이터 셋 평균 정확도:0.7969 전체 23개 속성의 약 1/4 수준인 6개의 PCA 컴포넌트마능로도 원본 데이터를 기반으로 한 분류 예측 결과보다 약 1~2% 정도의 예측 성능 저하만 발생했다. 1~2%의 예측 성능 저하는 미비한 성능 저하로 보기는 힘들지만, 전체 속성의 1/4 정도만으로도 이정도 수치의 예측 성능을 유지할 수 있다는 것은 PCA의 뛰어난 압축 능력을 잘 보여주고 있다. PCA는 차원 축소를 통해 데이터를 쉽게 인지하는 데 활용할 수 있지만, 이보다 더 활발하게 적용되는 영역은 컴퓨터 비전(Computer Vision)분야이다. 03. LDA(Linear Discriminant Analysis)LDA개요LDA(Linear Discriminant Analysis)는 선형 판별 분석법으로 불리며, PCA와 매우 유사합니다. LDA는 PCA와 유사하게 입력 데이터 세트를 저차원 공간에 투영해 차원을 축소하는 기법이지만, 중요한 차이는 LDA는 지도학습의 분류에서 사용하기 쉽도록 개별 클래스를 분별할 수 있는 기준을 최대한 유지하면서 차원을 축소합니다. PCA는 입력 데이터의 변동성의 가장 큰 축을 찾았지만, LDA는 입력 데이터의 결정 값 클래스를 최대한으로 분리할 수 있는 축을 찾습니다. LDA는 특정 공간상에서 클래스 분리를 최대화하는 축을 찾기 위해 클래스 간 분산과 클래스 내부 분산의 비율을 최대화 하는 방식으로 차원을 축소합니다. 즉, 클래스 간 분산은 최대한 크게 가져가고, 클래스 내부의 분산은 최대한 작게 가져가는 방식입니다. 다음 그림은 좋은 클래스 분리를 위해 클래스 간 분산이 크고 클래스 내부의 분산이 작은 것을 표현한 것입니다. 일반적으로 LDA를 구하는 스텝은 PCA와 유사하난 가장 큰 차이점은 공분산 행렬이 아니라 위에 설명한 클래스 간 분산과 클래스 내부 분산 행렬을 생성한 뒤, 이 행렬에 기반해 고유백터를 구하고 입력 데이터를 투영한다는 점입니다. LDA를 구하는 스텝은 다음과 같습니다. 클래스 내부와 클래스 간 분산 행렬을 구합니다. 이 두 개의 행렬은 입력 데이터의 결정 값 클래스별로 개별 피처의 평균 백터를 기반으로 구합니다. 클래스 내부 분산은 행렬을 SW, 클래스 간 분산 행렬을 SB라고 하면 다음 식으로 두 행렬을 고유백터로 분해할 수 있습니다. 고유값이 가장 큰 순으로 K개(LDA변환 차수만큼) 추출합니다. 고유값이 가장 큰 순으로 추출된 고유벡터를 이용해 새롭게 입력 데이터를 변환합니다. 붓꽃 데이터 세트에 LDA 적용하기붓꽃 데이터 세트를 사이킷런의 LDA를 이용해 변환하고, 그 결과를 품종별로 시각화해 보겠습니다.사이킷런은 LDA를 LinearDiscriminantAnalysis 클래스로 제공합니다. 붓꽃 데이터 세트를 로드하고 표준 정규 분포로 스케일링합니다. 1234567from sklearn.discriminant_analysis import LinearDiscriminantAnalysisfrom sklearn.preprocessing import StandardScalerfrom sklearn.datasets import load_irisiris = load_iris()iris_scaled = StandardScaler().fit_transform(iris.data) 2개의 컴포넌트로 붓꽃 데이터를 LDA 변환하겠습니다. PCA와 다르게 LDA에서 한 가지 유의해야 할점은 LDA는 실제로는 PCA와 다르게 비지도학습이 아닌 지도학습이라는 것입니다. 즉, 클래스의 결정값이 변환 시에 필요합니다. 다음 lda 객체의 fit() 메서드를 호출할 때 결정값이 입력됐음에 유의하세요. 1234lda = LinearDiscriminantAnalysis(n_components=2)lda.fit(iris_scaled, iris.target)iris_lda = lda.transform(iris_scaled)print(iris_lda.shape) (150, 2) 이제 LDA 변환된 입력 데이터 값을 2차원 평면에 품종별로 표현해 보겠습니다. 소스 코드는 앞의 PCA 예제와 큰 차이가 없습니다. 12345678910111213141516171819202122import pandas as pdimport matplotlib.pyplot as plt%matplotlib inlinelda_columns=['lda_component_1', 'lda_component_2']irisDF_lda = pd.DataFrame(iris_lda, columns=lda_columns)irisDF_lda['target']=iris.target#setosa는 세모, versicolor는 네모, virginica는 동그라미로 표현markers=['^', 's', 'o']#setosa의 target 값은 0, versicolor는 1, virginica는 2, 각 target별로 다른 모양으로 산점도로 표시for i, marker in enumerate(markers): x_axis_data = irisDF_lda[irisDF_lda['target']==i]['lda_component_1'] y_axis_data = irisDF_lda[irisDF_lda['target']==i]['lda_component_2'] plt.scatter(x_axis_data, y_axis_data, marker=marker, label=iris.target_names[i])plt.legend(loc='upper right')plt.xlabel('lda_component_1')plt.ylabel('lda_component_2')plt.show() 04. SVD(Singular Value Decomposition)SVD 개요SVD 역시 PCA와 유사한 행렬 분해 기법을 이용합니다. PCA의 경우 정방행렬(즉, 행과 열의 크기가 같은 행렬)만을 고유벡터로 분해할 수 있지만, SVD는 정방행렬뿐만 아니라 행과 열의 크기가 다른 행렬에도 적용할 수 있습니다. 일반적으로 SVD는 m X n 크기의 행렬 A를 다음과 같이 분해하는 것을 의미합니다. SVD는 특이값 분해로 불리며, 행렬 U와 V에 속한 백터는 특이벡터이며, 모든 특이벡터는 서로 직교하는 성질을 가집니다. Σ는 대각행렬이며, 행렬의 대각에 위치한 값만 0이 아니고 나머지 위치의 값은 모두 0입니다. Σ이 위치한 0이 아닌 값이 바로 행렬 A의 특이값입니다. SVD는 A의 차원이 m X n 일 때 U의 차원이 m X m, Σ의 차원이 m X n, V의 차원이 n X n으로 분해합니다. 하지만 일반적으로는 다음과 같이 Σ의 비대각인 부분과 대각원소 중에 특이값이 0인 부분도 모두 제거하고 제거된Σ에 대응되는 U와 V 원소도 함께 제거해 차원을 줄인 형태로 SVD를 적용합니다. 이렇게 컴팩트한 형태로 SVD를 적용하면 A의 차원이 m X n일 때, U의 차원을 m X p, Σ의 차원을 p X p, V의 차원을 p X n으로 분해 합니다. Truncated SVD는 Σ의 대각원소 중에 상위 몇 개만 추출해서 여기에 대응하는 U와 V의 원소도 함께 제거해 더욱 차원을 줄인 형태로 분해하는 것입니다. 일반적인 SVD는 보통 넘파이나 사이파이 라이브러리를 이용해 수행합니다. 넘파이의 SVD를 이용해 SVD 연산을 수행하고, SVD로 분해가 어떤 식으로 되는지 간단한 예제를 통해 살펴보겠습니다. 새로운 주피터 노트불을 생성하고 넘파이의 SVD 모듈인 numpy.linalg.svd를 로딩합니다. 그리고 랜덤한 4 X 4 넘파이 행렬을 생성합니다. 랜덤 행렬을 생성하는 이유는 행렬의 개별 로우끼리의 의존성을 없애기 위해서입니다. 12345678# 넘파이의 svd 모듈 임포트import numpy as npfrom numpy.linalg import svd# 4X4 랜덤 행렬 a 생성np.random.seed(121)a = np.random.randn(4, 4)print(np.round(a, 3)) [[-0.212 -0.285 -0.574 -0.44 ] [-0.33 1.184 1.615 0.367] [-0.014 0.63 1.71 -1.327] [ 0.402 -0.191 1.404 -1.969]] 이렇게 생성된 a 행렬에 SVD를 적용해 U, sigma, Vt를 도출하겠습니다. SVD 분해는 nummpy. linalg. svd에 파라미터로 원본 행렬을 입혁하면 U 행렬, Sigma 행렬, V 전치 행렬을 반환합니다. Sigma 행렬의 경우, S=UΣV 에서 Σ행렬의 경우 행렬의 대각에 위치한 값만 0이 아니고, 그렇지 않은 경우는 모두 0이므로 0이아닌 값의 경우만 1차원 행렬로 표현합니다. 12345U, Sigma, Vt = svd(a)print(U.shape, Sigma.shape, Vt.shape)print('U matrix:\\n', np.round(U, 3))print('Sigma value:\\n', np.round(Sigma, 3))print('V transpose matrix:\\n', np.round(Vt, 3)) (4, 4) (4,) (4, 4) U matrix: [[-0.079 -0.318 0.867 0.376] [ 0.383 0.787 0.12 0.469] [ 0.656 0.022 0.357 -0.664] [ 0.645 -0.529 -0.328 0.444]] Sigma value: [3.423 2.023 0.463 0.079] V transpose matrix: [[ 0.041 0.224 0.786 -0.574] [-0.2 0.562 0.37 0.712] [-0.778 0.395 -0.333 -0.357] [-0.593 -0.692 0.366 0.189]] U 행렬이 4 X 4, Vt행렬이 4 X 4로 반환됐고, Sigma의 경우는 1차원 행렬인 (4,)로 반환됐습니다. 분해된 이 U, Sigma, Vt를 이용해 다시 원본 행렬로 정확히 복원되는지 확인해 보겠습니다. 원본 행렬로의 복원은 이 U, Sigma, Vt를 내적하면 됩니다. 한 가지 유의할 것은 Sigma의 경우 0이 아닌 값만 1차원으로 추출했으므로 다시 0을 포함한 대칭 행렬로 변환한 뒤에 내적을 수행해야 한다는 점입니다. 1234# Sigma를 다시 0을 포함한 대칭행렬로 변환Sigma_mat = np.diag(Sigma)a_ = np.dot(np.dot(U, Sigma_mat), Vt)print(np.round(a_, 3)) [[-0.212 -0.285 -0.574 -0.44 ] [-0.33 1.184 1.615 0.367] [-0.014 0.63 1.71 -1.327] [ 0.402 -0.191 1.404 -1.969]] U, Sigma, Vt를 이용해 a_는 원본 행렬 a와 동일하게 복원됨을 알 수 있습니다. 이번에는 데이터 세트가 로우 간 의존성이 있을 경우 어떻게 Sigma 값이 변하고, 이에 따른 차원 축소가 진행될 수 있는지 알아보겠습니다. 일부러 의존성을 부여하기 위해 a 행렬의 3번째 로우를 ‘첫 번째 로우 + 두 번째 로우’로 업데이트하고, 4번째 로우는 첫 번째 로우와 같다고 업데이트 하겠습니다. 123a[2] = a[0] + a[1]a[3] = a[0]print(np.round(a, 3)) [[-0.212 -0.285 -0.574 -0.44 ] [-0.33 1.184 1.615 0.367] [-0.542 0.899 1.041 -0.073] [-0.212 -0.285 -0.574 -0.44 ]] 이제 a 행렬은 이전과 다르게 로우 간 관계가 매우 높아졌습니다. 이 데이터를 SVD로 다시 분해해 보겠습니다. 1234# 다시 SVD를 수행해 Sigma 값 확인U, Sigma, Vt = svd(a)print(U.shape, Sigma.shape, Vt.shape)print('Sigma Value:\\n', np.round(Sigma, 3)) (4, 4) (4,) (4, 4) Sigma Value: [2.663 0.807 0. 0. ] 이전과 차원은 같지만 Sigma 값 중 2개가 0으로 변했습니다. 즉, 선형 독립인 로우 백터의 개수가 2개라는 의미입니다(즉, 행렬의 랭크가 2입니다). 이렇게 분해된 U, Sigma, Vt를 이용해 다시 원본 행렬로 복원해 보겠습니다. 이번에는 U, Sigma, Vt의 전체 데이터를 이용하지 않고 Sigma의 0에 대응되는 U, Sigma, Vt의 데이터를 제외하고 복원해 보겠습니다. 즉, Sigma의 경우 앞의 2개 요소만 0 이 아니므로 U 행렬 중 선행 두개의 열만 추출하고, Vt의 경우는 선행 두개의 행만 추출해 복원하는 것입니다. 123456789# U 행렬의 경우는 Sigma와 내적을 수행하므로 Sigma의 앞 2행에 대응되는 앞 2열만 추출U_ = U[:, :2]Sigma_ = np.diag(Sigma[:2])# V 전치 행렬의 경우는 앞 2행만 추출Vt_ = Vt[:2]print(U_.shape, Sigma_.shape, Vt_.shape)# U, Sigma, Vt의 내적을 수행하며, 다시 원본 행렬 복원a_ = np.dot(np.dot(U_, Sigma_), Vt_)print(np.round(a_, 3)) (4, 2) (2, 2) (2, 4) [[-0.212 -0.285 -0.574 -0.44 ] [-0.33 1.184 1.615 0.367] [-0.542 0.899 1.041 -0.073] [-0.212 -0.285 -0.574 -0.44 ]] 이번에는 Truncated SVD를 이용해 행렬을 분해해 보겠습니다. Truncated SVD는 Σ행렬에 있는 대각원소, 즉 특이값 중 상위 일부 데이터만 추출해 분해하는 방식입니다. 이렇게 분해하면 인위적으로 더 작은 차원의 U, Σ V로 분해하기 때문에 원본 행렬을 정확하게 다시 원복할 수는 없습니다. 하지만 데이터 정보가 압축되어 분해됨에도 불구하고 상당한 수준으로 원본 행렬을 근사할 수 있습니다. 당연한 얘기지만, 원래 차원의 차수에 가깝게 잘라낼수록 원본 행렬에 더 가깝게 복원할 수 있습니다. Truncated SVD를 사이파이 모듈을 이용해 간단히 테스트해 보겠습니다. Truncated SVD는 넘파이가 아닌 사이파이에서만 지원됩니다. 사이파이는 SVD뿐만 아니라 Truncated SVD도 지원합니다. 일반적으로 사이파이의 SVD는 scipy.linalg.svd를 이용하면 되지만, Truncated DVD는 희소 행렬로만 지원돼서 scipy.sparse.linalg.svds를 이용해야 합니다. 임의의 원본 행렬 6 X 6을 Normal SVD로 분해해 분해된 행렬의 차원과 Sigma 행렬 내의 특이값을 확인한 뒤 다시 Truncated SVD로 분해해 분해된 행렬의 차원, SIgma 행렬 내의 특이값, 그리고 Truncated SVD로 분해된 행렬의 내적을 계산하여 다시 복원된 데이터와 원본 데이터를 비교해 보겠습니다. 1234567891011121314151617181920import numpy as npfrom scipy.sparse.linalg import svdsfrom scipy.linalg import svd# 원본 행렬을 출력하고 SVD를 적용할 경우 U, Sigma, Vt의 차원 확인np.random.seed(121)matrix = np.random.random((6, 6))print('원본 행렬:\\n', matrix)U, Sigma, Vt = svd(matrix, full_matrices=False)print('\\n분해 행렬 차원:', U.shape, Sigma.shape, Vt.shape)print('\\nSigma값 행렬:', Sigma)# Truncated SVD로 Sigma 행렬의 특이값을 4개로 하여 Truncated SVD 수행num_components = 4U_tr, Sigma_tr, Vt_tr = svds(matrix, k=num_components)print('\\nTruncated SVD 분해 행렬 차원:', U_tr.shape, Sigma_tr.shape, Vt_tr.shape)print('\\nTruncated SVD Sigma값 행렬:', Sigma_tr)matrix_tr = np.dot(np.dot(U_tr, np.diag(Sigma_tr)), Vt_tr) # output of TruncatedSVDprint('\\nTruncated SVD로 분해 후 복원 행렬:\\n', matrix_tr) 원본 행렬: [[0.11133083 0.21076757 0.23296249 0.15194456 0.83017814 0.40791941] [0.5557906 0.74552394 0.24849976 0.9686594 0.95268418 0.48984885] [0.01829731 0.85760612 0.40493829 0.62247394 0.29537149 0.92958852] [0.4056155 0.56730065 0.24575605 0.22573721 0.03827786 0.58098021] [0.82925331 0.77326256 0.94693849 0.73632338 0.67328275 0.74517176] [0.51161442 0.46920965 0.6439515 0.82081228 0.14548493 0.01806415]] 분해 행렬 차원: (6, 6) (6,) (6, 6) Sigma값 행렬: [3.2535007 0.88116505 0.83865238 0.55463089 0.35834824 0.0349925 ] Truncated SVD 분해 행렬 차원: (6, 4) (4,) (4, 6) Truncated SVD Sigma값 행렬: [0.55463089 0.83865238 0.88116505 3.2535007 ] Truncated SVD로 분해 후 복원 행렬: [[0.19222941 0.21792946 0.15951023 0.14084013 0.81641405 0.42533093] [0.44874275 0.72204422 0.34594106 0.99148577 0.96866325 0.4754868 ] [0.12656662 0.88860729 0.30625735 0.59517439 0.28036734 0.93961948] [0.23989012 0.51026588 0.39697353 0.27308905 0.05971563 0.57156395] [0.83806144 0.78847467 0.93868685 0.72673231 0.6740867 0.73812389] [0.59726589 0.47953891 0.56613544 0.80746028 0.13135039 0.03479656]] 6 X 6 행렬을 SVD 분해하면 U, Sigma, Vt가 각각 (6,6) (6,) (6,6) 차원이지만, Truncated SVD의 n_components를 4로 설정해 U, Sigma, Vt를 (6,4) (4,) (4,6)로 각각 분해했습니다. Truncated SVD로 분해된 행렬로 다시 복원할 경우 완벽하게 복원되지 않고 근사적으로 복원됨을 알 수 있습니다. 사이킷런 TruncatedSVD 클래스를 이용한 변환사이킷런의 TruncatedSVD 클래스는 사이파이의 svds와 같이 Truncated SVD 연산을 수행해 원본 행렬을 분해한 U, Sigma, Vt 행렬을 반환하지는 않습니다. 사이킷런의 TruncatedSVD 클래스는 PCA 클래스와 유사하게 fit()와 transform()을 호출해 원본 데이터를 몇 개의 주요 컴포넌트(즉, Truncated SVD의 K 컴포넌트 수)로 차원을 축소해 변환합니다. 원본 데이터를 Truncated SVD 방식으로 분해됨 U*Sigma 행렬에 선형 변환해 생성합니다. 새로운 주피터 노트북을 생성하고, 다음 코드를 입력해 붓꽃 데이터 세트를 TruncatedSVD를 이용해 변환해 보겠습니다. 12345678910111213141516from sklearn.decomposition import TruncatedSVD, PCAfrom sklearn.datasets import load_irisimport matplotlib.pyplot as plt%matplotlib inlineiris = load_iris()iris_ftrs = iris.data# 2개의 주요 컴포넌트로 TruncatedSVD 변환tsvd = TruncatedSVD(n_components=2)tsvd.fit(iris_ftrs)iris_tsvd = tsvd.transform(iris_ftrs)# 산점도 2차원으로 TruncatedSVD 변환된 데이터 표현, 품종은 색깔로 구분plt.scatter(x=iris_tsvd[:, 0], y= iris_tsvd[:, 1], c= iris.target)plt.xlabel('TruncatedSVD Component 1')plt.ylabel('TruncatedSVD Component 2') Text(0, 0.5, 'TruncatedSVD Component 2') 왼쪽에 있는 그림이 TruncatedSVD로 변환된 붓꽃 데이터 세트입니다. 오른쪽은 비교를 위해서 PCA로 변환된 붓꽃 데이터 세트를 가져다 놓았습니다. TruncatedSVD 변환 역시 PCA와 유사하게 변환 후에 품종별로 어느 정도 클러스터링이 가능할 정도로 각 변환 속성으로 뛰어난 고유성을 가지고 있음을 알 수 있습니다. 사이킷런의 TruncatedSVD와 PCA 클래스 구현을 조금 더 자세히 들여다보면 두 개 클래스 모두 SVD를 이용해 행렬을 분해합니다. 붓꽃 데이터를 스케일링으로 변환한 뒤에 TruncatedSVD와 PCA 클래스 변환을 해보면 두 개가 거의 동일함을 알 수 있습니다. 12345678910111213141516171819202122from sklearn.preprocessing import StandardScaler# 붓꽃 데이터를 StandardScaler로 변환scaler = StandardScaler()iris_scaled = scaler.fit_transform(iris_ftrs)# 스케일링된 데이터를 기반으로 TruncatedSVD 변환 수행tsvd = TruncatedSVD(n_components=2)tsvd.fit(iris_scaled)iris_tsvd = tsvd.transform(iris_scaled)# 스케일링된 데이터를 기반으로 PCA 변환 수행pca = PCA(n_components=2)pca.fit(iris_scaled)iris_pca = pca.transform(iris_scaled)# TruncatedSVD 변환 데이터를 왼쪽에, PCA 변환 데이터를 오른쪽에 표현fig, (ax1, ax2) = plt.subplots(figsize=(9, 4), ncols=2)ax1.scatter(x=iris_tsvd[:, 0], y= iris_tsvd[:, 1], c= iris.target)ax2.scatter(x=iris_pca[:, 0], y= iris_pca[:, 1], c= iris.target)ax1.set_title('Truncated SVD Transformed')ax2.set_title('PCA Transformed') Text(0.5, 1.0, 'PCA Transformed') 두 개의 변환 행렬 값과 원복 속성별 컴포넌트 비율값을 실제로 서로 비교해 보면 거의 같음을 알 수 있습니다. 12print((iris_pca - iris_tsvd).mean())print((pca.components_ - tsvd.components_).mean()) 2.339760329927998e-15 4.85722573273506e-17 모두 0에 가까운 값이므로 2개의 변환이 서로 동일함을 알 수 있습니다. 즉, 데이터 세트가 스케일링으로 데이터 중심이 동일해지면 사이킷런의 SVD와 PCA는 동일한 변환을 수행합니다. 이는 PCA가 SVD 알고리즘으로 구현됐음을 의미합니다. 하지만 PCA는 밀집 행렬에 대한 변환만 가능하며 SVD는 희소 행렬에 대한 변환도 가능합니다. SVD는 PCA와 유사하게 컴퓨터 비전 영역에서 이미지 압축을 통한 패턴 인식과 신호 처리 분야에 사용됩니다. 또한 텍스트의 토픽 모델링 기법인 LSA의 기반 알고리즘입니다. 05. NMF(Non-Negative Matrix Factorization)NMF 개요NMF는 Truncated SVD와 같이 낮은 랭크를 통한 행렬 근사(Low-Rank Approximation) 방식의 변형입니다. NMF는 원본 행렬 내의 모든 원소 값이 모두 양수(0 이상)라는 게 보장되면 다음과 같이 좀 더 간단하게 두 개의 기반 양수 행렬로 분해될 수 있는 기법을 지칭합니다. 4 X 6 원본 행렬 V는 4 X 2 행렬 W와 2 X 6 행렬 H로 근사해 분해될 수 있습니다. 행렬 분해는 일반적으로 SVD와 같은 행렬 분해 기법을 통칭하는 것입니다. 이처럼 행렬 분해를 하게 되면 W 행렬과 H 행렬은 일반적으로 길고 가는 행렬 W(즉, 원본 행렬의 행 크기보다 작고 열 크기와 같은 행렬)로 분해됩니다. 이렇게 분해된 행렬은 잠재 요소를 특성으로 가지게 됩니다. 분해 행렬 W는 원본 행에 대해서 이 잠재 요소의 값이 얼마나 되는지에 대응하며, 분해 행렬 H는 이 잠재 요소가 원본 열(즉, 원본 속성)로 어떻게 구성됐는지를 나타내는 행렬입니다. NMF는 SVD와 유사하게 차원 축소를 통한 잠재 요소 도출로 이미지 변환 및 압축, 텍스트의 토픽 도출 등의 영역에서 사용되고 있습니다. 사이킷런에서 NMF는 NMF 클래스를 이용해 지원됩니다. 붓꽃 데이터를 NMF를 이용해 2개의 컴포넌트로 변환하고 이를 시각화해 보겠습니다. 12345678910111213from sklearn.decomposition import NMFfrom sklearn.datasets import load_irisimport matplotlib.pyplot as plt%matplotlib inlineiris = load_iris()iris_ftrs = iris.datanmf = NMF(n_components=2)nmf.fit(iris_ftrs)iris_nmf = nmf.transform(iris_ftrs)plt.scatter(x=iris_nmf[:, 0], y= iris_nmf[:, 1], c= iris.target)plt.xlabel('NMF Component 1')plt.ylabel('NMF Component 2') Text(0, 0.5, 'NMF Component 2') NMF도 SVD와 유사하게 이미지 압축을 통한 패턴 인식, 테그트의 토픽 모델링 기법, 문서 유사도 및 클러스터링에 잘 사용됩니다. 또한 영화 추천과 같은 추천 영역에 활발하게 적용 됩니다. 사용자의 상품(예: 영화) 평가 데이터 세트인 사용자 - 평가 순위 데이터 세트를 행렬 분해 기법을 통해 분해하면서 사용자가 평가하지 않은 상품에 대한 잠재적인 요소를 추출해 이를 통해 평가 순위를 예측하고, 높은 순위로 예측되는 상품을 추천해주는 방식입니다(이를 잠재 요소 기반의 추천 방식이라고 합니다). 06. 정리지금까지 대표적인 차원 축소 알고리즘인 PCA, LDA, SVD, NMF에 대해서 알아봤습니다. 많은 피처로 이뤄진 데이터 세트를 PCA같은 차원 축소를 통해 더욱 직관적으로 이해할 수 있습니다. 무엇보다도 차원 축소는 단순히 피처의 개수를 줄이는 개념보다는 이를 통해 데이터를 잘 설명할 수 있는 잠재적인 요소를 추출하는 데 큰 의미가 있습니다. 이 때문에 많은 차원을 가지는 이미지나 텍스트에서 PCA, SVD 등의 차원 축소 알고리즘이 활발하게 사용됩니다. PCA는 입력 데이터의 변동성이 가장 큰 축을 구하고, 다시 이 축에 직각인 축을 반복적으로 축소하려는 차원 개수만큼 구한 뒤 입력 데이터를 이 축들에 투영해 차원을 축소하는 방식입니다. 이를 위해 입력 데이터의 공분산 행렬을 기반으로 고유 백터를 생성하고 이렇게 구한 고유 백터에 입력 데이터를 선형 변환하는 방식입니다. LDA는 PCA와 매우 유사한 방식이며, PCA가 입력 데이터 변동성의 가장 큰 축을 찾는 데 반해 LDA는 입력 데이터의 결정 값 클래스를 최대한으로 분리할 수 있는 축을 찾는 방식으로 차원을 축소합니다. SVD와 NMF는 매우 많은 피처 데이터를 가진 고차원 행렬을 두 개의 저차원 행렬로 분리하는 행렬 분해 기법입니다. 특히 이러한 행렬 분해를 수행하면서 원본 행렬에서 잠재된 요소를 추출하기 때문에 토픽 모델링이나 추천 시스템에서 활발하게 사용됩니다.","link":"/2020/12/04/study/python_machine_learning_perfect_guide_ch06/"}],"tags":[],"categories":[{"name":"study","slug":"study","link":"/categories/study/"}]}